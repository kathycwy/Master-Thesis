"Count","PublishDate","VisitDate","Url","Content"
"1","2023-03-22","2023-03-24","https://www.section.io/blog/llms-supercloud/","Supercloud has the potential to significantly impact the field of Large Language Models (LLMs) (e.g. ChatGPT) in coming years. Supercloud is a term used to describe a new paradigm in cloud computing that is characterized by the use of a decentralized, distributed architecture. This architecture enables organizations to seamlessly harness the power of multiple cloud providers and data centers all the way to the Edge, enabling them to create a more resilient, scalable, and cost-effective infrastructure. So, what are the implications of supercloud for LLMs? Scale, Speed and Resilience The decentralized and distributed nature of supercloud could provide a more scalable and resilient infrastructure for running LLMs. This could enable faster training and deployment of models, as well as more reliable and consistent performance. LLMs require enormous amounts of computational resources for their training and deployment, and this presents significant challenges for organizations that need to scale their infrastructure to accommodate these demands. However, the decentralized and distributed nature of supercloud offers a potential solution to this problem. In a supercloud architecture, computational resources are distributed across multiple cloud providers and data centers, which enables organizations to harness the power of multiple providers to run LLMs. This means that organizations can scale their infrastructure more easily and quickly, without the need for significant upfront investment in hardware and software. Additionally, the distributed architecture of supercloud provides inherent resilience against failures, which can help to ensure more reliable and consistent performance of large language models. This is because the distributed nature of supercloud means that if one node fails, there are other nodes available to take over the workload, ensuring that the model remains operational and available to users. Furthermore, the use of supercloud can enable faster training and deployment of large language models. This is because the distributed architecture of supercloud allows for parallel processing, which means that different parts of the model can be trained simultaneously on different nodes. This can significantly reduce the time required to train the model, which can be a significant advantage in applications that require rapid iteration or deployment. Overall, the use of supercloud could provide a more scalable and resilient infrastructure for running LLMs. The distributed nature of supercloud enables organizations to harness the power of multiple cloud providers and data centers, which can help to scale their infrastructure more easily and quickly and ensure more reliable and consistent performance of their models. Additionally, the use of supercloud can enable faster training and deployment of large language models, which can be a significant advantage in applications that require rapid iteration or deployment. Sharing and Bias Management The use of supercloud could also help to address some of the ethical concerns surrounding LLMs. One major concern is the potential for bias in the data used to train the models. By using a decentralized and distributed architecture, organizations could leverage a wider range of data sources and reduce the risk of bias. LLMs have been the subject of some ethical concern in recent years, particularly when it comes to issues of bias. One of the main concerns is that the data used to train these models is often biased toward certain demographics or perspectives, which can result in the model reproducing and amplifying that bias. One way that supercloud could help to address this issue is by enabling organizations to leverage a wider range of data sources when training their models. In a decentralized and distributed architecture, data can be sourced from multiple cloud providers and data centers, which can help to ensure that the training data is more representative of diverse perspectives. Furthermore, by leveraging a distributed architecture, organizations can also ensure that their models are more resilient to adversarial attacks or other forms of bias. For example, by using multiple cloud providers, organizations can reduce the risk of a single point of failure, and ensure that their models are less susceptible to tampering or manipulation. Another advantage of supercloud is that it can help to improve transparency and accountability in the training and deployment of large language models. By using a distributed architecture, organizations can more easily track and audit the data sources and computational resources used to train their models and ensure that the process is transparent and free from bias. Finally, the use of supercloud could also help to promote greater collaboration and knowledge-sharing within the AI community. By leveraging a decentralized and distributed architecture, organizations can more easily share their training data and models with others, and promote greater collaboration and transparency in the development of large language models. By leveraging a decentralized and distributed architecture, organizations can ensure that their models are more representative of diverse perspectives, more resilient to adversarial attacks, and more transparent and accountable in their development and deployment. As the use of large language models continues to grow, it will be important for organizations to consider how they can use supercloud to promote ethical and responsible AI development. Conclusion The implications of supercloud for LLMs are significant. The combination of these two technologies could lead to more intelligent, natural language interactions with cloud services, as well as improved collaboration and communication between different cloud providers and data centers. Additionally, the use of a decentralized and distributed architecture could help to address some of the ethical concerns surrounding LLMs. As the cloud computing landscape continues to evolve, it will be interesting to see how these two technologies continue to intersect and shape the future of cloud computing."
"2","2023-03-15","2023-03-24","https://www.section.io/blog/supercloud/","As the world becomes increasingly digitized, the demand for powerful and versatile cloud computing solutions continues to grow. Enterprises today require cloud infrastructure that is not only flexible and scalable but also offers high availability and low latency, even in the most remote locations. This is where the concept of the Supercloud comes into play. The Supercloud is a term used to describe an architecture that combines the benefits of edge computing, multi-cloud, and multi-location compute to create a robust and efficient computing environment. In this blog post, we will explore how the Supercloud is a super set of Edge, Multi-cloud, and multi-location compute and why it is essential for modern enterprises. What is the Supercloud? The Supercloud is a relatively new concept that aims to provide an all-encompassing cloud solution that combines the best features of various cloud computing architectures. It essentially provides a comprehensive cloud infrastructure that combines edge computing, multi-cloud, and multi-location compute to create a powerful and versatile computing environment. Edge computing refers to a distributed computing architecture that processes data at the edge of the network, closer to the end-user. This approach reduces latency and enhances the user experience by minimizing the time it takes for data to travel from the user’s device to the cloud and back. Multi-cloud, on the other hand, is an approach that involves using multiple cloud providers to create a more resilient and flexible computing environment. Multi-location compute is a term used to describe a computing architecture that spans across multiple geographical locations, allowing for greater scalability and resiliency. The Supercloud combines these three approaches to create a cloud architecture that can deliver a seamless computing experience to users regardless of their location. It offers the benefits of edge computing by reducing latency and enhancing the user experience, the flexibility of multi-cloud by allowing organizations to use multiple cloud providers, and the scalability and resiliency of multi-location compute. Why is the Supercloud essential for modern enterprises? Modern enterprises operate in a highly connected and dynamic environment that demands a computing infrastructure that can keep up with their requirements. The Supercloud offers several benefits that make it an essential solution for modern enterprises: Reduced latency and enhanced user experience: By leveraging edge computing, the Supercloud reduces latency and enhances the user experience by processing data closer to the end-user. This approach is particularly beneficial for applications that require real-time processing, such as gaming and video streaming. Greater flexibility: The Supercloud allows organizations to use multiple cloud providers, providing greater flexibility in terms of resource allocation, workload management, and cost optimization. Scalability and resiliency: The Supercloud architecture is designed to be highly scalable and resilient. By leveraging multi-provider and multi-location compute, the Supercloud can distribute workloads across multiple providers and geographical locations, ensuring high availability and reducing the risk of downtime. Improved security: The Supercloud can enhance security by leveraging edge computing to process data closer to the end-user and letting organizations build defence in depth, reducing the risk of data breaches and cyber attacks. Introducing Section.io Section.io is an innovative Supercloud solution that offers a powerful and versatile cloud infrastructure. It combines edge computing, multi-cloud, and multi-location compute to create a comprehensive cloud solution that can meet the requirements of modern enterprises. Deploy apps directly from your GitHub account, your Docker Containers from your container registry or deploy complex microservice apps into a supercloud Kubernetes namespace."
"3","2023-02-27","2023-03-24","https://www.section.io/blog/distributed-microservices-performance/","“Optimization happens constantly, automatically, and transparently to the operations teams and the end users.” Traditionally we have hosted applications in centralized locations. (Cloud hosting being the more recent incarnation of that hosting structure). Some of us have run “hub and spoke” models with the spokes fostered by legacy CDNs. This model means we have “always on” centralized hosting for the whole application and only some parts of the applications running in distributed locations on CDNs. Ultimately, we have been limited to run on a distributed footprint only those parts of an application that have not resulted in overly burdensome operational overheads for our Development and Ops teams to deploy and manage. Why Not Decentralize the Whole App? What if we can move to a decentralized hosting model rather than the hub and spoke of cloud + CDN? What are the implications for; The end-user experience of the application Application deployment and operations overhead The cost of hosting. We would expect application distribution to result in one major positive and two serious negatives; Better performance experience for end users Significantly more complex Operations How can you deploy the application to many locations How can you choose and manage many locations The cost of hosting to increase significantly Your application would be always running in every distributed location all the time. Conducting a Study on Section The following study was conducted to analyze these three aspects when running a distributed microservice application on the Section platform. TLDR With Section, application distribution means; Significantly better application performance for end users everywhere No change to operations flows - it feels like working with a centralized application Cost Optimization - Run only where and when users need the application Deploying a Microservices Application We deployed a microservices application consisting of an Nginx Server and a Fluent D data collector sending logs from the application to external logging repository (in this case we followed Section’s New Relic integration steps). Both of these were deployed in Docker containers. For K8s folks: When deployed to Section, they appear in the K8s dashboard Section provides as follows: The experience of deploying an application to Section is exactly as though deploying to a single cloud location (or for K8s users, like deploying to a single K8s cluster). There is no operational burden to deploy to Section’s multi-location global platform. Use existing workflows including Kubernetes workflows The development teams and dev environment can stay exactly the same. The operations team can continue to collect and analyze logs from existing centralized locations Section’s platform automatically took care of providing the following in the few minutes of set up; A URL - https://weathered-sun-8105.section.app/ DNS set up SSL certs for the domain (Let’s Encrypt) DDoS Protection Traffic and Measurement We used New Relic’s synthetic monitoring to generate traffic from all parts of the world and to measure the performance of the application for users in those parts of the world. We set up three traffic groups firing both ping and full-page tests every minute. North America - on the top row of our dashboard Europe - middle row Asia and ROW - bottom row Here is the traffic monitoring dashboard: Testing the Application Centrally Deployed The status quo for deploying an application would be to choose a hosting location based on an arbitrary or best guess as to where the ideal hosting location may be for your end users. Say, for example, you choose either the West or East of the USA for your application as a Cloud hosting location. To test the status quo we used a static location selection policy to deploy the microservices application to just one US East location. In this case, New York. Transparently to Ops teams and end users, in just a few minutes, Section’s platform takes care of; Resetting networking (DNS, Anycast) to route traffic to this location Deploying the Nginx container in this location and starts serving traffic from it when ready Deploying the Fluent D container in this location (which starts sending logs to New Relic when ready) At this point we can see from the following North American centric view that Washington, Columbus, and Montreal all performing reasonably well but the Western locations are having a sub-par experience. This is what you would expect from a simple, single location application deployment strategy. All locations not in the locale of the chose hosting centre must suffer. Distributing the Application to Chosen Locations What if we can deploy the application closer to those users in the Western parts of the USA? By making a few simple tweaks to Section’s location selector, we can ask the Section platform to deploy this microservice application to six specific locations across North America. Note that at this point we have Section’s Automatic Optimization turned off and we are making a “Static” or always-on policy selection whereby we are choosing the locations in which the application will be always deployed. This resulted in the deployment pattern as follows in about 10 minutes: Again, without any operations activity, other than selecting the locations, Section sets the networking (DNS, Anycast, SSL Management, DDoS Scrubbing, etc) to route traffic to all these locations and sends traffic to the best available location automatically The Nginx container is deployed to all these locations and each starts serving traffic The Fluent D container is deployed to all these locations and each starts sending logs to the central collation point - ie New Relic Immediately we see a performance improvement for users in the Western parts of North America in the order of 5 to 6 time better! However, we can also see that the European users and those in Asia/ ROW are still experiencing a sub-standard experience. (we did see a moderate improvement in user experience in Asia thanks to the closer proximity of US West locations) Distributing the Application Intelligently So what if we let Section’s Adaptive Edge Engine take control of the application placement and simply deploy the application to the best locations for the application at any point in time? By turning on Automatic optimization in the Section Console, the application will be placed into endpoints across the world to optimize for latency. With all locations across the globe generating traffic at this time, the Section’s AEE reconfigured the deployment footprint of this application in 10 minutes to run in the following locations: And as we expect, user experience all over the globe is significantly improved. Of note, the AEE decided that to find the optimal balance between locations utilized (ie cost) and performance, fewer locations in North America were needed than the 6 we manually selected. It chose Seattle and Los Angeles for the Western locations. We can also see a short slight increase in request time while the new Western North American locations were brought into service and the others removed. Importantly, no requests were dropped by the distibuted hosting footprint during this automated adaptation of the network footprint. Distributing the Application Dynamically So we know that in the real world, traffic ebbs and flows throughout any 24-hour period and indeed week to week or month to month. Why should we keep all these global locations operating at all times? What happens if traffic dissipates from some locations? Say, it’s late at night in North America. Real Internet application traffic is not stagnant so our hosting should not be stagnant The Automatic Optimization of Section’s AEE is constantly remaking placement optimization decisions. The AEE will detect a traffic change and then, in minutes, re-optimize the footprint for performance. To test this we turned off all the monitors emanating from North America. In response, the AEE then decided the following locations make sense in the absence of traffic from North America. The AEE again reshaped the delivery network by withdrawing unneeded locations. All without dropping any requests from any location. The following shows the traffic performance from each location in the absence of any traffic from North America. Let’s check if the AEE can detect an increase in traffic from any location and how the AEE responds in order to continously adapt and deliver optimal placement for user experience. Let’s say the traffic increases from North America and at the same time, traffic from Europe dissipates. In 10 minutes, the AEE shifts the application around, bringing it up in North America and removing the unneeded locations in Europe. Again, of note, no requests are dropped through the period. The AEE routes North American requests to the next best location while the application is in the 10-minute deploy period to the North American PoPs. During the deploy period; Post Deploy period: As the traffic from North America is detected, it is immediately routed to the next best location until the North American locations are available. When the AEE detects those locations are up and ready to receive traffic, the AEE starts sending traffic there and then user experience in North America is again optimal. All this happens constantly, automatically, and transparently to the operations teams and the end users. Cost Optimization This adpative, “run only where you need to” behavior allows for cost as well as user experience optimization. We do not need to run the application in every location at all times as would otherwise be the case in the absence of the Auto Optimization capabilities of Section’s AEE. What have Ops been Doing? It is worth noting for each of the distribution methods discussed above, there has been no change to the core development or operational practices for the application. Logs continue to stream to a central location, the team can SSH onto any delivery box at any time and the Dev team can continue to develop the application in the same way they always have."
"4","2023-01-10","2023-03-24","https://www.section.io/blog/persistent-volumes-feature-launch/","We’re excited to introduce the immediate availability of Persistent Volumes in all 184 Section Points of Presence. Persistent Volumes allow your pods to persist data beyond a pod’s lifetime, making it easier for you to run applications such as databases and share data between pods. “With Persistent Volumes, we’re enhancing the features organizations can use to deliver their applications globally and adding even more utility for Section users” – Andy Piggott, CPO. Persistent Storage helps address a wide range of use cases including: *horizontal scaling of a pod for access to data replicas or caches *keeping data beyond a pod restart to avoid repopulating *storage for databases such as Postgres, MySQL and SQLite *implementing a document or object store “Customers have been requesting Persistent Storage for a while now, and we’re excited to announce that it’s finally here! With this feature, a whole world of new applications becomes possible on Section, and it’s available over 180 points of presence from today” said Dan Barholomew, CTO & Cofounder. To help get customers started with Persistent Volumes, we’ve put together some guides: *Read our Persistent Volumes Explainer to find out more about Persistent Volumes and how they work. *Our latest tutorials will show you some examples of getting started with Persistent Volumes and includes one on using Postgres with Persistent Volumes. Whether you’re looking to run a database on Section, share a distributed cache or take snapshots of your application, Section’s Persistent Volumes open up new ways to deliver pieces of your application stack – we can’t wait to see how you use them!"
"5","2023-01-30","2023-03-24","https://www.section.io/blog/developer-sphere-of-influence/","In traditional cloud deployments, the ability of developers to dictate and influence the overall experience and delivery of applications is really quite limited. Want to make your application faster or more secure? There’s only so much you can do. But that sphere of influence is growing – rapidly – and developers would be wise to consider the larger view of what that means for user experience, and how they build their apps. Let’s back up a bit to understand what’s changing, and what it means for the DevOps community. In a traditional hyperscaler deployment (AWS, Azure or GCP, for instance), the cloud instance exists within a datacenter protected behind a firewall. At the far end of the connection sits the application user. Between that user and the application is a many-hop pathway comprised of the user’s ISP, the intermediate provider, the internet backbone, the hyperscaler ISP and then, finally, the application instance inside the datacenter. Everything except that last bit is outside of the control of the application developer. Have security or performance concerns about that user experience outside the datacenter? Too bad. This is one of the fundamental reasons that Content Delivery Networks (CDNs) were created almost 30 years ago. What a CDN does is move some of the application (typically, content) out of the datacenter and place it closer to the user. This allows the developer to influence the user experience, in this instance making delivery of content – say, the images on a website – more efficient and responsive. Or it might entail adding a security layer to prevent bad actors from stealing user data or penetrating your application from the outside. But as applications have become more sophisticated, the staticn things that a CDN can do have become comparatively more rudimentary. Extending Developer Control Enter distributed edge computing. What distributed compute does is create a general-purpose workload platform that extends from the datacenter all the way out to the user’s local ISP, in the process dramatically extending what is within the application developers’ sphere of control. By swallowing what can be termed the “middle mile” of the internet into your scope, developers are able to mitigate and manage many of the issues that might arise across any of those links. Let’s look at an example. Most users and developers have little understanding of what goes on in that middle mile: it’s actually a complex series of buying and peering agreements between link providers. And the interests of providers may not always align with those of users and developers. For instance, the fastest/lowest latency route from user to server might not be the cheapest route, and it will be entirely up to those interim providers which route to choose. As a developer or user, you get what you get. The idea behind distributed compute is to upend this paradigm, giving developers much greater control over the entire deployment. This argument often focuses on improvements in latency that come from moving application workloads out to the edge, i.e., closer to the user. And while that’s incredibly valuable, less thought is being given to the larger implication of claiming more responsibility for – and control over – overall application outcomes by developers. By participating in these middle-mile decisions, the central opportunity for developers is balancing various parameters that impact application experience: availability, cost, throughput, responsiveness, security and privacy, compliance, etc. In a traditional cloud environment, this all happens outside your sphere of influence. With a CDN, you might get some limited, rudimentary opportunities. With distributed computing, all of these things are now in your control, and can even vary based on time, location, load, etc. And best of all, you get this control on a global basis, ensuring the optimal experience whether your users are in San Francisco, Singapore or Stockholm. One proponent of distributed compute is Netflix, which was once one of the primary users of the CDN model to ensure efficient static video content delivery for users. Over time, Netflix realized that running a CDN was not only extremely expensive, it also didn’t give them the level of control over different parameters around programmable content they were hoping for. So they built their own distributed network. In other words, they built their own Section. New Considerations for Application Design Extending this sphere of influence presents tremendous opportunities for developers. But it also gives them new factors to consider in designing applications. One primary consideration is application database calls. Developers have been taught a three-tier architecture for dynamic web design: the browser HTML, the server application and a database. In our traditional cloud deployment, the server application and database will sit in the same datacenter, and possibly the same rack. This proximity means that there’s little incentive to maximize efficiency in database calls… the link is short, fast and effectively free, so a chatty application is not a concern. When the round trip is under a millisecond and costs basically nothing, a thousand calls only takes a second. But move that application out to the edge, and all these calls back to a central database suddenly become a significant consideration. First, it hampers the performance gain you were seeking by moving closer to users – now each call might take 10 milliseconds. Moreover, it can also cause cloud costs to skyrocket because you’ve got a significant amount of chatty round-trip traffic between the edge application and a central database. Consider an ecommerce example using GraphQL. The notion of GraphQL is to expose a single API, while aggregating responses over many different backend services. So instead of a single customer order database, we now have a bunch of different microservices wrapped by a GraphQL aggregator. In our ecommerce example, when the application asks for the checkout page, GraphQL will typically send the product name, description, images, cost, quantity, etc. – multiplied across each item in the cart. In a traditional centralized deployment, this doesn’t matter. But distribute the application and that’s a lot of traffic between the front and backend. There are different ways around this, but here are two: first, you could write the GraphQL calls to be less chatty. For example, maybe it sends all descriptions for products in a cart in one go. If I’ve got five items in my cart, I’ve now cut that traffic by a factor of five. Another approach is to recognize that the contents of the cart are specific to the individual user, but things like product names are shared. The developer could thus distribute GraphQL and an application cache out to the edge. By making the interaction around specific basket contents a non-cacheable call, while caching all of the shareable content (names, descriptions, etc.) in a local file, you would eliminate most of the database round trips entirely. When Facebook developed GraphQL they experienced this exact challenge. The company’s answer was to create a GraphQL component called Data Layer, and its job is to distinguish between shareable and non-shareable items and work out how to minimize resulting interactions with backend services. It’s important to note that this isn’t a new problem. As soon as applications reach any scale, it’s typical for their database to fall over. There have long been three solutions: buy a bigger database, which is how Oracle has made its money. Or you can optimize your database, which is why the industry of DBAs exists. Or add caching layers into the application and write more efficient code to minimize database calls. As we move into the distributed compute age, this same need for efficient database call design is not only important for application scalability, it’s also an important consideration for application distribution. Does this imply you need to redesign an application for distributed compute? Not at all; we can get you there today, possibly for less than you’re paying for your centralized cloud deployment. However, it does mean that over time you should think through the new opportunities and considerations offered by more complete control over application delivery. Democratizing Distributed Deployment with Section Companies like Netflix and Facebook have already grappled with these issues and settled on distributed compute as the best possible answer for user experience. Yet they obviously have massive teams and budgets to not only build these distributed networks, but also manage them daily. Organizations without billion-dollar budgets can get this same ability to command, control and optimize a distributed application platform through Section – without having to bother with the underlying network. This significantly extends the sphere of influence for developers, allowing them to balance and optimize a wide variety of factors that were previously outside their control. Moreover, they can stay focused on the application, and not worry about network operations. And that’s important, because as we all know, with great power comes great responsibility. Embracing a distributed compute paradigm is both simple – when you use Section – and opens a whole world of opportunities and considerations in future application design. As developers extend their sphere of influence all the way to the user, they’ll have to start thinking through the implications. If you’d like to get started today on deploying your containers over this type of modern global network, get started now."
"6","2022-12-15","2023-03-24","https://www.section.io/blog/2022-year-in-review/","What a year 2022 has been for the planet, for Edge and distributed Application Landscape, and for Section. As we emerge from the post-Covid shadows and embrace a slightly tilted financial environment following the events in Ukraine, there still seems to be no slowdown in the global interest in leveraging distributed compute and networking to deliver new and/or superior user experiences. Telcos, ISPs, CDNs, Hyperscalers, and Mid Tier hosting are all working up and working on their offerings to support application use cases for application distribution. At Section, we have had a year of bringing to a head, the technology build-out that we have been working on for the last few years and we are thrilled to expose our open, industry-leading, cost-optimized application distribution platform for general developer and operations team use. Looking back over 2022, a few highlights of our year have included: Customers Recognizing the continued value delivered by the Section platform and team we were pleased to welcome all our new customers in 2022, help existing customers expand, and see all our major customers re-sign for additional term commitments to Section. KEI Launch Following the build of our Adaptive Edge Engine, we turned our attention to user interaction with that complex, massively distributed, and dynamic system. We wanted to provide an experience for users that felt like working with one static location and using toolsets that are open industry standards. We did not want to create an opinionated, closed platform. Hence, our product and engineering team built and released this year, our Kubernetes Edge Interface (patent pending) which essentially presents our dynamic, distributed footprint as though it is one single Kubernetes cluster. We present a single Kubernetes API endpoint for each Project on Section which will address the user’s workload wherever it may be across the footprint at any time. Digital Activation As a final layer to opening up the ability for developers to move and manage workload super simply on the Section platform, we released our digital activation on-ramp which empowers any developer to deploy their containers and microservice applications to the Section global footprint in minutes. While this outcome leverages all the underlying power of our distributed Kubernetes platform and devs can interact with the platform using all the standard Kubernetes tooling such as Kubernetes Dashboard or Kubectl, the on-ramp also delivers the opportunity to work with the distributed footprint where a user has zero prior knowledge of Kubernetes. Additional Network / PoPs Our ever-expanding network of locations this year included the addition of nearly 20 high-quality edge network locations from the Lumen Network which provides exceptional distribution and peering on its network. We also added the 11 Linode locations to the Section network. More to follow in 2023. Distributed Application Landscape Earlier this year we published a broader view of the players who participate in the Distributed Application Landscape. That is those providers who help organizations move their applications to some variant of distributed footprint (multi-region, multi-cloud, or Edge). We are still not happy with the nebulous concept of Edge given the lack of any clear “edge of the Internet”. We prefer the concept of application distribution to describe what players are seeking to achieve and note that there are various levels of potential distribution possible. From just a few locations in a Hyperscaler or two through to on-prem or 5G distribution to thousands of locations. All are valid, relevant, and “could” be argued as Edge but all are clearly in the realm of application distribution. Patent Issued The Core of the Section platform we describe as our Adaptive Edge Engine. This takes care of all the complexities associated with orchestrating, moving, and managing workload and traffic across a dynamically cost-optimizing distributed footprint. We were pleased to have our patent issued for the “brains” of our AEE this year recognizing the originality of our approach to the large and very complex (but highly valuable) problem. Compliance In addition to continued compliance with PCI DSS audit requirements, this year, Section added SOC2 type 2 attestation demonstrating our continued commitment to Security and Reliability as cornerstones of our platform for our customers. 2nd DDoS Network To further enhance Section’s reliability, this year we added a 2nd major DDoS scrubbing network to our platform. We now have dually redundant DDoS scrubbing networks powered by two of the largest capacity DDoS scrubbing networks on the Internet. Eng Ed A few years ago we launched our Engineering Education program to help Engineering students publish their articles and gain experience and exposure for their thought leadership pieces. This year, we reached close to 1 million views a month of our Eng Ed students’ content with the most popular article drawing over 25,000 views a month. Our Eng Ed contributors have also reported successful job placements as a result of the work they have published as part of this global program. Community We continue to sponsor and support opensource projects and communities such as Varnish Cache, LF Edge, Linux Foundation, and the CNCF and we really enjoyed speaking at and participating in both Kubecon and Cloud Native Conferences in Europe and North America this year; what’s more, in person! So many great people in the community and the vast array of developing CNCF projects and surrounding proprietary offerings are fantastic. Awards It was pleasing to be recognized by partners, customers, and analysts this year, and receiving two awards (Frost and Sullivan’s Kubernetes Award for Product Innovation as well as the Intellyx Award for Digital Innovation was a thoroughly pleasant outcome for our team. Here we come 2023!"
"7","2022-03-31","2023-03-24","https://www.section.io/blog/kubernetes-edge-interface-announcement/","The pandemic has kicked digital transformation into overdrive: businesses have shifted more user engagement to application workloads, while users expect more functionality, responsiveness and availability. Moving workloads and data closer to users (to the edge) is the best solution, but managing the required distributed multi-cluster environments is hard – really hard. In fact, so much so that companies avoid doing it and cloud vendors like Google recommend against it. But what if you could skip all that and just manage edge workloads as if they were a single cluster? And use your existing Kubernetes and cloud-native tools? We’re incredibly excited to launch our new patent-pending Kubernetes Edge Interface (KEI), making it possible for organizations to deploy application workloads to the distributed, federated edge as easily as they would to a single Kubernetes cluster. As our CEO, Stewart McGrath, says, “Edge deployment is simply better than centralized data centers or single clouds in most every important metric – performance, scale, efficiency, resilience, usability, etc. Yet organizations historically put off edge adoption because it’s been complicated.” He goes onto say: With Section’s KEI, teams don’t have to change tools or workflows; the distributed edge effectively becomes a cluster of Kubernetes clusters and our AEE (Section’s patented Adaptive Edge Engine) automation and Composable Edge Cloud handles the rest. Stewart McGrath, CEO, Section That highlights the three main breakthroughs with KEI: It effectively turns the edge into a “cluster of clusters,” meaning you can gain all the benefits of a distributed multi-cloud, multi-region, multi-provider deployment – i.e., Section’s Composable Edge Cloud – with the simplicity of managing a single cluster. As an extension to the Kubernetes API, KEI allows you to use familiar tools and workflows, like kubectl and Helm, to manage and control your edge environment. No more learning specialized tooling, different workflows or proprietary flavors of Kubernetes, just work as you always have, and Use KEI to generate policy-driven controls for AEE to automatically tune, shape and optimize application workloads in the background across Section’s Composable Edge Cloud. For example, if you’re looking to apply a simple application workload policy such as “run containers where there are at least 20 HTTP requests per second”, you can define this with a simple declaration in your application manifest, apply the configuration using kubectl, and AEE will continuously find and execute the optimal edge orchestration for that outcome. It’s that easy. Here are some other things you can do with KEI: Configure service discovery, routing users to the best container instance Define complex applications, such as composite applications that consist of multiple containers Define system resource allocations Define scaling factors, such as the number of containers per location, and what signals should be used to scale in and out Enforce compliance requirements such as geographic boundaries or other network properties Maintain application code, configuration and deployment manifests in an organization’s own code management systems and image registries Control how the Adaptive Edge Engine schedules containers, performs health management, and routes traffic Simply put, there is no easier, better or faster way to deploy and control application workloads at the edge. As you can imagine, we’ve been hard at work to bring KEI to life, and we’re proud and excited to share it with the larger community. Get in touch to start using the Kubernetes Edge Interface today."
"8","2022-03-14","2023-03-24","https://www.section.io/blog/how-to-derisk-cloud-region-selection/","When building a cloud application, one of the first decisions to consider is where to deploy it. In fact, Google’s helpful document on Best Practices for Compute Engine Regions Selection states “When to choose your compute engine regions… Early in the architecture phase of an app, decide how many and which Compute Engine regions to use.” Considerations for selecting the regions, according to Google, include latency, pricing, machine type availability, resource quotas and more. It goes without saying that this is not just relevant for new apps; existing apps that need to scale with a growing user base should take these factors into consideration, particularly if the user base is distributed or even global. Google specifically notes latency is a “key consideration for region selection” as high latency can lead to an inferior experience. The document walks through the impact on latency of various cloud deployment patterns – including single region deployment, distributed frontend in multiple regions and backend in a single region, distributed frontend and backend in multiple regions, and multiple parallel apps – and discusses various strategies and best practices that can be used to mitigate latency issues, including premium tier networking, cloud load balancing and cloud CDN, local caching and app client optimization. To be frank, this all has us scratching our collective heads. To begin with, the document is 19 pages long. That’s 19 pages on how to choose where to deploy your app. Moreover, we find it a bit contradictory. For instance: When selecting Compute Engine regions, latency is one of the biggest factors to consider. Evaluate and measure latency requirements to deliver a quality user experience, and repeat the process if the geographic distribution of your user base changes. …while elsewhere… Even if your app serves a global user base, in many cases, a single region is still the best choice. The lower latency benefits might not outweigh the added complexity of multi-region deployment. In other words, latency is critically important, so important that you should repeat this process as the geographic makeup of your user base evolves, but not so important that it’s worth the added complexity of a multi-region deployment. Actually, in one sense this is a point we agree on wholeheartedly; we’ve written extensively on the barrier that complexity can create when scaling your application. However, we don’t think the right answer is to limit your application to a single region. The right answer is to get rid of the complexity. We’ve written an entire paper on why organizations are modernizing applications with distributed, multi-cluster Kubernetes deployments. We think it’s critically important, and we discuss the benefits at length, including availability and resiliency, eliminating lock-in, improving performance and latency, increasing scalability, lowering cost, enhancing workload compliance and isolation, and more. And yes, trying to self-manage a multi-region cloud deployment can be complex. That’s why we don’t make customers do it. At Section, we like to think of this as the de-risk deployment strategy. Or, more succinctly, the “don’t make me pick a region” approach to cluster deployment. We don’t mean to pick on Google, they are a valued part of our Composable Edge Cloud. And in truth, this is a conundrum for any public cloud platform – deploying to more than one region is hard, yet staying with a single instance is a de facto admission that a portion of your user base will have a sub-par experience. This is the best-practice methodology for region selection, according to Google’s document: Now that you have considered latency requirements, potential deployment models, and the geographic distribution of your user base, you understand how these factors affect latency to certain regions. It is time to decide which regions to launch your app in. Although there isn't a right way to weigh the different factors, the following step-by-step methodology might help you decide: See if there are non-latency related factors that block you from deploying in specific regions, such as price or colocation. Remove them from your list of regions. Choose a deployment model based on the latency requirements and the general architecture of the app. For most mobile and other non-latency critical apps, a single region deployment with Cloud CDN delivery of cacheable content and SSL termination at the edge might be the optimal choice. Based on your deployment model, choose regions based on the geographic distribution of your user base and your latency measurements: For a single region deployment: - If you need low-latency access to your corporate premises, deploy in the region closest to this location. - If your users are primarily from one country or region, deploy in a region closest to your representative users. - For a global user base, deploy in a region in the US. For a multi-region deployment: - Choose regions close to your users based on their geographic distribution and the app's latency requirement. Depending on your app, optimize for a specific median latency or make sure that 95-99% of users are served with a specific target latency. Users in certain geographical locations often have a higher tolerance for latency because of their infrastructure limitations. If user latency is similar in multiple regions, pricing might be the deciding factor. We prefer a simpler alternative. Just deploy to Section, and we’ll take care of the rest."
"9","2022-07-29","2023-03-24","https://www.section.io/blog/considerations-for-global-saas-deployment/","For software companies, offering their software as a SaaS solution versus an on prem or cloud only model means; Shorter customer deployment times which equates to shorter time to revenue An opportunity to add upsell and cross sell product lines which means both increased revenue per customer, and a more complete (and therefore competitive) product offering. However, companies looking to offer their software to customers as a SaaS solutions to a global customer base face a stark choice: treat customers outside of the primary geography as second-class citizens, or embrace a globally distributed deployment model. TLDR Organizations are leveraging Section to create wholistic SaaS offerings of their core software so their customers can adopt their software with just a DNS change. Section Features for SaaSification Simple application deployment and management (Kubernetes Edge Interface) Section’s global footprint (Composable Edge Cloud), Add on security and performance upsell and cross sell modules Section’s simple integration points and reference application. Examples Include: Optimizely Leveraging Section to build a global image optimization platform Quant CDN Using Section to build a new type of Content Delivery Network Wallarm Relies on Section to power it’s API Security Platform Drupal Association Powers their Drupal Steward security platform with Section SaaSification Overview For organizations that are early in their maturity, the first option is typically the default choice. These companies will pick a particular cloud instance for deployment, such as AWS EC2 US-East, and customers within or close to that region will enjoy a premium experience. As customers get geographically further from that particular region, application performance, responsiveness, resilience and availability will naturally degrade. In short, this is the default “good enough” cloud deployment focused largely on a home-grown user base. Image sourced from AWS As companies get serious about their global business opportunities, they increasingly shift to the second choice by moving part or all of their application (and its data) closer to the users. This geographic proximity typically improves the user experience across the board. It is, in a nutshell, the beginning of an edge compute model. Unfortunately, while offering substantial technical and business benefits, edge computing can also be considerably more complex to manage than a centralized cloud instance. In part, this is because it strives to treat global users as true first-class customers, and thus must take their particular requirements into account. Let’s look at some of the key considerations, and then talk about how to address them. Regulatory compliance Different countries and regions have unique rules and regulations, and truly global deployments must take these into consideration. A notable example is the European Union’s General Data Protection Regulation (GDPR), which governs data protection and privacy for any user that resides in the European Economic Area (EEA). Importantly, it applies to any company, regardless of location, that is processing and/or transferring the personal information of individuals residing in the EEA. For instance, GDPR may regulate how and where data is stored for your application. Moreover, many countries have copied the protections of GDPR for their own citizens. While the intricacies of regulatory compliance are outside the scope of this post, it’s important to recognize that these requirements exist, and that organizations should partner with network and compute providers that have experience adhering to compliance standards. Geographic workload and performance optimization Global application usage varies in the same way it varies for any one particular geography, that is to say: by time and customer density. Users wake up, log on, finish their day and log off, and usage is heaviest where the most target customers are located. At the global level, the difference is one of scale: usage patterns generally follow the sun and dense population centers can be simultaneously “online” but geographically far flung. From a compute perspective, this means that systems must be optimized to account for these global variances, both from a performance standpoint, as well as a cost efficiency standpoint. In fact, beyond just “optimized”, global networks should be actively and intelligently tuned to account for shifting usage patterns in real time. For example, Section’s patent-pending Adaptive Edge Engine (AEE) focuses on compute efficiency by using machine learning algorithms to dynamically adapt to real-time traffic demands. By spinning up compute resources and making deployment adjustments, the AEE places workloads and routes traffic to the most optimal locations at a given time. So, rather than a team of site reliability engineers (SREs) continuously monitoring and adjusting resources and routing, companies have the advantage of leveraging intelligent automation across a distributed edge network. Global network coverage A closely related consideration is network coverage and accessibility. Simply put, are edge compute resources available where my users need them to be? Partnering with a single vendor whose coverage doesn’t extend where needed is very limiting, yet aggregating different vendors into a “global” network can significantly increase complexity, cost, and risk. This is especially true if those vendors have different compute, deployment, and observability parameters, and you’re the one that needs to manage around those differences. Similarly, in light of the above performance optimization, are you the one who has to manage the spinning up and spinning down of compute resources across different vendors to cover different geographies? An equally important consideration: does my chosen partner(s) cover not only where I need to be today, but where I am likely to see future growth? And if they do, are they able to dynamically adapt compute resources across their network(s) to adapt to real-time traffic demands? Global observability When it comes to DevOps, the last thing organizations want is to discover performance or security issues after the fact based on user complaints, yet this observability isn’t easy to achieve across distributed systems. Ideally, organizations need real-time visibility into traffic flows and time series to evaluate performance and diagnose issues. Logs should be easy to consume and search as needed. Performance metrics need to be easy to visualize and available not only in real time, but also at varying levels (network, domain, and edge service) to quickly identify patterns. Global resilience While the ability to spot issues is critical, it’s better that these issues – at least as they’re related to the edge compute platform – not surface in the first place. Yet it’s an unfortunate reality that provider networks do, occasionally, go down. Witness the recent AWS outages that took down a wide variety of applications in the U.S. This points to the notion of building for resiliency at the global level; if a single provider network goes down, there should be built-in fault tolerance to redirect to other providers. Ideally, this resilience and fail-over should be automated based on health checks that detect outages and dynamically reroute traffic to healthy endpoints. SaaSification with Section While it is certainly possible to self-manage global application deployment, most software organizations don’t want to get into the business of managing the intricate complexities of distributed networks. With Section, software vendors can leverage an out-of-the-box Solution that gives them all the benefits of global deployment without the burden of building and managing it. The Section platform: Distributes Kubernetes clusters across a vendor-agnostic global network of leading infrastructure providers; Intelligently tunes your edge network to ensure workloads are in the optimal locations to maximize performance and cost benefits, and; Includes built-in observability, resilience and security needed to make sure your application stays up and running. On top of that, Section’s Recommended Partners offers a number of possible add-on services (e.g. web application firewall, performance optimization, A/B testing, etc.), to extend the value of your software. Ultimately, Section gives organizations all the business and performance benefits of global deployment, while allowing them to concentrate on their core business rather than distributed network management."
"10","2022-04-26","2023-03-24","https://www.section.io/blog/optidash-leverages-section-to-deliver-global-image-optimization/","Large images can dramatically impact site or app performance. Optidash’s solution optimizes those images, but the company needed an edge platform that provided the performance, scale, reliability and reach to address a global market. Section’s innovative edge platform offered the simplicity to deploy quickly, along with the control to fine-tune configuration. Accelerating Visual Content Site and app designers know that people engage more with visual content. But large image files can significantly impact online performance, not to mention hosting fees, bounce rates and page ranking. Large images mean a lot of unnecessary bytes being pushed to millions of users worldwide. The answer is to compress images as much as possible without impacting image quality. Unfortunately, manually optimizing all those images is both difficult and time consuming, leaving organizations in a bind. That’s where Optidash image optimization comes in. Optidash uses machine learning and highly specialized infrastructure to dramatically compress images (an average of 40%) on-the-fly with no loss in quality. The company’s API, apps, plugins and SDK are designed to allow companies to integrate with Optidash in minutes to make workflow more efficient. The company designed its solution to deliver optimization anywhere, in real time and at scale, but to reach both organizations and users globally meant it needed to deploy to the edge. “We are constantly going through our code to determine where we can shave one millisecond here and three there, and optimizing to improve peak processing workloads,” noted Przemek Matylla, CEO at Optidash. “This is a solution designed around performance and scale, which means a global user base. We specifically designed with the edge in mind.” The only question was which edge provider to partner with, and Matylla already knew the answer: Section. Simplifying Edge Deployment at Scale Section’s Edge Hosting Platform is the simplest way to deploy and control cloud-native apps for distributed global delivery. The platform offers Kubernetes-native tooling, CI/CD pipeline integration, a broad range of security and operational tools, and a complete single-pane-of-glass edge observability suite. Moreover, the patented Adaptive Edge Engine (AEE) automatically orchestrates and intelligently scales edge workloads to meet real-time traffic demands. In short, Section allows organizations seeking a distributed footprint to concentrate on innovating applications, not managing infrastructure. What Matylla liked best about Section’s Edge Platform was its combination of simplicity and control. “I was really blown away by the volume of settings and control Section exposes for the end user.” Przemek Matylla, CEO, Optidash “On one hand it’s almost like autopilot, you connect your origin and it just works,” he says. “But for people like me that want to dig into Varnish and config files, in just a few clicks I can see absolutely everything about the edge in the dashboard. For someone used to traditional CDNs, that ability to have both ease of use and total access is super cool.” Optidash heavily relies on Kubernetes for managing its container workloads, making Section’s Kubernetes-native tooling particularly important. Section’s patent-pending Kubernetes Edge Interface (KEI) makes it possible to deploy application workloads across a distributed, federated edge network as easily as they would to a single Kubernetes cluster, effectively turning the edge into a “cluster of clusters.” Teams can continue to use familiar tools and workflows, like kubectl and Helm, to manage and control the edge environment. Moreover, KEI can be used to generate simple policy-driven controls for Section’s Adaptive Edge Engine, allowing it to automatically tune, shape and optimize workloads in the background across Section’s Composable Edge Cloud. “With Kubernetes we can quickly and automatically spin up pods to fill available CPU and RAM capacity, allowing us to squeeze every bit of efficiency out of our machines,” notes Matylla. “In my opinion, if you’re not using Kubernetes you’re doing it wrong, so the fact that I can use tools like kubectl to control Section edge deployments is fantastic.” Scaling the Edge Optidash now processes millions of images a day (an average of 100 images a second) for customers on Section, and has optimized more than 5,000,000,000 images to date. The company also has ambitious plans going forward. It is working on a new product that uses artificial intelligence and machine learning to improve image presentation, which is crucial for industries like real estate or automotive sales that often rely on poor-quality images for online sales. Matylla plans to also make that product available at the edge with Section. Optidash is also working closely with Section to develop the ability to deploy specific high-performance workloads to targeted, dedicated hardware and server configurations on Section’s Edge Platform. This would allow Optidash and other companies to distribute the most demanding workloads to the edge for better responsiveness. “Image processing is compute-intensive, so we have very specific hardware configuration and load requirements for some of our core services,” explains Matylla. “I want to be able to target precise infrastructure down to the container level on Section’s Composable Edge Cloud to move those intensive workloads closer to users. I really value that they’re working with me to make that happen.” Improved performance isn’t the only benefit Optidash values for those core workloads; improved reliability and resilience are perhaps even more important. “I think one of the things people fail to appreciate is that a properly federated edge virtually eliminates the risk of service loss when a datacenter or provider goes down. I’ll tell you what keeps me up at night – the idea of basically going out of business due to service loss that’s out of our control. That resiliency is something I really value about Section’s Edge Platform.” And it’s something that is top of mind as Optidash continues to grow its business at the edge with Section."
"11","2022-11-14","2023-03-24","https://www.section.io/blog/wallarm-section-case-study/","“Section really understands how to optimize cloud-native environments from a technical perspective,” Diaz says. “That strong technical expertise makes them a perfect complement to our offering.” Applications and APIs are omnipresent – and growing exponentially. According to Postman’s 2021 State of API Report, there will be 1.7 billion active APIs by 2030. Today, a whopping 83% of all web traffic is the result of API calls. Understandably, Gartner predicted that APIs would “become the most frequent attack vector, resulting in data breaches for enterprise web applications.” We’re seeing the release of new API security exploits almost every single day. And if trends continue, that number could soon exceed 17 new API exploits per day Fast and Easy Application & API Security Wallarm’s end-to-end API security products provide robust protection for APIs, microservices and serverless workloads running in cloud native environments. Security and DevOps teams from leading tech companies and global 2000 enterprises choose Wallarm to gain unique visibility into malicious traffic, robust protection across their whole API portfolio, and automated incident response for product security programs. The company offers customers both a complete API and application security platform as well as a web application and API protection (WAAP) solution for any environment, including cloud-native, multi-cloud, edge or even on-prem. While new threats continue to emerge, organizations’ environments are also expanding and growing increasingly complex – opening the door to security holes. And that’s precisely why Wallarm turned to Section to extend the benefits of its security solutions without the burden of implementation headaches or ongoing maintenance. With Section powering Wallarm, users no longer have to worry about managing the deployment on their own. Unlike traditional WAFs, Wallarm security managers don’t have to fine-tune security rules or hard-code them manually. Filtering and traffic analysis happen automatically, allowing users to dissect traffic and attacks – without overpaying for unnecessary traffic – while the system returns fully automated rules for users. At the same time, Section enables Wallarm to provide additional services such as globally distributed hosting, layer 3 and layer 4 distributed denial of service (DDoS) protection, and secure socket layer (SSL) termination at the network level. As a result, customers receive a low-maintenance experience with the added benefit of an extremely low total cost of ownership. More Than Just Your Typical Vendor Relationship We’re all too familiar with the typical vendor relationship. You pay for a service, and (hopefully) you get an appropriate level of service in return. And most companies do an adequate, if not fine, job achieving that goal. But finding those organizations that truly go above and beyond – to really get to know your business, your products, your team and even your customers – is often like finding a needle in a haystack. And that’s exactly what Wallarm found in Section. “In all honesty, I feel like they are an extension of our company,” says Alex Diaz, Wallarm’s VP of Security Solutions. “We have seamless communication with their team. We meet with them every two weeks, so there’s constant visibility in terms of what’s going on between us. But I can also ping them on Slack with questions and count on them to respond quickly and help whenever we need them.” The partnership doesn’t end there. The two companies have successfully pursued a number of pieces of business together over the past year. And they look to continue doing more. As Diaz observes, “I would like to see us pursue more joint opportunities together in the future because we have proven that it’s working. Having a joint offering has been very beneficial for us.” With an increasing return to normalcy, Diaz anticipates that this could include more appearances together at trade shows and in-person customer meetings to “highlight just how well we work together.” A Perfect Technology – and Technical – Match The two companies started working together in late 2020 because Wallarm liked what it saw in terms of Section’s flexibility, robust CDN and unparalleled ability to deploy Wallarm’s filtering nodes and services in new data centers whenever and wherever needed. The fact that the Section team had demonstrated experience working with similar challenges was the icing on the cake. Yury Larichev, Wallarm’s Chief Revenue Officer, notes, “From a technology perspective, it’s a perfect fit because Section is so familiar with handling proxy deployments. Section is so easy to use, and it’s flexible enough to allow us to deploy anywhere we want. Wallarm’s filtering nodes can be deployed to sit on top of Section’s platform in a docker container, so our customers can be deployed and provisioned and protected within a matter of minutes.” But it is more than just Section’s technology that elicits high praise from the Wallarm team. Diaz shares that the company’s technical expertise – particularly when it comes to understanding cloud native deployments – was also a deciding factor. “Section really understands how to optimize cloud-native environments from a technical perspective,” Diaz says. “That strong technical expertise makes them a perfect complement to our offering.” Diaz concludes by observing, “When you put it all together, Section is a complete solution.” Ready to jump in? Deploy your first workload to Section in minutes."
"12","2022-08-29","2023-03-24","https://www.section.io/blog/kentik-case-study/","Kentik is the network observability company. Its platform is a must-have for the network front line, which is why network and cloud professionals from market leaders, such as IBM, Box, Zoom and countless others, turn to Kentik for the visibility needed to plan, run and fix any network. The company gives network pros what they need to make sense of network, cloud, host and container flow, internet routing, performance tests and network metrics, so they can make their business-critical services shine. Test and Monitor Everything Kentik offers customers the ability to ingest telemetry from different sources – across clouds, data centers, edge, WAN, SD-WAN and so on – to understand exactly what’s happening on any network. One way to do this is by observing flow logs from routers and devices to analyze the traffic flowing through a given network. In doing so, Kentik enables customers to keep an eye on device health and workloads, plan for capacity, identify trends and potential problems, and more. The company also offers the ability to simulate a customer’s network traffic by synthesizing traffic from different geographies around the world. The process – known as Kentik Synthetics – simulates visitor requests to test latency-sensitive applications. This is done through an “agent” that Kentik deploys around the world, giving customers access to performance data from cloud and other hosting providers. The broader and more distributed Kentik can make this worldwide agent footprint the better, as it provides greater performance insight. But deploying and managing agents on networks around the globe is a significant pain point for the company, for all intents and purposes forcing Kentik to assemble and operate its own distributed global network from a variety of vendors. That is, until they found Section. Bolstering Kentik Synthetics Whether it’s digital experience monitoring, traffic troubleshooting or emulating the performance of global users, Kentik Synthetics relies on a global network of these testing agents to allow customers to verify infrastructure, application and services performance levels across cloud and hybrid networks. Kentik turned to Section for help expanding customer access to these agents on an increasing number of networks around the globe. Section’s cloud-native hosting system continuously optimizes orchestration of secure and reliable global infrastructure for application delivery, a key selling point for Kentik’s CEO Avi Freedman: “As a network observability company, Kentik has a global view of the internet combining passive and active measurements. Partnering with Section allows us to quickly and easily augment our edge deployments, and their cloud-native platform and partnerships make it easy and affordable to integrate as we continually expand our footprint.” Opening Doors to Untapped Networks Of the many thousands of global data centers around the world today, choosing the right one to host an application – or synthetic agent – is a largely manual and arbitrary process. Section’s sophisticated, distributed and clusterless platform intelligently and adaptively manages workloads worldwide around performance, reliability, compliance, cost or other developer intent to ensure applications run at the right place and time. This made Section’s solution precisely what Sunil Kodiyan, Kentik Synthetics Product Manager, needed for agent deployment. “I like Section’s transparency in terms of costs as well as its flexibility around how bandwidth usage for ingress and egress is weighted based on overall CPU and memory usage. Section makes it straightforward and easy for us to understand our overall consumption. What’s more, Section allows us to have greater diversity in our agent network and agent fleet. They were able to get us access to networks and geographies that previously were inaccessible to us.” Support You Can Count on Getting started with Section could not have been easier. The company’s solutions engineers developed a proof of concept, and from there Kentik was able to kick off the engagement and deploy quickly. Before long, Kentik’s site reliability engineers were formulating scripts based on the deployment plan, allowing them to further automate processes by leveraging Section’s platform. When help was needed, Section’s support team was standing by Kentik’s side. As is often the case in any new engagement, a few minor support tickets emerged during the initial deployment. But Kentik’s Site Reliability Engineer, Jenna Sprattler, noted that Section’s support team was “responsive, helpful and resolved our issues quickly.” She was particularly fond of the Section support team’s email distribution list that auto-generates help tickets as well as the availability of team’s Slack channel for quick, one-off requests. “Not only has Section’s support console been easy to navigate, but its user interface for deployments makes it easy to see our environments, what we’ve deployed and the resources we’re utilizing,” says Sprattler. “It’s fantastic, efficient and extremely helpful!” A Partnership Designed to Grow “Our overall experience with Section has been very positive,” Kodiyan says. “We’ve easily accomplished our goal to increase our agent fleet count, growing the deployment by about 20% through Section. This is significant because the wider our presence globally, the more confidence our customers have knowing that if they need an agent in a specific geography, they can spin one up and start testing from that location.” Kodiyan sees plenty of room for Kentik’s partnership to grow in the months and years ahead. “We look forward to expanding our partnership, especially in diverse geographic networks that are difficult for us to enter, like Asia, Africa and others.”"
"13","2022-03-03","2023-03-24","https://www.section.io/blog/quantcdn-leverages-section-edge-as-a-service/","When QuantCDN was looking to create a new type of content delivery network (CDN), it needed an edge provider that could deliver the required simplicity, performance and global reach. Section’s innovative Edge as a Service offered the right building blocks and flexibility without the cost and complexity of starting from scratch. Rethinking Modern Website Delivery It’s often necessary to fundamentally rethink how we do things in order to advance. Such is the case if you’re going to upend the traditional CDN paradigm to dramatically improve website performance, security, and cost. And that was the objective in launching QuantCDN. Modern websites are built around dynamic site content, meaning pages are generated from various components in response to user requests. Content becomes modular, and is managed through a content management system (CMS) on the backend and assembled in real time either by the server or at the browser. The problem is dynamic content generation is relatively resource intensive, both in the backend databases and web servers needed to handle traffic, and in the latency it injects as pages are compiled. Moreover, it opens up the attack surface by allowing the insertion of malicious code. Yet, in reality, most websites consist of relatively static content – content that is sitting on complex and expensive dynamic infrastructure. If this content was delivered as static, pre-assembled pages, it would significantly improve site performance and responsiveness, lower costs, and increase security. That was the inspiration for QuantCDN. Founder & CEO Stuart Rowlands realized that for many organizations, from governments to major enterprises and brands, static content promised significant benefits. “The trick was to turn the traditional paradigm on its head,” explains Rowlands. “We needed to push content from the source the second it changes, and host it at the edge for immediate delivery in response to user requests.” Rowlands and his team set about building the underlying technology, plug-ins and APIs needed to automatically track and push content changes from popular content management systems (CMS). But QuantCDN still needed a platform to host the static content. “Part of the power of QuantCDN is that we leverage edge computing, so content is both physically closer to users, and can readily scale as traffic requires. That said, we’re a software company… the last thing I want my team doing is managing a distributed network.” Stuart Rowlands, Founder & CEO, QuantCDN Fortunately, Rowlands had an ace up his sleeve: he already knew about Section’s Edge as a Service. Simplifying Edge Deployment Section’s Edge Operating System is the simplest way to deploy and control cloud-native apps for distributed global delivery. The company’s Edge as a Service platform offers GitOps-based workflows, Kubernetes-native tooling, CI/CD pipeline integration, a broad range of security and operational tools, and a complete single-pane-of-glass edge observability suite. Moreover, its patent-pending Adaptive Edge Engine (AEE) automatically orchestrates and intelligently scales edge workloads to meet real-time traffic demands. In short, Section allows organizations seeking a distributed footprint to concentrate on managing the application, not the infrastructure. For QuantCDN, Section offered the robust framework the team needed to build and deploy its solution, without the complexity involved with other, alternative approaches. Rowlands also appreciates the observability in the Section platform, calling it “key” to maintaining the expected performance and accessibility of QuantCDN. “We considered a range of solutions, but each represented a higher barrier to entry,” says Rowlands. “Section provided the right building blocks and flexibility without the burden and cost of assembling this from scratch, and the observability to ensure it’s all operating as expected.” Among the benefits of working with Section, Rowlands and his team were able to leverage Section’s HTTP Extensions and Recommended Partner applications, which allows companies to combine pre-packaged security and performance solutions with Node.js or custom containers and serverless workloads. Thanks to Section’s flexibility, alongside the static edge, QuantCDN offers full support for traditional CDN functions such as caching, WAF, etc., so customers can use QuantCDN as a one-stop solution across a mix of modern and traditional projects. “When I’m looking for a technology partner, the things that matter are stability, security, flexibility, support and relationship,” notes Rowlands. “Having worked with a multitude of cloud providers, Section has stood out by ticking all those boxes.” Abstracting the Edge In fact, for Rowlands and his team, the Section platform has become a central part of QuantCDN, and he only sees that growing in the future. “Section is unique in the market. I can’t point to any other providers that offer the same level of flexibility and control. Section has taken the pain out of managing and scaling workloads across multi-region and multi-cloud, offering the right level of core complexity abstraction to build on.” Stuart Rowlands, Founder & CEO, QuantCDN Going forward, the QuantCDN team plans to be at the forefront of helping organizations tackle increasing DDoS attacks and other site security risks, as well as challenges associated with increasing web infrastructure complexity, increasing costs, increasing traffic, and the ongoing shift to serverless workloads. Section promises to be integral to those plans. “I only see Section’s uniqueness in the market increasing,” Rowlands notes. “As you go from running a select set of payloads to arbitrary custom payloads, the floodgates open in terms of what’s possible for a developer to do at the edge. Once you enable serverless workloads in any segment – with all the core complexities around scalability, global distribution, compliance, and even standardized costs all solved for – the headaches otherwise associated with a multi-cloud, multi-region deployment just go away.” Rowlands sums up his experience by noting the close alignment between Section and QuantCDN in the problems they’re trying to solve. “Workload complexity and cost are only increasing, while users have steadily become accustomed to instant response – leaving organizations between a rock and a hard place,” he says. “That’s the challenge we’re trying to solve with content delivery, and it’s the same challenge that Section solved for us with their edge technologies.”"
"14","2020-01-16","2023-03-24","https://www.section.io/blog/temple-webster-case-study-sitespect/","This customer story was produced jointly by SiteSpect and Section, in cooperation with Temple & Webster. Temple & Webster is Australia’s leading online furniture and homewares retailer. The brand offers more than 150,000 products, the largest range of any online retailer in the category, so website testing and optimization plays a central role for the business. Since 2018, Temple & Webster’s digital marketing, product, dev-ops and technology teams have worked with SiteSpect to optimize its online store, and they’ve seen substantial benefits. Calvin Brodie, Head of Product at Temple & Webster says, “For a company of our size, a 1% revenue increase is a 7-digit number, so every chance for optimization counts.” Conversion improvements have a big impact on the company’s bottom line. Improving conversion means improving the customer experience too. “If you’re not continuously running optimization tests, you’re probably frustrating your customers and you’re definitely leaving money on the table.” -Calvin Brodie, Head of Product, Temple & Webster With a long-standing investment in web optimization and testing, Temple & Webster have refined its approach and embraced optimization site-wide, especially by using Origin Experiments©, SiteSpect’s unique “in the flow” server-side approach, to refine and improve the functionality of its digital experiences beyond look and feel. Temple & Webster’s Key to Success Temple & Webster had tested other optimization tools in the past, but felt that they fell short when it came to really moving the needle. Specifically, they found tools that focused primarily on client-side look and feel changes didn’t provide the level of sophistication that they required, and any server-side offerings fell short in complexity, workflow, or integration capability. Because of SiteSpect’s patented reverse proxy architecture, they were able to start experimenting at the origin without disrupting the existing code or impacting site performance. Regarding Origin Experiments©, Mike Henriques, CIO at Temple & Webster says, “You simply must be doing this. We haven’t seen a tool that has allowed us to do as much complex and statistically significant testing as we have been able to do with SiteSpect.” Since implementing SiteSpect in 2018, Temple & Webster consistently has at least four Origin Experiments© running and four client-side experiments running at any given time. Henriques describes SiteSpect as, “complexity at our end, and simplicity on the SiteSpect side.” SiteSpect integrates completely with the Temple & Webster online presence, meaning the team can test anything and everything related to their site, whether it pertains to an internally built tool or a third-party service. Importance of Data-Driven Decisions Temple & Webster have always provided a customer-focused, data-driven digital experience, and with a strong testing and optimization program, they can understand the impact of any change on any channel. Brodie mentions that, “the most valuable insights aren’t necessarily the obvious big wins or losses. Instead, the most valuable results are those that differ from our hypotheses, common wisdom or accepted UI or UX ‘best’ practices.” While there are some great standards out there about what type of copy, font, or search algorithm work best, the actual results are incredibly site and context specific. As Bodie says, “when the data differs from expectations, that’s where you really make data-driven decisions.” He also noted that SiteSpect’s technology provides better data accuracy, more seamless integration and visibility across the entire customer journey compared to what they experience with tag-based vendor tools. Testing Across the Company With the measurable successes that Temple & Webster have seen with SiteSpect, and the “second to none” support, as Brodie and Henriques noted, testing and optimization has begun to spread throughout the organization. Temple & Webster merchandising and marketing teams have taken some of the A/B testing into their own hands, meaning that they can more efficiently validate and improve their work. Henriques says that SiteSpect is “enabling more of the business to be self-sufficient in testing some of its own hypotheses, which they couldn’t do before. And that’s a hugely positive driver of our culture of data-driven innovation.” Integrating With Their Architecture When Temple & Webster began exploring SiteSpect as a potential solution for its digital business, it first needed to assess whether SiteSpect would work within its architecture and technology stack. This evaluation was not only focused around analytics and other data tools, which offered no issues for SiteSpect, but also its network infrastructure, which included Section’s Edge Compute Platform. It didn’t take long to realize that integrating SiteSpect with Section would be no problem. Due to its patented architecture, which is very similar to a Content Delivery Network (CDN), SiteSpect is able to develop routing rules that seamlessly integrate with any CDN and edge solutions like Section. During early discussions, Henriques mentioned that “it is apparent for us that SiteSpect would be best suited to sit within our Section layer.” Recognizing the value for Temple & Webster, along with the opportunity for others to leverage a joint solution, the SiteSpect and Section teams worked together to create a new deployment option within Section, rather than routing outside of Section. “This goes to show how flexible both SiteSpect’s and Section’s platforms are, and how great their DevOps and technical support teams are in order to accommodate our needs. Since migrating to this new deployment, the traffic routing and customer experience have been seamless, and the performance and stability have improved tremendously.” -Mike Henriques, CIO, Temple & Webster In addition to SiteSpect, Section’s Edge Compute Platform offers many other out-of-the-box, containerized edge modules, such as page caching, image caching, web application firewall, bot blocking, and more. Spotlight: A Mobile Page Redesign Recently, Temple & Webster launched a page redesign for their mobile site. As part of the process, they tested all of their important metrics on the new experience. After that initial round of testing, they discovered that the results were inconclusive, and it wasn’t clear which components of the site were helping or hurting conversions. This type of result is fairly common in testing if a lot of changes appear in one variation, and it’s a stepping stone to further inquiry. From this initial result, the team worked with SiteSpect to break down the new mobile page into smaller pieces, creating new variation groups for each change. Temple & Webster then rolled out each change on its own to learn the impact each element had both individually and in conjunction with other elements via multivariate testing. By isolating each change, the originally flat result took on a whole new shape. It turned out that just one element caused a negative turn in conversions, and that without that single element, the redesign increased conversions significantly. With this knowledge, Temple & Webster not only fine-tuned their mobile page redesign, but also found a potentially problematic site component that they could move forward to test and then address site- and device-wide. Conclusion Since implementing SiteSpect, Temple & Webster have taken advantage of the breadth of capabilities it offers — using Origin Experiments© extensively, putting optimization in the hands of many departments, and collecting accurate and thorough data about their user experience. Brodie and Henriques emphasize the partnership with Section and SiteSpect consultants as key to their success."
"15","2019-10-22","2023-03-24","https://www.section.io/blog/adept-mobile-api-caching-routing-normalization/","Adept Mobile is a leading provider of mobile applications in the sports and entertainment space, implementing multi-platform mobile experiences for partners such as the NFL, NBA, NHL, MLS, CHL, and many others. Each application experiences extreme volumes of traffic during game days/times around which the Adept Mobile team is responsible for delivering consistently performant user experiences. Adept Mobile originally came to Section several years ago, when they became frustrated with legacy CDN solutions that failed to offer the level of flexibility and control that their applications require. After achieving many early successes with Section in terms of both performance and development workflow improvements, the Adept team continues to evolve their solution set alongside the Section platform, tailoring custom edge solutions to optimize in-app experiences. The Challenge Each Adept application ingests massive amounts of league, team, and athlete data to deliver rich content experiences to app users. As an example, the San Jose Sharks offer fans rich, dynamic in-app experiences, including exclusive content, live game streaming, game day alerts, secure digital ticketing, and stadium guides. Adept Mobile provides similar app experiences for teams across the league, as well as other major professional sporting leagues. Recently, the Adept team approached Section’s engineers looking to solve challenges associated with fetching data across various API endpoints. Not surprisingly, APIs from the NFL, CHL, NHL, etc are all configured a little differently, and some have rate limits that require workarounds. Adept Mobile leverages a React front-end for their platform. A key challenge that the Adept team is seeking to solve is how to normalize data flowing in from the APIs on the client-side before it hits the React app. The Solution The Section team has helped Adept Mobile implement reverse proxy caching and routing layers in front of each API endpoint, which means that the React app can be normalized to fetch for a single endpoint regardless of the sports league. The front-end SDK fetches from the specified endpoint which renders the React view layer, allowing them to alleviate load off of the league’s APIs and work around rate limits. What’s Next Section’s API caching layer has been instrumental in alleviating load from the API, but perhaps more exciting is what’s coming next. Currently, the Adept application is handling data normalization with some code on the client-side. However, Adept is actively working with the Section platform to dynamically create tables without having to worry about the format of the incoming API data. To achieve this, Adept is leveraging Section’s Node JS module at the edge to handle the normalization. The best part is that the logic that the Adept application is currently using to normalize data on the client-side is already written in JavaScript. This means that the Adept team can simply copy the existing application code into the Section Node JS module and be up and running at the edge immediately."
"16","2019-07-08","2023-03-24","https://www.section.io/blog/adore-beauty-deploys-nuxt-app-at-the-edge/","Melbourne-based Adore Beauty is Australia’s longest running online beauty store and official stockist of over 200 leading global beauty brands. From humble beginnings in her garage in early 2000, then 21-year old Kate Morris set out to provide a better cosmetics shopping experience. While the company has no doubt experienced tremendous growth in the years since, that same commitment to customer experience remains at the core of the Adore Beauty brand. In e-commerce, a large part of delivering an exceptional customer experience lies in the online shopping experience itself. Adore Beauty employs top technical talent focused on continuous innovation to deliver more personalized shopping experiences. They run a highly customized BigCommerce back-end with a lot of heavy integration with Section to maximize caching performance. The Problem Adore Beauty first started leveraging the Section platform several years ago to improve performance and reliability during the peak holiday sales season. After experiencing impressive sales lift through various optimization methods, the technical team at Adore Beauty began to expand beyond simple site acceleration techniques and queueing to take advantage of other benefits that Section’s Edge Compute Platform offers. “We started off by simply using Section to speed up the site and to provide a queueing system when our website would get overloaded. Since then, we have added services and now cover caching, WAF (via ThreatX), A/B Testing (via SiteSpect) and image optimization (via Cloudinary).” -Gareth Williams, CTO, Adore Beauty Recently, Gareth approached the Section team with a new challenge. In an effort to deliver faster, more personalized customer experiences, his team was interested in running their Nuxt application at the edge, closer to their end users. The Solution The team at Adore Beauty is leveraging Section’s Node JS edge module to serve their Nuxt app closer to end users, resulting in reduced latency and significant cost savings as a result of not having to send all requests back to the central infrastructure. For Adore Beauty, the front-end framework (i.e. Nuxt app) calls a Laravel API which houses all product, category, and pricing information. Other requests, such as shopping cart functions are routed to the origin (i.e. BigCommerce) for handling. API Caching In order to avoid performance degradation associated with API calls back to the origin on every request, the Adore Beauty team is leveraging API caching to cache commonly used API routes in a varnishapi Varnish Cache Module. ""proxychain"": [
        {
            ""name"": ""varnishstatic"",
            ""image"": ""varnish:6.1.1""
        },
        {
            ""name"": ""varnishhtml"",
            ""image"": ""varnish:6.1.1""
        },
        {
            ""name"": ""nuxt"",
            ""image"": ""nodejs:10.11.0""
        },
        {
           ""name"": ""varnishapi"",
           ""image"": ""varnish:6.1.1""
        }
    ],
 CI/CD Pipeline Integration Another key feature that is helping streamline operations for the Adore Beauty team is integration into their Bitbucket CI/CD pipeline for releases. A simple docker container run from within Bitbucket allows Adore Beauty to check code into their Bitbucket pipeline for testing before being copied back into Section to trigger a global edge deployment. Edge Hosting Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Learn more and get started on a free plan DevOps Support As CTO, Gareth says that one of the biggest challenges that he faces in his role is how to stay agile. They don’t yet have a dedicated DevOps skillset internally, and with their migration to microservices, they need to maximize their team’s productivity by writing great code and pulling together the best solutions. By running their app through Section’s Node JS edge module, the Adore Beauty technical team doesn’t have to worry about the complexities that come along with hosting, deploying, and scaling. Instead, they can focus on writing code that continues to optimize the user experience. “We don’t need to be concerned about hosting, deploying, and scaling our app. We simply write code, test it and hand this over to Section to run.” -Gareth Williams, CTO, Adore Beauty Getting the app deployed as close to the customer as possible solves a challenge that the Adore Beauty team is not ready to manage internally, which increases speed and redundancy and is backed by the experience of Section’s engineering team. “It means our team can stay lean and focused. We can add value to the customer by pulling together all our microservices and relying on Section to do the scaling,” says Gareth. Looking ahead, Gareth’s aim is to migrate as many microservices as possible to be hosted and scaled by Section. He says, “This will mean our entire focus will be on orchestrating and improving the customer experience.”"
"17","2021-06-22","2023-03-24","https://www.section.io/blog/drupal-steward-powered-by-section/","Earlier this month, the Drupal Association officially launched the Drupal Steward program, offering protection for highly critical and mass exploitable vulnerabilities across over 1 million websites built on Drupal’s open source web content management framework. Operated jointly by the Drupal Association and the Drupal Security team, Drupal Steward is a globally distributed web application firewall built on Section’s Edge as a Service platform. It essentially acts as a ‘virtual patching’ solution that allows DevSecOps teams to reduce risk and improve operational efficiency around critical security updates. Drupal makes up a significant percentage of the web – approximately 1 in 30 websites run on Drupal – hence, protecting this ecosystem means protecting a large portion of the online community. Innovation through Collaboration Through this partnership, Section is proud to continue our commitment to support open source communities while also helping the Drupal Association accelerate their path to innovation. For the Drupal Association and the Drupal Security team, building and operating the Drupal Steward solution on their own would have required a significant investment in additional resources, costs that would likely need to be reflected in the pricing. With Section’s Edge as a Service, however, they are able to offload the infrastructure costs and operational burdens associated with a globally distributed security solution, resulting in their ability to offer affordable (and critical) website protection for their community. Beyond powering the underlying technology for Drupal Steward, we have also been able to facilitate a technology partnership between the Drupal Association and Section client QuantCDN, who generously provided a fork of their own codebase for managing organizations, domains, and customers as a springboard for launching the Drupal Steward dashboard. Register for Drupal Steward Looking Ahead Over the past six months, the Drupal security team has been hardening the end-to-end systems, processes, and testing around the Drupal Steward program in preparation for its release. Back in December, they released Drupal Steward’s first activation report, a recap of the program’s quietly successful initial run. With the public release this month, Drupal site owners can now signup and activate Drupal Steward on their accounts, receiving immediate site protection for as little as $10/month (depending on request volume). As more Drupal site owners capitalize on the benefits of Drupal Steward, Section and the Drupal Association continue to explore additional ways for Section’s Edge as a Service platform to improve Drupal application delivery, including extending extra layers of functionality at the Edge. With the flexibility, control, simplicity, and scalability that Section’s edge technologies offer, the possibilities for what Drupal developers and operations teams can achieve at the Edge are limitless."
"18","2021-09-16","2023-03-24","https://www.section.io/blog/section-selected-for-5g-open-innovation-lab/","We’re excited to announce that Section has been selected by global industry manufacturing leader CNHI America’s 5G Open Innovation Lab (“5GOILab”) to participate in their 12-week global applied innovation ecosystem program. The 5GOILab program aims to create a collaborative ecosystem connecting rapidly evolving digital transformation industry leaders to relevant innovation and platform capabilities. “Section is excited and honored to participate in this program. 5G brings new challenges to placing, scheduling, and routing traffic to edge applications. Section’s innovation in this space will help unlock the full potential of 5G.” - Daniel Bartholomew, CTO, Section Section is one of 12 companies selected to participate in the program through a rigorous screening process. The Fall 2021 program is designed to develop solutions encompassing a broad range of technologies such as Edge, ranging from AI to advanced driver assist systems, cybersecurity, IoT, natural language processing, and advanced wildfire fighting technology. During the 12-week program, Section will work with the Lab’s partners to accelerate opportunities around 5G, edge computing, and more in order to help fuel the global enterprise move to digital (Industry 4.0). “CEO Stewart McGrath and the team at Section have been at the forefront of this technology space, embracing the opportunity for computing on the edge. Section will undoubtedly play an important role in bringing developer applications to customers on the edge more effectively. We are thrilled to have the chance to work with them.” - Jim Brisimitzis, Founder/General Partner, 5G Open Innovation Lab We are excited to participate in this highly selective program alongside the other 11 partners; Brodmann17, DarwinAI, Juganu, Metrolla, MixMode, OasisWorks, OnClave, Pano, Polte, Symbl.ai, and Thrugreen, and look forward to contributing to this globally impactful work in order to help improve the Internet. About the 5G Open Innovation Lab The 5G Open Innovation Lab (“5GOILab”) is a global applied innovation ecosystem for corporations, academia and government institutions working together with early-and later-stage start-ups to fuel the development of new capabilities and market categories that will transform the enterprise. They give startups at all stages unparalleled access to open platforms and markets needed to create, test and deploy new use cases and innovations for 5G and 5G-enabled technologies, including artificial intelligence and edge computing. They deliver actionable knowledge and market insight that helps their partners and member companies deliver value to customers and grow their revenue and leadership positions. Ultimately, the Lab is a global catalyst committed to the transformation of enterprises utilizing intelligent software-defined platforms. To learn more about the Lab and its ecosystem of companies, please visit their website here or follow them on Twitter at @5GOILab. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. All other trademarks are trademarks of their respective companies. About Section Section’s Edge as a Service technologies enable SaaS, PaaS, and application builders to accelerate their path to Edge. By removing the burdens associated with infrastructure provisioning, workload orchestration, scaling, monitoring, and traffic routing, Section helps innovators quickly and easily move their services closer to their end-users so they can cost-effectively deliver faster and more secure digital experiences. Section is the operating system for Edge. With Kubernetes and GitOps at its core, the Section platform provides DevOps teams with consistency, observability, flexibility, and control to simplify management of distributed multi-cloud and Edge environments. Section’s patent-pending Adaptive Edge Engine (AEE) dynamically shifts distributed workload placement and steers traffic to meet organizational and strategic objectives, including compliance, security, performance, and cost."
"19","2021-08-20","2023-03-24","https://www.section.io/blog/section-names-walt-noffsinger-vp-product/","Section is proud to announce the appointment of Walt Noffsinger as its new Vice President of Product. With a background of 30+ years of large-scale business transformation in senior leadership roles across acclaimed public and private sector organizations, he will provide valuable insights to help drive Section’s growth. “We’re very excited to welcome Walt to our leadership team,” said Stewart McGrath, Co-Founder & CEO of Section. “With significant leadership in the IT space at companies ranging from startups to enterprises such as IBM, and an incredible depth of product and product management experience, Walt is a fantastic addition to our team and will help to propel Section into the next stage of development. We look forward to the success driven by his product leadership.” In order to help Section and its customers get from where they are to where they want to be, Noffsinger’s approach focuses on the art of the possible, with an eye for practicality. The application and adoption of edge computing technologies is driving the industry in exciting new directions. Helping customers leverage new advantages like edge is fundamental to his approach. Additionally, keeping the user and developer experience running as smoothly as possible while driving improvements will be a key target. “We need to ensure current customers realize the full potential of distributed edge computing as we’re revolutionizing the approach to optimizing where their workloads operate. We will streamline our product and product management processes around problem solving, feature development, and user stories—introducing new technologies at the right time for customer adoption. This will allow us to continue providing essential capabilities without missing a step, so that we, and our customers, can continue to thrive while we’re building out our overall Section mission to Improve the Internet.” - Walt Noffsinger, Section VP of Product This appointment follows the recent announcements of Paul Savill and Deborah Diaz joining Section’s Board of Directors. As Section’s VP of Product, Walt builds on the diversity of experience, perspective, and leadership represented among Section’s executive team and will serve as a key driver and advisor as we continue to build our innovative edge technologies."
"20","2021-07-20","2023-03-24","https://www.section.io/blog/section-welcomes-paul-savill-to-board-of-directors/","Section is proud to welcome Paul Savill, Senior Vice President of Product Management and Services at Lumen Technologies, to the company’s Board of Directors. With over 25 years of experience in the telecommunications industry, Paul’s business acumen combined with his deep technical expertise makes him a natural fit to support and advise Section as we continue to pioneer edge technologies for modern enterprises. “We’re thrilled to welcome Paul to Section’s Board of Directors,” said Stewart McGrath, Co-Founder & CEO of Section. “Coming from an engineering background and having served in several product leadership roles, he understands Section’s innovative vision within the technology landscape and will serve as a valuable resource to help guide us in our next phase of growth.” “I’ve had the privilege of getting to know Section and its leadership team over the course of the last couple years, and I’m excited to now engage on the board to help broaden the success of the company and fulfill its great potential.” - Paul Savill, SVP of Product Management and Services at Lumen Technologies Following Section’s recent Series B funding round led by Lumen Technologies, Paul joins as the fourth member of Section’s growing Board of Directors bringing his wealth of industry knowledge to the table to help Section into the next phase of growth."
"21","2021-04-26","2023-03-24","https://www.section.io/blog/section-announces-series-b-funding-round/","We’re thrilled to announce that we have just closed $12 million in Series B funding, a very exciting milestone for Section. “It is a reflection of the hard work and vision that has gone into our business from day one. It is also an indication of the future opportunity ahead of us.” - Stewart McGrath, Co-Founder & CEO This latest financing round was led by Lumen Technologies whose mission to help facilitate the latency-sensitive, data-intensive needs of next-gen applications and emerging technologies aligns with our own to enable application providers to deliver faster and more secure digital experiences at the Edge. Two of Section’s existing investors Foundry Group and Next Frontier Capital also played a significant role in the round, as did new investor Andreas Family Office. “Section has been a key Lumen technology partner around Edge application delivery on the Lumen Platform. Lumen and Section share a common vision that compute can and should be dynamically available anywhere digital interactions occur. We look forward to continued work together to deliver solutions that drive secure digital experiences at the Edge.” - Paul Savill, Senior Vice President, Enterprise Product Management and Services This latest influx of investment will help propel us into the next stage of our growth as a leading Edge as a Service (EaaS) provider. We’re excited to be a key part of the ecosystem driving the enablement of edge computing forwards. Our goal as an EaaS platform is to simplify the complexities associated with distributed computing, while also providing the deep level of customization and flexibility that DevOps teams need to build more performant, secure, and scalable applications. This past year, we have been focused on the release of several groundbreaking new technologies, including: Adaptive Edge Engine (patent-pending) The Adaptive Edge Engine (AEE) intelligently and continuously tunes and reconfigures edge delivery networks to ensure edge workloads are running in optimal locations to meet real-time traffic demands. Node.js Edge Hosting Section’s Node.js Edge Hosting solution empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Traffic Monitor The Section Traffic Monitor gives DevOps engineers, application owners, and business leaders an easy way to quickly visualize how traffic is flowing through their edge architecture, delivering valuable insights that transcend functional boundaries. With the Series B funding, we intend to invest further in the pioneering work of our Edge as a Service platform, enhance our stellar team, and drive forward our mission to deliver a better Edge for Dev and Ops Engineers. “Our mission at Section is to improve the Internet by empowering innovators with simplicity, flexibility and control at the Edge to create better digital experiences. We know the Edge is key to a better Internet versus legacy centralized cloud as end users demand faster and more secure experiences. Section helps development and operations teams access the power of the Edge with the simplicity of cloud.” - Stewart McGrath, Co-Founder & CEO of Section"
"22","2021-01-27","2023-03-24","https://www.section.io/blog/edge-as-a-service-the-next-big-thing-in-tech/","Software as a Service (SaaS) was the largest market segment in the cloud in 2020 and Gartner forecasts it to grow to $117.7 billion by the end of this year. Not far behind comes Platform as a Service (PaaS), a development and deployment environment in the cloud, and Infrastructure as a Service (IaaS), instant computing infrastructure, provisioned and managed over the cloud. So, with compute architectures becoming increasingly more distributed, is Edge as a Service (EaaS) poised to be the next big thing in tech? What is Edge as a Service (EaaS)? A large portion of applications, particularly enterprise apps, leverage a heterogeneous network of public clouds, private clouds, on-premise data centers, and edge. A shift toward this kind of hybrid IT infrastructure has been accelerated due to the pandemic and increased demand for high-speed networks and by extension, edge computing. Helping meet this demand is Edge as a Service (EaaS). As defined by Edge IR, EaaS providers are “companies that offer a platform realizing distributed cloud architectures and integrating the edge of the network in the computing ecosystem.” Simplifying and Accelerating the Path to the Edge Cloud providers have offered numerous unique benefits to developers in the cloud hosting space, from the ability to easily scale to being able to quickly provision new resources, whether virtual machines or databases. Another benefit of the cloud is its geographic reach. Further, the cloud doesn’t offer just one service model, but multiple options for how to set up your environment to meet your specific set of needs, such as via IaaS or PaaS. Now is the time to expand these same principles to edge computing. The trends towards microservices and distributed computing architectures are driving this need. When you go from a deployment model of selecting a single cloud endpoint (e.g. ‘US East’) to running an application across hundreds, or even thousands, of endpoints across the globe, the complexities grow exponentially. EaaS offers organizations confronting a lack of skill sets within their internal IT teams, the ability to benefit from the expertise and resources of an EaaS provider who can provide turnkey solutions for customizing and managing these complex systems. Essentially, Edge as a Service offers an easy ‘Buy’ option for those facing the Build vs. Buy dilemma. Consider, for example, an organization like Chick-fil-A. Most people know them for their menu, not their technology. However, Chick-fil-A is revered among many tech enthusiasts as one of the first to successfully adopt an edge networking Kubernetes container orchestration deployment that is managed with a GitOps approach. Chick-fil-A made the decision to invest in an expert IT team to build and manage these systems at a time when the available vendor solutions were silo’d and disconnected, making it difficult to integrate. However, as Edge as a Service has matured to apply the same open source principles that led Chick-fil-A to develop a homegrown system, organizations today have the advantage of being able to leverage the same solutions without the burdens of building and managing these systems. This, in turn, gives them the ability to innovate while not diverting focus from their core business. In the same way that SaaS revolutionized software delivery and managed to become the default software-delivery model over the course of just one decade, we anticipate EaaS to be the next big tech disruption. How EaaS benefits SaaS and PaaS providers There are many ways in which Edge as a Service can add value to existing “as-a-Service” solutions. Edge as a Service offers SaaS providers new deployment options Software as a Service (SaaS) is defined as a centrally-hosted application offering specific services such as email (Gmail) or storage (Dropbox) via a subscription-based software licensing model. Edge as a Service enables SaaS providers to deliver faster, more secure experiences for their customers by leveraging the innovative benefits of edge computing. With EaaS, SaaS providers gain instant access to an edge deployment model for their solutions without having to build and manage their own edge network. Section recently partnered with Wallam to power its Wallarm Cloud WAF, which provides protection across applications, APIs and serverless workloads. Working with Section’s Edge as a Service, Wallarm Filter Nodes are distributed around the world providing protection at the edge without any agent installation necessary on the client’s side. “Section gave us a fast path to market to introduce a cloud-native offering for our WAF solution.” - Wallarm Edge as a Service enables PaaS & hosting providers to quickly and easily build and integrate edge services Platform as a Service (PaaS) offers a digital framework for developers to customize as they build, test and deliver their own applications. The PaaS or other hosting provider manages the infrastructure and operating systems, so that the developer can concentrate on managing the applications and building software. Edge as a Service enables PaaS in unique ways by: Presenting the opportunity to extend edge services not previously available on the platform, for example, a turnkey CDN for hosting providers, which can open up new revenue streams. Acting as the glue between edge services (software) and edge infrastructure - edge provisioning, workload orchestration, scaling, traffic routing One such example of a PaaS provider leveraging Edge as a Service is Drupal using Section to power the Drupal Steward security program for its community. Harnessing the flexibility and control of the Section platform, Drupal was able to build and offer a powerful Community tier package that provides affordable protection against the exploitation of highly critical vulnerabilities within an application. “The only edge platform that enabled us to build and release Drupal Steward at the Edge.” - Drupal Association EaaS: A New Approach to Networks Users are demanding ever greater simplicity, scale, and agility. As the demands on the world’s networks grow, older approaches to building applications and networks are becoming unsustainable. A new approach is necessary, which builds on cloud computing to place latency-sensitive and latency-critical workloads as close as possible to the end user - at the edge - in order to improve performance and reduce data backhaul. Edge as a Service abstracts many of the complexities associated with edge computing adoption, making it easier for modern engineering teams to adopt a new compute paradigm that delivers better digital experiences, and ultimately improves the Internet as a whole."
"23","2021-01-20","2023-03-24","https://www.section.io/blog/multi-cloud-hybrid-it-infrastructure/","Gartner predicts, “By 2021, over 75% of midsize and large organizations will have adopted a multi-cloud and/or hybrid IT strategy.” Meanwhile, IDC says it expects 2021 to be “the year of multi-cloud as the global COVID-19 pandemic reaffirms a critical need for business agility”. While the big public cloud providers tout themselves as one-stop shops, covering all of an organization’s cloud, data and compute needs, the reality is quite different. In practice, enterprises of all sizes are turning to multi-cloud and increasingly hybrid IT environments, with requirements for infrastructure to be deployed across traditional, cloud and edge compute environments. In this article, we will look at some of the top drivers behind the move, why edge is an increasingly critical part of a multi-cloud/hybrid IT infrastructure, three primary challenges connected to managing this kind of approach, and paths to help solve them. Top 6 Drivers Behind Multi-Cloud and Hybrid IT Organizations operating and serving customers locally and/or globally need to consider how best to make their applications available, responsive and high-performing no matter where the end-user (or end device) is located. There are many more drivers behind the move towards a multi-cloud and/or hybrid IT approach. Here are the top six. 1. Agility Multi-cloud and hybrid IT environments offer great flexibility. Many businesses are turning to these models with the desire to avoid vendor lock-in, and gain from the ability to leverage best-of-breed solutions, depending on the specific needs of the task. Another benefit, particularly for medium and large enterprises, is the flexibility for different business units within the same organization to use different cloud/edge providers for different services. 2. Architecture Modern applications are adopting a more modular style or microservices-based approach, enabling them to span more than one cloud provider or consume services from different clouds. Kubernetes clusters (orchestrating containers), for instance, are often used to simplify multi-cloud management 3. Resilience of Systems Multi-cloud or hybrid IT can help protect a company’s critical business application and data, and ensure availability by offering backup and recovery capabilities. It also provides improved disaster recovery in the event of an unexpected incident, from a power outage to a natural disaster - by offering the ability to replicate your resources in another cloud/type of compute away from the affected area. 4. Scalability Multi-cloud and hybrid IT allows businesses to scale their storage up or down across regions depending on real-time demand. Ideally, there is a seamless transition between providers, allowing businesses to invest in the level of space, security and protection they need for each data segment. 5. Cost The ability to select the services that best meet the needs of the business also allows businesses to take advantage of cost efficiencies, while avoiding vendor lock-in to a single provider’s fixed network and/or software stack. 6. Performance A multi-cloud strategy, particularly one that leverages edge computing, allows businesses the maximum opportunity to optimize for performance. Cloud computing has inherent performance and latency constraints, such as the synchronization of huge amounts of data with online storage. A hybrid approach to IT infrastructure enables the leveraging of edge computing and processing of data locally while using the cloud to transmit and store only the necessary data. Edge Computing, Multi-Cloud and Hybrid IT Infrastructure Multi-cloud complexity is expected to grow even more with the adoption of more distributed applications, such as edge/IoT and machine learning as noted in a recent Flexera report. Edge computing is increasingly becoming an important part of a multi-cloud/hybrid approach to IT infrastructure as CIOs work out how to: Guarantee digital performance for their remote employees working at the edge of the network; Manage the integration of emerging technologies such as AI, IoT and blockchain; Handle the volume and velocity of data generated in an always-on digital landscape and from sensors and IoT endpoints Increase resource efficiencies; and Reduce friction and speed up innovation for developers. As noted in the CRN 2021 predictions, “By integrating the network edge into their cloud strategy, developers have the ability to easily deploy services at the edge without having to be concerned with the operational overhead of managing more infrastructure… With integrated development and deployment pipelines, developers can move application services and functions from the cloud into network edge locations. This will help create more responsive and dynamic applications.” Edge Computing and Cloud are Complementary According to Santhosh Rao, Senior Director Analyst, Gartner, “Cloud computing and edge are complementary, rather than competitive or mutually exclusive. Organizations that use them together will benefit from the synergies of solutions that maximize the benefits of both centralized and decentralized models.” For edge computing to work best alongside the cloud as part of a hybrid IT infrastructure, the edge should be where network-sensitive and latency-critical applications run. One such example is AI inference taking place at a regional edge, with model training and tuning occurring in the centralized cloud. Edge and cloud need to talk to each other, for instance, about how much transfer of data should happen between them vs. being locally stored at the edge. 3 Challenges of a Multi-cloud/Hybrid IT Infrastructure Challenges connected to running a multi-cloud or hybrid IT infrastructure include complexity, a lack of skills, provisioning, IT interoperability and integration, application certification, and change management/tracking. Let’s examine three of these challenges in more depth. 1. Complexity While working with multiple clouds offers a great deal of flexibility, it naturally brings with it an additional layer of complexity that needs to be carefully managed. Deploying to and maintaining a single cloud instance is dramatically different than orchestrating workloads across hundreds or thousands of endpoints. Businesses that use a multi-cloud and/or hybrid IT approach need to crystallize their approach into a formal strategy. One tangible way to deal with complexity is to use abstraction to map complex data from the cloud or edge into the type of visualization you need for each specific task. 2. Lack of Skills IDC’s META CIO Survey 2020 revealed that 39% of respondents saw “a lack of skills as an obstacle, hindering their rollout of cloud strategies”. The lack of public cloud and edge platform expertise is driving organizations towards rethinking how they approach their infrastructure needs. “To achieve success in their multi-cloud journeys”, says IDC, it “advises businesses to take stock of their applications and develop a phased modernization roadmap for each one, thereby enabling them to make appropriate cloud decisions for the multi-cloud era.” 3. Provisioning In a multi-cloud set-up, before you can deploy applications to one cloud provider, you need to provision more than one environment. A lack of unified provisioning tools means that IT has to serialize much of public and private cloud service delivery, which often results in bottlenecks. There are ways to help streamline this process, such as through Infrastructure as Code (Iac). IaC allows DevOps teams to establish more repeatable processes through automation. Benefits of this include improved speed, greater consistency, reusability of code, more flexibility, and more straightforward version control. Building and Managing a True Cloud/Edge Deployment Model When you think beyond major cloud providers to a true cloud/edge deployment model, consider the challenges of managing workload orchestration across hundreds, or even thousands of, edge endpoints. Section fills this gap for teams who don’t have the resources and/or expertise to build and manage these complex systems. The Section Adaptive Edge Engine intelligently and continuously tunes our customers’ edge delivery networks to ensure that edge workloads are running the optimal compute for the specific application based on real-time traffic demands. Section is already providing Edge as a Service to many organizations utilizing a multi-cloud and/or hybrid approach to IT infrastructure. Contact us to learn more, or get started now."
"24","2020-06-12","2023-03-24","https://www.section.io/blog/infrastructure-as-code-cross-cloud-consistency/","To understand Infrastructure as Code (IaC), first let’s look at how infrastructure has traditionally been managed. Historically, a consumer of infrastructure would manually file a ticket and at the other side of the ticketing queue, someone else would log into a management portal and provision that piece of infrastructure. This could involve mounting servers on racks, installing operating systems, and/or connecting and configuring networks. After hardware was requisitioned and installed, if the need receded, the money spent couldn’t be reallocated. So, while demand typically varied, an owned and operated data center historically had no elasticity. The next step was virtual machines (VMs). This involved buying less, but bigger hardware and allocating portions of resources. This didn’t solve the overall problem of elasticity, but it did reduce the amount of provisioning required, as software installation could be bottled up as part of a standard image. This manual provisioning process worked okay when infrastructure demands were relatively low and didn’t involve a high infrastructure churn, which was indeed the case for many private data centers. Adapting to the Cloud As businesses increasingly migrate to the cloud and require greater on-demand scalability, the traditional infrastructure management processes aren’t sustainable. At the same time, applications have undergone a shift in their architectural patterns with the introduction of microservices and containers. Hence, engineering teams now need to support, not just the management of the infrastructure, but the design of the services running on that infrastructure, to take advantage of cloud provider services. This is one of the reasons behind the widespread adoption of Kubernetes (k8s), which along with cloud resources, offers the scalability that modern engineering teams demand. Armon Dadgar, Hashicorp CTO, pinpoints two main reasons for the need for change in how infrastructure was provisioned: (i) the move to a cloud environment, which is predominantly API-driven; (ii) a much higher elasticity of infrastructure than previously. We also need to take into account the overall shift within IT towards DevOps and agile practices, which has dramatically reduced software development cycles. Supporting DevOps Practices In a cloud/edge environment, development cycles are typically shorter than in a traditional data center setting, and releases are more frequent. This means features appear more often, and progress can be seen more frequently. Production feedback also arrives sooner, lessening the risk that what has been built won’t work for end users. Because IaC promotes higher repeatability through declarative languages that can be version controlled, humans no longer have to set up every piece of infrastructure by hand, leading to a reduction in human trial and error. The demand for cloud/edge infrastructure is typically cyclical and, in order to optimize efficiency, should be scaled up or down depending on load at a given time. It makes sense to only use the infrastructure you need and spin it up or down to meet real-time demands. IaC makes this possible. “Most outages are self-inflicted” This was a point made in a recent talk on IaC by Brendan Burns, Kubernetes co-founder and DE for Microsoft Azure. As he highlighted, it is rare that anyone intends to break the system and cause a multi-hour or multi-day outage, but accidents happen and lead to performance problems. Burns says, “The primary reason that these accidents happen is these snowflakes.” By snowflakes, he means every time a developer makes an imperative change to the world that will never exist again. Then when you have to do a rollback, you have to recreate it, but you can’t. By moving to a more declarative, cloud-native mode of operations, Burns describes us as moving to a world “where we are automating everything that we do, where we are securing and addressing the compliance and really achieving all of the goals that we set out to do in moving from the imperative world of scripting to a more declarative, more reproducible future.” “And so we have to move from this world where we have a bunch of snowflakes to this world where we are automating everything that we do, where we are securing and addressing the compliance and really achieving all of the goals that we set out to do in moving from the imperative world of scripting to a more declarative, more reproducible future.” Brendan Burns, Kubernetes co-founder and DE for Microsoft Azure We can go one step further with the idea that DevOps might eventually create a new category: FinOps, where automation leads to the ability to set up and tear down infrastructure in an automated fashion to best optimize costs – for example, to match a day/night cycle or seasonal trends. Of course, that leads to challenges around how to store, view, and investigate bottlenecks that are the result of a greater amount of metrics. Underpinning the cloud-native approach to application delivery is the need to reduce operational complexity and maintain freedom of choice. DevOps teams thrive on flexibility and control over their workloads, particularly at the edge. Part of Wikipedia’s definition of DevOps includes “establishing a culture and environment where building, testing, and releasing software, can happen rapidly, frequently, and more reliably.” IaC is a critical part of this. The benefits of IaC Some of the benefits to utilizing IaC include: Speed: IaC allows you to create and configure infrastructure elements in seconds simply by running a script. You can do it for every environment, from dev to prod, passing through staging, QA, and other areas. IaC can help make the whole software development cycle more efficient. Consistency: This approach codifies the provisioning management process so that every time it’s performed, it can be automated, meaning one-off configurations are avoided. Human error is much less likely to creep in when the infrastructure management process is operationalized instead of performed manually. Reusability: Since an IaC system is deployed from source code, it is easy to reuse over time simply by executing the same code again. Organizations can reuse the same system whenever necessary. They can also refine the source code over time, as well as concepts like testing, project structure, and monitoring. Version control: As defined by Git, version control is “a system that records changes to a file or set of files over time so that you can recall specific versions later”. In simpler terms, version control allows you to see who has changed what when. This transparency of documentation allows you to look back at every version of a file so that you can easily revert files or the entire project to a previous state, compare changes over time, and see how a modification may have caused a problem. It provides the transparency and visibility lacking in a traditional point and click environment. Flexibility: There is real benefit to not being locked into an infrastructure layer in a similar way that you can benefit from when adopting a multi-cloud strategy. DevOps teams leverage a range of different tools when it comes to IaC, e.g. Terraform, bash, Go, CLIs or simply APIs that are scripted. This approach enables teams to employ a common deployment process across multiple providers. It also typically leads to lower costs associated with infrastructure management, and allows DevOps teams to focus on what matters most instead of performing manual, slow, error-prone tasks. Summary Infrastructure as Code enables DevOps teams to establish more repeatable processes through automation, which results in benefits that span across organizational functions. IaC is a key technology underpinning Section’s flexible deployment options across its Composable Edge Cloud and is driving innovation around more performant and efficient edge computing."
"25","2019-07-29","2023-03-24","https://www.section.io/blog/section-devops-technology-stack/","DevOps teams enjoy the flexibility and control that the Section platform gives them in choosing where and when to run edge workloads. Driving that flexibility and control are sets of open source tools and integrations that developers are deeply familiar with. Here’s a breakdown of the Section technology stack used by (or integrating with) Section. GitOps GitOps is an approach to managing infrastructure where code/configuration is managed in a code repository such as Git. GitOps has several benefits, including infrastructure changes becoming an immutable changelog of everything that has happened to that environment, and DevOps teams use tooling they already know to manage their environments. GitOps allows developers to bypass dashboards and management consoles in order to manage their environments directly as version-controlled code. Version Control Developers are increasingly using Git as the go-to tool to manage entire technology stacks, allowing teams to seamlessly collaborate, review, deploy, and rollback when necessary. Customer configurations within the Section platform are backed by Git repositories, so developers can use familiar tools to easily pull code down alongside their application codebase to configure, test and optimize while minimizing the risk of pushing errors into production. CI/CD Section pushes to production many times today. CI/CD (Continuous Integration/Continuous Delivery) is an important practice for DevOps teams, allowing for more frequent and reliable delivery of code changes through driving development teams to implement small changes and check-in code to version control repositories on a regular basis. CI/CD has become an important practice to enable teams to achieve higher velocity and deliver value faster to client. As most of today’s applications involve developing code across a range of different platforms and tools, teams need a mechanism to integrate and validate their changes. Section provides flexibility to DevOps teams to integrate with their preferred CI/CD tools, including Jenkins, Bitbucket, and CircleCI, to name a few. These integrations enable hooks to connect our GitOps workflow into customers’ CI/CD platform of choice. Edge Workloads Module Marketplace No one likes vendor lock-in. One of the greatest benefits of the Section platform is the freedom that it gives engineers to run the software that best suits their application. The Section Module Marketplace includes readily available open-source and best-of-breed options for web application firewalls (WAFs), bot blocking, caching, server-side A/B testing, image optimization and more. Our marketplace has taken many of the tools and frameworks used by many organizations and containerized them. Once containerized and conforming to our module contract, these tools are available to be deployed (and enjoy all the benefits of running) on the Section platform. Custom Workloads The container-based nature of our platform means that virtually any containerized workload can run on our globally distributed edge network, giving developers the flexibility to run their own custom edge workloads in the locations that are most suitable for their applications. Orchestration & Provisioning Orchestration We recently migrated the Section platform from a legacy scheduler to Kubernetes. The Kubernetes migration led to a range of benefits, including greater flexibility, resource efficiency, scalability, low downtime, and improved performance during the entire production lifecycle. Read more on how Kubernetes is helping to power the growing move towards edge computing. Provisioning In terms of provisioning, Section uses Terraform. Terraform has been called the “next generation of configuration orchestration systems” because it brings a whole new layer of features and functionalities to the table. It works with any cloud and enables secure and straightforward design, management and improvements for infrastructure-as-code. Furthermore, Terraform is highly flexible, helping provision any application written in any language to any infrastructure. Infrastructure Hyperscale Section is a software company; we own no hardware. Instead, we leverage the global networks of a number of providers and move workloads where it makes the most sense for our clients. Section is provider-agnostic, seamlessly integrating with large hyperscalers such as AWS, Google Cloud, Azure, DigitalOcean, and RackCorp. Section takes away the need to manage your provider so you can focus on your edge needs. Localized High-Performance Infrastructure As the demands for faster, more secure and more scalable applications continue to build, more compute is being moved closer to end-users, leading to the introduction of more specialized infrastructure providers. Packet is one such provider, offering a range of automated infrastructure deployment models, from a bare metal cloud to an on-premise software solution, and custom solutions in between. We want your application to work beautifully everywhere; Packet helps Section deliver on that mission by providing access to raw infrastructure within milliseconds of users around the world. Emerging As the edge computing ecosystem continues to evolve, we’re seeing new infrastructure emerge on a hyper-local scale. Section is at the forefront of this movement, currently working with providers such as Packet and Vapor to deploy workloads at cell tower base stations in Chicago and Boston, with additional locations being considered. Observability Finally, as a team of engineers, we know first-hand how critical observability is to maintaining and optimizing application performance, security and scalability. It’s not just building an edge platform that creates business value; it’s operating it efficiently. Section leverages a series of tools, including Prometheus, Grafana, and the ELK Stack, to provide deep insights into every layer within and across the Section platform. These insights enable developers to optimize, tune, and debug their production environment."
"26","2018-10-22","2023-03-24","https://www.section.io/blog/devops-continuous-delivery-edge-compute/","To keep pace with technology, engineers must be able to conduct quick deployments in a safe, reliable and repeatable way. DevOps and continuous delivery (CD) help to support a more responsive and flexible, quicker time-to-market across the software delivery cycle; an increasingly important asset in a market in which IT departments are squeezed to achieve a quicker pace of innovation. For years, development and operations were kept separate with developers focused only on code, and operations on keeping the code running. The disconnect between departments typically led to long QA cycles and production deployments happening only infrequently (this still happens in many companies, particularly larger ones). However, developers, frustrated with production not updating their code quickly enough, began to write software that would automate operational tasks, and operations personnel began to contribute their knowledge back into the software written by the developers. The resulting new field has been dubbed DevOps. DevOps enables a blurring of boundaries between development and operations and accordingly, quicker application lifecycles; shorter, more efficient QA sessions; and a significantly larger number of deployments. DevOps and Continuous Delivery at the Edge As developers gain more control over the provisioning of IT resources and everyday operations, they require more flexibility, transparency and visibility than traditional CDNs can offer. Continuous Integration / Continuous Delivery (CI/CD) software pipelines are useful in managing environments in which the risk of instituting changes is significant by turning the changes into smaller, verifiable units. This is essential for edge computing as a lack of physical access can make it difficult to mitigate problems. Small incremental changes lessen the risk of a major flaw being introduced to the whole system. Instituting changes in this way means that any problems can be rapidly detected and prevented before having a wider impact. Netflix’s CD platform, Spinnaker, provides a good example of CD at the edge. Now open source, Spinnaker is a multi-cloud CD platform, which integrates DevOps and CD at the edge. It was initially developed internally within Netflix to enable development teams to release software changes with confidence. While it was being developed, the head of the Edge team at Netflix was working on a separate deployment management process. The two team leads realized that they were both aiming to conduct fast deployments in a safe, reliable and repeatable way and decided to fuse their projects to make a “continuous delivering infrastructure management platform”. Changes to applications in Spinnaker can now trigger a redeployment of the complete server fleet.. as a result of “Spinnaker’s use of immutable infrastructure in the cloud”. Other organizations are following suit. Just recently, GitHub Actions was launched. Actions allow developers to go beyond just hosting code on the platform to also run code, allowing developers to automate their workflows from a task as simple as creating notifications all the way to building a full continuous integration and delivery pipeline. Sam Lambert, GitHub’s Head of Platform, described Actions to TechCrunch as “the biggest shift we’ve had in the history of GitHub”. Why DevOps Teams Prefer Section Section is unique among CDNs and Application Delivery Controller solutions in having been specifically designed to accelerate, scale and secure websites with the complete development cycle in mind. Furthermore, Section is is the only CDN and/or Application Delivery Controller solution to offer delivery of a complete DevOps suite, including: Configuration as code Worldwide configuration change propagation and cache clear, which happens instantly Real time diagnostics and insights Total integration with typical application development workflow Modular software options to deploy and upgrade path control (all modules have consistent configuration-as-code and diagnostics capabilities) Software options include image optimization, server-side multivariate testing, Varnish Cache, 3 different WAF modules and more Choice of where Section runs e.g. either as a CDN and/or behind the firewall as an Application Delivery Controller The flexibility and transparency Section can offer in facilitating DevOps and CD at the edge is second to none. Through the utilization of container technology (Docker and Kubernetes at the core), Section architecturally differs from every other CDN and/or Application Delivery Controller solution in the market."
"27","2022-09-12","2023-03-24","https://www.section.io/blog/distributed-application-monitoring/","Section’s Kubernetes Edge Interface (KEI) and Adaptive Edge Engine (AEE) have dramatically simplified distributed application deployments, allowing DevOps teams to use familiar workflows and tooling (kubectl, Helm, etc.) to move apps to distributed footprints, then control and manage those workloads using simple policy-based rules. In short, the distributed footprint becomes one big cluster that’s automatically optimized in the background based on rules you set, while teams continue to manage application updates and deployment as they would with a single cluster. But what about ongoing operational requirements? How can and should you be monitoring app distributed applications? Multi-cloud and edge platforms and services must be flexible enough to adapt to different architectures, frameworks and programming languages. Take, for example, when a developer works with Section; they can continue to work with their existing code and continer repository, similar to any other Continuous Integration and Continuous Delivery (CI/CD) workflow. The platform ultimately supports code portability for developers building across many different runtime environments, handling the deployment of any code changes as needed. But Section’s vision doesn’t stop there. To quote Stewart McGrath, Section’s cofounder and CEO, “since we’re in a DevOps world, not only do we need to think about the application development lifecycle and the CI/CD workflow, but we need to think about the observability and management of the distributed application.” Observability for Distributed Systems: Diagnostics and Telemetry Speed of innovation is a key differentiator for businesses in a competitive market, so it’s not surprising to see how many SaaS, PaaS, and application builders are now getting onboard with new and more dynamic ways to accelerate apps through distribution. Developers should always look for ways to iterate quickly, yet safely. Observability helps these DevOps teams move quickly with greater confidence. Even if you’re using KEI and AEE to automate distribution optimization, deployment best practices demand a comprehensive monitoring strategy; it’s critical for developers to have a holistic understanding of the state of their application at any given time. But that observability becomes increasingly complex when you consider distributed delivery nodes across a diverse set of infrastructure from different providers. As reported in a recent Dynatrace survey of 700 CIOs, “The dynamic nature of today’s hybrid, multicloud ecosystems amplifies complexity. 61% of CIOs say their IT environment changes every minute or less, while 32% say their environment changes at least once every second.” To add to the complexities of trying to keep up with dynamic systems, that same report revealed that: “On average, organizations are using 10 monitoring solutions across their technology stacks. However, digital teams only have full observability into 11% of their application and infrastructure environments.” An effective solution for multi-cloud, muti-cluster or distributed system observability should be able to provide a single pane of glass to draw together data from many locations and infrastructure providers. This kind of visibility is essential for developers to gain insight into the entire application development and delivery lifecycle. The right centralized telemetry solution allows engineers and operations teams to evaluate performance, diagnose problems, observe traffic patterns, and share value and insights with key stakeholders. Monitoring with Section’s Composable Edge Cloud Section provides a Composable Edge Cloud on which your application can run consisting of a global network of compute locations. Kubernetes Console To see real time feedback of where your traffic is running, Section provides the access to the standard Kubernetes Console which is presented as though the various locations on which an application may be running at any one time are running in one single cluster; even though they may be any where in the world. In the Kubernetes console users can SSH onto every single container immediately in any location to diagnose and debug. Users can also use the standard Kubernetes logging functionality. Analytics Integration In addition, users can easily integrate with their existing application analytics tools such as Grafana, Splunk and Datadog. Summary Monitoring is essential for effecitve operation of any modern application. However, monitoring complexity increases significantly when face with instrumenting distributed systems. Section provides such monitoring instrumentation out of the box with every application deployed. Section’s ability to continuously optimize the orchestration of secure and reliable global infrastructure for application delivery was a key selling point for Kentik co-founder and CEO Avi Freedman: “As a network observability company, Kentik has a global view of the internet combining passive and active measurements. Partnering with Section allows us to quickly and easily augment our edge deployments, and their cloud-native platform and partnerships make it easy and affordable to integrate as we continually expand our footprint.” If you aren’t already on the Section platform, try it for free today."
"28","2021-02-11","2023-03-24","https://www.section.io/blog/sectionctl-cli-tooling-for-edge-computing/","The command line continues to be an essential tool for developers. It helps developers get more done. There are multiple benefits of a more responsive, faster UI. Systems hygiene tasks such as managing local processes and source control are frequently done through CLIs. Many cloud computing services, continuous integration products, and increasingly APIs now have their own CLIs. Introducing sectionctl: Section’s CLI Tooling Here at Section, we recently built and released our own CLI tool, sectionctl. We built sectionctl while launching our Node.js Edge Hosting solution. We wanted to create a developer-friendly experience for getting your Node.js applications running at the network edge. “We are trying to make an unfamiliar experience a familiar one.” - Lindsay Holmwood, Director of Product, Section We went through a couple of different ideas for how to approach this, and settled on the CLI utility. We realized that there were several small steps that needed to be followed before the Node.js edge app could be launched. These include: Issuing certificates - When you set up with Section, you automatically receive free SSL certificates for all your apps. When building a new onboarding workflow for Node.js apps, we decided to automate the step of issuing the certificates. We have now added this to sectionctl. Setting up domains - We want to let you easily see which domains are mapped to which applications. When you run the final command to deploy the app (sectionctl deploy), there are multiple arguments to parse. Since you may not know your account ID, application name, etcetera, this step makes it easier to see that information. Plus, if you have other workloads outside the Node.js experience – for example a more traditional CDN workload – sectionctl works really well for that too. We went for a CLI partly because we wanted to make it easy for you to see all your applications, domains, and certificates in one developer-friendly package. We decided it was better to build a small Swiss army knife of tools to do all these things as opposed to pinging people between a web interface and a CLI to perform different tasks. Our Developer Goals with sectionctl sectionctl isn’t the first command line utility that Section has had. Back in the early days of Section, we had a CLI that was written in Ruby, which was essentially a helper for each of your apps. It involved quite a different way of working when compared with sectionctl – you had to be in your Section config repo to run it. Since it was written in Ruby, there was also quite a bit of set up, maintenance, and dependencies to manage to get it to run. We took a different approach with sectionctl. We wanted a single binary you can easily download and install. It’s completely self-contained. We’ve tried to emulate other modern CLI experiences, in particular the Heroku CLI and kubectl from Kubernetes (which helped inspire the name). We wanted to provide a familiar developer experience to people who have never used our tools and platform before. We’ve deliberately built in user experience patterns and in-app help to make it easier to interact with CLI utilities. One such example is tab completion. If you’re typing code and get stuck, you can hit ‘tab’ to get feedback from sectionctl with suggestions for which flags you can use to do your deployment. As developers, we spend so much time in the console getting stuff done. Our ultimate goal for the developer experience here, as with the entire Section platform, is a simple, flexible developer experience that puts you in control. We wanted to build something that devs could enjoy working inside, benefitting from a feeling of familiarity despite never having used it before. We’d like people to have the same comfort they get when deploying an app to the cloud when deploying to the Edge with sectionctl. The challenge for us was to do the hard work to make solving problems easy for users. Using sectionctl for Ongoing Management and Observability While we built sectionctl specifically for the app creation and deployment process, we quickly realized that after the first 30 minutes when you have a real app that’s up and running, you’re going to need insight into what’s happening inside it. We have integrated two features for precisely these tasks: (i) sectionctl ps and (ii) sectionctl logs. sectionctl ps The sectionctl ps command lets you see the state of an application. It will return the status of all the different workloads running on Section’s Edge. sectionctl logs The sectionctl logs command lets you see deep into Section’s infrastructure, piping the logs from all edge endpoints back into your terminal. This allows you to see the complete log output of your app, both in the normal access logs (HTTP) and at an application level (error, warning, and debug logs). We see both of these features as a critical part of the ongoing maintenance of your apps, letting you see at a glance how they’re deployed and functioning, so you can quickly discover which weird and wonderful errors you need to deal with! How sectionctl and Traffic Monitor Work Together for Observability The Section Traffic Monitor and sectionctl work together by, on the one hand giving you a summary view of your traffic, and on the other, a more granular view of your application performance when you need it. The Traffic Monitor provides a visual experience to understand how your workloads are being deployed in different regions. This dovetails with sectionctl logs, providing deeper insights into what’s happening inside your edge workloads. This allows you to view all the output from all the workloads running on the Edge, providing an extra level of observability and debugging. In the sectionctl logs command, we provide flags to let you filter by output. By default, when you specify your app, you are immediately drinking from the firehose, seeing all the edge endpoints your app is running in. Even if you are getting a reasonable number of requests, you will be obliterated by the sheer amount of data. The flags let you specify what you want to see so that you can get specific information about a particular endpoint, application ID, etc., making it easier to narrow in on an instance of a problem. Looking Ahead While we built sectionctl specifically to help users get their Node.js apps running at the edge, it has a huge amount of applicability to other workloads, such as databases – the holy grail of persistent storage at the edge. We think about databases as another type of workload running alongside an app. We’re expecting that people will use sectionctl to get a database running, attach it to an application, and set up global replication of their data at the edge. We’re excited about what sectionctl is going to help people create. If you’re interested in testing out sectionctl, visit the sectionctl documentation to get started."
"29","2020-10-07","2023-03-24","https://www.section.io/blog/traffic-monitor-edge-observability-tooling/","Edge computing is complex. We can attest to this, having built an edge compute platform from the ground up. One of the fundamental challenges that we’ve faced along this journey, is helping people understand exactly what their edge stack is doing and the value that it’s delivering. Dashboards and graphs just don’t tell the full story. Over the past year, we’ve been working on some innovative observability tooling to help users better understand, troubleshoot, and optimize their edge environments. Our goal: Go beyond standard dashboards and graphs to give users an immersive and holistic Edge observability experience. The recent release of the new Traffic Monitor gives DevOps engineers, application owners, and business leaders an easy way to quickly visualize how traffic is flowing through their edge architecture, delivering valuable insights that transcend functional boundaries. (For more on the value that observability delivers, read our article Without Observability, DevOps is Doomed.) Finding inspiration When we started this journey, we took inspiration from some notable industry leaders, including Netflix and Kiali. Back in 2015, Netflix talked about building their innovative Flux tooling, giving them visibility into traffic coming into Netflix from the Internet and being routed to their AWS regions. Taking inspiration from Netflix Flux We were also intrigued by what Kiali was doing with their observability tooling around Istio service mesh, providing a visualization that answers the questions: What microservices are part of my Istio service mesh and how are they connected? Taking inspiration from Kiali Enlisting the design experts In order to create a newly imagined edge visualization, we went searching for an experienced design team to help us translate our vision to product. Our search led us to MetaLab, whom you may know for their work designing Slack. Through extensive research, cross-functional collaboration, creative discovery, user testing, and iteration, we arrived at designs to guide our product development. Transforming pixels to product Now the hard part - turning static designs into a dynamic, data-driven visualization! Fortunately, the Section engineering team responsible for the buildout was involved through the entire design process. This allowed us to design to what the underlying technology would support, while also providing intimate familiarity with design intentions and decisions. With Prometheus-based metrics driving the visualization, the Section Traffic Monitor introduces a real-time display of traffic activity on your web applications to more effectively: Understand how global traffic is flowing through your Section environment; Evaluate edge module throughput; Detect errors, including origin-fetch and module-related errors; and Share valuable metrics with key stakeholders. Making hard decisions As with any significant product innovation, we had to make some hard decisions along the way. Here are some of the notable challenges that we faced. Display of Edge regions One of the primary benefits of edge computing is the expansive global edge network that allows application logic to be processed closer to end users. However, showing hundreds/thousands of edge locations on a map would very quickly consume the display and get overwhelming. So, for the first iteration of the Traffic Monitor, we opted to consolidate edge points of presence (PoPs) into regions. As we continue to gather user feedback, we’re working to better understand how to deliver the most valuable insights around Edge locations and will iterate based on these findings. Simplification without over-simplification As we mentioned at the start, edge computing is complex, and there’s no shortage of data available to display. One of the hardest challenges through this process has been prioritizing the importance of information for the majority of users. Where possible, we’ve attempted to surface the most important data, while still providing access for users to dig into deeper insights. Looking ahead, we’re exploring the idea of customizable views based on user-defined metrics. Requests vs. responses There was a lot of back-and-forth over the direction of the traffic dots, or more precisely, whether to represent traffic as requests, responses, or both in the animation flow. We explored all of the above, and ultimately landed on responses as the best indicator of system performance and health. Bi-directional traffic was too confusing, and request traffic didn’t really highlight the function that the Edge plays within the application architecture. Flat Earth? We can assure you, we’re not flat-Earthers! … As much as we wanted to depict Earth in its true form, we favored showing a holistic view over a partial view that would require user intervention or automated rotation. Even so, there are trade-offs in this approach as well. For example, if you watch the traffic transition from Oceania to the US West region, it appears that traffic is traveling across the globe, when in fact, it’s the next closest region to serve the traffic. This is another area where we’ll continue to seek innovative solutions. Bringing the Adaptive Edge Engine (AEE) to life Aside from the functional benefits that the Traffic Monitor provides, it also allows users to visualize how Section’s underlying (patent pending) Adaptive Edge Engine (AEE) technology works. The AEE dynamically provisions edge resources and routes traffic based on real-time demands to optimize for performance, security, cost, and other strategic objectives. (See article Creating the Mutual Funds of Edge Computing.) In the display below, you can see Edge regions spinning up and down across a 24-hour cycle as traffic patterns ‘follow the sun’. Delivering value There are so many use cases for teams to derive value from this visualization. A few practicle examples of how DevOps engineers are leveraging Section’s Traffic Monitor: Identify origin-fetch errors Detect a DDoS attack Understand cache hit rate Use traffic patterns to guide key business decisions Demonstrate Edge value to business leaders If you’re interested in learning more about the Traffic Monitor or want to walk through how to get more value out of your Edge insights, contact our team of solutions engineers."
"30","2020-10-05","2023-03-24","https://www.section.io/blog/observability-devops/","Speed of innovation is a key differentiator for businesses in a competitive market. Developers working at the bleeding edge of technology are always looking for ways to iterate quickly, yet safely. Observability helps DevOps teams move quickly with greater confidence. As Arijit Mukherji laid out in his opening talk at a recent Splunk event, DevOps and observability go hand-in-hand. “Move fast and break things. If we don’t move fast, the competition will beat us. That’s the reality. 52% of Fortune 500 companies have disappeared since 2000. Without DevOps services, digital transformation is doomed because it won’t be able to keep up. Without observability, DevOps is doomed. Observability provides me with vision. Imagine DevOps as a fast car, but to stop it from going in circles or crashing headlong into a wall, I need to know where I’m going. That’s the lens observability provides.” - Arijit Mukherji, Distinguished Architect, Splunk (formerly CTO at SignalFx) Are There Three Pillars of Observability? Kaslin Fields, Developer Advocate at Google (who illustrates cloud-native concepts as comic books) describes observability as “the board you see at an amusement park that tells you the wait time and whether or not a ride is closed”, i.e. knowing your systems so you know what’s going on. Robust observability tooling can help you iterate more quickly by providing visibility into every stage of the development lifecycle. The three commonly defined pillars of observability are logs, metrics, and tracing. (Thanks to O’Reilly for the definitions.) Logs “an immutable, timestamped record of discrete events that happened over time” Logs let you see who the last person in the system was before you and help surface highly granular local information. Since a log is simply a string or a blob of JSON or typed key-value pairs, any data can be represented in the form of a log line. There are three forms of event logs: plaintext - the most common format of logs; might be free-form text; structured - becoming more popular, typically in JSON format; binary - in the Protobuf format, MySQL binlogs, or the pflog format. Metrics “a numeric representation of data measured over intervals of time” Metrics allow you to track trends and patterns in the behavior of a system over set intervals of time, making them ideally suited to building dashboards that reflect historical trends. Tracing “a trace is a representation of causally related distributed events that encode the end-to-end request flow through a distributed system” Traces are a representation of logs. They offer visibility into the path traveled by a request and its structure. Traces help system admins understand the amount of work performed at each layer and preserve causality by using happens-before semantics. Despite widespread adoption of these definitions, there is a counter argument out there that these three pillars are too narrow and there should be a wider definition of observability tooling. “You may achieve observability with all three, or none [of the pillars] - what matters is what you do with the data, not the data itself.” Ben Sigelman, CEO & Co-Founder, LightStep His mindset is that these pillars are most appropriate for enterprises of “planet-scale”, such as Google, Facebook or Twitter, and engineers at smaller organizations need to find “a new scorecard for observability.” Charity Majors, CEO of Honeycomb, instead of listing preset tools defines a set of technical specifications that observability tooling should have. The debate continues. The Growing Popularity of Observability “In 2020, observability is making the transition from being a niche concern to becoming a new frontier for user experience, systems and service management in web companies and enterprises alike.” – James Governor, RedMonk There are various key drivers of the rise in observability interest and adoption over the last few years. These include a rise in uncertainty in systems and apps and an increase in complexity tied to the rise of microservices and containerization. Visibility into these complex ecosystems is essential. Feature management and experimentation require the ability to query the system from end to end at any given time in order to understand performance levels, detect bugs and execute fixes. In addition, as mentioned, there is an increased demand for regular changes and updates. Indeed, change is the goal in today’s development environment and observability provides the visibility necessary to give developers confidence to move quickly. There is often a debate over whether observability is only necessary when companies reach a certain scale. However, in Charity Majors’ words, “Developing software with observability is better at ANY scale. It’s better for monoliths, it’s better for tiny one-person teams, it’s better for pre-production services, it’s better for literally everyone always. The sooner and earlier you adopt it, the more compounding value you will reap over time, and the more of your engineers’ time will be devoted to forward progress and creating value.” Charity Majors, CEO, Honeycomb The Role of Observability at Every Stage of the Development Lifecycle There are several critical initiatives where observability should play a particularly central role. These include: Hybrid/multi-cloud monitoring Cloud migration Application monitoring Incident response However, it’s clear that observability plays a critical role in shining a light on software systems at every stage of the development lifecycle. It provides an essential feedback loop for Engineering, Ops, SREs and DevOps teams to improve end user experience. Ultimately, observability is about understanding performance from the end user point of view. How well does your request execute from end to end? It’s essential to understand the entire service delivery chain. Observability provides that lens from the perspective of the user. Finally, observability is a mindset and set of standards as much as it is a technology. The tools you adopt and how you use it will, of course, depend on your specific needs."
"31","2020-12-21","2023-03-24","https://www.section.io/blog/nodejs-edge-hosting-release/","We are excited to announce Section’s Node.js Edge Hosting solution, which offers new opportunities for application developers to deliver more performant applications at the Edge. Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Developers can now build server-side applications with the performance of a CDN. Why Node.js? According to multiple industry surveys, JavaScript is the most used programming language, and Node.js tops out as a key technology that developers are using today. JetBrains - The State of Developer Ecosystem 2020 StackOverflow - 2020 Developer Survey Node.js is optimally suited to build scalable network applications, as it allows many connections to be handled concurrently. The callback is fired after each connection is issued, and will sleep if there is no work to be performed. Section’s Node.js edge module has a straightforward workflow that makes deploying changes easy, alongside a dynamic visualization so you can continually assess how your application is performing. Some common use cases for Section users include: Server Side Rendering at the Edge (improve SEO, reduce load on origin servers, deliver faster load times) Single Page Apps (offload the complexities of hosting your SPA and its assets) Micro APIs (build small, targeted APIs for specific use cases and enable faster responses on user queries) Static Site Hosting (serve your web application from Section’s Edge) Why Run Node.js Workloads at the Edge? There is often a disconnect between writing and running code for developers, especially when it comes to optimizing the distribution of that code. Few developers have experience building distributed systems. Edge compute platforms, like Section, enable developers to migrate more services out of centralized cloud infrastructure to leverage a wealth of benefits (performance, scalability, security, flexibility) along the edge continuum. By running Node.js workloads as locally as possible at the edge and eliminating unnecessary data exchange between the cloud data center and the end user, latency is lowered and hosting costs are reduced. Edge computing is particularly beneficial for applications and microservices that are latency sensitive and/or latency critical. Edge Hosting Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Learn more and get started on a free plan Backed by Section’s Adaptive Edge Engine Section’s Node.js Edge Hosting solution is backed by the patent pending Adaptive Edge Engine (AEE), which intelligently and continuously tunes and reconfigures your edge delivery network to guarantee the optimal compute for your application. In recent tests comparing the performance benefits of running a Node.js application on the Section Edge vs Cloud, the AEE performed up to 7x faster than the cloud. Supported by a Developer-Friendly CLI Experience Getting started with Node.js Edge Hosting is fast and easy with the Section CLI (command line interface). Once you’ve downloaded sectionctl, a series of simple commands allows you to create and deploy a Node.js app on Section’s Edge in minutes. sectionctl deploy --account-id 1335 --app-id 7171
 | APP INSTANCE NAME | APP STATUS |            APP PAYLOAD ID            |
|-------------------|------------|--------------------------------------|
| nodejs-flwp6-3b4  | Running    | 69be5c29-9f02-41dc-bed0-27cff1cbbbaf |
| nodejs-7xzhf-0d3  | Running    | 69be5c29-9f02-41dc-bed0-27cff1cbbbaf |
 Section Case Study: Adore Beauty Adore Beauty is Australia’s longest running and most successful online beauty store, recently leading the ASX’s largest IPO of the year. The team at Adore Beauty place a strong priority on continuous digital innovation in order to deliver an exceptional customer experience and personalized shopping experiences. They run a customized e-commerce back-end with heavy integration with Section to leverage optimum caching performance with the goal of improving performance and reliability, particularly during the peak holiday sales season. The team at Adore Beauty is leveraging the Section Node.js edge module to serve their Nuxt app closer to end users. The front-end framework (the Nuxt app) calls a Laravel API, which houses product, category and pricing information. The origin (BigCommerce) handles all other requests, such as shopping cart functions. Using the Node.js edge module in this way has led to lower latency and significant cost savings due to no longer sending all requests back to the central infrastructure. Read the full case study here. Contact us to learn more about how you can benefit from Node.js Application Hosting at the Edge, or get started now."
"32","2020-10-26","2023-03-24","https://www.section.io/blog/aee-outperforms-cloud/","TL;DR In tests, Section proved to be 7.04x faster than cloud at the 95th percentile, while maintaining the same spend level. Today, we are excited to announce the results of a recent series of tests to determine the performance benefits of Section’s Adaptive Edge Engine (AEE). The AEE (patent pending) is a dynamic, context-aware workload scheduling and traffic routing technology. Our ML forecasting engine dynamically provisions edge resources and routes traffic in response to real-time demands. This allows developers to take advantage of bespoke, dynamic edge networks, optimized across a range of attributes, including performance, cost, security/regulatory requirements and carbon-neutral underlying infrastructure. For more background on the strategic thinking behind the AEE, check out this article - Creating the Mutual Funds of Edge Computing. AEE vs. the Cloud We recently ran a set of experiments comparing an application running in Section controlled by the AEE against the same application running in a major cloud provider. We have not sought at this stage to define the scalability or size of the system’s capabilities (this will come later!), but rather to demonstrate a set of basic actions and outcomes under controlled conditions. Test Design Our test design involved the following parameters for the test agents: We built three geographically distributed servers to generate HTTPS traffic, situated in: US East, Asia East, Europe West. Each test agent executed the same traffic pattern, starting at 9am local time for each region. The load pattern represents a standard 24 hour period of traffic: busy in the day and quiet at night. We developed the tests in JMeter. For the test targets, we created a Node.js application to respond to the JMeter agents with an HTTP 200 OK status code. The application was deployed into Section and the major cloud provider without any changes to get it working across both platforms. The Node.js application deployed in Section was configured to use one of the AEE’s pluggable optimization strategies. In this case, we used a Minimum Request Location Optimizer Strategy. This strategy involved deployment of the application to Points of Presence when the geographic source of traffic closest to a PoP was above 25 requests per second. In the major cloud provider, the application was deployed statically, as in single datacenter hosting. Test Results The test patterns created approximately 5.1 million responses per case. Across these observations, Section Edge case latency had a mean of 147 milliseconds (ms) and median of 81 ms. The latency for the Cloud case had a mean of 700 ms and median of 184 ms. Distributions of HTTP latency are right-skewed, as indicated by mean values that exceed their medians. To meaningfully characterize the spread of the right tail of the distribution, we computed the semideviation. The semideviation is an analogue of the standard deviation that only considers the values above (or below) the mean. The “upper” semideviation used here summarizes the risk of slower-than-average performance. The semideviation of Section Edge latency was 238 ms and that of Cloud latency was 4,873 ms. At the extremes, the 95th and 99th percentiles were 458 ms and 474 ms for Section Edge and 3,225 ms and 7,491 ms for Cloud. A statistic that characterizes risk at the extreme tail of a distribution is the conditional value at risk (CVAR). The CVAR is the average of values that exceed a specific percentile of the distribution; the average outcome in the tail. The CVAR(95), the mean latency above the 95th percentile for Section Edge is 517 ms and for Cloud it was 6,883 ms. This can be interpreted as: end-users experiencing latency in the upper 5% of the distribution, will see average latency of 517 ms and 6,883 ms for the Section Edge and Cloud cases, respectively. The CVAR(99) values are 699 ms for Section Edge and 13,940 ms for Cloud. Analysis of the Test Results The results indicate that making requests of an app deployed on Section Edge is consistently faster, with markedly less extreme risk than making requests of the exact same application with a Cloud deployment. The Section Edge latency distribution has lesser mean and median than the Cloud distribution and the semideviation (measure of spread above the mean) is an order of magnitude less for the Section Edge case than the Cloud case. The 95th and 99th percentiles of the Cloud distribution are 7 or more times greater than for Section Edge. The mean latency at the extremes of the distributions (e.g. CVAR) are an order of magnitude less for the Section Edge case compared to the Cloud case. The CVAR, as an arithmetic average, is sensitive to the spread of values in the upper tail region under consideration. The very high value of CVAR(99) for the Cloud case shows us that the upper 1% of latency values are of very large magnitude. The CVAR(99) for the Cloud case is nearly 20 times the mean of that distribution (recall that the only observations that were analyzed were those with response codes of 200). On the other hand, the CVAR values of the Section Edge latency distribution are less than 3 times the magnitude of the mean. In fact, the average latency for the slowest 1% of Section Edge users (i.e. CVAR(99)) is equal to the average latency of all users in the Cloud case. The value proposition of edge compute Successful use of edge compute will necessitate the use of automated systems to dynamically migrate workloads into, out of, and across a rapidly growing pool of edge Points of Presence (PoPs). Despite its performance benefits, edge computing will simply be too expensive for many workloads without this kind of automated workload orchestration. Workload Orchestration An intelligent system which dynamically determines the optimal location, time and priority for application workloads to be processed on the range of compute, data storage and network resources from the centralized and regional data centers to the resources available at both the infrastructure edge and device edge. Workloads may be tagged with specific performance and cost requirements, which determines where they are to be operated as resources that meet them are available for use. - Open Glossary of Edge Computing v2 Sustainability matters The benefits of Section’s Adaptive Engine are not limited to latency improvements and cost efficiencies, but also enable us to contribute to wider tech sector efforts to build a more sustainable future. With projections of 6 billion Internet users by 2022 (75% of the world’s population), efforts to reduce tech-related energy consumption are crucial. Section’s AEE tunes the requested compute resources at the edge continuously in response to performance, traffic served and the application’s required compute attributes. We use the minimal amount of compute necessary to provide maximum availability and performance. For a deeper dive on how Section’s AEE drives more sustainable computing models, check out this recent talk from Dr. Kurt Rinehart, Section’s Director of Information Engineering: Comparing Cloud vs Edge efficiencies using context-aware, location-optimized workload scheduling. Keep an eye out for our next post in this series, digging into the inner workings of the AEE."
"33","2020-07-10","2023-03-24","https://www.section.io/blog/data-science-edge-computing/","At Section, we are working to leverage data science and machine learning methods to create the mutual funds of edge computing. In finance, Modern Portfolio Theory revolutionized investment practices by providing an analytical framework to balance the risk in a portfolio with the expected reward. No longer does an investor need to study and select individual stocks. Rather, they specify their appetite for risk versus reward, and then analytically identify optimally efficient portfolios to suit. The future of the edge will see this same improvement-through-simplification by supporting optimal trade-offs between cost and performance for each business case. This will deliver enormous benefits for all users of the Internet as we will have the best performing and most secure web applications delivered to us by a more efficient Internet. Cost-performance optimization in edge computing While operators and infrastructure providers race to build and expand their edge networks, and software providers build out tooling to give developers access, there’s a big, hairy challenge that not many are talking about – how are engineers expected to efficiently manage workload orchestration across thousands of points of presence? The value proposition of edge compute is an optimization of performance versus cost, access, and other limitations. Edge Points-of-Presence (PoPs) will have higher performance and higher cost, so unless cost is not a concern, it won’t be desirable to move all workloads to the edge. Successful use of edge compute will require automated systems to dynamically migrate users to workloads that move into, out of, and across a rapidly expanding pool of edge PoPs. The data science behind edge workload orchestration So, how do we go about applying similar mathematical frameworks that have been adopted in the modern investment landscape to simplify edge workload orchestration? The cost/performance frontier According to Investopedia, “the efficient frontier is the set of optimal portfolios that offer the highest expected return for a defined level of risk, or the lowest risk for a given level of expected return. Portfolios that lie below the efficient frontier are sub-optimal because they do not provide enough return for the level of risk. Portfolios that cluster to the right of the efficient frontier are suboptimal because they have a higher level of risk for the defined rate of return.” Using the efficient frontier as a guide, we are working to apply this same approach to edge workload scheduling using cost and performance as our constraints. What this means in practice is that developers can define their level of accepted cost for a given level of expected performance. Aside from certain security-related scenarios (like regulation or geo-fencing), developers shouldn’t have to think about where their compute workloads are running, as long as cost and performance parameters are being met. Furthermore, most developers don’t want to deal with the complexities of edge workload orchestration. Section’s Adaptive Edge Engine (patent pending) provides a solution that abstracts the underpinning monitoring, decisions, and execution to provide developers with a trusted, turnkey solution to optimize application performance. Constructing the cost/performance frontier The first requirement of an efficient portfolio is a suitably diverse pool of options providing scope for optimization. In this case, the pool of all PoPs is growing each year and will continue to grow at an increasing rate. With the wider distribution and availability of edge PoPs provides high cost, high performance options that did not previously exist. Given these options, we need reliable estimates of the cost and performance for each. It is perfectly acceptable if there is uncertainty about these estimates, because that uncertainty can be handled in the optimization and in the ongoing learning process behind the scenes. Given the options and relevant data measures above, we can easily trace the efficient frontier. However, before we can solve for an optimum, we must know more about the preferences and requirements of the customer. We manage this through elicitation identical to when your 401k manager asks you to categorize yourself as a Risk-Averse, Moderate, or Aggressive investor. Investors in each category will sit in a different location on the efficient frontier and we similarly allow customers to articulate how they uniquely balance cost and performance for the case at hand. With these ingredients, we can compose an optimal portfolio of PoPs and deploy the customer workload accordingly. Once initiated, the process itself is ongoing. Portfolios need re-balancing as the underlying dynamics change. In our case, and this is where the investing portfolio breaks down, the major driver of changes in the portfolio is changing web traffic volumes and (originating) locations. The dynamic nature of incoming request traffic, along with other factors, require us to continually solve for the best portfolio, re-deploying as necessary to maintain optimal results. The final piece is harnessing the realized results in a feedback loop that we use to tune the models feeding and driving the optimization at the heart of this process. At Section, we can leverage this knowledge across time and across all customers and PoPs to build a comprehensive view of the cost vs. performance landscape. Vendor-agnostic networks & open source technologies One of the keys to realizing the full potential of edge computing is interoperability. A truly expansive global edge network will not be built by or rely on a single provider. Furthermore, operators need to provide easy and open access for software developers to flexibly deploy/remove edge workloads on/from their infrastructure. Many organizations are forming to help push progress on this front. Similar to what CNCF has done for the cloud native ecosystem, LF Edge (also under the umbrella of the Linux Foundation) is helping drive open source technologies, education and adoption around and within the emerging edge ecosystem. TL;DR At Section, our vision is simple - Improve the Internet. By applying a proven model and creating the mutual funds of edge computing, we will do just that."
"34","2020-12-18","2023-03-24","https://www.section.io/blog/7-edge-computing-predictions-2021/","Edge computing was one of the only accelerated technologies in 2020. As people worldwide had to lean more heavily than ever on digital solutions, Internet infrastructure was called into question for capacity limits. Many users saw broadband speeds drop by as much as half. Netflix, Amazon Prime and YouTube were asked to reduce the quality of their streams to improve network speeds. The attractiveness of a decentralized form of compute has become increasingly obvious. As we leave 2020 behind and look ahead to 2021, edge computing will continue to benefit from increased investment and innovation. Here are Section’s predictions on edge computing for the year ahead. 1. Edge will go mainstream in 2021 Two of the largest research agencies, Forrester and Gartner, are each predicting that 2021 is the year that edge computing will enter the mainstream. “Public cloud entities won’t disappear, but they won’t dominate the future of distributed computing. Their culture is based on massive data centers and tight control of the architecture, the exact opposite of what is needed to serve customers locally. Vendors with a winning edge strategy will do better.” Forrester Predictions 2021 “Edge computing is entering the mainstream as organizations look to extend cloud to on-premises and to take advantage of IoT and transformational digital business applications. I&O leaders must incorporate edge computing into their cloud computing plans as a foundation for new application types over the long term.” Gartner 2021 Strategic Roadmap for Edge Computing The use cases for edge computing are vast, from multi-player gaming to cutting-edge healthcare to customized finance solutions. A diverse array of industries is driving the push towards edge computing, looking to benefit from increased scalability, greater reliability, enhanced regulatory compliance options and better performance. The global rise in data use, the continued push towards innovative technologies like VR and AR, the growth of 5G networks, and a long-term remote workforce will all drive an increased demand for edge computing. For edge to go mainstream, persistent data at the edge must be conquered. Developments on a number of fronts have shown we can move persistent data to the edge to let dynamic applications execute at the edge with the data right there beside them. This means edge will be much more than just a layer for caching, streaming, object manipulation or other ephemeral applications. 2. Continued migration to cloud native software development will set more organizations up for edge enablement Those companies who embrace cloud native will position themselves for the next step in the digital transformation journey and make edge computing business and operational models more viable. The developer-led movement towards cloud native is characterized by an increasing number of developers adopting cloud native principles when constructing applications. Benefits include easier application lifecycle management, faster deployment, greater flexibility and scalability, and from a business point of view, cost savings. While many SaaS providers offer cloud-based deployment options today, a significant majority have not yet moved towards edge enablement, which will allow them to leverage even greater benefits and act as a launchpad for digital innovation. Cloud native technologies such as Kubernetes allow organizations to maximize efficiency. Section has experienced the benefits of Kubernetes at the edge first-hand, for instance, leveraging flexible tooling, running edge workloads anywhere along the edge continuum, and the ability to be infrastructure-agnostic. “The technical evolution of edge will follow the evolution of application development, delivery and operations, relying on repeatable and reusable microservices at scale.” Gartner 3. Automation will become the standard for edge scalability Automation can be programmed in response to an application’s required compute attributes, traffic served, and performance. This is driving innovation toward more performant and efficient edge computing.. Paths towards this include: Infrastructure as Code (IaC), which enables DevOps teams to establish more repeatable processes through automation. Facebook’s Autoscale, its energy-sensitive load balancer that reduces the number of servers that need to be on during low traffic periods. Section’s own Adaptive Edge Engine, which continuously tunes and reconfigures our customer’s edge networks to ensure we are running the optimal compute for their workloads. Additionally, automation has the potential to play a significant role in the needs of the tech industry to reduce overall energy consumption by offering a more sustainable energy model. 4. 5G is poised to support IoT at scale As 5G becomes steadily more available, it will offer reduced latency to compute at the network edge, particularly when combined with mobile edge computing (MEC). 5G will also offer a way to connect edge devices, especially in the consumer realm. An area currently being much discussed in the press is the fact that major telecom networks will provide 5G with most Americans likely to have access to 5G connectivity some time in 2021, yet this won’t add up to continuous national coverage. Private 5G networks will step in to fill the gap, being deployed and enabled by manufacturers like Huawei, Nokia and Ericsson. There will also be implementations of private 5G in settings that require low-latency, high-speed, high-density wireless connectivity, particularly industrial settings, such as logistics centers, manufacturing plants, and ports. Use cases for 5G in these settings include AR/VR for remote inspection, surveillance and remote monitoring. Enterprises that want to keep their data wholly onsite will need, in addition to a private network, the ability to process data locally, by, for instance, equipping its devices with edge AI chips to enable them to perform those computations online. 5. AI learning will move to the edge This shift will be possible due to new inference chips e.g. from Intel, Hailo and Nvidia, in addition to new machine learning (ML) techniques, such as reinforcement and federated learning. Edge AI chips allow ML tasks to happen on-device instead of in a remote data center. Moving processing and storage closer to users and “things” that are generating the data not only lowers latency, but also addresses concerns around bandwidth, autonomy and data privacy. Edge AI chips will likely appear in an increasing number of consumer devices, including high-end smartphones, tablets, smart speakers, and wearables. Edge-based ML will also enable a wider set of uses, including a wide range of industrial applications, from optimization of production to predictive maintenance. 6. Front-end development at the edge will come into its own Static site generators (SSGs) have become increasingly popular in recent years due to their simplicity, security benefits, and their ability to serve content quickly. Developers are turning away from content management systems (CMSs), such as WordPress and Drupal, because of the challenges that slow them down. SSGs apply data and content to templates, process them, and output a folder with the resultant pages and assets. This shifts the work away from “request time” (users ask for the view of a page) to “build time” (unrelated to when users ask for the view). This architecture breaks the relationship between the amount of visits to a site and the overhead of generating the views to service all of those visits. JavaScript developers are turning more to SSGs, in particular Jamstack, because of its use of modern technology and JS frameworks, offering a strong developer experience. Jamstack enables lower latency partly because of its ability to serve pre-built files over an edge compute platform. By serving Jamstack projects directly from the edge, much faster speeds and superior performance can be achieved. 7. We will see accelerated data center growth at the edge The recent Omdia Edge Report forecasts that the number of servers deployed at the edge will grow from around 2.5 million in 2020 to 3 million in 2021. By 2024, 5 million servers (26% of shipped servers) will be deployed at the edge. Price Waterhouse Cooper, meanwhile, predicts that the market for edge data centers is expected to almost triple by 2024. The global edge market for edge data centers will surge to $13.5 billion in 2024 compared to $4 billion in 2017. They note five factors driving the growth in edge data centers, including the arrival of 5G, IoT proliferation, a widening data gap, the adoption of SDN and NFV tech, and video streaming and AR/VR. Telcos and enterprises are driving most of the growth at the edge (compared to hyperscalers driving overall data center capacity growth). Edge data centers offer the potential to help reduce latency, overcome intermittent connectivity, and store and process data closer to the end user."
"35","2020-12-07","2023-03-24","https://www.section.io/blog/challenges-of-distributed-databases-at-the-edge/","Global Internet traffic in 2021 will be equivalent to 135x the volume of the entire Global Internet in 2005, according to Cisco. Globally, Internet traffic will reach 30 gigabytes per capita in 2021, up from 10 gigabytes per capita in 2016. Drivers of the huge increase in data volume include networked smart devices, emerging technologies – like IoT, 5G and AI – seeing rapid uptake, and manufacturing IIoT. Remote working has also contributed to the trend toward distributed data across 2020 and this looks set to largely continue through 2021. In parallel to this, end users have ever growing expectations for reliable connectivity, superior performance, and fast speed of service. Demand is growing for edge computing, which offers many advantages in meeting these needs. By bringing data processing and storage as close as possible to the end user, edge computing offers benefits in speed, reliability and scalability, not to mention efficiency savings. Edge computing is a fast-growing market, with Statista forecasting global revenue to reach $9 billion by 2024. Meanwhile, Gartner predicts that by 2025, three-quarters of enterprise-generated data will be created and processed at the edge (compared to just 10% in 2018). Before edge computing can truly deliver on its promise, however, the challenge of distributed databases at the edge needs to be solved. To date, edge computing workloads have been mostly stateless, but changing edge workloads are driving the need for persistent data at the edge. Using cloud and on-premise databases is not the ideal solution. We need to figure out the most efficient way to process the tsunami of data at the edge. Why Conventional Distributed Databases Don’t Work at the Edge Conventional distributed databases depend on the centralized coordination of stateful data, scaling out within a centralized datacenter. They rely on a specific set of design assumptions, including: A reliable data center local network (low latency, high availability, few network splits) Precise timekeeping using physical clocks and network time protocol (NTP) to ensure coordination Decent consensus mechanisms due to low latencies and high availability An alternative approach with geo-distributed databases (a single database spread across two or more geographically distinct locations) has a very different set of design assumptions: Unreliable wide area networks Lossy timekeeping Dynamic network patterns with sporadic partitions Consensus is expensive and slow with geo-distributed users at scale. The mechanisms of coordination restrict how many participants or actors can take part in a network of coordinating nodes. For stateful edge computing and geo-distributed databases to operate at scale and handle real world workloads, edge locations need to find a way to work together in a way that is coordination-free, which allows edge devices to move forward independently when network partitions do occur. Distributed systems need to be designed to work on the Internet within an unpredictable network landscape, use a form of time-keeping that isn’t lossy, and as stated, not be dependent on centralized forms of consensus. What are the Challenges of Persistent Databases at the Edge? 1. The Distributed Nature of Edge Computing Systems Edge computing systems are highly distributed by design. However, distributed systems require the designer to make decisions about sources of truth, synchronization, and replication. This can result in systems where users can access data and applications independently. Each edge device needs to work on its own to perform its function, however these devices also need to share - and synchronize data with other edge devices and nodes. Coordinating several edge devices while simultaneously enabling them to work independently has proved to be a continued challenge for designers of distributed systems. 2. Stateless Data at the Edge Edge computing is straightforward when the data is stateless or when state is local (when a device maintains its own state or is trivially partitionable). What do we mean when we talk about stateless data For one, there is no stored knowledge of, or reference to, past transactions in stateless data; examples include HTTP, IP and DNS. Stateless transactions consist of a single request and a single response, and typically use CDN, web, or print servers to process the short-term nature of requests. Up until recently, most edge computing use cases have been stateless. Why is Stateless Straightforward? Stateless works well for web applications that present static media and query database tables to inform applications. Stateless, database-centric applications also work well for performing batch analytics on historical data. In stateless application design, application services don’t have to remember what they’ve done in the past. The database records an application’s state, and any time the application needs to do something, it will ask the database for information. This works well for data that is stored in one location, such as centralized cloud data centers. The Real World isn’t Stateless In reality, the majority of applications are stateful; they depend on data from previous request/response transactions in order to inform subsequent requests. Consider, for example, a banking application that keeps a ledger of expenses and deposits. In order to maintain the current balance, it must draw on insights from previous transactions, or state. In distributed computing architectures, each edge node has its own local context that should inform the data it generates. This context ideally needs to be maintained and made locally available to applications, so that the latency savings that edge computing promises can be delivered on. When application context is available at the edge, it makes it easier to identify relevant insights from the full dataset and discard only the unnecessary information. Stateless Design Causes Latency Another significant challenge for stateless data at the edge is the centralized coordination necessary, which counters gains made on latency. If a request needs to travel across the network to a centrally stored database, latency is added with each trip (sending a data packet over a local network, compared with computing locally at the edge, comprises a difference in magnitude of 107). Network latency, even with advances like 5G, will always be subject to the speed of light, thus constrained by what’s physically possible. Approaches to Building Stateless Edge Applications Several approaches to building stateless edge applications have emerged, each with their pros and cons. These include: Data filtering at the edge combined with data analysis in the cloud Pro: This reduces the amount of data transmitted over networks. Con: Due to the speed of light, waiting for the cloud will always be slower than processing data at the edge. The use of IoT gateways or edge data centers Pro: This approach moves the database closer in proximity to edge deployments, reducing the distance that network packets have to travel to reach a database and collect the required information. Cons: Delays – There will inevitably be delays in the network, which will cause increases in latency by orders of magnitude. This kind of delay for latency-critical applications (autonomous vehicles, IIoT, AR/VR, gaming) is unacceptable. Consistency – Because the data is spread over more locations, the likelihood of network partitions increases, and thus the likelihood of data inconsistencies being introduced increases. 3. Why We Need Stateful Data at the Edge Increasing numbers of use cases for edge computing are demanding the processing of stateful data, which is more complex and challenging than stateless. What is stateful data? Stateful data comes with information about the history of previous events and interactions with other devices, programs, and users. Stateful applications use the same servers every time they process a user request. “Without stateful data, the edge will be doomed to forever being nothing more than a place to execute stateless code that routes requests, redirects traffic or performs simple local calculations via serverless functions… these edge applications would be incapable of remembering anything of significance, forced, instead to constantly look up state somewhere else.” - Chetan Venkatesh and Durga Gokina, founders of Macrometa Corporation Stateful Use Cases at the Edge Stateful is useful for applications that require more context about users or end-user devices, to deliver more personalized experiences. These include: Mobile apps IoT devices Multi-player gaming platforms Virtual or augmented reality IIoT and manufacturing systems e.g. retail inventory management, and logistics, fleet tracking and management Stateful, real-time edge computing will enable latency-critical applications by providing the means for processing and distributing streaming data across complex systems without compromising on speed. The Challenges of Stateful Data and Edge Computing There are challenges involved with performing stateful computing at the edge.These include - for the above edge use cases, the ability to sync stateful data with guaranteed consistency. This is essential, for instance, to avoid lag in real-time gaming or prevent freezes in real-time streaming video calls. Without reliable consistency, different applications, devices and users will see different versions of data, leading to unreliable applications, data corruption and data loss. How do you manage and coordinate state across a range of edge locations or edge nodes and sync data with guaranteed consistency? Do Edge-Native Databases Provide the Solution? One solution being worked on are edge-native databases, which are geo-distributed, multi-master data platforms capable of supporting multiple edge locations without the need for coordination. While they don’t require centralized forms of consensus, they can still guarantee consistency and arrive at a shared version of truth in real-time. These databases promise to overcome the data processing limitations experienced till now at the edge. An additional benefit is that they won’t require developers to have a specialist knowledge of how to design, architect or construct these databases. Conclusion According to Morgan Stanley, manufacturers, since 2010, have “collected 2,000 petabytes of potentially valuable data, but discarded 99% of it.” The 1% that is being processed makes use of Big Data. The challenge is how to filter through the remaining 99% to most efficiently extract value. When application context is available at the edge, it is far easier to identify relevant insights from the full dataset and discard only the unnecessary information. True transformation will be possible when algorithms can have access to massive datasets and innovations in ML can be applied to huge volumes of streaming data in real-time without a dependency on historical datasets. When this is solved and distributed architectures can handle persistent data at the edge, it promises a huge breakthrough for the applications of tomorrow and the edge use cases they will enable."
"36","2018-07-16","2023-03-24","https://www.section.io/blog/what-is-the-application-edge/","Web applications are increasingly complex by nature and rapidly evolving in a landscape of increased consumer demand for higher quality experience, faster interaction and a low tolerance for security breaches of personal data. While cloud computing and the emergence of the large cloud hosts has caused a centralization of compute resource, the next wave of compute innovation is happening at “the edge”. Edge Compute Many applications stand to benefit from the scalability, reliability, performance and regulatory compliance options that computing at the edge can offer. The global surge in data use, the continued rise of innovative technologies such as VR and AR that require high bandwidth and low latency, the explosion of the Internet of Things (IoT) and the growth of 5G networks are all fueling the need for, and enabling the rise of edge computing. Change at the infrastructure level is essential to support these new forces; not to mention the increasing adoption of distributed computing architecture and an increasingly diverse and specialized hardware ecosystem. Unsurprisingly, a robust community is growing around edge computing and new resources are becoming available as such, including the recently released State of the Edge Report and Open Glossary, which are a great foray into creating a common language and understanding of the edge. Matt Trifiro (Vapor IO) and Jacob Smith (Packet) are the Co-Chairs behind the two ventures. They worked with colleagues at Ericsson (UDN), Arm and Rafay to create the inaugural Report to “cut through the noise, bring some order to the discussion, and build a community that cares deeply about edge computing and the innovations that will be required to bring its promise to fruition”. The 93-page Report covers a lot of ground both in terms of what edge computing is in its theoretical sense and the pragmatic details of what a real-world edge computing architecture is comprised of. Key findings include: Cloud computing as it currently exists cannot alone support the demands placed on it by emerging applications and business needs; Edge computing and cloud computing are not mutually exclusive. There are current and future opportunities for hybrid architectures that can simultaneously make use of both infrastructures efficiently; Edge computing resources can be situated on either the operator side or the user side of the last mile network. Operator-side resources are referred to as belonging to the “infrastructure edge”, whereas operators on the user side are known as “the device edge”; Device edge resources are often limited by power and connectivity issues; the infrastructure edge has more potential for scalable resources to parallel a centralized cloud data center (albeit at a reduced scale); The mainstream adoption of cloud native technologies in recent years is a significant enabler for a healthy edge computing ecosystem. One of the most interesting sections of the Report is that on ‘Pushing Applications to the Edge’. The potential for “new and previously impractical classes of applications” is rich - from the possibility of unthought of applications that process huge amounts of data to new use cases like autonomous vehicles. There is also a discussion of future developers building edge native applications, as well as existing applications being rewritten to be edge enhanced to optimize their performance and reduce their impact on the network. Some of the applications discussed include large-scale Industrial IoT (IIoT) systems, IoT, autonomous cars, AR/VR, smart cities, video games, AI, machine learning and edge content delivery. User demand for real time data interactions, the requirement for complex application logic execution in real or near real time to facilitate user experience, in addition to the increasing compute power becoming available at the edge, all adds up to mean that application engineers will increasingly need control and transparency over that Edge Compute Platform. While there is no theoretical edge to the Internet (it being an interconnected mesh of compute), an Edge Compute Platform can be defined as the last place application logic is executed before the application reaches beyond the control of the application provider. Typically, this has historically occurred as the application moved behind a security perimeter that is not managed by the application provider, such as a browser or phone application. Traditionally, the Edge Compute Platform would be an Application Delivery Controller (such as a load balancer) or a Content Delivery Network. However, the IoT wave is changing this view of edge. We are now considering the Telcos and the IoT devices themselves as elements of the edge itself. By 2020, Gartner predicts that there will be 20.4 billion ‘connected things’ in use worldwide - up from 8.4 billion things in 2017. These devices will increasingly be located in rural and remote areas as well as in cities close to large data centers as they tend to be today. There are multiple reasons for why the IoT benefits from edge computing over traditional cloud data centers, ranging from the potential for remote geographies to access their power to the necessity for latency in systems like autonomous vehicles to the challenges of meeting compliance requirements (particularly in the era of GDPR) when data is not processed locally. IoT devices like smart home cameras or connected control systems can benefit from localized data processing as much of the data they create is local in scope. Single IoT devices will typically be fairly low bandwidth, but millions in aggregate have the potential to bring down networks. Sending large volumes of data to a centralized data center instead of processing it at the infrastructure edge with only certain results being extracted to be sent upstream will become increasingly impractical. In its end of 2017 forecast, IDC predicted that by 2019, 15% of manufacturers that manage supply chain processes and data-intensive production will be utilizing cloud-based execution models that rely on edge analytics to enable real-time visibility and improve operational flexibility. Logic being executed at the edge is becoming increasingly complex as we have moved from simple image and static object caching (the problem CDNs initially set out to solve) on to application logic functions responding to real time data ingestion, such as: Full page caching based on cookies set or type of user interaction Image manipulation and optimization based on browser type Request blocking or tracking based on real time behavioural analysis Content composition or rewriting in response to analysis of user interaction and external data feeds More complex data consumption and logic execution activities are undoubtedly on the way. The Edge is Not Ready Innovation at the Edge Compute Platform has been hampered by the “network-first” approach of traditional Content Delivery Networks. The legacy CDNs have limited software flexibility, hard capital expenditure requirements and fail to provide sufficient transparency and/or control over the edge for application development and operations engineers. Increasingly, edge delivery networks are emerging that rely on highly specific connectivity to ISPs at a regional level or between application facilities. In other places, specialized hardware specifically tuned to certain needs of the application is being developed. Edge networks are also operating application specific software and relying on highly customized configurations. Most applications still rely to some extent upon executing code in relation to a dataset that is proximal to the user making the request. This further limits the potential for CDNs to deliver the next-gen content users are increasingly requesting. As distributed infrastructure has become more possible and more a part of the mainstream, the approaches towards managing widely dispersed infrastructure have also evolved. The emergence of DevOps is in part driven by increasingly complicated infrastructure deployments. Configuration management has become a key part of the infrastructure ecosystem offering a powerful toolset for managing global systems. Furthermore, infrastructure as code is a necessity for managing these global edge delivery networks. We believe that web software engineers and operations teams should be empowered to build, control, test and manage their own Edge Compute Platform. As the edge is executing application logic, the level of control and transparency should be the same as it would be if edge compute were part of the core application. How Our Platform Enables You to Innovate at the Edge Compute Platform Cloud computing has helped development and operations teams innovate and change without worrying about the underlying infrastructure. We are enabling developers and operations teams to build and innovate at the Edge Compute Platform by providing a level of flexibility, transparency and control, which to date has been unavailable in any other edge solution. We provide developers and operations engineers with a platform consisting of three things: A flexible library of edge logic software. We provide developers with a range of software from which they can choose the right solution for their application. We continually update and expand the available software in our library so that developers are not caught out with outdated software or prevented from accessing the Edge Compute Platform features they need. Developers and Operations engineers have the control and flexibility to upgrade and update when they are ready. A flexible edge compute fabric. We provide developers and operations engineers with the choice of where they would like their edge to run. Developers and operations engineers can run the edge close to their infrastructure, at specific locations they specify or take advantage of the federation of edge compute infrastructure that we have created. A DevOps centric control layer. Our platform includes a DevOps centric control layer, which has been built API and GIT first. Our control layer adheres to the principles of fast change and fast feedback with audit and transparency. Our control layer provides full integration with modern software development processes to allow development teams to move quickly and confidently and leverage the full capabilities of their Edge Compute Platform. We do not: Build the edge proxy software itself, but rely on industry leading software; both open source projects and proprietary. Build edge compute, but instead rely on the vast array of deployed compute available - from the large cloud providers to the localised or boutique hosting providers and beyond - to compute available within transit providers and at the locations of end users and content creators. Remove or otherwise inhibit users from having full access to and control of the edge compute software. By offering greater levels of transparency and control, we expect our customers to be able to extract far more from their Edge Compute Platform than has ever previously been possible. Section Platform Our mission is to improve the Internet by providing developers and operations engineers with superior access to and control over a more powerful Edge Compute Platform. We believe that every web application can be enhanced by a more effective use of a more powerful Edge Compute Platform. The Section platform makes this possible for modern developers and operations engineers. The Section platform should be a pleasure for developers and operations engineers to use. It should solve the problems they face with respect to transparency and control of their Edge Compute Platform. Developers and operations engineers should feel in control and empowered when they drive the Section platform to achieve superior application outcomes. The Section Platform is: A flexible library of edge logic software A flexible edge compute fabric A DevOps centric control layer We will continue to invest in our platform by: Adding additional software that users are interested in using at their edge Adding the means for providers of software to make their software available on Section’s Edge Compute Platform Adding additional compute capacity and locations to our Compute Fabric Providing more granular compute selection options to users so that they can tailor an edge compute solution for their application Providing a means for users to join compute capacity to the Section compute fabric Consistently improving the user experience as they interact with, drive and manage their Edge Compute Platform Maintaining and optimizing the performance and security of the core platform Simplifying quality control, compliance, and governance."
"37","2018-08-16","2023-03-24","https://www.section.io/blog/why-did-a-varnish-cache-hit-take-so-long/","Users of the platform occasionally ask: “Why is the time to first byte (TTFB) so large even though Varnish Cache indicates it is a cache hit?” In this post, we will take a moment to address this niggling question. The user will first notice this issue when they examine the time to first byte of the asset in question within their browser dev tools and look at a cache debug header. A cache hit typically takes around 5 to 100 milliseconds for the TTFB - the exact time depends on the latency between the user and the cache. Occasionally, however, they will see this process take several seconds; hence the question. The usual culprit is the Varnish Cache behaviour known as Varnish Cache request coalescing or request collapsing. Varnish Cache Request Coalescing If an asset is not in cache and Varnish Cache receives multiple requests for the same URL, it will coalesce these into a single request to the web application. Once the web application responds with the asset, Varnish Cache will examine the response and if determined to be cacheable, will serve all the coalesced requests from the single response. For more information on this, see: Varnish Cache docs Consider this example scenario: Varnish Cache is set up to cache HTML for www.example.com and has an empty cache. It sends HTTP response header “x-cache” with value “hit” or “miss” for debugging. The web application is slow and takes 20 seconds to generate HTML files (lots of DB queries, third party API calls etc). Let’s pretend the latency between user to Varnish Cache, and Varnish Cache to web application, is negligible. Time = 0s: User A requests www.example.com/product.html User A’s request hits Varnish Cache, which performs a lookup in cache and doesn’t find it. Varnish Cache sends the request on to the web application. Web server receives request for product.html and starts to generate the HTML. Time = 5s: User B requests www.example.com/product.html User B’s request hits Varnish Cache, which performs a lookup in cache and does not find it. Varnish Cache sees a request already in progress from user A and coalesces user B’s request to request A. Time = 10s: User C requests wwww.example.com/product.html User C’s request hits Varnish Cache, which performs a lookup in cache and does not find it. Varnish Cache sees a request already in progress from user A and coalesces user C’s request to request A. Now there are three requests: from users A, B and C for the same asset to Varnish Cache, but only one request from Varnish Cache to the origin. Time = 20s: Web application finishes generating /products.html and sends a response to Varnish Cache. Varnish Cache examines /product.html and determines it is cacheable. It serves the asset to user A along with the two other requests in the queue. User A receives the response, and sees x-cache: miss. The time to first byte is 20 seconds. User B receives the response, and sees x-cache: hit. The time to first byte is 15 seconds. User C receives the response, and sees x-cache: hit. The time to first byte is 10 seconds. Time = 25s: User D requests www.example.com/product.html User D’s request hits Varnish Cache, which performs a lookup in cache, finds the asset and immediately serves it to the user - in contrast to the earlier responses. User D receives the response, and sees x-cache: hit. The time to first byte is 5 to 100ms - as expected for a standard cache hit. Why does Varnish Cache do this? Imagine you run an online shop and your origin server can take a few seconds to generate an HTML page. You send out a Tweet telling tens of thousands of users about a sale event and your Tweet links to the same handful of sales pages. Suddenly, users on the site increase from a few dozen to several thousand. If these pages were not already stored in cache, the traffic spike can cause havoc to the web application. Even if the web application had auto scaling, instances always take time to spin up before serving traffic. With Varnish Cache sitting in front of the servers (and when configured correctly to handle HTML caching), instead of thousands of requests hitting the web application at once, now there are only a handful of requests that the server can easily handle. What does this Look Like in the Section Logs? Section records detailed logs about each and every request the platform serves. In the screenshot below, you can see a typical example of Varnish Cache request coalescing behaviour. In the screenshot, Varnish Cache-ncsa-logs show a request was made to Varnish Cache at 17:06:23 for the homepage, labelled A in the screenshot of the logs. This was a cache miss, so a request was then made by Varnish Cache to origin. We log this as the last_proxy-access-log record, in which you can see the time the origin took to respond with the home page as 25,615ms (25 seconds). During this time, another user requested the homepage at 17:06:41, labelled as B in the screenshot. Request B was “coalesced” together with request A. When the origin finally answered request A, Varnish Cache used the same response for request B which was a cache hit, however, this took a long time because the origin was slow to respond to request A. Edge-access-logs then indicate that response A was completely served to the user at 17:06:49 and response B was served at 17:06:53. The difference in serving time is an indicator of network congestion between Section and the end user B, normally caused by poor bandwidth on user B’s Internet connection. Section specialises in getting the best performance out of Varnish Cache. Learn more about how Section can help you improve Varnish Cache performance and gain valuable insights into real world behaviour."
"38","2022-08-18","2023-03-24","https://www.section.io/blog/making-hard-things-easy-is-hard/","Making complicated things easy has long been a prime objective of technological innovation. Let’s imagine, for example, you’ve launched a new application that finally addresses a perennial business problem. User adoption is skyrocketing. You’ve made it, or you’re at least well on your way. But as user growth increases and expands, have you addressed the location challenge? The most common response to this hypothetical question likely speaks to the fact that you’re relying on one of the hyperscalers to host the app with the goal of ensuring high availability and, presumably, high performance. But have you considered where you really want to deploy that app? More specifically, have you thought about where that app should be running at any given moment in time? The answer is almost always some variation of, “Well, I want the best performance and I want it at the lowest cost.” However, these two expectations are in direct conflict with one another. If you want the best performance, you’ll typically need to spend increasingly larger amounts of money to deploy that app in more and more data centers. But if you want the lowest cost, then you’ll need to pick just one or a handful of facilities – and your performance will suffer to the tune of higher latencies for your customers around the globe. These are just a few of the challenges and trade-offs that Kurt Rinehart, Section’s Director of Information Engineering, recently discussed when he joined Bart Farrell for a Data on Kubernetes Community Talks event. Kurt points to the need for intelligent systems, driven by business goals and strategies, that still maintain simplicity, flexibility and control for DevOps teams. He explains how Section helps users address these challenges with its Adaptive Edge Engine, which continuously tunes and reconfigures your edge delivery network to ensure your workloads are running the optimal compute for your application. When it comes to trade-offs, Kurt describes the all-too-common scenario where, in order to ensure a high degree of performance, you’re likely running the application across all of your data centers 24/7/365. It’s understood that you’re going to pay a significant amount of money to host your app in regions where traffic is low because your users are asleep. This trade-off is accepted because developers and engineers shouldn’t have to manage the ongoing optimization of increasingly complex distribution strategies. But that is why we should instead take a computational approach to meet real time traffic demands. Check out the full episode and hear more from Kurt about how Section is solving the location challenge by optimizing hosting for proximity and cost – dynamically and in real-time – giving users the benefits of distributed applications and services without the complexities of having to manage them."
"39","2022-07-06","2023-03-24","https://www.section.io/blog/turbocharge-graphql/","GraphQL is a widely adopted alternative to REST APIs because of the many benefits that it offers, including performance, efficiency, and predictability. While the advantages are significant, many developers become frustrated with latency challenges when implementing GraphQL. One solution to this is deploying the application across a distributed system. Let’s dive into these challenges and discuss some solutions that can be implemented today. TLDR? Get started for free or jump straight into tutorials for deploying distributed GraphQL on Section: Hasura Hasura with AWS RDS and Aurora Postgres Hasura and Supabase Postgraphile and Supabase GraphQL with Apollo Router GraphQL with Apollo Server Contact us to let us know what you need or get started now. Why GraphQL? GraphQL was built by Facebook in 2012 to combat issues with their mobile application. It was having to fetch so much data via their existing API that it was bogging down the user experience. They ended up completely rebuilding that model. Their challenge: the need to make the Facebook app scalable within a mobile application that was limited in resources. They asked, why does my app need all this information from my REST API. Why do I have to make multiple API calls to get that information? And, ultimately, why can’t there be just one call that returns all the information needed, and only that information? Hence, the creation of GraphQL. How Does GraphQL Work? GraphQL deliberately moved away from the twenty-year-old REST API model, with the Facebook team saying, instead we want to fetch content with just one HTTP request and we want it to return only the information that we need. GraphQL’s tag line is “Ask for what you need, get exactly that”, aiming to make it easier to evolve APIs over time, and enable more powerful developer tools. The query language is designed to replace schema stitching and solve challenges associated with REST APIs, including separation of codes, brittle gateway code and coordination. Since it was open sourced in 2015, GraphQL has become highly popular because of the flexibility and efficiencies it offers. Last year, The Linux Foundation set up the GraphQL Foundation to build a vendor-neutral community around it. Caching Challenges with GraphQL However, there are various challenges with GraphQL, particularly when trying to connect data across a distributed architecture. One of the main difficulties involves caching, which CDNs are unable to solve natively without altering their architecture. With GraphQL, since you are using just one HTTP request, you need a structure to say, “I need this information”, hence you need to send a body. However, you don’t typically send bodies from the client to the server with GET requests, but rather with POST requests, which are historically the only ones used for authentication. This means you can’t analyze the bodies using a caching solution, such as Varnish Cache, because typically these reverse proxies cannot analyze POST bodies. This problem has led to comments like “GraphQL breaks caching” or “GraphQL is not cacheable”. While it is more nuanced than this, GraphQL presents three main caching issues: Duplicate Cache - When there is a duplicate value for more than one key (i.e. when more than one URL leads to the same response or cache value). Overlapping Cache - This happens when, due to aggregation, sections of the response/cache are the same with almost no differences, so that instead of the APIs being cached atomically, the GraphQL API call is cached, which takes longer than independent and asynchronous calls to each API. Cache Times - Cache expires time (or TTL) becomes challenging, leading to portions of the GraphQL response becoming stale and immediately hurting the cache-hit ratio. Some CDNs have created a workaround of changing POST requests to GET requests, which populates the entire URL path with the POST body of the GraphQL request, which then gets normalized. However, this insufficient solution means you can only cache full responses. For the best performance, we want to be able to only cache certain aspects of the response and then stitch them together. Furthermore, terminating SSL and unwrapping the body to normalize it can also introduce security vulnerabilities and operational overhead. The next challenge is that you need to have both a client application and a server to be able to handle GraphQL requests. There are some great out-of-the-box solutions out there, including Apollo GraphQL, which is an open source framework around the GraphQL client and server. While the GraphQL server is easy to implement at the origin, it becomes significantly more complex when trying to leverage performance benefits in a distributed architecture. What Solutions Are Available? GraphQL becomes more performant by storing and serving requests closer to the end user. It is also the only way to minimize the number of API requests. This way, you can deliver a cached result much more quickly than doing a full roundtrip to the origin. You also save on server load as the query doesn’t actually hit your API. If your application doesn’t have a great deal of frequently-changing or private data, it may not be necessary to utilize edge caching, but for applications with high volumes of public data that are constantly updating, such as publishing or media, it’s essential. Distributed GraphQL on Section’s Edge Compute Platform Section’s Edge Compute Platform offers a platform to build a distributed GraphQL solution that is fully configurable to address caching challenges without having to maintain a distributed system. We give developers full flexibility and control to distribute GraphQL servers, such as Apollo, to get the benefits of stitched GraphQL bodies, along with an optimized edge network and the opportunity to configure caching layers to meet performance and scalability requirements. Here’s an example of what an edge node might look like: Let’s say you decide to use a JavaScript-based GraphQL server, such as Apollo. You can easily drop it into Section’s Node JS edge module and add a caching layer behind it. There are significant benefits to utilizing Varnish Cache (or other caching solution) behind GraphQL servers to cache API requests, particularly when it comes to reducing load time and increasing the time to deliver. With Section, you have the further benefit of managing your entire edge stack in a single solution. Apollo Federation Apollo Federation is another solution to provide an open source architecture for building a distributed graph. Or, in other words, it offers the opportunity to implement GraphQL in a microservices architecture. Their goal: “Ideally, we want to expose one graph for all of our organization’s data without experiencing the pitfalls of a monolith. What if we could have the best of both worlds: a complete schema to connect all of our data with a distributed architecture so teams can own their portion of the graph?” While the Apollo Federation solution helps address scalability challenges, developers are still faced with the same caching challenges. Because of its flexibility, the Section platform can also provide a platform for distribution of the Apollo Federation solution, so that developers can add and configure caching layers to improve performance and efficiency. Do you have specific GraphQL challenges you’d like to discuss? Chat with a Section Engineer"
"40","2020-07-22","2023-03-24","https://www.section.io/blog/server-side-rendering-edge-nodejs/","As you build your website, you inevitably make a series of foundational decisions that will impact the entire architecture of your applications. One of these core decisions is where to implement rendering logic in your application. Approaches to Rendering There are several approaches to rendering (thanks to Google for the definitions and diagrams). Decisions around these approaches typically revolve around performance impact, including Time To First Byte (TTFB), First Paint (FP), First Contentful Paint (FCP), and Time To Interactive (TTI). SSR: Server-Side Rendering Rendering a request for a page as HTML on the server, on each request. CSR: Client-Side Rendering Rendering an app in a browser, by manipulating the DOM. Rehydration “Booting up” JavaScript (JS) views in the browser so they reuse the server-rendered HTML’s DOM tree and data. Pre-Rendering Running a client-side application at build time to capture its initial state as static HTML. Each approach has pros and cons in relation to performance. In reality, you can opt to use different approaches for different pages. Netflix, for instance, server-renders its relatively static landing pages, while prefetching the JS for interaction-heavy pages, giving the heavier client-rendered pages a better chance of quick loading. In this post, we’ll focus on server-side rendering (SSR), starting with a quick overview of what SSR is, then explore how to do it with Node.js, finishing with a look at SSR at the Edge. What is Server-Side Rendering? Server-side rendering is the process of taking a client-side only single page application (SPA) and rendering it to static HTML and CSS on the server, on each request. SSR sends a fully rendered page to the client. The client’s JS bundle then takes over and the SPA framework can operate as normal. Why is this important? Improved performance - The wait time needed to download, parse, and execute the JS code in the browser is eliminated. With static websites or pages, SSR can be useful as it generates the full HTML for a page on the server in response to navigation. This avoids additional round-trips for data fetching and templating on the client since it’s taken care of before the browser gets a response, therefore, server-side rendering helps you get your website rendered faster. Faster load times - SSR generally produces a fast First Paint (FP) and First Contentful Paint (FCP). Faster load times equal a better user experience. Improved SEO - Another benefit of using SSR is having an app that can be crawled for its content even for crawlers that don’t execute JavaScript code. This can help boost SEO. Social sharing - With SSR, you get a featured image and elaborate snippet when sharing your website’s content via social media. This isn’t possible for just client-side rendered apps. SSR is often used to build complex applications that involve user interaction, rely on a database, or where content changes happen frequently. When content changes often, users need to see the updated content as soon as it is available. It also helps applications that have tailored content depending on who is viewing it, such as social media sites and applications in which you need to store user-specific data, such as email and user preference while also delivering SEO. Using Node.js for SSR Node.js is a server-side JavaScript runtime designed to build scalable network applications. SSR breaks many of the assumptions behind how Node.js can be best used, because it is compute intensive. Node.js can handle large amounts of asynchronous I/O in parallel, but it runs into limits on compute. As the compute portion of the request increases (as it does during SSR), concurrent requests will slow because of contention for the CPU. This becomes important for SSR when a server process handles multiple concurrent requests. The other requests being processed will delay the concurrent requests. This issue worsens as concurrency increases. This means that when Node.js is used for SSR, it’s being used not for its concurrency model (as it is for many applications) but rather for its library support and browser characteristics. There are ways to handle concurrent requests in parallel, such as by running multiple processes of Hypernova via the built-in Node.js cluster module along with a buffering reverse proxy to handle communication with the clients, as has been done successfully at Airbnb. Some JS frameworks, such as server-side rendered React apps use Node.js for the server, which is an important difference from traditional server-rendered apps. Edge Hosting Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Learn more and get started on a free plan Server-Side Rendering at the Edge Utilizing Firebase hosting and dynamically generating content with serverless functions will allow you to store content in an edge platform or CDN cache. This allows you to compress and cache content near end users for lower latency in the network call, meaning when the next user visits the website, it won’t have to do the generation of content again. It will simply serve it from the local Edge closest to that end user. This further improves performance and takes load off the server. Looking at a real use case, we worked with our client Adore Beauty to deploy their Nuxt app at the Edge using the Section platform. We used Section’s Node.js module to spin up a SSR framework for Vue.js, but it could be used for any other front-end framework. As a result, Adore Beauty now has a distributed Vue.js Server-Side Rendering framework for which they no longer have to think about the infrastructure on which their app runs. Section has also configured Adore Beauty’s Bitbucket Pipeline to automatically deploy to Section on a git push to their master branch. This example can be adapted to other frameworks, web servers, and use cases. The possibilities of using Node.js for applications you want to run at the edge are as varied as your imagination."
"41","2019-05-08","2023-03-24","https://www.section.io/blog/server-side-render-single-page-app/","This article breaks down how to use Section’s Node JS module to spin up a server-side rendering framework for Vue JS (or any other front-end framework). The transformative benefits that edge computing promises cover a wide range of objectives and use cases. While many are obsessing over IoT applications, others in more mainstream industries, such as e-commerce, are starting to adopt microservices strategies to reap the benefits of edge computing. We recently collaborated with one of our e-commerce clients, Adore Beauty, to deploy their Nuxt app at the edge using the Section Edge Compute Platform. While this walk-through is specific to Nuxt.js (a Vue.js framework), the same fundamentals can be adapted to other single page apps, such as React or Angular. Architecture Overview Section sits between the client and origin server, acting as a proxy for web traffic requests. Within Section’s container-based (Docker/Kubernetes) platform, all edge modules exist as a single container, including the Node JS module. Everything is managed through code, meaning there is no manual work needed during deployments. Let’s take a look at each of the components within this architecture. Edge Proxy Every Section application proxy stack implicitly begins with the Edge Proxy. The Edge proxy is the endpoint to which the User-Agent (e.g. web browser) connects, and it has a few responsibilities: Performing the TLS handshake for HTTPS connections Routing requests to the corresponding application edge module stack Implementing the HTTP/2 protocol Request correlation Serving custom error pages Request enrichment with geo headers, etc. Users may choose to configure any number of edge modules within their edge stack. This example is highlighting the Node JS module, but most customers employ additional module layers, such as caching, bot blocking, web application firewall (WAF), etc., to meet the demands of their application. Within the Section platform, users can configure the order of their edge stack, along with the individual module configuration itself. Node JS module Inside the Node JS module, on Port 80, there is an Nginx web server. The server configuration, or location blocks, are specified by the user in the server.conf file that lives in the root of the Node JS module repository. Some requests you may want to pass along to the Node JS module and others along to the next-hop, whether that’s the next module in the stack (e.g. image optimizer module, A/B testing), or in this example, directly to the Last Proxy. Inside the Node JS module lives the repository that contains the source code for the application that you want to run at the edge. Within this code base, there are a few minimum requirements that must be included: There must be a package-lock.json file present. You must have an npm start script in your package.json file to tell the container how to start the web server, or to start the application in general. Your web server, whether using built-in Nuxt, Express.js, native Node.js HTTP module, Koa, etc., must listen on process.env.PORT to determine exactly which ports the application runs on. (We have adopted these specs as a standard from other providers like Heroku and AWS Lambda.) In this example, the basic flow upon first deploy is as follows: Copies the custom server.conf file and puts it into /etc/nginx/ directory Runs npm install to install node modules Runs npm start to start the application, which in turn kicks off the start script that: Runs npm run build, which runs nuxt build Runs nuxt server to start the built-in Nuxt web server using process.env.PORT Starts Nginx to start accepting requests, where routing is controlled based on configurations sample start script: ""scripts"": {
  ""dev"": ""nuxt"",
  ""build"": ""nuxt build"",
  ""start"": ""npm run build && nuxt start"",
  ""generate"": ""nuxt generate"",
  ""serverbuild"": ""nuxt build"",
  ""lint"": ""eslint --ext .js,.vue --ignore-path .gitignore ."",
  ""precommit"": ""lint-staged"",
  ""ngrok"": ""ngrok http 3000 -region=au""
}
 For Adore Beauty, the front-end framework (i.e. Nuxt app) calls a Laravel API which houses all product, category, and pricing information. This initial version of the Node JS module uses blue-green deployments for updates, but we’re actively working on migrating to a rolling deployment model, where Kubernetes will add an annotation to the deployment which triggers a new deployment, spins up a new pod for the Nuxt app, and routes traffic over when it’s healthy. Last Proxy The Last Proxy uses the X-Forwarded-Proto HTTP Header to determine which protocol to use when communicating with the Origin server. For Adore Beauty, the Origin is a Magento instance, where all of the shopping cart and checkout functionality is managed. The Results Adore Beauty now has a distributed Vue.js server-side rendering framework for which they needn’t worry about the infrastructure on which their app is running, and have configured their native Bitbucket pipeline to automatically deploy to Section on a git push to their master branch. We’ve outlined a specific use case here, but hopefully, you can begin to see how this example can be adapted to other frameworks, web servers, use cases, etc. In fact, this is what excites us most about the future of edge computing … the possibilities are endless! What will you create?"
"42","2019-12-09","2023-03-24","https://www.section.io/blog/edge-configuration-management-cd-pipeline/","Speed of innovation is what differentiates leading businesses in today’s competitive environment. Developers pioneering at the edge are always looking for ways to iterate faster and more reliably. Continuous integration, continuous delivery, and continuous deployment are the industry baseline for rapid innovation at the edge. Continuous deployment (CD) is an extension of continuous integration and is used to reduce cycle and lead time time in between writing a new line of code, and that code running in front of users in production. To achieve continuous deployment, developers depend on infrastructure that runs and instruments the pipeline of automated steps that change the code running in production. This ensures that every change is predicted to work, the code change applies successfully, and the code behaves as expected after deployment. Benefits of Adopting Continuous Deployment Over the last decade, Continuous Deployment has emerged as the industry standard for making software changes because: It reduces the change failure rate. Teams that deploy multiple times a day, with less than 24 hours of lead time, experience less degraded service, and fewer remediations. It reduces the time to restore service when defects are introduced. Teams that deploy multiple times a day restore service at least 24x faster than teams who deploy at most once a week. It unblocks innovation. Organizations that embrace Continuous Deployment practices perform better on profitability, operating efficiency, and market share. It reduces the delay on ROI. By getting code running in front of users faster, CD helps organizations find product/market fit faster. Continuous Deployment vs Continuous Delivery We often see the terms Continuous Delivery and Continuous Deployment used interchangeably, but there is an important distinction between the two to call out. Continuous Deployment automates change promotion from one stage of the pipeline to the next without the need for human intervention. While this drastically increases the pace of iteration, it also initially carries more risk in pushing errors to production. Continuous Delivery requires human intervention to promote changes, typically at the last step, but often in subsequent stages. Both types of CD can be applied to applications and infrastructure. In fact, while CD is often discussed in the context of application lifecyles, there is a growing body of research to show CD’s impact on delivering and deploying infrastructure services continuously, including lowering defect rates and improving software quality for applications. Building CD into Edge Configuration Management In distributed compute architectures, deploying and scaling workloads can become much more complicated. Most development teams would rather focus on writing and shipping code that delivers value to users, than complex networks or infrastructure. Key Principles To manage changes to your edge configuration through a CD pipeline successfully, there are three key principles worth adhering to: Optimize for fast feedback. Identify steps within the pipeline that need optimizing by tracking execution time on individual stages. From the time you push a change to version control to making the change live should take no longer than five minutes. Fast feedback is important for quickly ensuring your changes meet business needs and cutting out technical debt and unnecessary costs. Chunk your changes, test immediately. Instead of changing multiple things in batches and then testing for the effect, interleave the changes and tests, and stop execution immediately if the tests fail. By turning changes into small, verifiable units, you lessen the risk factor. Push all changes through the pipeline. You lose the benefits you’re striving for if you accommodate changes outside the process. Breaking Down the Pipeline Push - An engineer issues a change to the configuration and pushes it to a repository. The changes can either be reviewed through an established process or pushed directly into master. Detect and trigger - The repository is either regularly polled or a hosted version control system, such as GitHub, calls out via a webhook to detect the change and trigger a build. Build artifacts - The build establishes dependencies and builds software artifacts (if any) that will be deployed further along the pipeline. Build infrastructure - The build communicates with an IaaS system in order to build the required network, compute, storage and load balancing structure. Orchestrate infrastructure - The build uses a configuration management tool to provide the service by stringing together the provisioned infrastructure. Testing: Testing after each change is essential to ensure that the CD has been effectively implemented. “Change one, test one” is a useful rule of thumb. Make all your changes then immediately test them. This means you’ll get fast feedback if a change fails the test, allowing you to automatically halt any other changes until you debug and fix the problem. What to Look for in Edge Workload Management Tooling As more organizations adopt edge computing architectures, one of the main considerations when evaluating edge workload management tools is how configuration changes are integrated into CI/CD pipelines. Here are some key features to look for: Granular Control Source code management - a correctly configured source code management (SCM) tool will underpin a stable platform for project development, which offers file access management, expedited version control, and natural storage of all essential project files, including the operative code and all linked script libraries, databases and integrated development environment (ICE) configurations. CD systems depend on this kind of in-depth control. Configuration as code - configuration as release-ready code will allow you to configure and manage your infrastructure, and leverage your CI pipeline to set up automated workflows, meaning instant configuration changes. Programmability API-first approach - automatically executing changes without relying on human intervention will let you derive the most significant gains from the implementation of CD. Instant Validation/Invalidation Integration into standard development lifecycle workflows is essential. Instant rollback when necessary - ensure your edge platform supports instant rollbacks and configuration changes so you don’t end up having to wait for indefinite periods of time and can instead make changes instantly. Real-Time Visibility Quick action requires robust observability tooling: logs, metrics, tracing. Integration with Popular CI/SCM Tools Jenkins - an open source server-based, cross-platform CI tool, offering broad plug-in support for various builds and databases; entirely written in Java and available for public use under a free software license at MIT. CircleCI - a commercial command line interface, which has a reputation for ease of use and speed. Used by various high profile clients, including Facebook and Spotify. Travis CI - easily syncable with GitHub projects, Travis is a hosted service known for its advanced testing options and range of languages support. Bitbucket Pipelines - an integrated CI/CD service built into Bitbucket, with an easy set up, which lets you automate code from test to production. It’s often used in combination with CircleCI, which offers a free container while Bitbucket provides free private repos. Managed Infrastructure Services Eliminate overhead associated with building & orchestrating infrastructure. Autoscaling - Confidence in knowing that compute resources will expand and contract to meet performance and cost requirements. Best-of-breed edge management tools will allow you to focus on constantly identifying and eliminating bottlenecks in your CD pipeline to get your iteration time down. Section’s Edge Compute Platform is built to complement and integrate with your CI/CD workflows. If you’d like to discuss your specific CD edge needs, chat with a Section engineer."
"43","2019-11-22","2023-03-24","https://www.section.io/blog/how-to-adapt-to-browser-shared-cache-going-away/","You may have heard that browsers are moving away from using a single HTTP cache. By partitioning the HTTP cache based on the top level origin, Chrome, Firefox and Safari are tightening up security vulnerabilities that enable privacy leaks. While this helps protect websites from malicious attacks, there are potential performance and bandwidth consequences if you don’t adjust affected cache configurations. What Resources Are Affected? The new browser cache protocols prevent documents from one origin from detecting whether a resource from another origin was cached. In other words, if you use third-party CDN services to reference popular libraries, frameworks or other static assets, you will no longer be able to take advantage of extended performance benefits from those providers. Common examples include jQuery, JsDelivr, Google Fonts, UNPKG, and many others. There has been a lot of commentary around the effects of these changes and potential solutions, which can adequately be summed up by saying that the impact is still yet to be determined but most certainly expansive in nature. How To Maintain Performance While Protecting Against Cross-Site Leaks The most effective way to adapt to these changes is to bring third-party static assets onto your domain. There are several benefits to hosting static assets on your own origin, the most notable of which are improved performance and increased control. By hosting assets on your origin vs third-party domains, you reduce the number of TCP connections required when loading resources and enable modern protocols such as HTTP2 to work on these assets, which significantly speeds up page load times. You can further achieve improved performance by leveraging edge delivery services, such as Section, to distribute your cache and serve resources closer to end users. Having increased control over static asset delivery allows developers more freedom when optimizing performance within their overall application architecture. By bringing static assets on-domain, you can implement more granular configuration using solutions such as Varnish Cache to achieve better performance gains. Section also offers alternate origin routing and HTML rewriting, which enables you to move content on-domain while still fetching it from a 3rd party location. Bottom line is, it’s still early days in the rollout of these changes. In fact, browsers are at different stages in their solution decisions and implementation. We’ll be keeping a close eye on the situation as it continues to develop. In the meantime, developers should be considering how these changes might affect their applications and take steps to mitigate any potential negative impacts. Our engineers tackle these challenges daily, so if you’d like to chat through specific scenarios, feel free to reach out."
"44","2020-04-17","2023-03-24","https://www.section.io/blog/virtual-waiting-room-vs-scaling-infrastructure/","Recently, Kmart experienced some Twitter backlash when they launched an experimental online queuing system to help manage traffic surges on their Australian websites. While the queuing system is intended to keep the site online during high traffic periods, customers are criticizing Kmart for their decision to use a patchwork solution rather than invest in scaling their underlying infrastructure So, this begs the question, are virtual waiting rooms a cheap alternative to avoid investment in properly scaling resource capacity? If only it were that simple. What are virtual waiting rooms (aka website queuing)? Virtual waiting rooms are a way to limit the number of users on your website at one time. They are used to protect your website during unexpected peak traffic events, so that a portion of users are still able to navigate, search, and transact with the website, rather than all users suffering delays and outages. Virtual waiting rooms sit in front of your website, and only let users through when they think there is capacity on your website. There are an assortment of virtual waiting room or website queuing solutions around, some provided by third-parties to add to your site via a simple integration, others custom designed by the site owners from scratch. If you’ve ever bought tickets to a popular concert or sporting event online you’ve likely experienced such a system. Section offers a Virtual Waiting Room module for customers to deploy alongside other modules in their edge stack – for example caching, web application firewall (WAF), bot management, A/B testing, custom containers, etc. But why not just address the underlying concern and scale your infrastructure? In the modern world of elastic cloud infrastructure, automated scaling, and granular billing, it is tempting to suggest that we shouldn’t need to restrict the number of concurrent users accessing a service anymore. We should be able to add all the hardware we need to service the demand for the duration of the increased traffic, and then deprovision it all afterward, only paying for what was needed while it was being used. Scaling Out Scaling out, or adding more servers, is what the cloud excels at, but many existing software architectures were not designed to scale this way. One example is the single-master model often found in popular database systems: no matter how many replicas are added, there is always one server that needs to coordinate the rest and will be the bottleneck. Scaling Up Scaling up, or adding more CPU, RAM, disk, or network capacity to the same number of servers, is also an option in most clouds but can be tricky when applying such changes can disrupt availability to the service, or when you’re already using one of the largest instance types, or the large types are not available in the datacenter you need it to be in. Hybrid Approach One approach is to use a virtual waiting room to guide infrastructure scaling decisions. Building for a very large scale is simply waste for many systems until there is evidence that large scale needs to be supported. Using a queue allows the system to be online for most users instead of offline for all, and frees the engineers to focus on implementing better scaling solutions instead of fire fighting. Planning for traffic surges Recent circumstances have left many DevOps teams scrambling to support increased online traffic volumes, highlighting the need for ongoing preparation for these types of events. ITNews recently covered the pacing efforts of Coles Liquor, one of Australia’s largest liquor retailers, calling out years of background prep as the reason they’ve been able to handle the ‘digital stock-up surge’. “We have been doing a lot of stress testing over the last couple of years since I joined Coles.” Juan De La Pava, martech and performance manager for Coles Liquor Regular performance, load, and stress tests are an important part of the development lifecycle. It’s critical that teams continually reevaluate systems, resources, and processes to ensure there aren’t gaps or underinvestments in technology and/or people."
"45","2019-09-19","2023-03-24","https://www.section.io/blog/horizontal-pod-autoscaling-custom-metrics/","With Kubernetes as the backbone of Section’s Edge Compute Platform, we manage a large, complex network of globally distributed clusters with the goal of optimizing where and when our customers’ workloads are running to suit the needs of their application, while maintaining operational efficiency of the platform. In order to maximize efficiency, we need a mechanism to dynamically expand and contract resources based on the demands of our customers’ applications at any given time. We rely on the Kubernetes Horizontal Pod Autoscaler (HPA) to scale services in a reliable, predictable, and controllable manner. How the Horizontal Pod Autoscaler Works The HPA controller runs regularly to check if any adjustments to the system are required. During each run, the controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. The controller manager obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics). The HPA in its basic form is pretty powerful, but where the HPA starts to take on superhero status is when we start to introduce custom metrics to drive autoscaling intelligence. To start, there are three different variables that need to be set before using the HPA. You set these within the HPA configuration itself: maxReplicas minReplicas some type of resource, or something to scale on Resource type can be memory or CPU, which generally isn’t ideal for us to scale on because it can be extremely variable. Rather, we define and pull in custom metrics from Prometheus to get more granular in how we autoscale. Incorporating Custom Metrics from Prometheus We use the Prometheus Adapter for Kubernetes Metrics APIs to access the custom metrics on which to autoscale. Using custom-metrics-config-map.yaml as a starting point, we can see that the default settings already include CPU and memory. resourceRules:
  cpu:
    containerQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)
    nodeQuery: sum(rate(container_cpu_usage_seconds_total{<<.LabelMatchers>>, id='/'}[1m])) by (<<.GroupBy>>)
    resources:
      overrides:
        node:
          resource: node
        namespace:
          resource: namespace
        pod_name:
          resource: pod
    containerLabel: container_name
  memory:
    containerQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>}) by (<<.GroupBy>>)
    nodeQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>,id='/'}) by (<<.GroupBy>>)
    resources:
      overrides:
        node:
          resource: node
        namespace:
          resource: namespace
        pod_name:
          resource: pod
    containerLabel: container_name
 In this example, we’ll add some extra parameters to the config map. These are just PromQL queries with some substitutions that the Prometheus Adapter will fill in appropriately: Network received packets total - provided by the Node Exporter Request count - a custom metric that our pods expose for scraping to Prometheus. Not all our Pods have this metric, so packets received makes a decent proxy. Here’s what it looks like: rules:
- seriesQuery: 'container_network_receive_packets_total{namespace!="""",pod_name!=""""}'
  resources:
    overrides:
      namespace: {resource: ""namespace""}
      pod_name: {resource: ""pod""}
  name:
    matches: ""^(.*)""
    as: ""pps1m""
  metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)'
- seriesQuery: 'container_network_receive_packets_total{namespace!="""",pod_name!=""""}'
  resources:
    overrides:
      namespace: {resource: ""namespace""}
      pod_name: {resource: ""pod""}
  name:
    matches: ""^(.*)""
    as: ""pps20m""
  metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[20m])) by (<<.GroupBy>>)'
- seriesQuery: 'section_http_request_count_total{namespace!="""",pod_name!=""""}'
  resources:
    overrides:
      namespace: {resource: ""namespace""}
      pod_name: {resource: ""pod""}
  name:
    matches: ""^(.*)""
    as: ""rps10m""
  metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[10m])) by (<<.GroupBy>>)'
- seriesQuery: 'section_http_request_count_total{namespace!="""",pod_name!=""""}'
  resources:
    overrides:
      namespace: {resource: ""namespace""}
      pod_name: {resource: ""pod""}
  name:
    matches: ""^(.*)""
    as: ""rps1m""
  metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)'
 The Prometheus adapter has a custom language it uses where it will substitute in namespace or pod, which allows us to query the custom metrics API so the HPA can use it. Read documentation metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>}[1m])) by (<<.GroupBy>>)'
 Note, we can also rename the metric in the API with the following (seen in context above): name:
  matches: ""^(.*)""
  as: ""rps1m""
 In this example, we’re pulling in Requests Per Second (RPS) for each of our edge modules (e.g. WAF, caching, image optimization, etc.) so we can efficiently autoscale to accommodate the volume of requests at any given time. Observability with HPA and Custom Metrics While the HPA is tremendously powerful, it does require significant, ongoing tuning to realize its benefits. To this end, we use Grafana dashboards for a convenient way to view HPA activity, specific to a particular pod or environment. In the example above, we can see an environment scaling up to ten replicas (from two) over a 24-hour window. It’s important to note that each metric has its own scaling decision, which Kubernetes will collate and choose the highest value. For example, we might create a packets-per-second metric where we scale on a 20-minute window and on a 1-minute window. The 1-minute window can be useful to detect a big spike in traffic, whereas a 20-minute window can help indicate a longer-term traffic trend. Whenever making custom settings, it’s helpful to consider whether you’re looking to adapt for spiky traffic or for a day/night cycle traffic pattern. Of course, your work is likely not done once you’ve made a first pass at setting HPA values. Using a dashboard like the one above can help you adjust settings to suit your objectives. Do you want to run as few pods as possible to save money on nodes? Do you want to scale quickly to a maximum value? Appropriate settings will vary with the type of workload, and with ongoing software development, they may need adjusting over time."
"46","2019-03-15","2023-03-24","https://www.section.io/blog/prometheus-querying/","Prometheus has its own language specifically dedicated to queries called PromQL. It is a powerful functional expression language, which lets you filter with Prometheus’ multi-dimensional time-series labels. The result of each expression can be shown either as a graph, viewed as tabular data within Prometheus’ own expression browser, or consumed via external systems via the HTTP API. PromQL can be a difficult language to understand, particularly if you are faced with an empty input field and are having to come up with the formation of queries on your own. This article is a primer dedicated to the basics of how to run Prometheus queries. You will need only three tools: Prometheus server port forwarded from the local connection Simple cURL A data visualization and monitoring tool, either within Prometheus or an external one, such as Grafana Through query building, you will end up with a graph per CPU by the deployment. Prometheus Querying The core part of any query in PromQL are the metric names of a time-series. Indeed, all Prometheus metrics are time based data. There are four parts to every metric. Taking the varnish_main_client_req metric as an example: The parts are: Metric_name (e.g. varnish_main_client_req) One or more labels, which are simply key-value pairs that distinguish each metric with the same name (e.g. namespace=""section-b4a199920b24b""). Each metric will have at least a job label, which corresponds to the scrape config in the prometheus config. The value, which is a float64. When querying in the Prometheus console, the value shown is the value for the most recent timestamp collected. The timestamp, which has millisecond precision. The timestamp doesn’t appear in the query console, but if you switch to the Graph tab within Prometheus, you can see the values for each timestamp in the chart. Each distinct metric_name & label combination is called a time-series (often just called a series in the documentation). If each series only has a single value for each timestamp, as in the above example, the collection of series returned from a query is called an instant-vector. If each series has multiple values, it is referred to as a range-vector. These are generated by appending a time selector to the instant-vector in square brackets (e.g. [5m] for five minutes). The instant vector and range vector are two of four types of expression language; the final two are scalar, a simple numeric floating point value, and string, a simple string value. See Range Selectors below for further information on this. All of these metrics are scraped from exporters. Prometheus scrapes these metrics at regular intervals. The setting for when the intervals should occur is specified by the scrape_interval in the prometheus.yaml config. Most scrape intervals are 30s. This means that every 30s, there will be a new data point with a new timestamp. The value may or may not have changed, but at every scrape_interval, there will be a new datapoint. There are four types of metrics: Counters - A cumulative, monotonic metric. Counters allow the value to either go up, stay the same or be reset to 0 (on a process restart). varnish_main_client_req is an example of this, which provides the total number of HTTP requests that the varnish instance has handled in its life. Gauges - A non-monotonic metric. Gauges can go either up or down, giving the current value at any given point in time. An example is node_memory_utilisation, which provides the current percentage of memory used on each node. Histogram - This creates multiple series for each metric name. Sampled values are put into buckets. Sum & count metrics are also generated for each sample. Summary - The summary is similar to histogram in that it takes samples and creates multiple metrics, including sum & count. However, it creates quantiles (i.e. 50th percentile, 90th percentile, etc.) instead of buckets. We’re going to deal with counters for this analysis, as it’s the most common metric type. Query Structure The structure of a basic Prometheus query looks very much like a metric. You start with a metric name. If you just query varnish_main_client_req, every one of those metrics for every varnish pod in every namespace will get returned. If you do this in Grafana, you risk crashing the browser tab as it tries to render so many data points simultaneously. Next, you can filter the query using labels. Label filters support four operators: = equal != not-equal =~ matches regex !~ doesn’t match regex Label filters go inside the {} after the metric name, so an equality match looks like: varnish_main_client_req{namespace=""section-9469f9cc28d8d""} which will return only varnish_main_client_req metrics with that exact namespace. Regex matches use the RE2 syntax. If you’re familiar with PCRE, it will look much the same, but it doesn’t support backreferences (which really shouldn’t matter here anyway). You can also use multiple label filters, separated by a comma. Multiple label filters are an “AND” query, so in order to be returned, a metric must match all the label filters. For instance, varnish_main_client_req{namespace=~"".*3.*"",namespace!~"".*env4.*""} will return all varnish_main_client_req metrics with a 3 in their namespace that don’t also contain env4. Range Selectors By appending a range duration to a query, we get multiple values for each timestamp. This is referred to as a range-vector. The values for each timestamp will be the values recorded in the time series back in time, taken from the timestamp for the length of time given in the range duration. As an example, take varnish_main_client_req{namespace=""section-9469f9cc28d8d""}. If we add a [1m] range selector we now get this: We get two values for each series because the varnish scrape config specifies that it has a 30 second interval, so if you look at the timestamps after the @ symbol in the value, you can see that they are exactly 30 seconds apart. If you graphed these series without the range selector and inspected the value of the lines at those timestamps, it would show these values. Now, range-vectors can’t be graphed because they have multiple values for each timestamp. If you select the Graph tab in the Prometheus web UI on a range-vector, you’ll see this message: Error executing query: invalid expression type ""range vector"" for range query, must be Scalar or instant Vector A range-vector is typically generated in order to then apply a function to it to get an instant-vector, which can be graphed (only instant vectors can be graphed). Prometheus has many functions for both instant and range vectors. The more commonly used functions for working with range-vectors are: rate() - calculates the per-second average rate of increase of the time series in the range vector over the whole range. irate() - calculates the per-second average rate of increase of the time series in the range vector using only the last two data points in the range. increase() - calculates the increase in the time series per the time range selected. It’s basically rate multiplied by the number of seconds in the time range selector. Your selection of range duration will determine how granular your chart is. A [1m] duration, for instance, will give a very spiky chart, making it difficult to visualize a trend, looking something like this: For a one hour view, [5m] would show a decent view: For longer time-spans, you may want to set a longer range duration to help smooth out spikes and achieve more of a long-term trend view. Compare the three day view with a [5m] duration to a [30m] duration: Joining Series Technically, Prometheus doesn’t have the concept of joining series like SQL, for example, has. However, series can be combined in Prometheus by using an operator on them. It has the usual arithmetic, comparison and logical operators that can be applied to multiple series or to scalar values. A caution: if an operator is applied to two instant-vectors, it will only apply to matching series. A series is considered to match if and only if it has exactly the same set of labels. Using these operators on series achieves a one-to-one matching when each series from the left side of the expression exactly matches one series on the right side. So, taking the two series from the previous examples as separate instant-vectors: varnish_main_client_req{endpoint=""metrics"",instance=""10.244.24.68:9131"",job=""varnish"",namespace=""section-9469f9cc28d8d"",pod=""varnish-786d4648bd-lrlrc"",service=""p8s-varnish""} & varnish_main_client_req{endpoint=""metrics"",instance=""10.244.48.66:9131"",job=""varnish"",namespace=""section-9469f9cc28d8d"",pod=""varnish-786d4648bd-rfnjb"",service=""p8s-varnish""} If we apply an addition operator to these two to try and get a total number of requests in that namespace, nothing will be returned. This is because there are no series returned that have exactly matching labels. However, if we use the on keyword to specify that we only want to match on the namespace label, we get: Note that the new instant-vector contains a single series with only the label(s) specified in the on keyword. The ignoring keyword can also be used as an inverse of that to specify which labels should be ignored when trying to match. In this case the returned instant-vector contains a single series with all matching labels left after removing the labels in the ignoring set. It is worth pointing out here that Prometheus also has a number of aggregation operators. So if we really wanted to get the total number of client requests in a namespace, we would never actually do this because the pod names would change over time. The sum operator is much simpler: sum(varnish_main_client_req{namespace=""section-9469f9cc28d8d""}) by (namespace) Combining Instant-Vectors with Scalars You can also combine an instant-vector with a scalar value. Each series in the instant-vector has the value applied with the operator. For example: varnish_main_client_req{namespace=""section-9469f9cc28d8d""} * 10 results in each value for each series in the instant-vector being multiplied by ten. This can be useful for calculating ratios & percentages. Kubernetes Reference Series What if we wanted to know how many requests were being made against each node in our Kubernetes cluster? We can see this in the varnish_main_client_req series by looking at the instance label (e.g. instance=""10.244.48.66:9131""). However, this is not particularly helpful as the IP address given there is only the internal IP address of the pod as addressed by the service the Prometheus operator has used to identify which endpoints to scrape. The port is t that which the varnishstat promethues exporter publishes its metrics endpoint to by default. To address this, the kube-state-metrics and node-exporter Prometheus exporters publish a number of series that essentially exist to provide reference labels. These are series like kube_pod_info & node_uname_info. There is a kube_pod_info series for every single pod in the cluster. If we filter it down to the two pods we are interested in knowing more about, we can see what kind of information it provides: From this, we can see the name of the node, the IP address of the nodes and which ReplicaSet created the pods. Also note the value. The value for these reference series is always 1. The reason for this is so that you can join them with other series simply by multiplying without having to change the value of the original series, and still gain access to the new set of labels. So, to get our varnish_main_client_req series with the node label on them, we can do this : varnish_main_client_req{namespace=""section-9469f9cc28d8d""} * on (pod) kube_pod_info This returns us: Unfortunately because we needed to use the on keyword to match, that is also the only label we get back. Ignoring non matching labels won’t help here because pod is the only matching label. To fix this, we use the group_left or group_right keywords. These keywords convert the match into a many-to-one or one-to-many matching respectively. The left and right indicate the side that has the higher cardinality. So a group_left means that multiple series on the left side can match a single series on the right. The result of this is that the returned instant-vector contains all of the labels from the side with the higher cardinality, even if they don’t match any label on the right. So varnish_main_client_req{namespace=""section-9469f9cc28d8d""} * on (pod) group_left() kube_pod_info gives: This gives us back all the labels for the varnish_main_client_req series, however we still don’t have the node label. To solve this, the group_left and group_right keywords allow a label list to be passed in. The labels provided will be included from the matching lower cardinality side of the operation. So by doing: varnish_main_client_req{namespace=""section-9469f9cc28d8d""} * on (pod) group_left(node) kube_pod_info We can get all the information required: We can then take this over to Grafana to make a dashboard and chart, add this data to a graph panel, and clearly view it all. Avoiding Overloads and Slow Queries Sometimes graphing a query might overload the server or browser, or lead to a time out because the amount of data is too large. When constructing queries over unknown data, it is better to begin building the query in the tabular view of Prometheus’ expression browser until you arrive at a reasonable result set (i.e. hundreds as opposed to thousands of time series). Switch to graph mode only once you have sufficiently aggregated or filtered your data. If the expression continues to take too long to graph ad-hoc, you can pre-cord it using a recording rule. This is particularly relevant to PromQL when a bare metric name selector such as api_http_requests_total can easily expand to thousands of time series each with a different label. Also, expressions that aggregate over multiple time series will generate load on the server even when the output is only a small amount of time series."
"47","2019-08-29","2023-03-24","https://www.section.io/blog/how-we-create-use-service-blueprints/","Service Blueprints are a tool used to harmonize business and technical processes, particularly those that involve complex interactions. At Section, we are increasingly embedding them into our workflows to help create more clarity, transparency, and cohesion. Service blueprints are a critical part of the service design experience, aimed at putting the customer experience first through understanding the process by which a product is made and how an organization produces it. Service design has been a disruptive force over the last twenty years with many companies such as Apple, Starbucks, P&G and Nike building their companies around its principles. Service is often the thing that drives customers to and/or from a product. What is a Service Blueprint? “A service blueprint is a diagram that visualizes the relationships between different service components – people, props (physical or digital evidence), and processes – that are directly tied to touchpoints in a specific customer journey.” This definition comes from the Nielsen Norman Group, the founders of which are often credited with leading the field of user experience design (UX). Their website includes many useful UX tools, including this service blueprint example: The top layer involves time and scale – showing how long an action typically takes to complete. The next is evidence, whether physical or virtual, for instance, a website, an in-store discussion, a sales call, etc. Then there is the customer journey - from enquiry through to sales completion all the way to sign-up and delivery. The work of the company is split between frontstage actions: those elements which the customer sees that involve actions and technology; backstage actions: those things that the company does that take place behind the scenes; and support processes, processes which are instrumental to enabling the customer experience. The Benefits Service blueprints are increasingly used across industries to map out the service experience and make it more efficient. A well-defined service blueprint covers multiple areas in the company: from the business side to the technical, serving as a communications and planning tool. A service blueprint can be as simple or as complicated as you need it to be. Blueprinting is an especially useful approach to cross-functional efforts and those that involve numerous touchpoints. Each subsystem can be mapped out, from architecture to project planning. Instead of different parts of the company working in silos without knowing about the contribution of other departments, a service blueprint can provide an overall map for how a product flows through a service across an entire company. It is also a useful tool for working with external elements for clearly showing the responsibilities of each party, what the customer does vs. what the company does, etc. How we use Service Blueprints at Section At Section, we are increasingly integrating the use of service blueprints into our workflow engine. Service blueprints help us to provide support from start to finish by visualizing where a customer is within their journey, including where they might be experiencing pain points and more proactively resolve any issues. As the company grows, we are finding it to be a particularly useful tool for onboarding new employees, making it easy to see what each stage of the customer journey looks like and the actions necessary to enable it. We have recently started to use service blueprints to help us manage the implementations of our edge module. Here is an example of a service blueprint we have used for adding ShieldSquare, the real-time bot mitigation and management tool, to our customers’ tooling: The ShieldSquare blueprint maps out the various stages of the customer journey and its interactions with Section. The left-hand column is laid out in the same way as the example from the NN Group, moving from time to supporting processes. The first customer action shows a customer experiencing a bot attack, then finding the Section website and seeing that we have bot management software, including ShieldSquare. Next, they might have a pre-sales call and conversation with one of our engineers to discuss the product. This will involve a discussion around commercials, including how much it might cost per metric, its benefits and a quote. Next, we move to the ShieldSquare trial stage, followed by a baseline and monitoring stage and finally to sign up via a physical contract, which signs the customer into active mode. We could add multiple more layers to add higher fidelity, for instance, to address the complications of having suppliers. In this instance, it’s a stylized and simplified blueprint. Nonetheless, it allows both ourselves and our ShieldSquare customers to understand the boundaries of each of our actions. Our service blueprints are orthogonal – it doesn’t necessarily replace another planning tool. Indeed we use it alongside our long-form step-by-step written documents as a way of visualizing the process. By mapping out the processes we hold in our heads, it allows us to see more clearly where problems might be encountered, and better anticipate ways of solving them. Here’s an example of another service blueprint in use at Section for onboarding new customers: Other Useful Tools The NN Group has many further excellent Service Blueprint resources, including: Service Design 101 https://www.nngroup.com/articles/service-design-101/ UX vs Service Design, or how do they compliment https://www.nngroup.com/videos/ux-vs-service-design/ Service Blueprints Definition https://www.nngroup.com/articles/service-blueprints-definition/ More excellent Service Blueprint Resources from NNGroup https://www.nngroup.com/search/?q=service+design"
"48","2020-06-03","2023-03-24","https://www.section.io/blog/building-a-fifo-virtual-waiting-room/","Virtual waiting rooms, or online queuing systems, are a good, simple way to control traffic surges on web applications. While they aren’t intended to be a replacement for properly scaling infrastructure, they can help protect a website’s overall user experience by remaining online for a subset of users, while excess traffic is queued. At Section, we recently underwent a re-evaluation of the underlying logic behind our Virtual Waiting Room edge module. Previously, we were using a random selection model, which started to raise questions of fairness around user experience. So, we began evaluating FIFO and True Queuing models, each of which has its own benefits and drawbacks. Online Queuing Models There are several different models when it comes to virtual waiting room technology and these nuances can be broken down into logic around ‘queuing’. At a high level, queuing methods can be broken down into three different models. Random Selection In a random queuing model, application administrators set a maximum threshold for the number of concurrent visitors allowed on the website. Once this threshold is reached, subsequent users trying to access the website are placed into a holding area. As users exit and capacity becomes available for new users to enter the site, the technology will serve the next request, either from the queue (i.e. if a user in the queue refreshes their page), or from a new user attempting to access the site. Random queuing logic is solely based on the next request hitting the website and does not discern between users in the queue and new users, meaning that new users can essentially ‘jump the line’ ahead of users who have been waiting in the holding area. First In First Out (FIFO) As the name suggests, a true First In First Out (FIFO) queuing model is a linear model that lines up and admits users based on the order in which their requests were received. On the surface, this can seem like the ‘fairest’ model; however, if you’re only ever waiting on the first person in the queue, that user is potentially (and likely) holding up the rest of the line until they refresh or trigger a new page request, which can lead to a degraded user experience as a whole. There are instances where this model is appropriate, such as with online ticketing applications. Modified FIFO A modified FIFO model (like the one used for Section’s Virtual Waiting Room) applies more logic to queuing decisions by creating progressive layers of the queue. Application administrators still set a maximum threshold for concurrent users on the website. However, there is an additional layer that provides a mechanism to only grant access to people at the head of the queue. The decision to admit new users from the head of the queue is still based on the first request hitting the website, but this eliminates the possibility of users bypassing the queue. Building Section’s FIFO Virtual Waiting Room As we looked to improve upon our random selection Virtual Waiting Room model, our aim was to strike an optimal balance of good throughput and fairness. We started by documenting the desired flow. We were then able to translate this flow to technology decisions that include: A script to detect the amount of concurrent users on the website The ability for Section admins to set thresholds for visitor access allowances and size of the head of the queue waiting area. Session id objects that include a time to live (ttl) that determines active vs. inactive users in the head of the queue Additional Considerations There are several other factors to consider when evaluating a FIFO model, including handling of bots and exempt users. Bots You don’t want bot activity, such as Google or Bing bots, holding up spots in your queue. For this scenario, we’ve included logic that checks for a valid session id before placing users in the queue. Exempt Users Exempt users, such as site administrators, need to be able to bypass the queue. For these cases, you can configure a custom url that allows you to obtain an access cookie immediately. Summary Online queuing solutions are very effective at preventing websites from crashing during high traffic events. Virtual waiting room technology, and more specifically, the underlying logic driving it, should be evaluated based on the specific use case. Section’s Virtual Waiting Room offers flexible configuration options to meet a wide variety of requirements. Contact our team of solutions engineers to chat through your specific use case."
"49","2018-12-20","2023-03-24","https://www.section.io/blog/html-caching-magento-1/","Magento 2.3 is the latest iteration of the enterprise-class eCommerce platform, Magento, that many of our customers at Section use. In fact, it is estimated that one in four of all businesses use the Magento software. The open source platform officially began development in 2007 and the first public beta version was released seven months later that same year. It is known for its wealth of customizable options and extra adaptations, designed to suit a wide range of use cases and provide a host of useful features such as cart customization and robust security. Magento 2.0 was launched in 2015 with the goals of heightening user engagement, offering smoother navigation, and increasing conversion rates. With various architectural differences, Magento 2.0 boasts improvements in usability, speed and optimization. It has various new features, including a revised security directory, an advanced payment option and a “friendlier” approach to installation and updates. Despite the benefits that Magento 2 offers, there are many eCommerce stores who remain on Magento 1. As with any major system upgrade, companies and/or developers who have become comfortable with one version’s mechanisms and paradigms may prefer to stay with the original to avoid extra resource strain and/or maintenance and potential downtime. However, we believe that remaining on Magento 1 shouldn’t mean sacrificing performance. Section allows Magento 1 users to take advantage of Full HTML Caching by emulating the same behavior that Magento 2 provides out of the box, with just a small amount of additional work needed on the web application. How to Configure Full HTML Caching for Magento 1 We recently helped one of our eCommerce customers, Australia’s largest online retailer of costumes, accessories and party supplies, overcome some performance challenges that they were experiencing with Magento 1 by leveraging Section’s Full HTML Caching feature. Pages on the site are tagged with a custom HTTP response header that contains the product IDs of products displayed on that page. By working with Section’s Customer Engineering Team, now whenever they update a product, their Magento application automatically sends a cache flush API call to Section, whereby we then flush all product and category pages containing a specific product ID. We wanted to ensure that despite being on the older iteration of the platform, customers using Magento 1 could still take advantage of the benefits provided by Section’s Full HTML Caching feature. The main benefit is faster page load times, something which we have found consistently leads to more product page views and ultimately a higher rate of cart conversion and increased sales revenue. To benefit from these caching improvements, Magento 1 customers need to take the following steps: Lift personalized content, such as mini cart and login status, on web pages from HTML to AJAX, so that the HTML is generic; Either turn off CSRF* tokens, or inject them via AJAX calls; Pages that contain mostly personalized content (e.g. checkout, account, etc.) can be left as is, they can be excluded from caching; Product pages need to be tagged with a unique ID; this ID should take the form of a HTTP response header value (e.g. prod-ID: 12345); Any page, such as a category page, needs to be tagged with IDs of all the products on that page. The HTTP header should look like like this: prod-ID: 12345, 12346, 12223 A server side module/script needs to be created, which can call the Section cache flush API with a conditional cache clear of only items with a specific prod-ID; Finally, this module needs to be hooked into the Save button in Magento. Whenever a product page is edited and the Save button is pushed, this module should call the Section API to flush any page with that product in order to reflect the latest state. With the above in place, long cache times can be placed on HTML pages, drastically cutting down on the number of requests to origin servers. This should lead to much higher concurrent user counts, a faster user experience and overall reduced server cost. Contact us if you’d like assistance implementing performance improvements on your Magento site. * Cross-Site Request Forgery (CSRF) tokens are not needed on most pages. As defined by OWASP, CSRF is “an attack that forces an end user to execute unwanted actions on a web application in which they’re currently authenticated”. The Synchronizer Token, while a strong method to defend against CSRF, is dependent on session cookies and server-side state, which unfortunately negatively impacts cacheability. There are other ways to reduce the incidence of CSRF; several ideas for alternative solutions are listed here."
"50","2018-11-28","2023-03-24","https://www.section.io/blog/keeping-websites-online-during-click-frenzy/","Similar to Cyber Monday, Click Frenzy is Australia’s most highly promoted online mega-sale event that drives an entire nation of shoppers to retailer websites on a single day. Participating retailers anticipate big sales, but must also ensure that their websites are equipped to handle the traffic blitz. With Opportunity Comes Risk Click Frenzy, an initiative unique to Australia, started in 2012 and involves many of the country’s largest domestic and international retailers (including Target, Bing Lee, Myer, Microsoft and Dell) who run sales to attract a large wave of simultaneous buyers. The most recent one-day sales event took place on November 13, 2018, and the next is scheduled for February 26, 2019. In its inaugural year, the Click Frenzy site crashed almost immediately following the start of the sale period. The Australian public responded with a trending #clickfail hashtag on Twitter and the creation of a set of memes mocking the event’s failure. Many consumers bypassed the official site entirely, going directly to the participating retailers instead. These kinds of one day sales frenzies can drive serious profits. In 2015, Adobe found that Cyber Monday was the U.S.’ largest eCommerce sales day on record, pulling in more than $3 billion in sales. China’s equivalent, Singles Day, also held in November, generated $14.3 billion in sales that same year. Always aiming for record results, Click Frenzy and its partners must be able to accommodate potentially enormous spikes in traffic. Preparing eCommerce Websites for Traffic Spikes At Section, there are multiple ways in which we help website and app owners prepare in advance so that when traffic spikes happen, the technology behind them doesn’t fail. Three of the essential methods we use involve: Caching Dynamic Content - Making sure generic HTML and other dynamically generated content is cached so the web application is freed up to do more meaningful tasks, such as cart management and checkout. Caching Static Objects - Utilizing caching techniques, such as hole-punching, can dramatically decrease the load on your server by offloading large files such as image or video to reverse proxy servers. This leaves the website server open to easily handle the critical traffic. Developing a smart caching strategy is essential to ensuring scalability and improving page load time. Optimizing the Front End - Reviewing your site to see what might be causing slow downs, such as large images that can be compressed, old third party JavaScript that no longer has value or alternately too much third party JavaScript, which should be removed. Another solution is to employ overload protection services, such as a virtual waiting room, which lets you put a limit on the upper number of concurrent users on the site and display a queuing message to those waiting. This means that you can both provide a robust service to users already on your site who can successfully navigate, engage with and make purchases whilst keeping other users aware of how long they must wait until they can return to the website and complete their transactions. Once website users have completed transactions or left the website, the next set of waiting room users will be granted access. Many clients use New Relic to monitor and alert on web application health and make judgments about when to adjust virtual waiting room numbers. New Relic’s APM service enables you to view and analyze large amounts of data and gain insights in real-time that are immediately actionable. Additionally, New Relic’s Apdex scores serve as a valuable quick indicator of web application health and can be used to trigger alerts should the application encounter difficulties. Another critical high traffic event that Section has been involved with includes helping clients manage large spikes of traffic generated by the Shark Tank TV show. Section has helped multiple sites remain online through the Shark Tank experience and work simultaneously to improve site speed to deliver a superior user experience. We do this by minimizing traffic hitting the origin through carefully deploying a Varnish Cache configuration for each site, particularly focusing on areas that were susceptible to cache breakage, including session management, the completion of forms, shopping cart and checkout. Similarly, Section plays a central role in keeping retailer eCommerce websites online during Click Frenzy. We ensure that the bounce rate stays low due to technical issues, site speed stays optimal and that payments are processed without fail. This isn’t necessarily easy, especially when a great campaign leads to so much traffic that it looks to your web server like a DDoS attack is underway. Website scalability at such a time is imperative. In addition to the technological methodology we employ, Section’s Customer Engineering team can help you plan ahead for anticipated high-traffic events and come up with the right strategy to keep the site healthy and user experience great. We also ensure that we have the right team resources available to handle any unexpected issues that our clients may experience during Click Frenzy (or other events) and will arrange a post-event debrief to analyze what happened and prepare for the next."
"51","2016-11-10","2023-03-24","https://www.section.io/blog/cyber-monday-trends-ecommerce-performance/","Each year, more and more people conduct their shopping online, buying everything from holiday gifts and clothing to household essentials and even groceries through websites or mobile applications. Several trends have emerged out of this shopping phenomenon, including online-only businesses which undercut brick-and-morter costs and subscription services that send goods monthly. But perhaps the most lucrative marketing campaign for ecommerce sites globally has been the popularity of one-day-only blowout sales conducted exclusively online. These sales often take place in November to drive pre-holiday sales and include Cyber Monday in the US (November 30), Single’s Day in China (November 11), Click Frenzy in Australia (November 15) and Amazon Prime Day (July 15). The growth of holiday ecommerce sales As with all ecommerce trends, these one-day sales are a relatively new phenomenon. Cyber Monday was only officially named in 2005 as a way for ecommerce businesses to cash in on the traditional post-Thanksgiving shopping spree which starts with Black Friday. Cyber Monday deals are typically online-only and allow smaller stores and websites to offer discounts without needing to battle long lines of customers that are associated with Black Friday. Although a new “holiday,” Cyber Monday has proved to be extremely lucrative for ecommerce businesses, who now advertise sneek peaks at their deals weeks in advance and throw significant budget into promoting their sale. As ecommerce sales have grown so have Cyber Monday sales, and in 2015 a study by Adobe found that Cyber Monday was America’s biggest ecommerce sales day on record, with a total of over 3 billion dollars in sales. This was a 16% increase in sales from Cyber Monday 2014, so if the trend continues we can expect to see around 3.5 billion dollars in sales in 2016. While Cyber Monday is well known in the US, the world’s largest ecommerce sale is the “Singles’ Day” sale conducted by the Chinese ecommerce giant Alibaba on November 11th. In 2015 Singles’ Day generated 14.3 billion dollars in sales, more than 4x that of Cyber Monday. In 2016 the sale generated over 1 billion dollars in sales in the first five minutes, and ecommerce giants globally are looking to it as a barometer for their holiday ecommerce predictions. How to prepare websites for holiday sales As demonstrated by the above statistics, one-day sales can bring a great amount of traffic and revenue to ecommerce sites. But websites need to properly prepare for this influx in traffic, which is a massive increase over an average shopping day. In 2015, sites as large as Target, Nieman Marcus, and PayPal experienced outages due to an overload of traffic, and that could mean serious revenue losses: studies have estimated that an hour’s downtime could result in up to 8% in lost daily revenue, or hundreds of thousands of dollars on the largest ecommerce shopping day. Why weren’t these large sites able to stay online? It comes down to several factors, one being the unreliability of load testing. Load testing is a synthetic measure that uses scripts to estimate real user traffic, and is intended to help determine what amount of traffic a site can handle and identify weak points in the website architecture. Unfortunately, it is near impossible to simulate real user behavior: different browser types, connection speeds, number of page visits, and more are difficult to estimate and therefore test. This means when actual customers visit a website, their behavior could differ dramatically from what was tested. Another common misconception is that adding additional servers in a website’s hosting infrastructure will prevent any problems. While adding hardware will increase the number of visitors a website can serve, hardware still has limits so if a website underestimates their traffic it can still slow down or go offline. Caching increases scalability at peak load times The best way to ensure a smooth Cyber Monday and ecommerce holiday season is for websites to leverage a smart caching strategy that allows the majority of product pages to be served from a cache. This increases the scalability of a website by allowing it to serve many more visitors at once, and decreases stress on the website servers. By utilizing caching techniques such as hole-punching, which allows for large portions of pages to be cached while keeping personalization elements such as account information un-cached, ecommerce sites can cache nearly all of their pages. This leaves the actual website server open to handle the critical checkout process with ease. Another option is to use an overflow page which caps the amount of visitors on your website at any one time, allowing those in the process of browsing and completing transactions to finish their sale before another visitor is allowed on the site. Some overflow pages tell visitors what number they are in line to get into the site, while others may display a simple message letting customers know they will get onto the site if they keep the browser window open. While it may be difficult to predict how Cyber Monday will look in 2016, it’s clear from past years that websites need to be prepared for a large spike in traffic. Leveraging smart caching strategies and examining how customers interact with websites will prevent sites from going down during this crucial revenue opportunity. Contact us if you need help setting up a cache solution on your website and increaing scale for peak load times."
"52","2016-09-01","2023-03-24","https://www.section.io/blog/what-is-cashed-data-cache-definition/","What is a web cache and what are the benefits of cached data? Before we get into what exactly caching is, you need to understand why caching is important. Ultimately, the main benefit of caching is faster web pages. And faster web pages lead to a better user experience, which means happier website visitors. Multiple studies have shown that users visit more pages on a website when it loads faster. For a media company, this could mean more articles and ads viewed, and for an eCommerce site, it means that customers view more products. In fact, it’s even been shown that faster web pages lead to increased conversions and revenue, and a web page delay of just 1 second leads to an average 7% loss in revenue. Faster web pages also mean search engines view your website more favorably, improving your SEO value and meaning more people find your site. How does a cache work? Now that we know why caching is important, let’s take a look at what it actually means and how caching makes your website faster. Every time you visit a web page, you are using your web browser to request and assemble that page from the website’s server. The server holds all the files needed to assemble that web page, including the HTML doc (instructions to build the rest of the page), the images, text, styling, and more. On average, your browser makes upwards of 100 requests back and forth from the website’s server to build a complete webpage. Without any type of caching, whenever you visit that page you make those requests all over again. And every other person visiting that web page is making the same requests. If there are lots of people accessing a page at one time, the server slows down and takes longer to deliver the web page to everyone. And slow web pages = unhappy visitors. Caching solves this problem by storing a copy of the assembled web page in a couple of different locations. This copy is temporarily stored somewhere other than the website server, so your browser doesn’t need to go all the way back there each time you load the same page. I’ll go through two major caching locations and how they work. Browser Caching: Why do I need to clear my cache? One way to cache content is to do it directly on the hard disk of your personal computer. Web browsers do this automatically for web pages you visit, so they don’t need to go back to the website server to download every single element again. For example, a website logo is often repeated on each web page. If that logo is in your browser cache, the browser doesn’t have to re-download it for every page on that website you visit. You may be familiar with the phrase “clear your cache” or “clear browser data” - this is one of the first things engineers ask you to do when troubleshooting why a web page isn’t showing up correctly, or you can’t see updates that have been made. Clearing your cache deletes all those files that have been saved on your computer, forcing your browser to go back to the website server and download a “clean” copy of the web page. Cached data on a server Web pages are also sometimes cached closer to the website server, rather than on your personal computer. When a website installs a cache on top of their server, they are keeping copies of the relevant files and instructions in that cache. When your browser requests data from the website server, it hits the cache first, and if the cache has a recent copy of the web page you requested, the cache delivers the assembled content directly to your web browser so your browser doesn’t have to travel back to the server. The website can control how often their cached content needs to be updated: if you are the first visitor to a web page after the cached content has expired, the cache will re-collect a new version of the web page from the server, then deliver that content to you and save it until that newer copy also expires. This graphic illustrates how it all works: What is cache hit vs cache miss?: To get into the terminology of all this, a “cache hit” means that the cache successfully delivered you the content the website has cached, whereas a “cache miss” means the cache didn’t have the relevant content stored and had to go back and request it from the server. The higher the cache hit percentage, the more often people are getting content delivered to them through the cache, meaning web pages are loading faster for them and there are less requests going to the website server, which also decreases server hosting costs for the website. How does a Content Delivery Network come in to play? Now that you’ve got the basics of caching, you may be interested in what type of content websites choose to cache and why, how you can set up a cache for your own website, and how Content Delivery Networks or CDNs are related to caching. We’ll cover all that in Part 2 of the Fundamentals of Caching: look for it on the Section blog next week. If you want a simple way to cache your web content, request a demo of Section’s easy-install cache solution."
"53","2016-08-11","2023-03-24","https://www.section.io/blog/speed-means-sales/","The importance of website performance is often overlooked, even though the Internet heavily influences many aspects of our daily lives. Over the past few months I have been investigating just how crucial and imperative optimal website performance is to the success of one’s company or organization. In order to examine the benefits of having a fast, readily available website, I performed research and analysis on 324 websites selected at random from a list of several thousand that run on Magento Enterprise - a leading e-commerce platform used by many online stores. There are numerous online resources that offer helpful tools to analyze and measure the performance of your website. For the purposes of my studies, I primarily utilized WebPagetest because of the variety of information that is displayed after conducting a performance test. However, there are also other resources available to examine the speed of a website such as Google’s PageSpeed Insights, GTmetrix, and Pingdom Website Speed Test. All of these tools provide fascinating data and metrics that will give you an indication as to the performance of your website as well as ways in which to improve its speed. The specific metrics that I observed in my research regarding page load time along with their respective meanings are as followed: Redirect Time to www: The amount of time it takes to fully redirect to the website’s www domain if it is in place for a “Bare Domain” request. HTML Load Time: Otherwise known as the Time to First Byte (TTFB), is the time in which the HTML document (the key to starting any page drawing in the browser) is delivered to the web browser. Start Render Time: The initial point in time in which the first non-white content (can be anything that is different from a blank page) becomes visible and is displayed on the web browser. First View Load Time: The time from the start of the initial navigation until the first time the page is loaded in the web browser. Speed Index: A calculated metric that describes how fast all of the page contents are visually populated. Examining these differing metrics for your website can help you to paint a picture of its overall performance, and determine areas in which speed optimization can occur. Magento Website Performance Research Findings Now that I have described the basics of my research, here are some of the most interesting findings and takeaways from the 324 Magento Enterprise websites that I evaluated. In my analysis, I compared the top 10 websites with the best performances vs. the bottom 10 websites with the worst performances, in addition to the overall averages for each individual metric. 241 of the 324 total websites analyzed underwent redirects. Of those 241 websites, 12% of them redirected to their www domain in an impressive 100 milliseconds or less. On the contrary, the average Redirect Time to www for the 10 worst websites was 1598.60 milliseconds. 11% of the websites had an HTML load time of 200 milliseconds or less. Only 5% of the websites with an HTML load time of more than 750 milliseconds effectively utilized a Content Delivery Network (CDN) on their bare domain. Of all 324 websites, the average Start Render Time was 2.88 seconds whereas the average First View Load Time was 7.25 seconds, which equates to an average difference of 4.37 seconds from when something first appears on the screen to when all of the page content has loaded. Only 6% of the websites had a Start Render Time of 1 second or less. Of the 251 websites that were classified as strictly e-commerce, the average First View Load Time was considerably worse at 7.15 seconds. The Speed Indexes of the top 10 best websites were all under 1156 milliseconds. In comparison, the average Speed Index for all 324 websites was 5142 milliseconds. What does all of this mean for you? Although there are a lot of factors that impact the page load time of your website, the most important thing to take away is that milliseconds ultimately do matter when it comes to optimal website performance. There are an abundance of studies that have proven this notion. When Google was contemplating whether to display 10 results or 30 results on their Google search pages, they found that traffic dropped 20% for the 30 results pages because of a ½ second page loading difference. Not only that, Google experienced 25% less searches for every 500 millisecond increase in page load time. Bing, another popular search engine, experienced a 2.8% drop in total revenue due to a one second delay in page load time. According to Amazon, they found that they underwent a 1% decrease in sales for every additional 100 millisecond in page load time, and a 2% increase in conversions for every one second of speed improvement. A widely renowned study made by the Aberdeen Group showed that a one second delay in page load time may lead to a 7% loss in conversions, 11% fewer page views, 16% decrease in customer dissatisfaction, degraded Search Engine Optimization (SEO), and a generally poorer user experience. In dollar terms: if your website earns $10,000 per day, this would mean a $250,000 loss in sales each year. Improve Magento performance with Section Section is a Magento Select Technology Partner and a website performance and security solution built to integrate seamlessly with Magento. Download our extension for Magento 2 to get performance improvements in a few simple steps, or Magento 1 accounts can sign up for an account here. Both options include a free 14 day trial. We’ve also produced a definitive guide on optimizing Magento for performance and scalability which reviews the best ways to increase the speed of your website."
"54","2017-08-10","2023-03-24","https://www.section.io/blog/page-load-time-bounce-rate/","It’s become common knowledge that the speed at which pages load is a crucial part of user experience, and recently page speed has become more and more important in other areas - Google’s search engine ranks pages based on page load time (specifically, time to first byte), and Facebook has announced that it will prioritize links that load quickly in its newsfeed over those that are slow to load, saying “We’ve heard from people that it’s frustrating to click on a link that leads to a slow-loading webpage. In fact, even more broadly on the internet, we’ve found that when people have to wait for a site to load for too long, they abandon what they were clicking on all together.” By doing this, Facebook is acknowledging that their users expect a fast-loading link, even if it is directing users to content outside of Facebook. How Page Speed Impacts Visitor Behavior Large websites including Amazon have conducted studies showing that users expect a page to load quickly, and in fact that slower page load times can lead to a significant drop in revenue - some estimates say up to 1% loss for every 100ms delay in page load time. Section’s own studies have also shown this, with leading ecommerce beauty site Adore Beauty finding that faster pages resulted in a 16.5% increase in conversion rate and a consistent improvement in revenue. But translating these studies from high-volume websites like Google and Amazon to the many thousands of small and mid-size websites out there can be difficult. Some may argue that page speed matters more for giant websites who are expected to perform well even under high traffic, while other studies have found it hard to measure the impact of page load time across thousands of websites. It can be difficult for websites to measure the impact of page speed on user experience themselves. Google Analytics takes a very basic measure of website speed as experienced by your visitors - they show overall page load time for a sample of 1% of visitors. For many websites the Google Analytics sample size would be extremely small and therefore skewed by the different connections of the end-users being sampled. More advanced tools like New Relic measure the various elements of page load time - including back end time, front end time, and total page load time - but do not link this data to marketing metrics like page views per session and bounce rate. Additionally, many of the often-cited page speed metrics are from several years ago, when end-users had different connection speeds (especially on mobile devices) and expectations around how fast pages should load. Now many users expect even faster page load times, and with the growth of ecommerce-only retail businesses and internet-only media companies, page speed has more of an impact than ever. So, If you are a medium-size ecommerce site with good brand awareness and a loyal customer base, does page speed really impact your bottom line, and how do you measure it? We are happy to announce that Section’s Real User Monitoring now includes business metrics that show how page speed impacts the number of pages viewed by a user and the average bounce rate. We’ve also conducted a study on a sample of websites (primarily ecommerce websites) already using Section’s RUM. The results clearly demonstrate that slower page speeds negatively affect both the number of pages viewed by a single visitor and the average bounce rate (the percentage of visitors who leave after only visiting one page). Page Speed and Bounce Rate The infographic below divides average total page load times experienced by real visitors into brackets of 1-second, 2-seconds, 3-seconds, etc. If a visitor went to multiple pages on a website, the page load time is averaged - so if the first page loaded in 4 seconds, and the second loaded in 2 seconds, they would be put into the “3 seconds” bracket to represent the average time a page took to load. As you can see, the bounce rate increases as page speed goes up, meaning that more people are only visiting one page the slower the first page they visit is. For users with an average page load time of 2 seconds, the bounce rate is quite low - only 9.61%. This goes up slightly for those with an average page load of 3 seconds, to 13.0%. There is a jump in bounce rate of 4.1% from 3 to 4 seconds, with 4 second page load time having a bounce rate of 17.1%, and a 5.1% jump from 4 seconds to 5 seconds. For those users experiencing an average page load time of 7 seconds, the bounce rate is 32.3%. Page Speed and Number of Pages Viewed So now we know that slower page load times mean less visitors going to more than one page, but how does page speed impact the total number of pages users visit? As you might expect given the impact page speed has on bounce rate, the faster pages load on average the more pages visitors go to. As shown in the above graph, those with an average page load time of 2 seconds view 8.9 pages on average, while those with an average page load time of 7 seconds view only 3.7 pages on average. The number of pages viewed consistently decreases as page speed decreases, and the difference in pages viewed between users averaging 2 second load times and users averaging 4 seconds is 3.x pages. This means that a 2 second delay in page load time could mean a user exits your site 3 pages earlier. How Page Speed Impacts Revenue What does all of this mean for your website? From these studies we can see that page speed does impact both number of pages viewed per session and the bounce rate. As any marketer knows, these are important metrics that can have a direct impact on conversion rate and revenue. Pages viewed are important because they show a visitor is engaged with your website and is staying on the site through several page loads. For ecommerce sites, more pages viewed often means a higher likelihood that a visitor will add one of the items on a page to their cart. It also could mean larger cart sizes. As we have found previously in A/B tests with a leading online beauty store, faster pages equal larger carts and more revenue. Pages viewed per session are also a crucial metric for media sites and other websites which rely on page views for advertising revenue. The cost and effort of acquiring a new visitor to your website is often high, so once they are on your site you want to make sure they will stay and view several pages to get a strong return on investment. The faster the pages load, the more likely they will view more pages and become a valuable visitor in terms of both advertising and likelihood to return to your website. Bounce rate is the percentage of visitors who come to your site and leave after only viewing one page. This metric can be impacted by many factors, including page content, page design, and how relevant your page content is to the search term or advertisement that brought visitors to your website. Page speed also impacts bounce rate, and because those who bounce only visit one page on your website, the load time of that first page is extremely important. If a visitor has never come to your website before, they will need to collect all assets from your web server or cache server. A very slow time to first byte or start render time could mean visitors leave your website before the first page has even loaded. The data in this post demonstrates that page speed does have a clear effect on both bounce rate and number of pages viewed, but it’s important to remember that every website is different, and content, design, and overall user experience will also impact these metrics. To measure the effect page speed has on your specific website and visitors, you can install Section’s Real User Monitoring and start collecting this data right away. Section’s RUM is included with all Section accounts, contact us to get started with a free 14 day trial."
"55","2016-09-19","2023-03-24","https://www.section.io/blog/tips-to-improve-website-performance-page-speed/","Reports by Google, Amazon, and others have consistently shown that fast page load time is one of the most important elements of website success: one study by the Aberdeen Group showed every 1-second delay in page load time led to a 11% decrease in page views and 7% loss in conversions. There are many ways you can optimize your website for better performance, but here are some quick tips you can do now to improve your web performance and page load time: 1. Do a quick, free analysis of your web page speed Tools such as WebPageTest will show you exactly what is taking the longest time to load on your webpage. Simply enter the URL of the page you’re looking to evaluate and wait for the results to come in. The below test is for the homepage of beauty store Sephora: there are over 200 elements that make up this page, and by looking at the left column you can see what is taking the longest time to load. For example, while the images are typically taking under 50 milliseconds to load, a Facebook plugin takes 182 ms. Learning how to read a web page test is extremely valuable: find out more in our study of the speed of 300+ Magento websites. 2. Ensure your images are optimized for web Use an image editing tool such as Photoshop or a free image re-sizer to size your images appropriately so you aren’t making users download large files. To do this, first find out what the maximum allowed image size is for your webpage or blog layout - it usually won’t be more than 800px width, and even if you upload larger images they will be sized down. If you use html tages such as and a width or height specification, you aren’t reducing the image size on your server, only how it appears on the page. Once you determine your ideal image size, save as that size. If you’re using an image editor, you can also save at 80% quality which further reduces the file size, or using an online resizer you can select options which reduce the file size by your desired percentage (we don’t recommend saving at more than 50% smaller). JPG and PNG are the best file types for web. 3. Reduce the number of plugins on your site Plugins like Google Analytics provide valuable information but can slow down page load time. These tools usually work by having you enter a JavaScript snippet somewhere in your website code (commonly in the header section), and when a user visits your page they need to collect this remote file before the page is fully loaded. If you have several of these snippits, say one for Google Analytics, one for sharing on Facebook, and one for sharing on Twitter, your page will take longer to finish loading. Check your HTML code regularly to make sure you still need all the plugins that are installed on your page. 4. Remove unnecessary code in your CSS files Your CSS file needs to load before your page becomes viewable, but if you (and others) have been building upon the same file for some time, it likely has a lot of superfluous code - every extra space or line will add up to a slower page! Go through the code yourself in case there are any hidden elements that aren’t being used, or use a free CSS minifier which will remove extra spaces and code for you. Here are some resources Google suggests for minifiying HTML, CSS, and JavaScript code. 5. Cache your pages By caching your pages you reduce the number of requests to your server, speeding up page load time dramatically. Caching also helps with scalability of your website during peak traffic times, such as when you are running an email marketing campaign or ecommerce sale. Check out our blogs on why caching is essential to web performance and what content you should be caching, then explore setting up open-source caching solutions such as Varnish Cache. To get an easy-to-setup, globablly distributed caching solution, get started for free or sign up for a free trial or our next-generation Content Delivery Network."
"56","2016-09-16","2023-03-24","https://www.section.io/blog/cache-definition-part2-varnish-html-cache/","We previously went through the fundamentals of caching, what it is, why it’s important for faster web pages, better SEO, and reduced server costs, and what the differences are between a browser cache and a cache installed on a web server. Now we’ll go through some information on what type of content is typically cached, how Content Delivery Networks or CDNs are related to caching, and some tools to cache your own web data and improve the performance of your website. What web content should I cache for optimal website performance? There are some types of files that are frequently cached by websites, some files that can be cached but that many websites do not cache because they are seen as “risky” (we’ll get into that later), and others that are never cached. Files that are frequently cached are ones that are the same for all users and don’t change often. They may include: Static images Logos and brand assets Stylesheets (the code that dictates the font, colors, etc used throughout the website) Javascript files that don’t change (for example, the Google Analytics javascript snippet that - measure website views) Downloadable files or other content Files that can be cached but rarely are include: Full HTML pages Javascript files or other code that changes more frequently Files that should not be cached include: User-specific data such as account information that is different for each visitor Any sensitive data, such as banking or credit card information Why aren’t HTML pages cached? The HTML document is the first piece of information that a web browser receives when it loads a web page. This document includes all the information needed to load the elements of a page, including stylesheets (which dictate the colors, fonts, and overall look of a page), logos, images, header and footer files, and more. The process to generate a web page’s HTML document is where most of your server resources are spent, but most CDNs focus on caching static files such as the ones in the above list, and do not cache full HTML documents. This is because the full HTML document is critical to the look of the web page, and if it is cached incorrectly it could result in a page whose layout appears completely off, or one that displays the wrong user account data. So, despite the considerable speed and resource-freeing benefits, it can be too risky for websites to cache their HTML documents if they aren’t able to properly test their caching configuration and ensure everything will work as expected on the live website. To cache an HTML doc, developers must have the ability to implement flexible configurations within their CDN, and also to test these configurations before they go live. Section provides these benefits (read more about them in our HTML caching whitepaper), but most CDNs do not allow for flexible configurations and do not have a CDN testing environment. This is the main reason that most sites still direct users back to their servers for the HTML document, slowing down page load time and increasings server costs for the website. Do I need a CDN to cache my website? There is an old assumption that the main feature of Content Delivery Networks is to store and delivery static cached content from server locations across the globe, despite that: Modern CDNs can do much more than cache static objects. You do not actually need a globally distributed server network (CDN) to take advantage of some of the benefits of caching. Although utilizing a CDN to cache objects can make the process simple, and includes the added benefit of distributed servers to deliver your content to worldwide users, you can install a cache on top of your local server without utilizing a CDN. What tools are out there for me to cache my web content? Varnish Cache is a commonly used caching solution which is open-source and can be installed and configured by anyone using Varnish Cache Configuration Language, although complex configurations can still be tricky. Platforms such as Magento, a leading ecommerce software, strongly recommend users implement Varnish Cache for faster page load times. Section also uses Varnish Cache as our caching layer, and gives users the ability to choose the Varnish Cache version they use and configure it for their application. Other caching softwares include Squid and Nginx. There are also caching solutions available for specific platforms, such as Wordpress. How do I quickly set up a cache solution? Section makes it easy to install and configure Varnish Cache for your specific application. We let you use the version of Varnish Cache that works best for you, and provide an easy user interface to configure the changes. In addition, Section provides developer-friendly features not seen in most CDNs, such as a testing environment, the ability to cache full HTML docs, and real-time logs and metrics so you can see what percentage of traffic is hitting the cache and what is going back to your server. Sign up for a free trial today or get started for free and more information on Section."
"57","2016-08-17","2023-03-24","https://www.section.io/blog/content-delivery-network-what-is-in-a-name/","Birth of the Term “CDN” Nearly 20 years ago, Akamai launched the first content delivery network. They rocked the web world by distributing reverse proxy servers globally into a number of Points of Presence (PoPs) to act as content caches and ran a layer of DNS over the top to select the closest PoP to the user. The impact for websites at the time was immediate. All of a sudden the images which were being served over and over again from the origin servers were being served from Akamai’s reverse proxy servers. This meant fewer choke points at the origin network and improvement in delivery speed into the browser due to lower latency. Content was being delivered from the Akamai network of reverse proxy servers. And the CDN name was born. CDN Evolution We understand from industry chit chat that Akamai started their network with Squid. Squid is an open source reverse proxy server which functions as a cache for static objects such as images. Squid was doing not much else at the time other than caching and subsequently serving content (largely images) to browsers. “CDN” made sense as a name for a network of reverse proxy servers performing this function. Fast forward 20 years and CDNs can now include a wide range of reverse proxy servers performing a range of functions. Some examples of reverse proxy servers now deployed by “CDNs” include: Nginx – open source server which can be run as a reverse proxy. Performing functions including caching, rewriting (e.g. running LUA or web application firewall. Cloudflare for example use Nginx extensively in their network. ModSecurity – An open source Web Application Firewall which can detect and / or block a range of requests based on parsing the request looking for a match to certain content. Cloudflare and Akamai both based their security products on older versions of this Reverse Proxy. Varnish Cache – An Open Source HTTP accelerator which caches content (including the whole page), and due to its programmability can be driven hard to perform a wide variety of functions in addition to caching (e.g. Load balancer style activity). Fastly built their CDN based on older versions of Varnish Cache. Max CDN also use Varnish Cache for parts of their CDN. JavaScript detection bot blocking (or Anti Scraping) proxies – Query and detect for a browser’s ability to execute Javascript before allowing that browser to send its request to the origin. Thereby blocking undesirable bots from scraping content or causing DDoS. Distil provide a network based on this style of Reverse Proxy. Google’s PageSpeed Module – Built and maintained by Google, this open source reverse proxy can perform a very wide range of Front End Optimisation activities to a webpage including resizing images, rewriting image types, engaging image lazy loading, prioritising critical CSS etc. Verizon built this into their CDN and companies such as Yottaa built their networks on using similar reverse proxies. Edge Rewriting Proxies – As noted above, Nginx running LUA can be used as an effective content rewriting proxy enabling developers to compile content for the web page at the edge of the network or change the nature of the content based on information provided to the edge by the browser. Useful for aspects such as JavaScript tag injection. Cloudflare use this technique for adding Javascript objects into the page and Akamai’s Edge Side Includes is a similar offering. The above are just a few of the many reverse proxies which can now be offered to users in a distributed network. By doing so, website owners and developers can take advantage of the awesome features these reverse proxies offer, without needing to worry about deployment, patching scalability etc. “CDN” Only Covers 10% of Modern CDNs Functionality What is apparent from the above is that CDN is no longer a relevant term to apply to networks running all these proxies. A modern network in front of a web server should no longer just be delivering static content. Such a network would only be making use of maybe 10% of the features and benefits of a distributed reverse proxy network. As discussed above, reverse proxies running in distributed networks can do many things including: Delivering content Blocking inbound requests, Modifying and improving whole pages and content Re rerouting requests Reporting and alerting on the status of the requests and the outbound content Selectively caching and serving entire webpages Serving alternate content based on rulesets Challenging spurious requests, etc These activities go far beyond Content Delivery. Unfortunately, even though the legacy CDNs have rolled out additional reverse proxies into their networks, these powerful features are often ignored or seriously underutilised because they are just “too hard” to set up and manage on those legacy CDNs. The locked down, production-only nature of legacy CDNs and poor visibility for developers and operations engineers makes their functionality, (beyond the basic static object caching) difficult and often risky to leverage. We Need a New Name for New Functionality Things have moved a long way in the last 20 years, but if you use a CDN only for static object caching, then CDN is what you are using. If you are looking for more performance, security and scalability from your distributed reverse proxy network, then let’s find a new name. You are going beyond the old school CDN. The team at Instart Logic coined a term to describe their network of reverse proxies- Application Delivery Network. I’m not sure this goes far enough as it still discusses a one way, limited conversation between browser and the network. At Section we publish a range of open source reverse proxies on our network. We don’t hide them in a “block box” or protect them with patents. As such, we can bring many reverse proxies to our users so they have a choice of which reverse proxies they want to run on our network. For example, at Section you can choose between Varnish Cache 3 or 4. We also provide our users with the tools to drive the proxies effectively. Developers can finally unlock the full potential of these reverse proxy servers in a safe and efficient manner. Section is a reverse proxy management platform which can be deployed in either a distributed fashion (like a CDN) or inside a firewall as an Application Delivery Controller (or both). It’s the first software defined reverse proxy management platform. So, how do we describe all that with a three letter acronym?"
"58","2016-09-07","2023-03-24","https://www.section.io/blog/varnish-waf-options/","It’s common that a modern web site will want the advantages of Varnish Cache’s excellent programming model in tandem with a WAF. We’ve been looking at this space for a while and we’ll show you what we have found. Let us know if you’ve seen anything else. WAF inside Varnish Cache We’ve seen a few projects work on getting a WAF to examine requests directly in Varnish Cache. An example of this is the Varnish Security Firewall. After over a year in the wilderness, a few recent commits have appeared on the project to make it compatible with Varnish Cache 4.0. There’s also the Varnish Firewall which was last updated in 2012. Both of these WAF options inside Varnish Cache contain XSS and SQL Injection protection mechanisms. We were unable to find any testing results using these systems. ModSecurity with Varnish Cache We searched for a Varnish Cache VMOD that facilitated the ModSecurity core library running inside Varnish Cache natively without success. ModSecurity runs effectively in Apache and Nginx, and using these as a WAF proxy behind Varnish Cache works well. User traffic comes to your site, and you use some SSL termination proxy to direct traffic to Varnish Cache. Varnish Cache is then configured to fetch from your ModSecurity WAF proxy layer. Then, configure the ModSecurity proxy layer to fetch content from your application. Considering the heavy performance impact of a decent ModSecurity ruleset, caching is recommended by the ModSecurity developers. Have a look at the ModSecurity Performance Recommendations where the first recommendation is to put Varnish Cache or another effective HTTP cache in front. Varnish Cache with other WAF Proxies Using the technique above you can also substitute ModSecurity for a different WAF. An actively developed proxy that takes a different angle on WAF is the NAXSI WAF that runs inside Nginx. At that stage you might ask yourself “Why would I use Varnish Cache when I have Nginx? Nginx is fast and has caching capabilities too”. We really like Varnish Cache’s VCL. It allows us to creatively solve problems with HTTP requests and responses without modifying the application. This allows us to somewhat separate caching concerns from our application. In nginx, the ngx_http_proxy_module provides caching capabilities that cache according to the cache rules sent from our origin responses. You’ll need to make sure your application’s web server is sending the right headers, and this can sometimes be done really easily in Varnish Cache. Final Thoughts When deploying these systems don’t forget your metrics and log management. Its great to surface your metrics and logs in a usable way that means you don’t need to log onto your servers to see what’s happening. You’ll probably also need some kind of alerting system that lets you know when something is going wrong. Have a look at our other blog articles for some ideas on metrics systems."
"59","2021-01-18","2023-03-24","https://www.section.io/blog/snapt-nova-waf-edge-module/","Section’s newest module, Snapt Nova, offers enterprise-grade edge security with the ability to centrally manage, deploy and control your security preferences. By deploying Snapt Nova on Section’s Edge Compute Platform, you can combine the application security and intelligence of Snapt while simultaneously leveraging the benefits of edge computing. The Snapt Nova WAAP and WAF solution allows you to protect your applications and data against disruptions, leaks, DoS botnets, application attacks, and fraud. You can also rest assured that Nova is a fully PCI-compliant WAF solution, helping prevent security breaches and payment card data theft in the present and in the future. The Snapt Nova module on Section is specifically designed for cloud native architectures and DevOps teams. For IT organizations managing hundreds, possibly thousands, of microservices, Snapt offers the ability to reduce complexity by scaling without limitations on any platform and any network while protecting you against common attack vectors used by hackers across locations, clouds, and environments. Key Benefits Key benefits of Snapt Nova include: Full protection against Layer 7 attacks The Snapt Nova module offers protection against a full spectrum of layer 7 threats (the layer of the OSI model just beneath the surface of user interfaces), including: DoS Data leaks SQL injections Remote code execution XSS attacks Scrapers Additionally, it extends multiple layers of defense for your application, including: Authentication Access management A+ SSL rating Full PCI compliance Granular telemetry, allowing you to determine potential threats in real-time With Snapt Nova, you get extensive logging and alerting abilities (Slack, Prometheus, etc.). Plus, Nova monitors offer more than just TCP data. Real-time telemetry includes: HTTP error rates Request rates HTTP POSTs Known vulnerability endpoints … much more Straightforward, flexible centralized security configuration Snapt Nova is centrally managed with near-zero latency communications, giving you access to real-time control, telemetry and intelligence. From a single pane of glass, you can easily apply blacklists, whitelists, rulesets, rate limits, and deploy and configure modules across multiple clouds and locations. Instant defense against the OWASP Top 10 and DDoS attacks When you deploy the Snapt Nova module on Section, you gain instant protection against the top 10 most critical web application security risks, including Automatic mitigation of DDoS attacks using real-time telemetry to make dynamic adjustments to traffic. AI and Machine Learning to automatically configure security and send alerts Snapt Nova leverages an intelligent ML Engine and AI-based autonomous decision-making, alongside real-time telemetry to automatically secure your applications. With this, the technology is able to deliver pre-emptive threat protection, as well as proactive recommendation of changes to your WAF. Auto-scaling as needed Snapt Nova’s ADC nodes are stateless, allowing you to scale automatically without any cap across locations, clouds and environments. This enables you to reduce management overhead and ensure your configuration remains consistent across your WAF instances for the specified application. Pay-what-you-use pricing model With a SaaS pricing model, customers save an average 30% on Cloud spend due to real-time auto-scaling and a lightweight footprint. Additionally, multi-cloud support allows you to automatically move capacity over to the lowest cost Cloud provider. The Snapt Nova WAF can be added to your application security perimeter with Section within minutes. Contact us to find out more about how to benefit from enterprise-grade edge security, or get started now."
"60","2020-07-01","2023-03-24","https://www.section.io/blog/securing-distributed-edge-network/","Security for edge computing is ipso facto a large and complicated topic. In our previous post, we looked at challenges specific to security at the edge. In this one, we’ll take a look at some of the ways in which the edge can be secured. A Kollective Distributed Devices report highlighted in TechRepublic recently showed that two-thirds of IT teams see edge computing as a threat to their organizations. Just over half of respondents said they expect to encounter challenges in ensuring complete security across all edge devices. What’s involved in edge security? There are multiple components involved in edge security at all levels of the edge continuum, including those listed below. Perimeter risk management As application architectures are becoming more distributed, the attack surface is growing. Millions of devices with a wide range of operating systems and update schedules are being brought into the enterprise, and workplace IT organizations need robust perimeter risk management strategies to secure them. These include: Web Application Firewalls (WAFs) WAFs block certain kinds of network traffic and allow legitimate traffic through. This prevents potential attackers from being able to communicate with your applications and services, thus preventing many types of security exploits. There are various ways to sort traffic into legitimate or unsafe categories. One way is through layer 3 firewalls, also known as network firewalls, which filter traffic based on the TCP/IP stack. Another approach involves layer 7, the application layer. This approach allows you to filter traffic based on the application or application service that the traffic is trying to reach, and the specific contents of that traffic. Intelligent WAFs automatically block threats based on your application’s unique threat profile. Encrypted tunnels Virtual Private Networks (VPNs) have become highly popular for enabling geo-blocking on websites and services, and bypassing government censorship without giving away who is doing the bypassing. A VPN does this by creating a tunnel between the end user and the Internet encrypting the Internet connection. In the instance of Stunnel, the most commonly used tool for encapsulating arbitrary data in an encrypted tunnel, OpenSSL is used to create an encrypted tunnel. SSL stands for Secure Sockets Layer, which is the same encryption used to encrypt web pages. Access control (virtual and physical) It’s essential to use access control to: Authenticate individuals to ensure they are who they say they are. Authorize individuals to access only the information they need to view and use within a company. At a high-level, access control involves restricting access to data through authentication and authorization. As all devices enter and exit the network, they must be subject to access control to ensure they can be trusted. Threat detection It’s important to use proactive threat detection technologies to detect threats early and thereby mitigate damage. Using monitoring tools to proactively run tests on your networks and endpoints means threats can be identified before they become full attacks or data breaches. Cybersecurity monitoring can detect a wider range of threats, improve visibility into threat risks, provide reports on suspicious activity when it is still low level and significantly bring down incident response time. Proactive security measures can help prevent attacks or decrease the damage when one does occur. Threat detection needs to involve protection against both known and unknown vulnerabilities. Application security Applications running at the edge need to be secured beyond the network layer for threats such as account takeover, OWASP injection attacks, API/feature abuse, bat bots, etc. This requires layer 7 protection. Ever since HTTP has become the universal app protocol, attackers have become more likely to scan for and exploit weaknesses within the app layer. The application layer is the closest layer to the end user and the user edge, meaning it provides hackers with the largest threat surface. Automating Updates and Patches Keeping devices up to date through automated patching is crucial for reducing the potential attack surface. You can often avoid data breaches by ensuring that patching of security holes is performed automatically. Many of the most harmful malware attacks leverage software vulnerabilities in common applications, such as browsers and operating systems. These kinds of programs require regular updates to keep them safe and stable. Summary: 5 Edge Security Solutions Adopt a Zero Trust Security Posture A high trust security posture was the norm in On Prem traditional data center settings whereas computing at the edge requires a low to zero trust security posture, similar to the cloud. Security capabilities need to be extended to all edge devices. According to Gartner, “enterprises need to develop defense in depth and manage edge computing stacks that must be assumed to be compromised - software and data.” An edge security strategy must also protect all network communications to/from the edge and ensure a secure software updates schedule. Another aspect to adopting a zero trust security posture is to centralize your secrets in a KV secrets engine. Access Control Establish access control for edge device authentication and trust assurance in order to protect the data analyzed and stored at the edge, including privacy and compliance. Each edge device must have a linked identity that is provisioned and can be clearly managed and secured. By establishing a trusted network of devices and data at the edge, the security of data can be more easily handled. Utilize AI Solutions AI is another way that the edge and its data can be secured. AI systems can be programmed with trusted historical data allowing them to continuously scan new information against the gathered historical data to find anomalies that may signal an intrusion. AI is able to analyze the massive quantities of data generated at the edge, helping speed up response times and support security operations. Minimize the Attack Surface It is necessary to take steps to minimize the attack surface as much as possible by ensuring that edge computing, hardware, software, applications, data and networking have security and self-protection built-in as part of the design process. This is true for the prevention of both virtual attacks and physical tampering and theft. Encryption According to Dave McJannet, CEO of HashiCorp, “If you can centralize secrets and credential management and you can encrypt all data in rest and in-flight in this cloud world, you’ve gone 99% of the way to addressing the security challenge.” For edge computing sites where the physical perimeter cannot be guaranteed, encryption of all data, whether in transit or at rest, is more likely to keep it safe even when the network is intercepted."
"61","2020-06-23","2023-03-24","https://www.section.io/blog/security-challenges-network-edge/","Whether data is at rest or in transit, protecting data is essential. Many of the security challenges faced by edge computing are shared with cloud computing, but there are several edge-specific security considerations that have emerged or heightened in the new edge computing paradigm. According to State of the Edge’s Data at the Edge report, there will be 175 zettabytes of data generated by 2025, a tenfold increase from 2016. Managing this enormous volume of data will be one of the key drivers of distributed architecture. As the scale of edge computing grows, security challenges specific to the edge need to be understood and tackled head-on. This is important for all areas of data - whether it’s payment information collected on point-of-sale devices at the edge or cameras used in public safety projects to examine aging infrastructure, we must institute rigorous security policies. Edge security challenges Security challenges specific to the edge include: An enlarged attack surface By definition, data at the edge is highly distributed. The scale of distributed computing and storage that edge computing requires is immense. Data for one application alone can be spread across dozens or hundreds of sites or nodes. Edge security practices also need to take into account the huge diversity of edge computing nodes and devices, and a flexible approach needs to be developed that adapts as needed within manageable guidelines. Beyond traditional information security visibility New security challenges exist around the fact that processing and storage at the edge typically exist outside of traditional information security visibility and control. Living strategic plans need to be developed beyond traditional data center security practices to include heterogeneous mobile and Internet of Things (IoT) computing security. A physical threat as well as a virtual one The smaller scale and diversity of physical locations means edge computing locations are more prone to physical tampering and theft. Remote edge locations typically have no IT staff, so this must further be factored into security and management strategy. This makes multiple layers of security even more important, such as encryption and multi-factor authentication. Limited compute capacity, depending on the edge device type Many edge devices, in particular IoT devices, have limited compute capacity, requiring a flexible approach to security. Minimum viable protection must be enabled by default. Many IoT devices never have their factory default or static username and password combinations changed by their users. We’ve seen the damage this can cause with botnets like the Mirai in 2016. Default passwords must be changed and control maintained through a centralized management dashboard that controls how devices interact with the computing environment. Connectivity challenges A base of constant network connectivity can’t be assumed for edge devices. Security controls need to continue to provide protection even if the edge system is disconnected from the management console, whether intermittently or for consistent periods. Companies can also reduce risks by not allowing direct connections between edge devices and the cloud except if they are essential for performing critical functions. Security practices and challenges differ along the edge continuum Security practices need to be implemented differently along the edge continuum. Specific approaches can be adopted within each tier in order to factor in important differences in the compute footprint, deployment scale and connectivity reliability, along with physical and network security challenges. The three main tiers within the edge continuum to consider as part of an edge security strategy are: On-Prem Data Center Edge (at the upper end of the user edge tier) This tier refers to server-class infrastructure situated within traditional, physically secure data centers. While considerably smaller, security tools in these kinds of settings are largely the same as those used in the cloud data center. However, some difference of approach is necessary due to the smaller scale and to support the coordination of Kubernetes clusters distributed across edge data center locations. On-prem data centers are typically more secure than smart device or constrained device edges, which tend to be deployed in semi-secure to easily accessible locations in the field. Smart Device Edge (the middle of the user edge) This tier comprises hardware (from consumer mobile devices and laptops to servers specifically for IIoT use cases, such as factory floors) situated outside physically-secure data centers, yet still able to support virtualization and/or containerization. Smart Device Edge IoT and compute resources can usually support robust security features, such as data encryption and multi-device authentication. Constrained Device Edge (the lowest extreme of the user edge) This tier refers to microcontroller-based devices that are highly distributed, such as sensors or actuators that perform little or no localized compute - all the way up to more capable devices designed to address time and safety-critical applications, such as Programmable-logic Controllers (PLCs). Constrained Device Edge resources often depend on upstream more capable devices for additional security measures. Often IIoT has isolated devices in this bracket; in order to drive new outcomes, it is important to connect them to networked intelligence. In our next post on security at the edge, we’ll look at what’s involved in edge security and approaches that can be taken to solve some of these complex challenges."
"62","2018-10-15","2023-03-24","https://www.section.io/blog/evolution-cyber-security-edge-compute/","Edge computing promises to improve efficiency when it comes to how data is gathered, processed and analyzed, but it also creates the potential for a whole new threat landscape. IoT devices are notorious for their security loopholes and as users rush to get ahead of the curve and implement compute at the edge, it is quickly becoming a new “front line” in the cybersecurity battle. At the same time, edge compute architecture offers new opportunities to implement more sophisticated security layers to protect against threats. As organizations try to stay ahead of the evolving technology paradigm, it is essential to build an appropriate edge cybersecurity strategy in response. Providing Protection At The Edge Edge security places a gateway between devices at the edge and the rest of a company’s computing resources. Workloads performed at the edge need to be built with the same level of protection that is traditionally done in data centers. This includes checkpoints such as identity management, data encryption, zero trust networks and patch management. Additionally, edge computing should include a disaster recovery (DR) plan. “Organizations lag when it comes to updating DR plans for edge computing,” says Dan Olds, partner at research firm OrionX Network. Some IoT devices have known vulnerabilities because the system installs are handled by end-users or perhaps run on uncommon operating systems. If these devices are mission-critical systems, edge computing provides a layer of protection and having a DR plan ensures that if breaches happen, risk is being managed. Meeting Threats Upstream One of the benefits of edge computing is that it allows you to scale with a workload regardless of location or size. As strains are put on the network, edge computing can scale up and down with demand. Not only does this ensure intelligent traffic steering that provides users with a high-quality experience, but it offers the security advantage of giving enterprises visibility up to the edge, meaning all potential threats can be met upstream where the trusted and untrusted zones meet. Many Assets Means Many Vulnerabilities Part of the challenge of securing all the devices on a network is knowing exactly what systems are on it. More non-IT personnel are managing connected devices, and in an effort to deploy them quickly, they can unintentionally bypass IT and the processes necessary to keep those devices secure. Mike Raggo, CSO at 802Secure, says, “If you’re responsible for asset management, you need to account for your IT assets across the company, whether these assets come in through IT or end user areas.” It is not only newer devices which create a challenge, but older ones as well. “The hardest thing about these older systems that have been connected over the past 25 years is that you can’t easily do discovery on them,” says Eddie Habibi, CEO of PAS Global. Through a combination of device detection and an asset management system, assets can be kept track of in order to identify and minimize risks. Since each of these devices presents a risk, cybersecurity at the edge ensures that the whole network is not compromised by a breach in a single device. Identifying all the devices is a crucial first step in providing protection. If you’d like assistance assessing your edge security infrastructure, talk to a Section developer."
"63","2018-12-26","2023-03-24","https://www.section.io/blog/edge-workload-considerations/","As the hype around edge computing continues to mount, engineering teams across the globe are asking themselves how they can achieve lower latency and greater efficiency by migrating more processing to the edge. From retail environments and fast food chains, to wind energy and autonomous vehicles, the use cases for edge computing are seemingly endless. For developers building and operating in these increasingly distributed systems, there are some fundamental workload considerations to keep in mind at the edge. Edge Workload Categories There are two main types of workloads for developers to consider when it comes to edge computing: out-of-band and inline (or in-band). The more straightforward of the two are out-of-band workloads, which could also be categorized as synchronous or transactional. Within this kind of workload, a client issues a request and the system issues a block on the response, such as in the case of static file delivery. Inline workloads are significantly more complex. Also thought of as asynchronous or non-transactional, these types of edge workloads contain custom logic to handle processing immediately upon ingestion of data, rather than sending it back to a centralized infrastructure to be processed. When inline workloads are introduced at the edge, the entire computing model changes. Excitingly, this is where some of the most promising (and challenging!) possibilities for edge computing lie. Edge Workload Components From a developer’s perspective, there are several components that guide decisions when approaching edge workload logic. Web Servers To date, load balancing and reverse proxies have been the primary distribution methods for legacy CDNs. However, as edge computing enters into the mix and workloads become more complex, software architects are increasingly turning to networks of containerized microservices to deliver more flexibility and scalability. Alternative Triggers ‘Serverless functions’ have seen increased adoption as a method of running logic closer to the end user. When serverless functions are combined with edge cron jobs that gather and send only the essential data back to the centralized infrastructure, this is where edge computing starts to take shape. As market demands continue to drive the need for increasingly specialized infrastructure, developers are now turning to a ’serverless for containers’ model to run their containerized microservices at the edge without the hassle of managing the allocation and provisioning of servers in close proximity to users. State Management Currently, there are several different state management models that people refer to in relation to the edge: ephemeral, persistent, and distributed. Distributed state management is the most noteworthy and challenging model for edge computing. Consider this common use case for distributed state at the edge with web application firewalls, in which security administrators strive to block traffic at each endpoint and as soon as one endpoint detects malicious traffic, the other endpoints need to concurrently know about it. Managing, coordinating and synchronizing data over a range of edge locations or nodes is difficult; some have even defined edge computing as “a distributed data problem”. When state is local (such as when an IoT device manages its own state) or it is stateless, edge computing is fairly straightforward. Likewise, when stateful computing is centralized in a cloud data center, it is relatively simple. However, attempting to perform stateful computing at the network or infrastructure edge is not an easy task. Managing and coordinating state across a range of edge endpoints with the guarantee of consistency is difficult. Messaging Every edge workload must be built to receive messages through low latency global message delivery, and API extensibility is critical for scalability. Observability When building distributed systems, traceability across the entire stack is essential. Developers and operators need effective mechanisms to be able to identify and diagnose issues when they occur and determine where they can make changes to optimize performance. Needless to say, edge computing is evolving in an increasingly complex landscape, and the considerations outlined in this article are just the tip of the iceberg. As an edge compute platform, the tooling that Section provides to empower developers at the edge is intent on solving for these challenges, opportunities and beyond."
"64","2018-12-17","2023-03-24","https://www.section.io/blog/dynamic-scheduling-edge-computing/","One of the most important topics in relation to edge computing is scheduling. In the future, it is perfectly feasible that every 5G base station will have a data center at its base. While these edge data centers will enable a massive amount of compute, there won’t be enough to run every application in the world at every tower at once. We need a system that is capable of optimizing workload scheduling to run in the right place at the right time. This is a hugely challenging problem that needs our attention. The types of edge workload scheduling models break down as follows: Static Scheduling: As we see in today’s Content Delivery Networks (CDNs), static scheduling is relatively straightforward with pre-set locations and predetermined configurations; Dynamic Scheduling: Scheduling that is latency or volume driven. This is where the most opportunity exists in relation to edge computing; Enforcement Scheduling: This involves circumstantial scheduling, such as in the case of location or data protection requirements (e.g. GDPR, PCI Compliance). Dynamic Scheduling Dynamic, or demand-based scheduling, in relation to latency or volume, presents the biggest challenges (and brightest opportunities) when it comes to the various types of edge workload scheduling. There are several factors that drive dynamic scheduling. For instance, a developer determines that certain responses should have specific latency thresholds, such as, “Geography isn’t important, I just want to be within 10 milliseconds of my users.” The same request in terms of volume might look something like, “I’m getting lots of traffic from Europe and while I don’t typically have traffic there, I want the system to detect a certain set of special circumstances and trigger the launch of various components in the right geographies within Europe.” Edge Workload Scheduling Challenges Scheduling offloaded service requests to cloud computing can exert a considerable drain on networks. When many service requests are offloaded, it becomes important to work out how to schedule service requests between different edge compute stations so that performance is guaranteed, but costs are kept to a minimum. Compromise between the two is inevitable in finding the optimum balance between value and performance. Scheduling becomes even more complicated when we take into consideration some of the complex dynamics of the edge. For example, in the case of the device edge, as terminal devices move locations and the service environment accordingly changes across time, working out how to make coherent and consistent dynamic scheduling decisions in line with the uncertainty of request patterns and the shifting environment is considerably challenging. Furthermore, what has been described as the Fourth Industrial Revolution is already upon us. By 2020, experts have predicted there will be up to 50 billion IP-enabled IoT devices alone. This is contributing to a massive explosion in data and data center traffic. As the number of terminal edge devices and mobile services grows, the service request scheduling problem becomes yet more complicated. Another major challenge with dynamic scheduling is the associated cost of startup and teardown. To better illustrate this concept, consider this race car scenario. Prior to competition, race cars are transported to tracks by (relatively) slow-moving trucks. Once the car arrives, there is an entire crew that prepares the car for competition. When the light turns green at the start of the race, the car goes fast. When it comes to dynamic scheduling, we need to build strategies to expedite the setup and teardown so that it is instantaneous… like Star Trek, beaming the race car into the track at the precise moment the light turns green, and beaming it out once the checkered flag falls. Traditional approaches to optimization born out of centralized systems, such as dynamic programming and combination optimization, will become more problematic, requiring high-complexity solutions that lead to problematic and long execution periods. Potential Solutions One of the more established potential solutions already in play is the Kubernetes Horizontal Pod Autoscaler. In the context of Kubernetes, there are two things you typically want to scale as a user: pods and nodes. The Horizontal Pod Autoscaler automatically scales the right number of pods in a replication controller, deployment or replica set as determined by observed CPU utilization, or other application-provided metrics with custom metrics support. The decision of when to scale is based on continuously measuring a preset metric and the moment it crosses a set threshold. An example: you want to measure the average CPU consumption of your pods and trigger a scale operation once your CPU consumption surpasses 80%, however, one metric doesn’t fit all types of application so the metric might vary. For a message queue, for instance, the number of messages existing in the waiting state might fit the metric; however, for a memory intensive application, memory consumption might fit the metric, so the percentage will vary across application type. Scaling down when the workload usage drops can also be implemented without causing disruption to the processing of existing requests. Another solution being explored is the creation of new algorithms, such as the Dynamic Service Request Scheduling (DSRS) algorithm created by engineers at the Beijing Information Science and Technology University, which aims to make decisions around request scheduling that optimize cost while guaranteeing a certain level of performance. They claim, “the DSRS algorithm can be implemented in an online and distributed way”. Conclusion Although the compute capacity will be enormous, edge data centers are not equipped to run all of the world’s applications at all times; nor is it economically viable for businesses to consider running all servers at all times. Therefore, whoever works out the most efficient solution for running workloads in the right place at the right time will truly enable the next wave of edge computing."
"65","2018-12-14","2023-03-24","https://www.section.io/blog/hardware-overshadowing-software-edge-computing/","Edge computing may soon make it possible to create a completely new generation of consumer and industrial applications. Those applications will benefit from lower latency and improved performance because of the proximity of processing near the end device. It is predicted that edge computing use cases will demand round-trip processing times of better than 10ms; that’s compared to around 40ms by traditional CDNs. While there is currently a lot of momentum around the topic of edge computing, one of the most striking observations from our team has been a significant lack of software-related conversations. For example, in a session at Edge Congress in Austin this fall, the speaker polled the crowd to find out its industry make-up. First, the people who represented data centers were asked to raise their hands - about half the audience complied; then those representing telcos were asked to make themselves known; the other half of the room raised its hands. Finally, the speaker asked who was representing software; in a session with over 200 participants, there were only 3-4 people (including Section’s two co-founders). Telcos and data center providers are increasingly embracing the market opportunities presented by edge computing, and are racing to deploy thousands more data centers in smaller more localized settings, such as at the base of cell towers. However, while they prepare to serve the needs of a new class of edge compute applications, not enough people are talking about how to develop software that is capable of running at scale across a manifold number of locations on this new distributed infrastructure. Software Developers are the Catalyst for Edge Computing Without software, the edge is simply computers. To assist in making this complex landscape useful to developers, facilitating the right kind of software and software development is essential, as is the construction of abstractions and systems that give developers the capacity to confidently interact with the edge in whatever way their application demands. Zac Smith, CEO of bare-metal cloud provider Packet (who we work with as an infrastructure partner), recently told The Internet of Business his philosophy regarding the need for a new approach to infrastructure: “We believe this shift requires a different approach to infrastructure – a more fundamental, agnostic one that can embrace variety and rapid changes in hardware. That’s why Packet’s technology focuses on automating at the lowest layer. We can automate nearly anything, and make it consumable to a developer or to her software at global scale.” We share a similar philosophy in our approach to the software we provide at Section through our Edge Compute Platform. Unlike a CDN that dictates what software a company is able to run in its data centers, Section provides a modular stack that developers can tailor for specific use cases. In doing so, developers have the control they need to be able to run any workload, anywhere. Yes, infrastructure is necessary. But in order to realize the full potential of edge computing, we need to not lose sight of the software developers who control the workloads that will make it a reality. For additional reading on this topic, check out What the Edge Means for Developers"
"66","2018-10-23","2023-03-24","https://www.section.io/blog/edge-computing-for-developers/","The ‘edge’ is dominating many conversations across the technology landscape today. 75% of enterprise-generated data will be created and processed outside of centralized cloud data centers and relocated to the edge by 2022, according to Gartner. However, beyond its potential for disrupting traditional cloud infrastructure, what does edge computing tactically mean for developers who are responsible for the day-in-day-out management of increasingly distributed web application architecture? The Benefits of Edge Computing for Developers The tactical benefits of edge computing for developers are manifold, including: Speed/Performance – Perhaps the most fundamental advantage of edge computing is the vast improvements in processing speed that can be achieved; by moving workloads closer to the end user, data doesn’t have to travel all the way back to the centralized infrastructure, thereby reducing latency. Security – One of the most touted benefits of edge computing is the potential for improved security on various fronts, including: sensitive data being processed close to the end-user or end-device, as opposed to being sent across the network to a data center; authentication and validation of the identities of end-users is an ideal task for the edge, as is the enforcement of API routing policies to ensure that end-user traffic gets to the right cloud environment Points of Presence – With edge compute, significantly smaller amounts of data are sent to the cloud or data center for processing, reducing total traffic load and shrinking the application attack surface. Content Delivery – Content that requires extremely low latency, such as gaming, VR and AR is particularly suited to delivery at the edge; similarly autonomous vehicles that require real-time data transmission of extremely large quantities will only be able to be realized at the scale their vendors imagine if new forms of computing, including edge compute and 5G are deployed. Personalization – At the edge, content can be easily personalized at an individual level through leveraging users’ dynamic content-based profiles to tailor each web experience. Monitoring – Critical infrastructure like oil and gas utilities are prime candidates for edge computing; safety monitoring in IoT devices such as pressure sensors or internet protocol (IP) cameras at the edge are increasingly being used to safeguard against problems. Edge computing enables the processing and analysis of data in real-time, sending control centers information as it happens so that DevOps teams can anticipate and stop any incidents before they occur. The Need for Flexibility A multi-cloud strategy can be the most effective way to deputize edge computing and benefit from a cheaper, more flexible and secure computing framework. The edge can be most effective in an adjunct role to the public cloud or data center, benefitting from those microservices that are latency-sensitive, such as identity enforcement/validation that SaaS applications can run at the edge for an optimized end-user experience. Each application has its own requirements for data processing and application delivery optimization. Above all, developers need flexible tools that can adapt to any system architecture. DevOps Principles & Selecting the Right Edge Platform The developers have been somewhat forgotten about in the conversation around the edge even though they are the ones on the front lines of the paradigm shift. Solution providers who are touting their expansion into edge computing are not necessarily backing it up with developer-centric offerings. In order to implement an effective edge compute infrastructure, it’s vital for developers to be able to extend the same Development - Staging - Production life cycles that they are accustomed to, extending the same DevOps principles that underpin their core application architecture. Again, flexibility is key. The Section Edge Compute Platform emerged out of the need for a more flexible CDN solution and has evolved into a comprehensive edge compute platform that extends far beyond content delivery and caching. Guided by a ‘for-developers-by-developers’ manifesto, we believe that the only way to move technology forward is to grant full transparency and control for developers to run any workload, anywhere. With Section’s modular Edge Compute Platform, developers have the freedom to select their own technology stack based on the specific needs of their applications and work in feature branches, modifying and testing in development and staging environments (without costly risks) before merging the branch back into production. The Section platform can be fully integrated with normal application development workflow and allows developers to leverage container orchestration to instantly effect immediate global configuration changes at the edge. Built on the Kubernetes system, Section’s Edge Compute Platform provides unmatched flexibility and scalability for engineers to develop against a distributed edge with comfort and confidence. As the demand for edge computing continues to expand, Section aims to empower developers to push the boundaries of the computing landscape by providing the tools to build and run edge workloads on their own terms."
"67","2018-07-19","2023-03-24","https://www.section.io/blog/cdns-and-devops/","A review of some of the CDNs available in the market today and commentary on their readiness for DevOps centric entineering teams. Overview of Ten CDNs Akamai Akamai is one of the world’s leading distributed computing platforms and generally considered to be the world’s largest CDN, serving between 15-30% of all web traffic. Its global network numbers over 240,000 servers in more than 130 countries. In addition to its CDN, media and software delivery services, Akamai has worked on building up a cybersecurity feature set. Its customers include FOX Sports, Honda Motor Co, NASDAQ OMX Group and Thomson Reuters. It was founded in 1998 by Daniel Lewin, a graduate MIT student and Tom Leighton, an MIT applied math professor and is still headquartered in Cambridge, Mass. Akamai is currently undergoing management changes with Elliott Management acquiring a 6.5% stake in the company last year. Amazon CloudFront Amazon CloudFront is the CDN side of Amazon Web Services (AWS) and offers a set of features (including website acceleration, video streaming, content download and static or dynamic content caching), along with multiple customizable options. CloudFront operates on a pay-as-you-go basis. CloudFront has servers in Europe, Asia, Australia, South America and several major cities in the U.S. It also operates 107 edge locations across five continents, giving CloudFront the capacity to offer web videos and other high bandwidth content locally to its consumers at an improved latency. Cloudflare Cloudflare was founded by Lee Holloway, Michelle Zatlyn and CEO Matthew Prince in 2009. Having started by launching a free plan for very small websites, it is now the most popular content delivery network provider globally, based on unique visitors received (6,300,000). Its DNS users number over 6 million. Nearly 10% of all requests for web pages go through Cloudflare’s global network of servers. In addition to CDN and web optimization services, Cloudflare offers DNS services and security offerings, including DDoS mitigation. Cloudflare’s headquarters are in San Francisco with four additional U.S. offices and two international ones (Singapore & London). Cloudflare has attracted media attention around its position on freedom of speech and its hosting of controversial websites. Fastly Fastly is a fairly recent entrant into the CDN landscape. Its “edge cloud platform” offers CDN services, security offerings, video and streaming, in addition to load balancing services. They built differentiation in the market through highlighting the ability to clear cache quickly and leverage the programmability of Varnish Cache (version 2.1). Over 3 billion people globally are served content via Fastly each month. Its HQ are in San Francisco, California with five additional offices (three in the U.S. and two in London and Tokyo). Fastly’s CDN customers include Reddit and Spotify. Imperva Incapsula Incapsula was founded in 2009 and originally operated under the cybersecurity company Imperva; security remains one of its key offerings with the company defining itself as “a security CDN”. That same year, Incapsula was spun out into its own company then brought back into the Imperva fold in 2014 where it remains as a product line within the parent company. Imperva Incapsula is a cloud-based application delivery platform that uses a worldwide CDN to offer application delivery, content caching, load balancing and failover services, in addition to web application security and DDoS mitigation. Its HQ are in Redwood City, CA. Level 3 (Now CenturyLink) The multinational CDN, telecom and ISP company Level 3 was acquired by CenturyLink in 2017 with its CEO Jeff Storey becoming CEO of CenturyLink. CenturyLink has kept the name Level 3 and continues to offer content delivery, core transport, IP, voice and video services for medium-to-large Internet carriers across the U.S., Latin America, Europe and in certain cities in Asia. Level 3 is a significant player as a music and video content provider, delivering Netflix and Apple’s services. Limelight Networks Limelight Networks is a Tempe, Arizona based company that offers global CDN services, particularly focused on helping companies securely deliver digital content, including live and on-demand videos, online gaming and operating system updates. Its network has over 80 PoP and delivers between 40-80 petabytes of data on a daily basis. The company distinguishes itself as a CDN by virtue of having its own private network. Limelight’s private network includes the lease of dark fibre and operation of DWDM hardware. The company recently announced a partnership with Chinese CDN Tencent Cloud, giving the two companies’ reciprocal rights to offer their services to each others’ customer bases, opening up a route into China for Limelight. Microsoft Azure CDN Microsoft Azure CDN is a customizable CDN for users looking for integration with Microsoft Azure’s larger stack of cloud tools for building, testing, deploying and managing applications and services through Microsoft’s global network of data centers. Initially, Azure didn’t offer use of Microsoft’s own servers; however, in May 2018, Microsoft launched itself as a provider within Azure CDN, enabling Azure customers to use and deliver content from Microsoft’s own global CDN network. Various new offerings were launched simultaneously, including new regional caching capabilities and use-your-own-SSL certificates. StackPath StackPath launched a couple of years ago billing itself as a Security-as-a-Service (Saas) firm, offering a range of edge services on a platform built for cloud scale. StackPath’s CDN offerings are built on those of companies it acquired - MaxCDN and Highwinds. Its current focus is on North America and Europe, and StackPath has a high density of PoP in both continents with multiple tier-1 carriers; it is beginning to grow PoP elsewhere, including three sites in Asia, one in South America (Sao Paolo) and another in Sydney. Additional features include built-in DNS, quality of service (QoS) control mechanisms, a free shared SSL (or the option to use your own SSL certificate) plus anti-DDoS mitigation and WAF services. Verizon Verizon (formerly Edgecast Networks) is a self-provisioning CDN favored by the telecom and hosting industries. Verizon Digital Media Services acquired Edgecast in 2013 and the company still uses Edgecast CDN as its name. The Edgecast CDN offers a range of features and capabilities, including static caching and dynamic content acceleration, security features (including WAF, DDoS protection, bot mitigation and DNS resolutions), global scalability, a self-service real-time analytics portal, APIs and rules engine, and optimized delivery for video streaming and mobile devices. How Ready for DevOps is the CDN Market? According to DORA & Puppet’s 2017 State of DevOps Report, enterprises that employ DevOps practices deploy code up to 30 times more frequently than their counterparts and 50% fewer of their deployments fail. Engineers following DevOps principles are expected to take both a holistic view of operations and engage in the day-to-day operation of software in order to understand how the software being deployed by an IT team is functioning across a diverse set of operating systems and platforms, and make small and large-scale system changes when required. As DevOps focused teams are at least partly responsible for the overall performance of the organization they work for, it is increasingly important for these engineers to have choices and flexibility over what kinds of systems and services are deployed company-wide; and even more important for all engineers to have control and transparency when it comes to the systems chosen. The rise in DevOps is partly linked to the rise in computing companies that are offering modular and containerized services, in which stacks are disaggregated, allowing services to be developed and released independently of one another. DevOps practices are often found within companies finding room for more loosely coupled teams - the aim being to empower teams and team members to make their own choices within the larger framework of the company. As developers exert more control and influence over IT provisioning and operations, CDNs are under increasing amounts of pressure to expose their services in a similar way to other cloud services, giving DevOps greater autonomy. Customer demand for continuous delivery is also fundamentally changing what companies expect from their IT providers. Certainly from a CDN perspective, DevOps centric engineering teams need more control, transparency and visibility than traditional legacy CDNs can offer. Some of the major CDNs like Akamai, Cloudflare and Fastly are starting to pay attention to the DevOps movement and are attempting to alter their services accordingly to make them more attractive to DevOps teams. Fastly, for instance, has increased the DevOps friendliness of its CDN by bringing an increased level of programmability to CDN software by using Varnish Cache and leveraging the benefits of Varnish Cache Configuration Language. Fastly has also improved the speed of deploy of configuration changes so that engineering teams do not have to wait so long for configuration changes to propagate, or suffer from mixed configuration states while in transition. Meanwhile, Akamai is seeking to simplify the alteration of applications in production and expanding the number of APIs that it exposes via toolsets for a host of different frameworks, including Varnish Cache, Terraform and WordPress. The aim is to grow the level of scripting and automation currently offered by Akamai’s CDN. However, even Akamai still lags well behind the DevOps movement in terms of the flexibility, control and transparency that modern DevOps centric engineering teams truly require to do their best work. When it comes to an Edge Compute Platform, DevOps centric engineering teams should have: Configuration as code Instant global configuration change propagation and cache clear Real time diagnostics and insights Complete integration with a normal application development workflow Choice of software to deploy and upgrade path control Section and Our Unique Place in the CDN Market Section is the only provider in the market to offer a modular Edge Compute Platform, which is built from the ground up to align with modern DevOps practices. Section was founded with the goal of creating a DevOps-centric platform, giving engineers control and flexibility over their Edge Compute Platform. By leveraging container technology (Docker and Kubernetes at the core) architecturally, Section is fundamentally different to all other CDN and Application Delivery Controller solutions in the market. Section is the only provider that can deliver against each of the DevOps system requirements: Configuration as code Work in Varnish Cache Configuration Language, LUA, ModSecurity Files etc. Code configuration is backed with a Git repository. Instant global configuration change propagation and cache clear Sub 200ms Cache clear globally, immediate global configuration change propagation. Real time diagnostics and insights See your enriched logs and metrics in real time in the Section portal. Query the logs in the Section portal itself for deep, immediate insights. Complete integration with normal application development workflow Run Section in every development environment by cloning a git branch (Dev PoP). Work in feature branches, change and test in dev, test and staging before merging the branch back into production. Experiment, test and make mistakes in Dev - not in production. Choice of software to deploy and upgrade path control The opportunity to choose between three different WAF modules (the rules-based WAF ModSecurity vs. the AI-driven Threat X and Signal Sciences) gives DevOps engineers an unparalleled level of flexibility to choose the right WAF for their enterprise needs; and couple it with their choice of other security offerings. Unlike the other CDNs, Section doesn’t require you to just choose one stack, but instead allows you to pick and choose the modules that make sense for your business needs. Further modules available on Section include Image Optimization modules, server-side multivariate testing, Google’s pageSpeed, OpenResty and Varnish Cache. New modules are regularly added. All modules have consistent configuration-as-code and diagnostics capabilities. Another important part of Section’s flexible approach is the fact that customers can pick where Section runs. Our container-based approach to HTTP traffic delivery means that our HTTP traffic control modules can be run in a distributed fashion either as a CDN, and/or behind the firewall as an Application Delivery Controller. The transparency and flexibility that Section is able to provide its customer base is leading to a growing popularity, particularly among global e-commerce stores (Adore Beauty, Esprit, Merrell among them) and large media companies like Universal Music Group. When the Foundry Group recently announced its leading role in Section’s $5.5M Series A funding round, the VC firm commented on Section’s unparalleled levels of flexibility. Foundry Group wrote, “Section fits into our Glue and Protocol themes as their PaaS connects compute infrastructure from multiple service providers for improved HTTP traffic delivery from a true federation of compute providers”; adding, “Stewart and Daniel recognized CDNs were not able to adapt to quickly enough to keep pace with modern software development practices”."
"68","2018-07-09","2023-03-24","https://www.section.io/blog/web-application-firewall-overview/","In this article we will outline what a WAF is, explore types of WAFs currently available and deployment options for those WAFs. What is a WAF? A web application firewall (WAF) is a special type of firewall that is deployed in front of HTTP applications and is designed to protect websites and web applications from popular web exploits that could compromise security, consume excessive resources or take your website offline. According to the Verizon 2017 Data Breach Report, 29.5% of breaches were caused by web application attacks (by far the most common vector). Companies are increasingly in the headlines for being hit by serious data hacks or experiencing DDoS attacks that force their business offline. Taking a proactive stance and deploying a WAF can stop these attacks from taking place and keep brand reputations undamaged. WAFs protect servers. The WAF acts as a reverse proxy forming a proactive layer of protection, sitting between your web application and incoming traffic. The reverse proxy server can either sit behind the firewall in a private network, or in a distributed network such as Content Delivery Networks. New WAF solutions hit the market each year. Choosing the right type of WAF for your organization and deciding whether to deploy in a distributed fashion or behind your firewall depends on the technology that fits best with your application. At Section, we are the only provider to offer the opportunity to choose your security stack modules and where you run them. We have both a rules-based WAF and two different behavioral WAFs in our library of Edge Compute Platform Modules. You can run your choice of WAF in a distributed fashion (on the Section cloud in the same fashion as a CDN), behind your firewall on your infrastructure (in the same fashion as an Application Delivery Controller), or on your custom or private cloud. Whichever solution you select, several benefits are gained by deploying the WAF on our network: (i) additional security via our network layer DDoS protection; (ii) the ability to run your WAF in your development environment for testing; (iii) complete access to DevOps logs and metrics to monitor the behavior of your traffic in real-time. Types of WAF The Traditional Rules Based WAF A traditional WAF works by applying a set of rules to an HTTP conversation to cover common attacks, such as cross-site scripting (XSS) or SQL injection. ModSecurity ModSecurity is our rules-based WAF offering. ModSecurity (sometimes called ModSec) is a veteran open-source cross platform WAF engine. It was developed by Trustwave’s SpiderLabs, which quickly became popular worldwide because of its open source availability and its great flexibility. The company describes itself as the “Swiss Army Knife” of WAFs since its platform is completely customizable. ModSecurity offers the engineer a powerful rules-based language, allowing you to apply rules only where you need to do so. Our GUI (graphical user interface) allows you to alter security settings in order to configure ModSecurity with this level of flexibility. The version of ModSecurity that we offer is the unmodified, open-source version. There are various existing rule sets out there, which can be applied to ModSecurity for those who want more of an “out of the box” solution; for instance, the OWASP ModSecurity Core Rule Set (CRS). The CRS is a set of generic attack detection rules for use with the ModSecurity platform designed to protect web applications from the OWASP Top Ten and other common attack categories. ModSecurity also offers protection for application specific attacks, including Wordpress, media websites, Magento, Magento Enterprise and other ecommerce stores. The ModSecurity WAF allows for HTTP traffic monitoring, logging and real-time analysis with few or no changes to existing infrastructure. After signing up for a Section account, and adding ModSecurity, you can adjust the rule settings and select whether you want to run the rule sets in “Detect” or “Blocking” mode in order to immediately start blocking malicious activity. Other Rules-Based WAF Offerings Various of the big CDNs continue to offer rules-based WAFs, including Akamai, Cloudflare, Fastly and Incapsula. Akamai remains the market leader CDN and security provider in terms of sheer size and revenue. Its Kona Web Application Firewall is deployed at the edge of the client’s network. The level of human expertise constantly monitoring the threat landscape for zero days and new vulnerabilities so that its clients don’t have to is probably Akamai’s greatest strength. Akamai will also customize its rule set according to what each client needs for its individual security posture; although some argue that the customer doesn’t have enough control over its own security postures. Cloudflare is a newcomer by comparison. Its WAF is rules-based and feature-rich to protect against the major attack types. However, the level of granularity offered is meager compared to that of Akamai; and one of the most significant claims against Cloudflare’s WAF is that it allows for too many false positives. Its default rule set is also regularly updated by the security engineering team to ensure that new significant vulnerabilities are accounted for. For any new threats discovered that could affect a large portion of their user base, new WAF rules are applied; perceived smaller threats are left untouched. Customers at the Enterprise level (roughly around $5,000/month +), however, have the ability to import an unlimited number of their own custom rule sets. Fastly was founded only five years ago and offers its WAF as part of its overall cloud security defense provisions. Like Akamai, security rules are enforced at the edge as part of Fastly’s relatively new “edge cloud platform”. Fastly builds third-party rules from the OWASP CRS, commercial sources, and open source available options, in addition to generating its own. Incapsula’s WAF also uses a custom rules engine, which it calls IncapRules. They block critical web application security risks, including the OWASP top 10. The Incapsula WAF is configured to be used out of the box; however, its security team can customize the default rules when requested. As at the other CDNs, the default rule set is regularly updated and new mitigation rules applied. Some users complain that scripting new firewall rules is too complicated and needs to be simplified; and also argue for more custom actions to trigger turning of and off Incapsula settings on different sites and applications. Security controls in an enterprise environment can be challenging to sync up with different teams using different sets of controls. Limitations to Rules-Based WAFs Detractors argue that rules-based WAFs are challenging to manage, disposed towards false positives and ineffectual against zero day attacks. Traditional CDN-based WAFs often offer less granularity than the newer solutions in the market meaning you have to turn on blocking across your entire site rather than being able to adapt security per feature or service. In terms of false positives i.e. clean traffic that is accidentally blocked, a strict rule set like the OWASP ModSecurity CRS can bring an overwhelming number of false positives, which can block good traffic as well as bad. It can take significant tuning to arrive at the right level of alerts. Usually either signatures need to be shrunk to a minimal number, which reduces security coverage, or time and money needs to be spent for identifying and testing new custom rules. In order to guard against both common threats and zero days i.e. the newest vulnerabilities out there, the WAF rule set must be continually monitored and updated in order to ensure protection. Even when careful monitoring occurs, zero day threats are called that precisely because they take place before the software is known to be outdated, or infected. Certainly an open source rules-based WAF like ModSecurity requires a certain amount of technical knowledge to be able to program and configure it; particularly if you are defining your own rule set. At Section, we also see this as an advantage as for those engineers who want that flexibility, ModSecurity provides it. Newer Learning/Behavioral WAFs The next-generation of intelligent Web Application Firewalls have been around for the last few years, and are increasingly popular because of their comprehensiveness, flexibility and reduced volume of false positives. Behavioral WAFs utilize behavioral learning techniques, which allow them to study visitor profiles and determine patterns based on behavior, and accordingly detect and block threats that reveal behavioral anomalies. As application architectures become more complicated, the legacy rules-based CDN WAFs are finding it increasingly difficult to keep up. Furthermore, the adaptive nature of the behavioral WAF means less time and money needs to be spent on in-house security experts. At Section, we currently offer two next-gen behavioral WAFs: Signal Sciences and Threat X. Signal Sciences Signal Sciences has taken a bottoms-up approach to security since its founding in 2014: the Venice, CA-based company was built by engineers and CISOs frustrated by “trying to make legacy WAFs work while embracing DevOps and Cloud”, and has accordingly focused on creating security products that address the challenges they directly experienced. Signal Sciences’ CEO, Andrew Peterson, has led the way in building a next-generation Web Application Firewall (WAF) that provides protection for web applications, APIs and microservices that prides itself on taking a human approach - as in Peterson’s words, “we’ve been in that harrowing security defender’s position”. The Signal Sciences WAF uses a combination of contextual information and cloud analysis to immediately block threats in real-time. Its customers include Yelp, Etsy and Grubhub. As of 2017, they protected 60 billion requests a week. 95% of Signal Sciences’ customers use full blocking mode for their production sites without false positives or the need for detailed tuning. A key part of the technology at Signal Sciences is the ability to provide detailed monitoring to reveal where attackers are focusing their efforts in order to protect the right parts of the infrastructure. Rather than taking the traditional CDN approach to WAFs and deploying them solely at the edge, Signal Sciences (and Section) recognize a WAF should be deployed at the location most convenient for the business – either distributed on the Section cloud or behind the firewall on a Section Origin PoP. An additional benefit of deploying Signal Sciences through Section is that you can run your WAF in your development environment for testing, allowing you to debug failures with confidence and flexibility before deploying to production. Threat X Threat X’s next-gen WAF utilizes a behavioral profiling and correlation engine to analyze attacks and eliminate false positives by grading risk level and progress across the ‘kill-chain’. It learns each site’s unique threat profile and automatically blocks suspicious and malicious traffic while protecting legitimate traffic. Like Signal Sciences, Threat X also covers risk to not just web applications, but APIs and microservices within hybrid cloud environments. The company’s intelligent WAF is backed by a security team who continually monitors the latest vulnerabilities and hacker trends, and makes adjustments accordingly - all aimed at reducing the workload (and associated cost) of its customers’ security teams. Similarly to Signal Sciences, Threat X was founded by engineers motivated by direct experience and frustration with legacy solutions and static signatures. Their goal from the outset was simple: “To reduce costly false positives, minimize the operational burden of maintaining traditional WAFs, and remain agile with the evolving threat landscape.” Adding Threat X to your website via Section is straightforward: you simply do a DNS change to point at the Section cloud and that means you can bring Threat X directly onto your website. The Threat X dashboard suite then offers a comprehensive view of all potential threats ranked by threat level, and demonstrates how Threat X responded to each one. The Denver-based company just raised $8.2M in a Series A funding round and will be further honing its WAF technology. We expect WAF technology to continue to develop rapidly over the next few years. We also expect the security specialist companies such as Threat X and Signal Sciences to build and deliver the most innovative products. Learn More Section will continue to provide flexibility and control for our users. Having the choice of WAF software on the Section platform means our users will not be trapped with aging technology. Having the choice of deploy location means you can decide whether a distributed WAF, a centralized model or a private cloud deploy model is best for your application. To learn more about how to get started with Website Security please download the full Website Security Guide. For more information on how Section can improve the security of your website, please contact us."
"69","2016-11-14","2023-03-24","https://www.section.io/blog/prepare-for-a-magento-security-incident/","A few weeks ago Magento published a very helpful article to their Security Center outlining the steps to follow after suffering a malware attack on your Magento site. It is a great resource to save away in case you ever find yourself in this situation, however, it is also beneficial to understand what exactly is involved so you can prepare now. Reading the article only after being attacked may leave you frustrated that you don’t have the information you need to proceed with confidence. Here are some steps that you can take today so that you’ll be ready. Prepare your website for a Magento security incident Ensure regular backups are being performed for your Magento database and installed files. You should also ensure these backups are tested regularly by restoring them to another server. Ensure you have the latest security patches and extensions installed for your Magento version and establish a process to review this regularly. You can also sign-up to be notified when Magento publishes new security issues. Ensure you have a non-production copy of your Magento deployment where you can test Magento core and extension updates safely before applying the changes to Production. This could be the same server you use to test the restoring of your backups. With Section you can also easily replicate your CDN configuration to your non-production Magento instance too. Review which users can login to Magento Admin and limit their permissions to the minimum they need to perform their job. Also encourage them to use strong passwords that are not shared with other systems and services they may use. Understand which Miscellaneous Scripts should be configured in your store’s HTML design and why they are there. Beyond these steps, you can also configure your Magento deployment to be more resilient to an attack: Protect Magento from password guessing. The whitelisting steps in this article can also be handled by Section so that the access attempts are intercepted before they even reach your origin web servers. Read and apply Magento’s other highly recommended Security Best Practices that can help protect your site from malware, and other attacks. Section also offers a Web Application Firewall service that can provide an extra layer of defence to protect your store from Internet threats. Contact us today to discuss how Section can improve the speed, availability, and security of your Magento store."
"70","2016-10-10","2023-03-24","https://www.section.io/blog/magento-2-differences-varnish-cache/","Magento 2 was released to merchants almost a year ago, and is a complete platform overhaul from Magento 1.x versions, rather than an update. There are several core differences between Magento 1.x and Magento 2 that are important for businesses and Magento developers and businesses to understand. Key Magento 2 Improvements# Codebase: This has been totally refactored with a focus on better performance. The codebase also has significant unit testing coverage which allows code changes to be validated by automated tests as part of the build / release cycle. This improves code quality on an ongoing basis. Database Architecture: Magento has retained the EAV model present in Magento 1 however there have been some optimizations, such as the ability to have 3 separate master databases in Enterprise edition: Checkout, orders, and product data can all each use a separate master database. Extension quality: The world of extensions in Magento 1 was an area fraught with danger. For Magento 2 there are many poorly built extensions that work at low traffic volumes or with a small numbers of product items but then have significant performance issues once traffic or product counts rise. The Magento 2 strategy has been to throw away all existing extensions and define a new process to build and submit a Magento 2 extension that involves rigorous code quality checks before an extension is allowed to be offered in the Magento marketplace. Application design for caching: Magento 2 is designed to work out of the box with Varnish Cache, a lighting fast HTTP acceleration tool. Content cached and served from Varnish Cache is the fastest you can send, and Magento 2 makes it easier to implement Varnish Cache on your website either locally or through a Content Delivery Network. Setting up Varnish Cache with Magento 2# There are several areas to consider when setting up Varnish Cache with Magento 2. They include: Varnish Cache Versions: As of October 2016, Varnish Cache has recently released Version 5. With Magento 2.x, you will want to run Version 4 or newer. Installing locally or on distributed servers: Varnish Cache can be set up on a dedicated server in your hosting infrastructure or through a Content Delivery Network that installs Varnish Cache on globally distributed servers that are closer to your end-users. What content to cache: The goal of optimizing your website with Varnish Cache is to have Varnish Cache serve as much of your website directly from Varnish Cache as possible. This includes images, static files (CSS, JavaScript, etc.) and the actual HTML page itself. This is known as Full Page Caching. When configured correctly, Varnish Cache and Magento 2 work well together and allow you to cache large portions of your website and HTML documents, resulting in a huge performance improvement for Magento websites. However, there are several tricky elements which need to be managed to ensure you are getting the most out of your Varnish Cache setup, including managing cookies in Magento, recognizing partially dynamic pages, and isolating dynamic components on a page. To learn more about setting up Varnish Cache with Magento including recommendations on hosting configurations, Content Delivery Networks, and how to configure Varnish Cache so that it serves as much of your Magento 2 HTML as possible, download our eBook today or contact us to speak with one of our Magento and Varnish Cache experts."
"71","2016-10-04","2023-03-24","https://www.section.io/blog/ebook-optimizing-magento-performance-security-scalability/","Improve your ecommerce site with our eBook on performance and scalability for Magento# In September we became a Magento Select Technology Partner, and for the past few weeks we’ve been hard at work compiling all our Magento knowledge so that merchants who want to improve their Magento site speed and scalability, get information on hosting for Magento, read about Magento cache options and more have a singular resource to turn to. We’re pleased to publish the final product today, a 47-page eBook, “The Definitive Guide to Optimizing your Magento Site for Better Performance and Scalability,"" which is now available to download for free. This guide aims to give an overview on why performance (page speed), scalability (the ability for your site to handle more traffic), and security are important to your Magento site, with examples from both well-known studies and our own A/B tests that demonstrate that any Magento site, no matter the size, can see increased page views and revenue from a better-performing site. We also give recommendations on how to measure and improve your site performance and scalability, including what questions you should be asking around hosting for Magento sites, if you need a Content Delivery Network, and what you need to know about caching in Magento. Learn how to Configure Varnish Cache for Magento The guide also includes instructional chapters on configuring Varnish Cache for your Magento site and how to program Magento 2 for better performance. These sections give your developers solid recommendations on how to improve your Magento site using open source resources they already have access to. We’re excited for Magento merchants to use this eBook as a way to improve their site before the holiday season, and can’t wait to see the performance and scalability optimizations you make. Here’s a sneak peak at the eBook’s contents, or download your free guide on Magento optimization now. Show me the Money: Why performance and scalability are important for your Magento website Data Driven: how to measure your web performance The Host with the Most: Choosing the right Magento hosting for you Getting to Know You: Introduction to Caching for Performance Digging Deeper: How Varnish Cache and Magento Work Together The Value of Your Network: Considering the Benefits of Content Delivery Networks The Meat of It: Programming for Performance on Magento 2 Bonus Security Section: Protecting your Magento Website from Attacks Get Started: Your Customizable Action Plan Section is a website performance, scalability and security tool that makes it easy for Magento developers to improve their website with a quick-install version of Varnish Cache, free SSL certificates, and globally distributed CDN. To start improving your Magento site performance today, sign up for a free trial or contact us."
"72","2016-09-08","2023-03-24","https://www.section.io/blog/sectionio-joins-magento-technology-partner-program/","We are pleased to announce that Section has become a Magento Select Technology Partner, a level of partnership that includes many other top-tier technology solutions designed to help Magento eCommerce customers get the most out of their websites. We’re excited about this partnership because we recognize Magento as the leading eCommerce solution for all kinds of businesses, from small boutiques to large, recognizable brands, and we have already been building our own tools to integrate seamlessly with the Magento system. As we have outlined in previous posts, eCommerce sites can benefit greatly from improved website performance, because so much hangs on a customer successfully completing their transaction on an online shopping site. With faster web pages, customers view more products and ultimately spend more money, resulting in increased eCommerce revenue (check out our eCommerce case study from Adore Beauty, a leading beauty sales Magento site). Magento is already encouraging all of its users to implement Varnish Cache on their Magento site so that images and other static files are cached and delivered more quickly to your customers, and Section makes the Varnish Cache implementation quick, simple, and globally distributed, further improving page load time. In addition, because Section is built to work with modern developer workflows, development teams can cache more content, including full HTML docs for their website, and test it all in a CDN development environment to ensure nothing goes wrong when changes go live. With our Magento partnership, we plan on building more Magento-specific products to optimize Magento websites. We will also continue to publish tips and in-depth articles on how to get the most out of Magento, so keep checking our website and blog for all your Magento performance needs. You can read the full partnership press release, or to get your Magento site started with Section sign up for a free trial of our CDN now."
"73","2017-05-12","2023-03-24","https://www.section.io/blog/magento-performance-magento-varnish-cache/","Magento is a popular ecommerce platform, but it can be slow to load, especially for stores with large databases of products. In their release of Magento 2, Magento has looked to improve performance by creating a system that can be easily integrated with Varnish Cache, a powerful HTTP accelerator and reverse proxy. As more ecommerce shops move to Magento, we’re hearing more questions about how exactly Magento 2 and Varnish Cache work together and how to troubleshoot common Magento-Varnish Cache issues. Varnish Cache and Magento Setup Varnish Cache is a type of reverse proxy, which means it sits in front of the Magento database and intercepts requests going back to Magento. Any requests that can be served from Varnish Cache’s caching server will be served to the user from Varnish Cache rather than going back to Magento or your origin server. If your website uses a load balancer, Varnish Cache will sit behind that. The below illustrates the difference between Magento 1 and Magento 2 with Varnish Cache. Magento 2 Full Page Cache One common mistake when using Magento 2 is believing that Magento 2’s built in “Full Page Cache” feature is sufficient for performance. The Full Page Cache mode is labelled by Magento as to be used in development mode, as developers are not able to see what is being cached and what is not being cached with this setting. In production, it is strongly recommended all Magento 2 stores use Varnish Cache for optimal performance. Magento and AJAX Magento 2 loads content in a way that is very different from Magento 1.x and from many other platforms. This is because Magento 2 heavily relies on AJAX calls for personalization and add to cart functions. AJAX stands for Asynchronous JavaScript And XML and allows webpages or parts of webpages to be updated or customized without being completely reloaded. This means personalization elements such as a user’s account and cart information are loaded after the page has initially loaded via AJAX. For caching, having AJAX calls for personalization instead of having these elements coded into the HTML document means that much more can be cached, including the full HTML document. As we have mentioned before, caching the HTML document (also called “dynamic content”) vastly improves the speed of websites as the HTML document is the first element of a webpage that is requested and loaded. By caching HTML, the origin server does not have to generate this piece of information for every visitor, which also dramatically reduces server load. While using AJAX is an excellent feature for performance, since many Magento developers are not used to inserting personalization in this way it can cause problems. When building features and extensions for Magento 2, developers and extension developers must be aware that account information, personalized recommendations, cart size, and more must be inserted by AJAX and not in the HTML. Installing and Troubleshooting Varnish Cache with Magento 2 Below is a basic guide to installing Varnish Cache for Magento 2 and common issues that arise in deployment. Step 1: Install Varnish Cache: First, install Varnish Cache with the default VCL (Varnish Cache Language) for Magento 2. This default VCL is set to cache both static objects and the HTML document out of the box. Check out the Magento docs for this first step. Step 2: Verify: Check that Varnish Cache is caching HTML correctly. There are many reasons Varnish Cache may not be caching HTML even if you think it is. Rather than checking just what the browser receives, you need to confirm Magento 2 is sending the correct Cache-Control response headers for HTML documents (such as the homepage) to Varnish Cache. Follow the Magento 2 guidelines to confirm appropriate cache control responses. Step 3: TroubleShoot Varnish Cache:### There are several common issues that may arise when first installing Varnish Cache and Magento 2. They are generally found in these areas: Codebase: Check your Magento 2 codebase for layout xml configuration issues. If any block is marked: cacheable=""false” then Magento 2 will not send appropriate cache-control responses. If for example the footer layout block is marked like the above, then no pages will be cached in Varnish Cache To review this scan your codebase for any blocks with cacheable=""false” attribute s Headers: Check Apache / nginx isn’t overwriting cache-control response headers (for text/html documents) Check all Apache configuration files (*.conf, .htaccess etc) Look inside Apache files for the text “Expires” to find and review all usage of mod expires Look inside Apache files for text “Header” to find and review all usage of mod header Check all nginx configuration for settings that overwrite response headers Magento Cache: Verify the Magento Cache Log in to your Magento server, or switch to, the Magento file system owner. Enter the following command: rm -rf /var/page_cache/* Access one or more cacheable Magento pages. Check the var/page_cache/ directory – if it’s empty, the cache is working. Step 4: Add functionality: We recommend adding additional functionality to the default VCL including normalizing query strings and overriding cache control response headers. Details on our VCL recommendations can be found here. For more information on Magento 2 and Varnish Cache, read this community post about how it works in Section. Section has a Magento 2 extension for Varnish Cache that gets you set up on our global delivery network and pulls in the default Magento 2 VCL all within the extension. You can download the extension or contact us with any questions. Get Started Today"
"74","2016-12-09","2023-03-24","https://www.section.io/blog/varnish-cache-magento-configuration-suggestions/","Magento 2, a complete reconfiguration of the popular ecommerce platform, is built for speed and is a vast improvement in website performance over Magento 1.x versions. One of the major differences in Magento 2 is that it is built to integrate with Varnish Cache, an extremely fast http-accelerator that is configurable to cache a variety of content, from static objects such as pictures to Javascript snippets and even full HTML documents. When set up correctly, Varnish Cache can vastly improve the speed of any website, and for ecommerce sites such as those using Magento, an improvement in speed of even 1 second can translate to improved conversion rates, larger cart sizes, and increased revenueb . Varnish Cache is configured using its own language, Varnish Configuration Language or VCL. Magento has provided a default VCL file for Magento 2 users which includes standard instructions to set up caching on a Magento 2 site. This VCL file utilizes Varnish Cache version 4. While this VCL will get Magento 2 sites started with cache setup, Section suggests adding several configurations to allow sites to cache more content and experience faster speeds and a higher cache hit rate. Achieving a higher cache hit rate also reduces load on the website origin server, resulting in lower hosting costs and allowing the origin server to focus on critical applications such as the checkout process. Varnish Cache VCL Additions for Magento Here are Section’s suggested additions to the Magento 2 default VCL file which will give websites a more advanced cache setup. These suggestions can also be utilized with other instances of Varnish Cache not related to Magento, and we would recommend every site using Varnish Cache implement these configurations. VMODs: Varnish Modules or VMODs are extensions which add to what can be configured directly in VCL. The Varnish Cache website keeps a list of VMODs, and you can learn more about how they work to add functionality to Varnish Cache here. Section recommends adding several VMODs: Varnish UUID Module/ import uuid; Allows you to generate unique random ID’s or hashes based on values that you provide. Can be used to build features that require unique IDs or hashing of strings. Varnish GeoIP Lookup Module/ import geoip; This allows you to look up a visitor’s GeoIP address and include instructions that take their location into account. This is especially important for sites with global visitors as it can send them to the correct site (www.mysite.co.uk vs www.mysite.com). You will need a GeoIP library installed to lookup the GeoIP: at Section we are able to gather city-level information. Header VMOD/ import header; This allows you to manipulate requests more than in the default VCL, making it easy to set and handle cookies and group and consolidate headers. This module is now included in a Varnish Cache Module Collection that has been put together - other features in the collection include variable support, advanced cache invalidates and more. See the full collection here. Normalizing query strings When visitors come to your site through an ad, email, or other tracked link the URL they visit will usually have a string attached so that activity can be tracked. For example, if traffic is served from google ads the string will include “gclid=XXX.” If Varnish Cache is not configured correctly, the cache will see each of these as unique URLs which would not be served from the cache, even though the page content is the same. Magento’s default VCL file includes instructions to remove the gclid so URLs with the gclid paramater can be served from cache, however we recommend excluding several other common tracking tags such as utm_source (also Google tracking) and mc_cid (mailchimp email tracking): utm_source|utm_medium|utm_campaign|gclid|cx|ie|cof|siteurl|mc_cid|mc_eid Overriding cache control response headers to improve browser caching When setup correctly, browser and server caches should work together so that when possible content is served from a browser cache which reduces the requests to both your web server and your cache server. For ecommerce visitors who go back to the same site and product pages over and over before making a purchase, the ability to serve product photos and more from a browser cache is very valuable. At Section we add instructions that allow Varnish Cache to see what objects are stored in the browser, which therefore do not need to be fetched from the server cache. The default Magento value for caching content is just a day - our VCL lets you set browser cache times to longer for items such as logo files that are unlikely to be changed each day. An example of how we configure Varnish Cache to cache items for longer can be seen in Github under the vcl_backend_response section. Read more about overriding cache control response headers to cache objects for longer times. Caching API calls Some API calls are cacheable, and we recommend adding configuration that caches these API calls for faster response and offloads repetitive API calls from origin servers. Sharing a cache between two Magento stores: The Magento 2 default VCL specifically prevents users from sharing an object between domain names with the below section. However, sites can share a cache for common objects - for example, all assets under /assets/images/ could be shared on country-specific versions of the same site. If desired, you can override this rule for a specific URL so that one cache can be used for multiple Magento store domains.  if (req.http.host) {
     hash_data(req.http.host);
 } else {
     hash_data(server.ip);
 } ```


### Device detection ###
The Magento default VCL doesn’t include device detection, and assumes that websites are responsive so that they mobile and desktop browser would be served the same page. However, some sites are setup with a separate mobile site such as m.mysite.com, and we recommend setting up VCL so the cache can be split by device type.

## Get started with Varnish Cache for Magento on our CDN ##

Section has a [quick-install version of Varnish Cache for Magento 2](https://marketplace.magento.com/sectionio-metrics.html) available in the Magento Extension Marketplace: this allows Magento users to pull in the Magento 2 default VCL and go live on our global CDN all within the Magento portal. To make the changes detailed above for additional Varnish Cache benefits, users can simply access and edit the VCL file in the Section portal.

To install Varnish Cache for Magento 1, [sign up for a Section account](https://console.section.io/) and [follow these instructions](/docs/magento/#magento-19-and-earlier) or [contact us](/contact-us/) for help.

<a href=""http://get.section.io/magento-guide-performance-speed/"">
<img src=""/assets/images/blog/oct-2016/ebook_cta1.png"" alt=""magento ebook"" style=""width:600px;"">
</a>"
"75","2017-01-06","2023-03-24","https://www.section.io/blog/magento-website-performance-varnish-cache/","Last month we presented a webinar with our partner Magento on website performance and its impact on revenue. Website performance is a crucial element of any website’s success and is particularly important for ecommerce sites who rely on the speed of their website to convert customers, encourage larger cart sizes, and ultimately gain more revenue. Ecommerce platforms including Magento recognize the importance of speed and scalability to their merchants and have been making improvements so they are built for fast pages and reliable performance at any volume of traffic. Magento 2, released in 2015, was built with performance in mind and to integrate with Varnish Cache, one of the best caching solutions available, and many have predicted that ecommerce site speed will become more and more important in 2017. Website speed matters for ecommerce The slides from this webinar can be seen below. In it, we reviewed: The definition of website performance How to measure website performance including metrics and suggested tools How to achieve better website performance, including little wins and big wins Caching and Varnish Cache How Content Delivery Networks work to improve performance Demystifying Website Performance and Its Impact on Revenue from section_io To learn even more about these topics and get started on improving your ecommerce website performance, you can download our guide to website performance and scalability. This guide includes in-depth information on all topics covered in the webinar, along with definitions of terms, a bonus website security section, and a 30-day action plan."
"76","2017-12-07","2023-03-24","https://www.section.io/blog/measuring-varnish-cache-logs-metrics/","So you’ve installed Varnish Cache, now what? How can you tell whether or not it gave you the performance improvement you expected? You’d be surprised how many people get Varnish Cache up and running and end up flying blind. Without proper logs and metrics you have few ways of knowing if Varnish Cache is actually doing what you think and giving you the performance benefits you are looking for. In addition, Varnish Cache often needs to be tuned after the initial setup to ensure that new items are properly cached and you are accounting for the behaviors of your specific users in your cache setup. How to Monitor Varnish Cache To understand how Varnish Cache is behaving you’ll need to process logs provided by Varnish Cache’s toolset. First you’ll need to configure varnishncsa to start writing logs and set up a log file rotation. You can then search logs using grep to find the requests you want to examine. Varnishstat is another tool that provides several interesting statistics that you can examine when you have a problem, or use as ongoing statistics to regularly check the health of your system. These logs will be in a very basic format and may need sifting through to fully comprehend what is going on. You can see a list of the Varnish metrics and counters to look at here. A more advanced setup can aggregate and consolidate this data in an easy to use way that can be utilized by both development and operations teams to diagnose and resolve issues. Solutions such as Section will provide detailed logs and metrics out-of-the-box, and we’ve also outlined some free tools below that you can use to set up logging yourself. First, to understand things like cache hit rates by content-type, user-agent and GeoIP: Use varnishncsa with a decent log format that captures a lot of data. Ship those logs to a centralised log processing system, using rsyslog or syslog-ng. Run Logstash using a syslog input or UDP input to receive the log lines. During Logstash processing, use the GeoIP filter and user-agent filter detection to enrich the data. Set up statsd from etsy, and point the Logstash output to statsd. We set statsd to flush aggregate data with means, medians and various percentiles to carbon-relay, the component of the Graphite stack that receives data. Carbon-relay pushes to carbon-cache, which persists the files. We then use graphite-web to perform ad hoc queries on the data. Secondly, for statistics from the instances of Varnish Cache: We run varnishstat as a collectd job periodically. collectd forwards the data obtained from varnishstat to our carbon-relay as above. carbon-relay sends the data to carbon-cache. We can then perform ad hoc queries on a single instance of Varnish Cache or look at the Varnish Cache cluster as a whole. Graphite-web supports creating dashboards, so you can use those ad hoc queries you find interesting and group them together to build reusable metrics for Varnish Cache that you can use to maintain the health of your system. In order to make these more manageable, you can use Tessera or Grafana to build dashboards with better interfaces that suit your system’s requirements without needing complex programming. At Section we use a combination of Graphite metrics with Grafana dashboards that are set up and ready to go in the Section portal. Varnish Logs In addition to monitoring tools you’ll also want to set up some detailed logging tools which will give you statistics on number of errors and other important information. We recommend setting up a centralized log system based on the ELK stack which will store logs and allow you to search and visualize them easily. As a quick refresher on the ELK stack, it consists of these three open source tools working together: ElasticSearch is a near-real time search engine that, as the name implies, is highly scalable and flexible. It centrally stores data so documents can be searched quickly, and allows for advanced queries so developers can get detailed analysis. ElasticSearch is based on the Lucene search engine, another open-source software, and built with RESTful APIs for simple deployment. LogStash is the data collection pipeline which sits in front of ElasticSearch to collect data inputs and pipe said data to a variety of different destinations - ElasticSearch being the destination for this data when utilizing the ELK Stack. LogStash supports a wide range of data types and sources (including web applications, hosting services, content delivery solutions, and web application firewalls or caching servers), and can collect them all at once so you have all the data you need immediately. Kibana visualizes ElasticSearch documents so it’s easy for developers to have immediate insight into the documents stored and how the system is operating. Kibana offers interactive diagrams that can visualize complex queries done through ElasticSearch, along with Geospatial data and timelines that show you different services are performing over time. Kibana also makes it easy for developers to create and save custom graphs that fit the needs of their specific applications. You can detailed instructions on setting up the ELK Stack from Digital Ocean. To set up an ELK stack for Varnish Cache, follow these basic steps: Run varnishncsa on your hosts, and use rsyslog or syslog-ng to ship the data to a Logstash endpoint. Configure Logstash to accept your data, and enrich the data with the various filters that Logstash provides. Configure Logstash to output your data to an elasticsearch cluster. Use Kibana to query the data in an ad hoc fashion, or build your own Varnish Cache management console. Using a combination of these logs and the metrics described above will ensure you have the answers to any questions about Varnish Cache hit rate, error rates, and more. Metrics to Examine Once you have a metrics system set up you’ll need to determine which metrics to look at to properly assess how Varnish Cache is working. While you will likely want to look at some specific metrics for your website, here are the major Varnish Cache metrics you should utilize to monitor the health of your Varnish Cache configuration. Cache hit rate: The overall cache hit rate is usually the most looked at cache metric because it demonstrates the percentage of requests that have been successfully served from cache. While you can use this to get a quick look at your cache performance, it is more important to understand what types of files are being cached and why certain file types are not being cached, as explained in the next section. Cache hit by asset type: By looking at cache hit rate by asset type (images, CSS, HTML document, Javascript) you can get a much clearer picture of how much content is being cached by VCL. Because websites have a large number of images, if all images are cached this will inflate the overall cache hit rate and hide the fact that the files which take longer for your server to produce - such as HTML documents - are not being cached. Cache miss: A cache miss means Varnish Cache looked for this item in the cache and did not find it - this could be because it is the first request for that item and it needs to be fetched from the back end before being cached, or because Varnish Cache thought it was a cacheable item but found for some reason it was not cacheable. If your VCL is configured well, you will have a low cache miss rate. Cache pass: A cache pass means an item is marked as uncacheable in a subroutine. While the item still passes through Varnish Cache to be delivered, Varnish Cache does not try to look it up in the cache or try to cache it before it is delivered. A high cache pass rate could mean you are not caching as much content as you should to achieve optimal performance. Time to serve by cache status: This metric will tell you how long an item takes to be delivered if it was a cache miss, hit, or pass. By looking at this metric you will see the difference in delivery times for each status. Time to serve by asset type: This metric shows you how long each asset type takes to be delivered. You can use this metric to tell what file types are taking longer to deliver, and thus optimize your caching configuration to cache as many of those files as possible. To learn more about how to get started with Varnish Cache, including writing Varnish Cache Configuration Language to cache content for your application please download the full Varnish Cache Guide. If you have specific questions about Varnish Cache and VCL check out our community forum or contact us at [info@section.io](mailto: info@section.io) and one of our Varnish Cache experts would be happy to help you."
"77","2017-12-01","2023-03-24","https://www.section.io/blog/varnish-cache-installation-tutorial/","Varnish Cache has quickly grown to be a popular reverse proxy software for accelerating HTTP requests and caching static and dynamic content. It’s an incredibly powerful way to speed up websites, increase scalability, and reduce server costs. Getting started, however, can be a bit of a headache. Before you dive into the fundamentals of VCL let’s take a quick look at everything you need to know about installing Varnish Cache. Installation Options When installing Varnish Cache you have a few options: Install the open-source version yourself, use one of Varnish Cache Software’s paid products which includes support and additional capabilities built on top of Varnish Cache, or deploy Varnish Cache within a Content Delivery solution such as Fastly (running modified Varnish Cache 2.1) or Section (running 7 versions of Varnish Cache up to 5.1.2). Below we go through the pros and cons to each solution so you can decide which option will be right for your web application: Open Source Varnish Cache Varnish Cache is an excellent open source project with a healthy community around it which is why many websites choose to install the open source version themselves. This has several benefits, the first and for some most important factor being that it is free to download and use. There also may be reasons your organization would prefer to deploy Varnish Cache on-premises rather than in a cloud-based solution. In addition, by using the open-source version you’ll always have the option to self-update to the newest Varnish Cache version. Varnish Cache is currently on version 5.1.2 and supports versions 4.0 and above with 4.1 being the current stable release. By using the open source Varnish Cache you can always rely on the Varnish Cache documentation, Varnish Cache Software documentation, and Varnish Cache community. You will also know exactly what VCL you are using and don’t have to worry about modifications which will make it more difficult to use these open source resources. However, despite being free this option does come with associated costs. These include the costs host your own Varnish Cache server and costs around using significant developer resource to set up and maintain the Varnish Cache server. In addition, because Varnish Cache out-of-the-box does not include user friendly monitoring and metrics tools, you will likely need to spend resource setting up your own logging and alerting system to track the performance of Varnish Cache and alert you if something goes down. It can also be quite complex managing Varnish Cache-specific setup issues such as the SSL termination needed to use Varnish Cache with HTTPS. Another downside of installing Varnish Cache yourself is the lack of structured support. While you have access to the Varnish Cache community you will not have immediate access to a designated support team who can answer questions or assist you while you are learning VCL or troubleshooting issues. Varnish Software Varnish Software is the commercial arm of Varnish Cache and the company offers several paid services built on top of the open source Varnish Cache. These include their core product Varnish Plus, Varnish Plus Cloud which deploys Varnish Cache Plus on cloud infrastructure, and Varnish API Engine for API management. Varnish Cache Software also offers Varnish Extend, a type of Content Delivery Network which is discussed in the next section. Varnish Plus is useful for your organization if you are looking for additional Varnish Cache modules and configurations and professional support offered by Varnish Cache Software. Varnish Cache Software can also handle SSL/TLS termination for you and provides an administration console. This can be a good option if you are looking to deploy Varnish Cache on-premises but want a better interface to work with than the bare VCL, need support, or have an advanced use-case. The downside of these solutions is that they can be prohibitively expensive, and some support levels offer just 20 support requests/year. The plans start at $31,000 for a three node cluster and go up based on needs and additional features. Content Delivery Network The final option is to deploy Varnish Cache globally using a Content Delivery Network. CDNs consist of two layers - the DNS layer which routes requests to the server closest to the user, and the reverse proxy layer which includes software such as Varnish Cache or other caching reverse proxies, web application firewalls, bot blockers, and more. Most traditional CDNs offer older caching proxies like Nginx or Squid, but there are a few CDNs which base their caching on Varnish Cache. As mentioned above, Varnish Cache Software has a Content Delivery Network “Varnish Extend” which is described as a self-assembled CDN. Varnish Cache Extend provides traffic management through Cdexis and instructions on setting up a custom CDN. This custom CDN could be a hybrid of commercial CDNs and private servers, a private CDN, or use Varnish Cache as an origin shield. This option allows you to create a CDN specific to your needs, however this will require a large amount of ongoing work to set up and maintain, and Varnish Cache Extend does not assist you with the actual implementation. In addition, Varnish Cache Extend only deploys Varnish Cache as a reverse proxy and applications would need to separately configure other reverse proxies such as WAFs. Therefore Varnish Cache Extend is only applicable for very large enterprises who want to build a custom CDN using Varnish Cache and get support directly from Varnish Cache. Fastly is a newer Content Delivery Network which is based on Varnish Cache 2.1. Fastly uses a modified version of Varnish Cache for caching static and dynamic content and also offers a rules-based WAF. Fastly also includes metrics, logs, varying levels of support and SSL/TLS certificates at an additional cost to their monthly fee. While Fastly takes advantage of the speed of Varnish Cache, it can be difficult to fully configure and test Fastly’s modified VCL without paying their professional services team for assistance. Because the VCL is modified, the open source documentation and community may not be able to assist you in configurations. Fastly does offer a rules-based WAF but does not have the advanced security solutions that other CDNs offer. Section is a flexible Edge Compute Platform which offers users a choice of 7 unmodified Varnish Cache versions including the latest release 5.1.2. Section also includes ELK Stack logs, robust Grafana metrics, free SSL/TLS certificates and certificate management, and a local Developer PoP for testing the full Varnish Cache and content delivery setup before pushing to production. By testing the full Varnish Cache configuration locally, Section users can cache more content without the risk of broken caching or session leakage. Section gives developers full control over their Varnish Cache configuration, so they are able to edit VCL directly to get the best possible performance out of Varnish Cache or get assistance from the Section support team. Section is also unique for content delivery solutions in that it allows users to deploy Varnish Cache on the Section global PoP network, on a private PoP network, or on-premises with all of the DevOps tools Section’s platform provides. Choosing the Right Solution for You When choosing the Varnish Cache deployment mode that is right for your application you should consider costs (including cost to maintain and set up the open source version), ease of use, accessibility to logs and metrics, and the type of support you think will work best for your development team. Writing custom Varnish Cache configurations involves learning VCL and continually tuning your VCL setup to get the best cache performance. For small organizations with fewer developer resources, a solution that provides guidance on VCL and access to Varnish Cache experts may be preferable to an open source installation. Applications that serve global audiences will benefit from a global deployment of Varnish Cache rather than a single Varnish Cache node installed in one location. Utilizing a CDN with Varnish Cache will allow them to get the fast performance of Varnish Cache’s caching combined with additional speed that comes with reducing the distance content needs to travel to users. Above all you should choose a solution which can adapt to your needs, gives you full control over your configuration, and allows you to test VCL before it goes to production. Many websites do not take advantage of the full power of Varnish Cache because they do not have the power to edit VCL and have no way of testing the full Varnish Cache configuration before going to production. This often leads to websites not utilizing the dynamic caching features Varnish Cache is known for out of fear that this caching will break in production. We strongly recommend using a tool like Section’s Developer PoP to test Varnish Cache before it goes to production. To learn more about how to get started with Varnish Cache, including writing Varnish Cache Configuration Language to cache content for your application please download the full Varnish Cache Guide. If you have specific questions about Varnish Cache and VCL check out our community forum or contact us at [info@section.io](mailto: info@section.io) and one of our Varnish Cache experts would be happy to help you."
"78","2017-08-02","2023-03-24","https://www.section.io/blog/varnish-cache-tutorial-vcl/","What is Varnish Cache Varnish Cache is a reverse proxy for caching HTTP, also sometimes known as an HTTP accelerator. It is most often used to cache content in front of the web server - anything from static images and CSS files to full HTML documents can be cached by Varnish Cache. The key advantages of Varnish Cache are it’s speed and flexibility: It can speed up delivery of content by 300 - 1000x, and because of the flexibility of its domain specific language, Varnish Cache Configuration Language (VCL), it can be configured to act as a load balancer, block IP addresses, and more. It is this combination of speed and configurability that has helped Varnish Cache grow in popularity over older caching reverse proxies like Nginx and Squid. Varnish Cache is an open-source project first developed by Poul-Henning Kamp in 2005, meaning it can be downloaded and installed by anyone for free. There are also several paid services which provide Varnish Cache as a service or hosted versions of Varnish Cache, including Varnish Cache Software (the commercial arm of Varnish Cache), Fastly (a Content Delivery Network running modified Varnish Cache 2.1), and Section (a Edge Compute Platform offering 7 versions of unmodified Varnish Cache up to 5.1.2). In this guide we will go through the basics of Varnish Cache and what you need to know to get started with VCL. By the end of this guide you should have an understanding of: The flow of traffic through Varnish Cache and your web server Enforcing HTTPs with Varnish Cache What type of content you can cache with Varnish Cache What is VCL and how each built in subroutine handles HTTP traffic How to use the Built-in and Default VCL files Methods for caching static objects Considerations including cookies and your origin configurations Methods for caching dynamic content Caching pages with personalization using hole-punching Extending Varnish Cache capabilities with VMODs Measuring the success of Varnish Cache Where the Varnish Cache Server Sits Varnish Cache is deployed as a reverse proxy, a piece of software that sits in front of a web server and intercepts requests made to that server, therefore acting as a proxy to the server a visitor is trying to access. The reverse part comes in because a reverse proxy acts as the server itself, whereas a forward proxy acts on the client or user side to, for example, block access to certain sites within a company network. Reverse proxies have a huge range of uses: They can examine traffic for threats, block bots, and serve cached content directly without traffic needing to go back to the origin server. The Varnish Cache reverse proxy can be configured to do many things but for this paper we are focusing on its main use, caching content. Varnish Cache sits in front of the origin server and any database servers and caches or stores copies of requests which can then be delivered back to visitors extremely quickly. When a visitor attempts to visit your website or application by going to your IP address, they will be redirected to first go through your Varnish Cache instance. Varnish Cache will immediately serve them any content that is stored in its cache, and then Varnish Cache will make requests back to your origin server for any content that is not cached. Users[Not supported by viewer] Origin Server[Not supported by viewer] The amount of content that is served from Varnish Cache depends on how you have configured your Varnish Cache instance (ie if you have set it to only cache images) in addition to how “warm” the cache is. When you first cache content and every time the set cache time expires the cache will be “cold” and the next time a visitor comes to your website it will need to fetch content from the origin before delivering it to that visitor. Varnish Cache fetches the content and, if is is cacheable, will then store it for the next visitor who can be served directly from cache. Content that is labelled as uncachable will never be stored in Varnish Cache and will always be fetched from the origin. Content might be labelled as uncachable if it has a max-age set at 0, if it has cookies attached, or because you don’t want it to be cached. By using VCL you can override headers set at your server that say content should not be cached, and even cache pages around content that is personalized. This means you should in theory be able to cache the majority of your requests and achieve a fast webpage for nearly all visitors. The next sections on VCL go more into detail on how to do this. Varnish Cache and HTTPS One hurdle of Varnish Cache is that it is designed to accelerate HTTP, not the secure HTTPS. As more and more websites are moving all of their pages to HTTPS for better protection against attacks, this has become something many Varnish Cache users have to work around. To enforce HTTPS with Varnish Cache you will need to put an SSL/TLS terminator in front of Varnish Cache to convert HTTPS to HTTP. One way to do this is by using Nginx as the SSL/TLS terminator. Nginx is another reverse proxy that is sometimes used to cache content, but Varnish Cache is much faster. Because Nginx allows for HTTPS traffic, you can install Nginx in front of Varnish Cache to perform the HTTPS to HTTP conversion. You should also install Nginx behind Varnish Cache to fetch content from your origin over HTTPS. HTTPS[Not supported by viewer] SSL/TLS Terminator [Not supported by viewer] HTTP[Not supported by viewer] SSL/TLS Terminator [Not supported by viewer] HTTP[Not supported by viewer] HTTPS[Not supported by viewer] 1[Not supported by viewer] 2[Not supported by viewer] 3[Not supported by viewer] 4[Not supported by viewer] 5[Not supported by viewer] 6[Not supported by viewer] 7[Not supported by viewer] 8[Not supported by viewer] In the above graph, the TLS/SSL terminator (such as Nginx) is sitting both in front of Varnish Cache to intercept HTTPS traffic before it gets to Varnish Cache, and behind Varnish Cache so that requests are converted back to HTTPS before going to your origin. As shown by steps 7 and 8, if Varnish Cache already has an item or full page in its cache it will serve the content directly through the first Nginx instance and will not need to request via HTTPS back to the origin. For detailed instruction on setting up Varnish Cache with HTTPS read this Digital Ocean tutorial. If you are deploying Varnish Cache via a paid service or content delivery solution they may be able to handle this for you: Section provides free SSL/TLS certificates for users and handles the SSL/TLS termination so users do not need to configure it separately. What Content to Cache with Varnish Cache Varnish is powerful because it is so fast, but even more importantly because it has such a wide range of abilities. Many caching proxies only focus on caching static items like images and CSS files, but due to its flexibility Varnish Cache can cache static items, the HTML document, and even pages that have personalized elements. When caching first came along in the 1990s it was usually tied to CDNs. CDNs focused on caching images and other content that is the same for all users, often on a separate URL from other content - for example, all images might be fetched from cdn.yoursite.com. While caching static content solved the challenges of websites in the 1990s - namely that both end-users and the data centers content was served from had much lower bandwidth than those of today - now with huge server farms and fast connections on both sides, to win the speed game you need to do more than caching static items. Caching a range of content is where Varnish Cache’s flexibility really shines: With Varnish Cache you can cache what is sometimes called “dynamic content” but usually refers to the HTML document, and cache content around personalized elements using a “hole punching” technique. Example of content that can be cached with Varnish Cache include: Images (png, jpg, gif) CSS stylesheets and fonts JavaScript Downloadable files Full HTML documents Caching the HTML document is where the real value of Varnish Cache comes in. The HTML document is the first piece of information delivered from the web server to the visitor’s browser and includes all the information needed to build a web page, including CSS files, text, and links to images. Before the HTML document is delivered to the browser, the visitor is looking at a blank page with no indication that the page is beginning to load. When the HTML document loads slowly it delays the time to first byte and start render time, which studies have shown are the most important metrics for both user experience and SEO. If your web server needs to generate each HTML document individually, you will always need to plan for the peak amount of traffic you expect on your website, as your servers could get overloaded with HTML document requests. Even if images and other linked files are cached, the HTML document will need to be generated for each visitor. This would mean if you have 200 visitors in 1 minute, the web server needs to generate 200 HTML documents. By contrast, if you cache the HTML document you both reduce the time it takes for it to be delivered to the user and reduce the load on your origin servers. If the HTML document is cached using Varnish Cache and set to live for just one minute, then each minute Varnish Cache will make one request to your backend server for the HTML document, and the other 199 requests will be served directly from Varnish Cache. The speed that Varnish Cache can serve a cached HTML document is extremely fast, often under 200ms, whereas a server generating and serving the HTML document often takes 1 second or longer. By caching the HTML document the web server only needs to generate 1 HTML document per minute which dramatically reduces the number of servers a website needs to plan for peak traffic. In the below graph, a website’s backend needs 12 servers to adequately prepare for peak traffic without the HTML document cached, and only 2 when the HTML document is cached and they can predict exactly how many requests per minute the server will get. This practice saves hosting costs while keeping servers free for critical transactions, and improves user experience by reducing the time to first byte and start render time. Varnish also allows for caching the HTML document even when it includes personalized elements such as cart size and account information. Using Varnish Cache Configuration Language and a technique called “hole punching” websites can configure their pages so that a majority of the content can be served from cache. In the next section we go into VCL and how to use it to cache all types of content. Understanding Varnish Cache Configuration Language The reason Varnish Cache is so flexible is due to Varnish Cache Configuration Language (VCL), the domain specific language for Varnish Cache. VCL controls how Varnish Cache handles HTTP requests, and can be thought of as a programming language for HTTP just as PHP is used for server side scripting. Understanding how VCL works is vital to getting a good outcome out of Varnish Cache, but this can be a challenge as most developers will not be familiar with VCL until they start working with Varnish Cache. To understand VCL you must have a grasp of what each VCL subroutine achieves as well as how the built-in VCL file interacts with the default VCL. Varnish Cache gives users two files on installation: default.vcl and builtin.vcl. The default file is blank and only contains comments. This is the file that you will edit to write VCL specific to your web application. The built-in VCL file gets triggered if you have not overridden each routine in the default VCL. If the default VCL is not edited, Varnish Cache will fall through to the the built-in VCL logic. The built-in VCL does have some instructions to cache objects, however because it’s default behavior is to not cache requests with cookies, and to not override headers set from the server, it often will not cache anything for modern web applications. Because of this, when getting started with Varnish Cache users must edit the default VCL to achieve a solid performance result for their application. Although Varnish Cache is built so that those requests that are not specifically called out in your default VCL go to the built-in VCL, at Section we recommend programming your default VCL so the vast majority of request are handled there instead of falling back to the built-in logic. This gives you more control over how each request is handled. If you have been caching static content with a basic system like Amazon’s CloudFront you could actually see a performance decrease if you switch to Varnish Cache without configuring anything. Although Varnish Cache is a faster and more sophisticated tool which will ultimately provide much better performance results, it requires some configuring to cache content for most visitors. For this reason, we highly recommend reading through the next sections and using other resources like Varnish Cache Software to get an understanding of what VCL will be needed for your application, rather than turning on Varnish Cache with only the built-in VCL. While the built-in VCL is safe in that it will not cache uncachable content, with an understanding of VCL and your application you will get a much better result than with the built-in configurations and any other caching solution. VCL Subroutines To be able to configure Varnish Cache you will need to learn basic Varnish Cache syntax as well as what each subroutine achieves. It’s important to note that the built-in Varnish Cache is always running underneath the VCL you write: Unless you write VCL that terminates a state each request will fall back to the built-in code. For example, you would write VCL saying “if this request matches these parameters then try to retrieve it from the cache” and all requests that do not match those parameters would fall back to what is written in builtin.vcl. The basic VCL request flow is illustrated below, where vcl_recv is the first subroutine that is executed after Varnish Cache has examined the basic information on the request such as verifying it is a valid HTTP request. The flow is divided into client side and back end requests, with all back end requests starting with vcl_backend. While the full Varnish Cache flow is more complex, this basic flow will get you started. It is important to understand the purpose of each subroutine in the below chart, however the ones you will likely alter to customize your Varnish Cache configuration are vcl_recv, vcl_hash, vcl_backend_response, and vcl_deliver. Receive Varnish receives and examines request (vcl_recv)[Not supported by viewer] User request<b>User request</b> Pass Get from back end and do not cache (vcl_pass)[Not supported by viewer] From vcl_pass? Don’t store in cache[Not supported by viewer] Deliver Deliver item from back end to user (vcl_deliver)[Not supported by viewer] Hash Assign a unique identifier to request and lookup in cache (vcl_hash)[Not supported by viewer] Item in cache? Item in cache?&nbsp; Item in cache? &nbsp;Item in cache? &nbsp; Lookup Action to identify item in cache and define one request from the next (vcl_hash) return (lookup)[Not supported by viewer] Yes (vcl_hit)[Not supported by viewer] No (vcl_miss)[Not supported by viewer] Cachable? &nbsp;Cachable?&nbsp; Fetch Fetch item from back end (vcl_fetch)[Not supported by viewer] Yes Store in cache[Not supported by viewer] Pipe Send request directly to back end and return to user (vcl_pipe)[Not supported by viewer] No Send to user[Not supported by viewer] From vcl_hash? Mark hit-for-pass[Not supported by viewer] Varnish Cache Flow by Section[Not supported by viewer] section.io is a Edge Compute Platform offering several versions of Varnish Cache. Varnish Cache is an open source HTTP accelerator. <font style=""font-size: 12px”><a href=""https://www.section.io/"">section.io</a> is a Edge Compute Platform offering several versions of Varnish Cache. <a href=""https://varnish-cache.org/"">Varnish Cache</a> is an open source HTTP accelerator.&nbsp;</font> Each of the above subroutines have different purposes and terminate with actions including VCL pass, VCL pipe, VCL Hash, and VCL deliver. To learn about how to write Varnish Cache Configuration Language to cache content for your application please download the full Varnish Cache Guide. If you have specific questions about Varnish Cache and VCL check out our community forum or contact us and one of our Varnish Cache experts would be happy to help you."
"79","2017-08-08","2023-03-24","https://www.section.io/blog/varnish-cache-503-error-guru-meditation/","If you use Varnish Cache on your website or are visiting a website that caches content with Varnish Cache, chances are at some point you will come across the Varnish Cache server error: Error 503 Service Unavailable / Guru Meditation with an XID number. A 503 error means that the web server trying to be reached is unavailable - this could be because it is overloaded, down for maintenence, or not fully functional for another reason. When you see this error in relation to Varnish Cache, it means that the website is using Varnish Cache to cache and serve content, and that Varnish Cache is unable to reach the back end server. Varnish Cache issues the Guru Meditation error when a connection has timed out or the Varnish Cache server has made too many requests to the back end server without getting a response. Instead of making an infinite number of requests to an unhealthy back end, Varnish Cache issues the 503 error to let the visitor (and website owner) it is likely the website manager is already working on a fix and your best bet is to try again later. If you are the website manager or owner and aren’t sure why you are getting this response, read below to find out. Fixing a Varnish Cache Server Error So, if your website is producing a Guru Meditation 503 error through Varnish Cache, how do you go about fixing it? First you’ll want to look at the logs for all the 503 errors by using varnishlog. You can get varnishlog to log 503 errors by using the below command from Varnish Cache:     $ varnishlog -q 'RespStatus == 503' -g request
 Regularly you will get 503 errors because your back end is down or unhealthy. In this case varnishlog could return something like “FetchError c no backend connection.” You should check the port Varnish Cache is trying to connect to, the origin server, and your HTTP service such as Apache or Nginx and see if all of that is operating correctly - if it is not, you’ll need to troubleshoot your back end. If your back end does seem to be up but you are still getting a Varnish Cache 503 error then there is something wrong with your web server’s connection to Varnish Cache or the Varnish Cache configuration. If your back end is responding but Varnish Cache is serving 503 we often find this is due to timeouts. You can change or add a .connect_timeout = Xs and a .first_byte_timeout = Xs in the backend default VCL section to a timeout length that works for your web server. Varnish Cache Software has more information on the various timeouts that can occur in Varnish Cache. Another tip is to disable KeepAlive so that idle connections will be dropped. This would look like the below:   ""origin"": {
       ""address"": ""origin.example.com"",
       ""disable_keepalive"": true
   }
 For more information on Varnish Cache download the Section Varnish Cache 101 Guide or contact us if you need help troubleshooting your Varnish Cache setup."
"80","2015-04-01","2023-03-24","https://www.section.io/blog/varnish-monitoring-using-graphite-for-metrics/","Varnish Cache is a great tool in your reverse proxy chain because its superior programming model allows you to make complex caching decisions that are unachievable in other reverse proxy servers. So, you spend a lot of time learning and deploying Varnish Cache. The next question is “How well is Varnish Cache working?” In order to understand how Varnish Cache is behaving you’ll need to process logs provided by Varnish Cache’s toolset. To start, you’ll need to configure varnishncsa to start writing logs (don’t forget to set up log file rotation!). You can then grep logs to find the requests you’re interested in. Also, varnishstat provides a bunch of really interesting statistics that you can examine when you have a problem, or over time to spot check the health of your system. You might want to see the information that varnishstat provides. Once you decide that your site needs high availability, you’ll be looking to do this on multiple servers. A more advanced setup can aggregate and consolidate this data in an easy to use way, that can be digested by devs who do ops, and ops who do dev. We like to use a few components to achieve this, in two parts. Also, all these tools are freely available on the Internet. Firstly, to understand things like cache hit rates by content-type, user-agent and geoip: Use varnishncsa with a decent log format that captures a lot of data. Ship those logs to a centralised log processing system, using rsyslog or syslog-ng. Run Logstash using a syslog input or UDP input to receive the log lines. During Logstash processing, use the GeoIP filter and user-agent filter detection to enrich the data. Set up statsd from etsy, and point the Logstash output to statsd. We set statsd to flush aggregate data with means, medians and various percentiles to carbon-relay, the component of the Graphite stack that receives data. carbon-relay pushes to carbon-cache, which persists the files. We then use graphite-web to perform ad hoc queries on the data. Secondly, for statistics from the instances of Varnish Cache: We run varnishstat as a collectd job periodically. collectd forwards the data obtained from varnishstat to our carbon-relay as above. carbon-relay sends the data to carbon-cache. We can then perform ad hoc queries on a single instance of Varnish Cache or look at the Varnish Cache cluster as a whole. Graphite-web supports creating dashboards, so you can use those ad hoc queries you find interesting and group them together to build reusable metrics for Varnish Cache that you can use to maintain the health of your system. In order to make these more manageable, you can use Tessera (thanks Urban Airship!) to build slick looking dashboards that suit your system’s requirements without any fancy programming. Finally, don’t forget your high availability in all that setup. You don’t want to lose metrics when your system is having a problem. So, if you’re using Varnish for Magento Acceleration, with Tupentine, perhaps you can use these techniques to make sure that you really are getting value out of our Varnish Cache instance. You might even be able to turn off a few of those application servers if you improve your cache hit rate a little."
"81","2018-02-26","2023-03-24","https://www.section.io/blog/varnish-cache-expiration-vs-evistion/","Varnish Cache Expiration vs Eviction When investigating cache hit rates in Varnish Cache it is important to differentiate between objects that have expired from the cache and objects that were evicted from the cache. An expired asset is one that has exceed the sum of its TTL(time-to-live) value and grace period and should be removed from the cache. An evicted asset, however is one that is removed from the cache because Varnish Cache has run out of space in memory and must delete a piece of data prematurely. The former is a regular part of the caching process, the latter can cause unnecessary performance problems. Monitoring your caching rates Varnish Cache’s varnishstat metric provides counters of how many cached objects have expired (MAIN.n_expired) and how many cached objects are evicted when the cache is full (MAIN.n_lru_nuked). If you are seeing poor cache hit rates and a lot of MAIN.n_expired in varnishstat, you may need to increase the TTL or take advantage of grace mode(serve stale while reevaluate — check out this article. You do need to consider whether adjusting these values may impact the work flow of your front end developers and their expectations of updates being visible etc. If on the other hand you are seeing poor hit rate, low MAIN.n_expired but high MAIN.n_lru_nuked counters, then this indicates that Varnish Cache has insufficient memory to properly store all the assets. You will need to increase the allocated memory by editing the Varnish Cache configuration file usually found in /etc/sysconfig/varnish. For example, Varnish Cache_STORAGE=""malloc,1000M” OR under DAEMOM_OPTS -s malloc,1000M \ Tracking the effect of your changes. Once the above changes have been made you should confirm that it has the desired effect and cache hit rates have improved. To see our guide on how to monitor Varnish Cache see this article If you would like to forego all the headaches of tweaking Varnish Cache and setting up custom logging, Section manages the memory allocation our deployed Varnish Caches on your behalf and full details of HIT/MISS/PASS rates are available in our metrics portal."
"82","2016-10-05","2023-03-24","https://www.section.io/blog/instant-global-cache-varnish-extension-magento/","Section has been helping Magento merchants make their sites faster, more scalable and more secure for several years. As studies have shown and Section Magento customers have proven, improvement in web performance consistently leads to more products viewed, more cart conversion, and increased revenue. As the busy holiday season approaches, it’s more important now than at any other time to have a fast, secure website that can handle an increase in web traffic without slowing down or going offline. Simple Magento Extension for Caching That is why we’re excited to announce our new extension that allows Magento merchants to easily install and configure Varnish Cache on the Section CDN from within the Magento Admin Portal. Using the Instant Varnish Cache Magento extension, Magento 2 merchants can get setup with Varnish Cache, a best-in-class caching solution for improved performance and scalability, and serve content from Section’s global server network, all without leaving their Magento interface. This extension will install Varnish Cache, provide step-by-step instructions on how to pull in Magento VCL (Varnish Configuration Language), and distribute that configuration globally on Section’s CDN. This simple solution will immediately provide a fast website experience for customers around the world by caching content to improve page load times. Caching content with Varnish Cache also reduces the work required by website servers, improving scalability as more customers visit your site, and saving merchants money on their operational expenses through reduced server costs. Easy Magento CDN Integration Other CDNs offer an integration with Magento, but Section is the first and only CDN that allows merchants to quickly setup on our system from within the Magento portal using the VCL recommended by Magento. In addition to globally distributed Varnish Cache, merchants also get the great benefits Section offers to all customers on our platform includiing: HTTP/2, Automated SSL/HTTPS certificates, Logs, Metrics and Alerts, Unlimited Cache Clears, IP blocking, Version Control, Test Environments and Local Development. To get started with this extension, go to our store on the Magento Extension Marketplace or if you have any questions please contact us for more information on improving your Magento website performance and security."
"83","2017-06-21","2023-03-24","https://www.section.io/blog/sectionio-partners-signal-sciences-web-protection-platform/","We are pleased to announce today that we have added the Signal Sciences Web Protection Platform to Section’s modular Edge Compute Platform. Both Section and Signal Sciences focus on bringing security, agile practices, and DevOps workflows to enterprises that are tackling the shift to cloud and modern application deployment. This addition to Section’s suite of reverse proxies for security and performance allows our customers to utilize this advanced Web Protection Platform within Section. To learn more or add Signal Sciences to your Section setup, please contact us. The full press release is below: Section Joins Forces with Signal Sciences to Bring DevOps and Agile Workflows to Website Security Boulder, CO, June 21, 2017 - Section today announced the expansion of its website security offerings with the addition of Signal Sciences Web Protection Platform (WPP), the security industry’s first Web protection platform. Both Section and Signal Sciences focus on bringing security, agile practices, and DevOps workflows to enterprises that are tackling the shift to cloud and modern application deployment. By working together, the two companies will provide Section customers the ability to utilize Signal Sciences WPP, a comprehensive threat protection and security visibility solution for web applications, microservices, and APIs on any platform, within Section’s modular Edge Compute Platform. “In the past, deploying web security solutions via content delivery networks was a challenge due to the inability for developers to fully test configurations and a lack of visibility into how the traditional WAFs blocks threats,” said Tyler Shields, Vice President of Marketing, Strategy, and Partnerships at Signal Sciences. “By bringing Signal Sciences and Section together, we’re able to break down the silos between development, operations, and security, providing security visibility, performance, and protection in a platform that integrates well into the customer’s current workflows.” Section and Signal Sciences were both started when their respective founders ran into difficulty implementing content delivery and security solutions while adhering to modern developer workflows. The solutions prioritize visibility through detailed metrics and monitoring, easy testing, and automation via continuous integration and continuous delivery practices. Section’s modular Edge Compute Platform is built using container technology, which allows it to offer a choice of technologies for website performance and security. With the addition of Signal Sciences, Section’s offerings now includes three Web protection tools (open-source WAF ModSecurity, intelligent WAF Threat X, and Signal Sciences Web Protection Platform), 7 unmodified Varnish Cache versions, Nginx with LUA for powerful Edge Side Rewrites, and Google’s PageSpeed for front end optimizations. “The ability for Section users to choose the web security solution that works for them, rather than being locked into black-boxed tools that legacy CDNs offer, is changing how people think of content delivery. We’re pleased to partner with Signal Sciences to bring our customers the most advanced website security platform on the market today,” said Stewart McGrath, CEO and co-founder of Section. Section’s core features include robust metrics, real time detailed logs, real user and synthetic monitoring, instant cache clear and configuration propagation, a global PoP network, and a Developer PoP for building and testing configurations locally. Over the coming months Section will deploy additional solutions for website performance, security, and scalability on its platform. Get Started with Signal Sciences and Section To get started with Signal Sciences and Section, please contact us. Get Started Today"
"84","2017-05-03","2023-03-24","https://www.section.io/blog/web-application-firewall-definition-website-security/","Web application firewalls have been around for over 20 years, but recent advancements in how they block bad traffic and are managed by development teams encouraged us to take a look at the history of firewalls, WAFs, and where website security is heading. What is a Firewall Computing firewalls were first developed while the Internet was still in its infancy in the 1980s. Their purpose was to act as a virtual shield between internal networks and servers and external networks (such as the Internet), so that traffic between the two could be monitored and blocked if it was deemed to be suspicious based on preset rules. The first iteration of firewalls looked at individual data packets to determine where each packet came from and if it matched rules that said it could pass through into the network. Subsequent firewalls inspect packets based on their state in a connection (ie, if it is part of an ongoing stream of data, the start/end of a data stream or if it does not relate to other packets), and future iterations of firewalls moved into the application layer. By contrast with firewalls, Web Application Firewalls or WAFs inspect HTTP traffic going to specific web applications, rather than traffic between servers. WAFs were first deployed in data centers, but are now often deployed in the cloud as a reverse proxy. This means the WAF is placed between a website’s origin server and a visitor’s browser, and acts as a proxy for the website origin server so that it can inspect traffic and either block it or pass it through to the origin. Types of Firewall and Web Application Firewalls The first WAFs were developed in the 1990s and the open-source WAF ModSecurity was first released in 2002 and is still in high use today. In addition to being a popular tool that web applications can setup and deploy themselves, ModSecurity serves as the backbone for many of the WAFs developed by Content Delivery Networks including CloudFlare and Akamai. WAFs aim to protect against web application-specific attacks including Cross Site Scripting, SQL Injections, Cookie Poisoning, known platform vulnerabilities and more. They prevent websites and apps from unknowingly letting hackers into their system or sharing user data. The majority of WAFs do this by employing a set of rules and using those rules to inspect traffic before it is let through to the website origin. ModSecurity and many other WAFs base their initial rulesets off of the Open Web Application Security Project (OWASP) Top 10 list, which has published a list of the top website attacks since 2003. The current list can be viewed here. Below we go into how these rules-based WAFs work and what other solutions have arisen in the WAF marketplace in recent years. How Firewall Rules Block Threats Rules-based WAFs deployed as reverse proxies inspect all traffic that attempts to connect to a website’s origin server against a list of rules and either blocks the traffic or lets it through. Whether these rules block or allow traffic in by default depends on if the WAF is set up with a negative model or positive model. Negative and Positive Security Models Negative security models, which have traditionally been the default WAF configuration, allow all traffic except that which meets rules showing it is a known threat. This configuration protects legitimate traffic from being incorrectly labelled as an attack, but also requires the use of a large database of rules and signatures of attack types to scan against. This type of configuration is a good solution for those looking to block known attack types and system vulnerabilities with less setup, as many WAFs come with automatic deployment of the OWASP Top 10 ruleset along with other databases of rulesets. A positive security model takes a different approach by blocking all incoming traffic unless it meets requirements that show it is not a malicious entity - for example location or browser type. This type of security requires less rules because it blocks traffic by default, but also requires websites to have an intimate knowledge of their visitor profile so that legitimate users are not blocked. Negative security rulesets operate on the guideline that the majority of attackers are using known vulnerabilities to exploit websites that do not have protection. While this may be the case, these WAFs require constant maintaining to ensure the new attack types discovered each day are included in that WAF instance’s ruleset. If a WAF is irregularly updated to include new attack types or vulnerability patches, that website becomes just as vulnerable to new attack types as it would be without a WAF. Positive security models more strictly limit the routes an attacker can take to gain access to a website, and because of this block against both known and unknown attack types and vulnerabilities. When first deployed, positive models may block a good amount of legitimate traffic in so-called “false positive” events, however by whitelisting visitor characteristics over a few rounds of rules editing the positive model will (when configured correctly) successfully allow in real visitors while blocking a wider range of malicious activity than negative models. Managing Rules for ModSecurity and other WAFs Both negative and positive model WAFs are based on using rules to either allow or deny visitors entry to a website. WAFs have operated on this rules-based approach since their inception, but more modern solutions have recently started to question this method due to some downsides of this approach. Rules-based WAFs can be inexpensive, relatively easy to install, and if updated and monitored regularly will block most attacks. Companies such as CloudFlare offer WAF services with protection against the OWASP Top 10 attacks starting at just $20/month, and provide additional rulesets at an upcharge. These basic WAFs are an attractive option for those looking for protection against some attacks at a low cost, but the true cost comes in the developer time it takes to maintain the rules that form the core of the protection. Taking ModSecurity as an example, here is how a typical deployment will go: ModSecurity is first used in “detect” mode, which tracks traffic and creates log entries but does not block threats. A developer will then review the logs from the detection stage to see if rules need to be added or adjusted. The WAF can be turned to blocking mode to start actively looking for and blocking traffic that meets the rulesets. As new vulnerabilities with platforms including WordPress, Magento and Drupal are discovered, a developer or team needs to update rules to capture these vulnerabilities. The security team will also need to create custom rules based on their specific threats - for example, ecommerce sites may see a large number of price scraping bots which need to be blocked, and some sites will see attacks from specific countries with which they are not doing business. Over time, many WAFs include thousands of rules that need to be constantly managed - this can cause cost to businesses through solutions that charge for adding additional rules, and significant labor cost. There has also been a history of these tools being deployed in detect mode and never moving to active blocking due to the difficulty of accurately identifying attacks based on rules and the fear of impacting legitimate traffic and hurting revenue as a result. Next Generation Firewalls The number of security threats is growing each year - in 2015, the number of incidents reported was 48% higher than in 2014 as reported by PWC global. Security threats are becoming more advanced, taking multiple routes to get into a website and specifically avoiding those which hackers know may be protected by a rules-based WAF. In addition, as the number and type of devices accessing the Internet becomes larger and more varied, attackers are given more pathways into a network. As ReadWrite puts it, hackers are now “calculated criminals focused on acquiring information in a data-laden marketplace.” All of this has led to a need for more sophisticated methods of website security and an uptick in intelligent or context-based security solutions. These WAFs, firewalls, and bot blocking tools do not rely on rulesets to block attacks and instead use complex systems to identify threats based on the combined actions they take against a website. By removing the rulesets that traditional solutions use, modern systems can stay one step ahead of attackers as they do not know what rules are being used against them. These “intelligent” or “learning” security solutions use contextual information such as location, device, time, and on-site behavior to built a complete profile of website visitors that allows them to either block or allow them in. Advanced techniques allow WAFs and bot-blocking tools to capture the attacks that are not found by rules-based solutions while still protecting legitimate traffic. An example of this advanced solution in bot blocking is Perimeter X. Bot blocking is crucial especially for ecommerce websites who are often victim to price-scraping bots and bots that hold up inventory in checkout carts. Perimeter X blocks bots while protecting real shoppers by giving each visitor a “Risk Score.” This score is based on behavioral analysis that includes factors such as mouse and click movement and timing, unusual web application requests, and hidden clicks. These techniques are able to defend against even the most sophisticated bots that use real browsers to take over accounts and can slip past older security methods. On the Web Application FireWall side, solutions including SignalSciences and Threat X look at the combined activity of potentially damaging traffic to determine if an IP address is in the early stages of an attack or collecting information in the background before starting an attack. Threat X tracks an attacker across seven stages of attack, to determine when and where hackers need to be stopped. While some activity may immediately set off blocking triggers, other activity is logged and watched in case it progresses. Using this system in place of a purely rules-based system means that hackers are stopped while real visitors who initially may look like or share attributes with hackers are not impacted. One reason the security industry is moving away from making simple yes/no decisions on traffic is the increased complexity of real user behavior that might at first look like malicious behavior: SignalSciences mentions that a user entering a name such as O’Toole in a form may be blocked in a traditional WAF as ‘ is a characteristic of a SQL attack. The SignalSciences solution aims to eliminate an attacker’s ability to use scripting to gain access to a website while reducing false positives by taking into account where information has been loaded (for example on a form fill vs a browser script) along with other factors. Despite these advancements, it hasn’t always been smooth sailing in terms of new security solutions: Early examples of learning-based security such as a WAF built by CloudFlare didn’t perform as expected because their results were different from those found with traditional rules based approaches. Intelligent solutions still need to gather contextual data around what “normal” traffic for a website looks like before programs are able to identify out-of-the-ordinary requests. However, the benefit of these solutions as opposed to rules-based systems is that data is examined and stored by a machine, so developers do not need to manually inspect traffic and decide what is expected or unexpected behavior on their specific website. Integrating Agile and DevOps methods into Security It’s clear the website security landscape is expanding as new techniques to combat malicious actors are regularly created. At the same time, modern development practices such as agile and DevOps are on the rise, and security companies are increasingly thinking about how to integrate these methods into their offerings. One key component of a DevOps workflow is the use of in-depth metrics and logs to continually assess and tune website configurations - and this is no different in the security space. Logs are essential for developers to understand the traffic coming through their site, what actions users are taking, and how security rules or learning systems are affecting that traffic. SignalSciences is one solution dedicated to improving this feedback loop by giving developers a clear dashboard with threat information in real time. Section is also committed to integrating with agile and DevOps principles by providing a local developer environment so engineers can see how their WAF and caching setup will impact the production website before going live. Section also gives all users ELK stack logs for inspecting detailed traffic and Graphite metrics visualized in Grafana for customizable graphs. Threat X believes that both developers and business people should be able to easily view threats and actions taken against threats, which is why their dashboards are consumable by even non-technical personnel. When next-generation solutions are combined with DevOps workflows, the security of websites and the ability for developers to immediately address threats increase dramatically. To learn more about the security solutions Section offers, contact us. Get Started Today"
"85","2016-12-16","2023-03-24","https://www.section.io/blog/cdn-reverse-proxy-cdn-comparison/","One of the most remarkable things about the CDN market in the 20 years since Akamai launched the first Content Delivery Network is how little attention has been paid to the technology these companies are built on. When you consider the function they serve, it is somewhat understandable. Your website is slow, you pay for a CDN and suddenly it is faster. You don’t really care how it works as long as you get the desired result. Yet when you think about how important the reliable delivery of website content is to the survival of millions of businesses around the world, it is strange there hasn’t been more attention paid to understanding the technology responsible for making sure the content reaches a company’s customers as intended. What if your web application’s code conflicts with the software running on your CDN? Isn’t that something you’d like to know before deploying to production? In our last post we reviewed the two main layers of a CDN, the DNS layer and the reverse proxy layer. If you consider the DNS layer and CDN PoPs the skeleton of a CDN, the reverse proxy layer is the real muscle behind a CDN. CDNs haven’t had any good reasons to lay out exactly how their reverse proxy technology works because the closer you look the easier it is to see how commoditized the market has become. Let’s take a quick tour under the hood of the most popular CDNs on the market today and look at which reverse proxies they use to improve their customer’s website performance and security. Akamai: The grandfather of CDNs, Akamai was the first to market a network of reverse proxy servers to bring cached content closer to the end user. While Akamai technically uses customized reverse proxies, they are likely to be a variation of Squid for caching and ModSecurity for security. Cloudflare: Cloudflare jumped into the CDN industry in 2009, bringing a free product option and good reputation for security to the market. To many, Cloudflare is synonymous with website security and like Akamai they built their security software on an older version of the ModSecurity reverse proxy which is now in NGINX. Their caching software is build on the NGINX reverse proxy. Fastly: A relatively new CDN, Fastly uses an early version of Varnish Cache for its reverse proxy servers. As one of the first CDNs to use Varnish Cache, Fastly is well liked by developers. It does not, however, have a security reverse proxy. Incapsula: Incapsula is a CDN that focuses on security and uses a version of ModSecurity to help stop malicious traffic and mitigate against DDOS attacks. Incapsula also offers caching but do not disclose if they use a modified open-source or proprietary reverse proxy. MaxCDN: Like Fastly, MaxCDN uses a variation of an earlier version of Varnish Cache for the reverse proxies on their network. MaxCDN does not have a reverse proxy for security. Section: Currently, Section offers Varnish Cache for performance and ModSecurity for security. Section is unique in that it offers developers the choice of several versions of Varnish Cache. More proxy options will be added to the platform in the coming months, giving developers more choice and control over how their content is delivered and secured. As you can see, there is a lot of overlap between the CDN providers when it comes to the reverse proxies their networks are built on. Unfortunately, CDNs aren’t transparent with the exact version of the reverse proxy software they are running or revealing any modifications they have made. At Section, we believe that the developers should have complete control and choice over the reverse proxies they depend on to deliver content to their end user. Learn more about our movement to free reverse proxies from the CDN black box, and get a free shirt to help spread the word."
"86","2016-12-14","2023-03-24","https://www.section.io/blog/content-delivery-network-architecture-cdn/","Content Delivery Networks can often seem shrouded in mystery: They appear on countless lists as one of the top ways to improve website performance, and there are many Content Deliver Network companies globally, but the basics of how CDNs actually accomplish all that they promise (such as improved website speed, ability to handle more visitors, and protection from attacks) are still unknown to many. Content Delivery Network History To unravel how CDNs work, we have to go back to the origin of CDNs. In the past few years the CDN industry has grown rapidly, and now almost 50% of web traffic is served through a CDN. Companies such as Netflix serve so much traffic that they have built their own internal CDNs to support the high volume of content they are serving to global visitors on a daily basis. However, CDNs were born in a very different time. In the late 1990s Internet usage was taking off and more and more visitors were accessing websites. Those websites were still mostly hosted in office data centers which had very small pipes connecting the web servers to the Internet. Each time a visitor tried to access a website, the browser would connect to the web server and then make subsequent requests to collect all the objects needed to build that page. Due to the size of the pipes connecting the web server to visitors on the Internet, as more people tried to access websites there quickly became a bottleneck at the website server, slowing down response time for everyone and sometimes taking sites offline. Content Delivery Networks came about in order to solve this problem of web servers getting overwhelmed by requests and slowing down. CDNs did this by installing a middleman in between browsers and servers that kept copies of the website in a cache. CDN servers had much bigger pipes than the web servers and were able to serve many requests at once. When a user requested a page from a website with a CDN, they reached a CDN server first, and were served a cached copy of the website from the CDN without having to make any requests from the website server. If the CDN server did not have a cached copy of the website, it would make a request back to the website server, greatly reducing the number of requests to the website server. The other main thing CDNs did to improve website performance was use multiple servers or Points of Presence located at various points around the globe. At this point visitors accessing websites also were using slow connections such as dial-up modems, so if a visitor in California was requesting a web page from a server in New York, the longer geographical distance could make a large impact on load time. By installing multiple distributed servers visitors could be directed to the server closest to them, reducing the distance requests needed to travel. CDN Architecture There are two main layers of a Content Delivery Network which perform the tasks described above: A DNS layer that directs users to the server closest to them A reverse proxy layer which imitates the website server and has additional functions such as caching or adding firewall protection. These components work together to reduce the distance visitors need to travel to get content and serve visitors from CDN servers rather than the website servers. They make up the essential features of a CDN, as the reverse proxy layer can include many different reverse proxies such as: A caching proxy like Varnish Cache A Web Application Firewall (WAF) like ModSecurity A bot blocking reverse proxy A reverse proxy that enables A/B testing Although CDNs may focus on different items such as performance or security, they are all relying on the basic setup of distributed servers + reverse proxies installed on those servers. At Section we use open-source versions of Varnish Cache for performance and ModSecurity for website security. We give developers complete control over how these reverse proxies are configured, meaning they can customize them to work for their specific website. To try out our tools, sign up for a 14-day free CDN trial or contact our team to learn more."
"87","2016-12-12","2023-03-24","https://www.section.io/blog/how-cdns-impact-website-uptime/","Content Delivery Networks are usually thought of for their benefits - they bring content closer to global end-users, reduce stress on origin servers, and add caching and security features that speed up websites and protect them from malicious traffic. However, CDNs can also add a lot of stress to development teams, and although CDNs are thought of as reducing downtime by bringing content delivery onto their network, if your CDN has many points of presence (PoPs) it can actually add to the time it takes to recover from downtime on your origin server or fix an issue on your site. This is due to the way CDNs cache content in global servers all over the world and serve that cached content to users. While caching content speeds up page load times, it can have detrimental effects when your website has an issue that needs to be fixed or experiences downtime. Below we review the two main scenarios in which a CDN with many PoPs such could have detrimental effects on website uptime. Your origin server goes down This could be due to an issue beyond your control, such as a hosting outage. As users around the globe visit your site, the global PoP caches capture and store the error, such as a 503 or 504 error message. Until the cache expires (which depends on how long you allow your cache to store pages), users will be served the error page even if the error itself was resolved within a few minutes. Different users may see different things depending when the PoP closest to them visited the origin server - pages will be serving error messages for some users, and be functional for others. Your team makes an error when updating your website As users around the globe visit your site, the global PoP caches capture and store the error. It could take over an hour for the change to propagate to all PoP caches, meaning some visitors will report an issue and others will not see it yet. Importantly, the website developers may not even see the error until the PoP closest to them has received the change. Once developers see the issue, they attempt to fix it and push the change to the production (live) website. Again, it will take time for the CDN PoPs to propagate the change to all users. Because most CDNs do not allow developers to test fixes locally, the first fix may not work and developers would need to push additional fixes. This results in an issue that could have been an easy fix taking multiple hours to resolve due to CDN propagation and the inability for developers to test CDN changes locally. So, how do you avoid these undesirable situations? There are several solutions: Use a caching tool that has the ability to serve stale content to users if your origin server is down at the time - Varnish Cache 4 and above, which is available on the Section platform or directly through Varnish Cache, is able to do this. Use a CDN with fewer, more powerful PoPs rather than one with thousands of PoPs around the globe. Having fewer PoPs greatly reduces the propagation time, allowing websites to see and fix errors more quickly. Use a CDN with an easy, instant cache clear function. Section gives customers the ability to clear the entire cache with one easy click within our portal, with no limit on cache clears and no cost per cache clear. Use a CDN which allows you to test changes you make before they go to production. By doing this, websites remove the likelihood that the CDN code will impact changes when they are pushed to production. Section is the only CDN on the market that provides developers with a local CDN environment for testing changes before they go live. Section’s CDN allows you to quickly propogate changes to our global PoPs, and provides developers with a local testing environment to test changes before they go live. In addition, we provide fully open-source versions of Varnish Cache so you can control all configuration changes. To try us out, sign up for a 14-day free trial."
"88","2016-11-02","2023-03-24","https://www.section.io/blog/more-pops-cdns/","A common question in the Content Delivery World and one that we at Section hear a lot is “How many PoPs does my website need?” To answer this question, first we will go into the background of CDN PoPs and why they were first utilized. What is a CDN PoP? Points of Presence (PoPs) are at the heart of the CDN infrastructure: when CDNs first came about, their primary purpose was to serve content from globally distributed servers or PoPs that were closer to a website’s end users than the website’s origin server. By placing PoPs all over the world, global visitors to a website would get directed to the nearest PoP to them, rather than having to travel back to the origin server. This solved a core problem in the earlier days of the internet, as hosting centers had low bandwidth and as more and more people were trying to access websites, there would be a bottleneck which caused websites to respond to requests slowly or collapse under the pressure of too many requests and go offline. CDN PoPs solved this problem by dispersing the number of requests going to the website’s server to many servers across the globe, and at the same time storing cached content on each PoP which could be immediately served to website visitors without going back to the website server. When a PoP cache was full (more on that below), certain content could be served directly from that PoP cache. The most common items to be cached on the PoP were static objects such as image files. Nowadays, CDNs are much more powerful and many more items including dynamic files can also be stored in a cache. As internet usage grew, legacy CDNs added more and more PoPs, and today those older CDNs have by some estimates hundreds of thousands of PoPs across the globe. But back to the question at hand - does your website actually need all of those PoPs? The case for fewer, stronger CDN PoPs In recent years modern CDNs have come along and challenged the notion that a higher number of PoPs is better by placing more powerful PoPs at strategic points along the internet backbone. The internet backbone is the series of cables that connect locations around the world to the internet: The further away an end user is from the internet backbone, the longer content will take to be delivered to them. By placing PoPs nearby the internet backbone, modern CDNs reduce the time it takes for content to travel between locations. In addition to placing PoPs in strategic locations, modern CDNs also have much fewer PoPs than legacy CDNs. Instead of tens of thousands, they may have under 50 PoPs around the globe. Website owners may worry that having less PoPs will slow down their website, but in fact for all but the smallest number of websites fewer PoPs will result in better caching and website performance. Here’s a quick explanation of why that is the case: When the first user visits a webpage (in this example we will use a website homepage), they will be directed to the nearest CDN PoP. However, that PoP will not have any cached content stored yet and will need to go back to the origin server to get the homepage content. The PoP will then deliver that content to the first visitor, and store a copy of the content that can be served from the cache to the second and subsequent visitors. The “cache hit” percentage indicates the percent of requests that can be served from a filled cache, and websites should aim to have a cache hit percentage that is as close to 100% as possible. To get into the math of it, if the homepage of a website has 100 visitors from the East Coast going to one PoP in New York, the first visitor would count as a “cache miss” and the rest would be “cache hits,” resulting in a cache hit rate of 99%. However, if those same visitors were spread over 10 PoPs along the East Coast, the cache hit percentage of each PoP would be 90% assuming the visitors are spread equally. If the visitors are not spread equally among the 10 PoPs, some PoPs would have a lower cache hit rate. A lower cache hit rate means both More visitors experiencing a slower load time due to the PoP cache not being filled Increased requests to the website server to fill the various PoPs Because of this, the only type of website that benefits from a CDN with thousands of PoPs around the world is one that serves only a few pages to a very distributed global audience. In fact, for many websites performance and cache hit ratio will improve with less PoPs. When Section worked with one website to turn off 100,000 CDN PoPs and switch to Section, we found that only about 10 PoPs were actually being utilized with the legacy CDN, and website performance improved when they switched to Section’s more modern, less PoP-heavy network. Page load time for today vs. yesterday and last week shows a performance improvement with less PoPs: Improve website performance with a Next Generation CDN Are you looking to improve your website performance and security with a modern CDN? Contact the Section team to learn more about our next generation CDN or sign up for a free 14-day trial of the only website performance solution built for developers."
"89","2014-09-23","2023-03-24","https://www.section.io/blog/how-many-pops-does-it-take-to-cache-a-website/","In CDN land there is a super debate over the number of PoPs (Point of Presence) required to get best performance for your website. Here is the match up: In the Left Corner, wearing blue trunks, is the all-time heavyweight argument of more PoPs is better. This argument states that having a wider distribution of lots of PoPs gets your content closer to the end user and thus speeds up the delivery In the Right Corner, wearing multi-coloured trunks, is the peoples favourite of Super PoPs are better. This argument states that fewer PoPs but placed in strategic Internet Peering locations will deliver a superior performance for your website. Having these arguments settled by coming out of two corners to duke it out in the ring sounds like fun but in reality, this will be settled mathematically. When do PoPs Cache? Firstly, let’s investigate when a PoP comes into play when serving a web page. When a user requests a webpage for the very first time, they will not receive anything from the cache PoP. This is because the PoP cache is not primed. The PoP has not seen the content before and wont have a copy so has to go back to the origin to fetch the content. As it serves out the content it may keep a copy of the files to serve to the next person who requests the page. In a simple configuration this would look as follows: This shows that the first request for the page will go back to origin while the second and subsequent requests for the page will be served cached content from the CDN. All other factors being equal, the pages will load faster for the users and there will be less compute cost (and time) from the origin servers. Mathematically, with one PoP, One Page (content wholly cacheable) and no cache expiry (see following), the cache hit ratio for say, 100 pages served would be 99%. 99 of 100 pages served would have their content served from the PoP. To keep the above simple, there are some very key assumptions made; There is one CDN PoP The 2nd and subsequent users are requesting the same webpage As soon as you start to flex either of these assumptions, the game changes significantly. Flexing both becomes even more fun. More Than One Webpage Served The majority of websites (but not all) do have more than one page which we serve to our browsers so the first page scenario described above will apply to every page served. We also need to consider that the cache life of assets for each page is limited. Due to a variety of factors, the cache life (or the time the asset will be stored in the PoP) will expire. let’s say we have 1000 requests (R) in one day for 100 different pages (P) and the cache expiry (ET) on the content in those pages is 12 hours. let’s also assume that the pages are requested equally by 50 users. The Priming Page (PP) will prime the PoP on the first page served ie PP=1. The cache hit ratio in a 24 hour period we would expect (with a single PoP) would be: (R/P-(PP*24/ET))/(R/P) or 80% As the number requests increase or the ET increases, the cache hit ratio increases. In reality most websites do not have an equal spread of pages served to customers, there will be some pages which have a much higher hit rate (like home page or category pages) with other pages having a much lower hit rate as they are requested less frequently (like a specific product page). More Than One PoP Now the maths starts to increase in fun. Just to keep things simple again, let’s assume you engage a CDN with two PoPs NP. And we have the same number of pages, requests and cache expiry time as above. Maybe we have half of your users connecting to one PoP and half of your users to the other. In order to serve cached content, each PoP must be primed first. Requests per PoP becomes relevant. What does this mean for our cache hit ratio? (R/P-(PPNP24/ET))/(R/P) or 60% Doubling the number of PoPs has decreased our cache hit ratio. What About at Larger Scale? What about a 2000 page website with say 3m pages viewed per month (100,000 per day) with a 12 hour cache expiry. 1 PoP = Theoretical Cache Hit Ratio of 96% 10 PoPs = Theoretical Cache Hit Ratio of 60% 100 PoPs = No content cached. The above assumes an even distribution of pages to each PoP, an even distribution of content served throughout the day, and an even distribution of each type of page served. What we find in practice is that there will be a weighting of different pages served. Category and Home pages are served more often than individual product and specific content pages. Graphically, this will look like the following: There may be a similar shaped graph for the distribution of content through PoPs depending on the number and location of the PoPs. And, more than likely, the traffic served will peak and trough at various times of the day. So it is the intersection of these curves which really determine the cache hit ratio rather than the simple linear equations I have outlined above. However, the number of PoPs, the number of pages served and the variety of different pages served are still key drivers of the cache hit ratio for a website. As a general rule; Less PoPs = Higher hit rate More Pages Served = Higher hit rate Fewer types of page served = Higher hit rate Greater concentration of pages served at particular times of day = Higher hit rate When is More PoPs Better? More PoPs is better when you have fewer types of page, lots of pages served and a very widely distributed website audience. Only in this instance can you get the PoP cache filled enough to give you the hit rate which means the short distance between the PoP and the end user can come into play. If there is a PoP close to a user and it’s not filled with the content the user wants, there might as well not be a PoP there in the first place! Do You Want it All? More PoPs and a Higher Hit Rate – Yes Please Is it possible to have a distribution of PoPs and a hit cache hit rate? Yes it is. For this you need a shared cache among the PoPs so that if the requested content is not present at one of the PoPs closest to the user, the content can be fetched from one of the other nodes rather than back to the origin servers. While this means the content for a first request is not always served from the PoP closest to the user, at least the origin infrastructure is not tasked with producing and sending content which is already out there (somewhere!). This would look like: Whats the right PoPs answer? 42? So when it comes to the right number of PoPs to run your website through, you should consider the volume of traffic your site serves, the depth or breadth of the pages and objects you serve and the geographic spread of your users. Assuming you have your HTML and static asset cache control headers all correctly deployed, ultimately probably the best offload and performance PoP piece you can include in your caching network is a shared cache."
"90","2016-05-30","2023-03-24","https://www.section.io/blog/turning-off-100000-cdn-pops/","What does it look like to turn off 100,000+ CDN PoPs? Is this going to hurt the performance of your website? This is a common question asked across the industry, We have previously covered this topic around the power of PoPs (Points of Presence), Are lots of PoPs better? Or are smaller numbers of “Super” PoPs better? https://www.section.io/2014/09/23/how-many-pops-does-it-take-to-cache-a-website.html In a followup we have looked at an actual customer turning off a CDN that comprises 100,000+ PoPs (216,000+ servers globally). Solution overview: Having been unable to achieve an appropriate HTML caching strategy with their existing CDN provider, Section were engaged to sit behind the current CDN provider so that the customer in question could leverage the Section toolset (Development workflows, Instantly available logs and detailed metrics) so that HTML content could be cached and traffic offloaded from origin servers. The implementation methodology above was chosen to be able to validate, firstly that HTML caching could be easily implemented with the Section platform and secondly to provide a safety net against the boogie man of “less than 100,000 POPs may result in worse performance”. Having successfully implemented HTML caching the next step was to turn off the 100,000 PoPs that were caching static content. This was the moment of truth for a specific example of switching from running on 100,000 PoPs globally to another CDN (Section) that has global reach but does not sell PoP count as its core feature. Results: Suspense over in the graph below we measure actual website speed via the New Relic Browser module. This is a RUM based performance monitoring tool that measures the page load performance of every single user and combines this into an overall metric. As you can see below, There was a slight performance improvement in network time and day on day trends. 100,000+ PoPs have been removed and performance has actually improved! Overall page performance: Day on Day performance showing a slight improvement in response times: Network time - This is the time taken for items to be downloaded (as opposed to time taken to generate content on server or process in browser): Additional details: As we had visibility of incoming connections from the legacy CDN we could see that the actual number of PoPs that were serving user content (and connecting back to origin servers when caches were missed) was far far less than 100,000. The actual number of POPs that were serving any volume of traffic was < 10. This makes sense considering that cached items are not shared between PoPs. If you had the same image or javascript file being requested via each PoP in turn you would have to make 100,000 requests for the same piece of content before you hit from cache. Summary: Hopefully this helps resolve the hype around very large CDN PoPs numbers, There may even be a web performance anti-pattern around having large numbers of PoPs. Either way we feel that what you cache (HTML in addition to static content) is far more important than thousands or millions of PoPs."
"91","2016-10-14","2023-03-24","https://www.section.io/blog/what-is-http2-benefits-of-http2/","Hypertext Transfer Protocol or HTTP is the communication protocol used by the World Wide Web that defines how messages are transmitted over the internet. HTTP/2 is the first major upgrade to HTTP/1 and is a major step forward in terms of website performance and speed of delivering content to browsers. An Overview of HTTP/2 The major difference between HTTP/1 and HTTP/2 is the ability for browsers using HTTP/2 to accept more content from a website’s server at once than was possible with HTTP/1. Because modern websites are so resource-heavy, this was a drawback to HTTP/1 which browsers and websites tried to get around by either using multiple connections to get web content or “sharding” content so that it was served from secondary domains - for example images.website.com in addition to your main domain, www.website,com. These hacks exposed issues with the underlying protocol which were set out to be resolved with HTTP/2. Some of the technical differences with HTTP/2 which lead to better performing websites include: It is binary instead of textual It is fully multiplexed which allows multiple request and response messages to be in transit at the same time It can use one connection to gather content instead of opening several connections It uses header compression to reduce the size of headers so they are delivered more quickly Sites that use HTTP/2 have been proven to have faster load times than both HTTP1 and SPDY, a protocol developed by Google to speed up HTTP/1 traffic which informed the creation of HTTP/2. Switching to HTTP/2 Modern browsers will accept traffic over the newer HTTP/2 protocol: In the USA, HTTP/2 ready web browsers account for 91.17% of all Internet users. In order to serve your site over HTTP/2 it must be fully encrypted with a TLS (SSL) certificate and all the content should be on one domain. Next you need to make sure your Content Delivery Network or hosting provider can support HTTP/2. If they can, you can then optimize your website for HTTP/2. Here are some useful resources on HTTP/2 and switching to it: Smashing Magazine - Getting Ready For HTTP/2: A Guide For Web Designers And Developers GitHub - FAQs on HTTP/2 Tourque Mag - A Quick Overview of HTTP/2 Section Can Help with your HTTP/2 Switch Want a quick, easy way to switch your website to the better performing HTTP/2? All clients on Section’s content delivery network automatically get SSL certificates for HTTPS and traffic served over HTTP/2, in addition to the website speed benefits seen with the use of our CDN and caching solution. Contact us if you’re interested in learning how Section’s globally distributed servers and HTTP/2 support can drastically improve your website speed in a few easy steps."
"92","2016-10-20","2023-03-24","https://www.section.io/blog/website-performance-speed-tools-analytics/","How to Measure Website Performance: Now that we know what to measure we need to know how to actually go about it. This is actually a bit more complicated than you might expect, as there are several types of tools that are better in certain circumstances than others. Remember that every time a user interacts with your website, the conditions can be very different. One user could be within a couple miles of the origin server, while another could be across the globe. Web speed could be impacted by the traffic hitting the server while testing. Maybe you are preparing for holiday website traffic and want to know how your website’s performance will be impacted by a surge in volume. There are several ways to measure your website, and they are more or less useful depending on your situation. We will outline these below with some recommendations for tools that we like. Synthetic Testing Also known as active monitoring, synthetic testing is performed by creating scripts that imitate an actual user accessing a site and measuring how long it takes for various elements to load. Synthetic testing is a fantastic way to quickly gather website performance data and identify obvious issues without having to wait for larger volumes of data. Most synthetic testing tools provide data in the form of a waterfall, a graph that breaks down exactly when each element of a page starts loading and how long it takes for each element completely reach the user’s browser. This is a great view for identifying bottlenecks, whether it be a slow loading HTML document, a third party widget, or a particularly large image. Unfortunately, synthetic tests have some significant limitations. First off, the metrics they provide only represent 1 browsing session from 1 specific location, so you can’t assume these metrics are applicable to all, or even most, of your users. Furthermore, a synthetic test must be scripted to take certain paths through a website, so they are not necessarily a good indicator of how an actual user would navigate through a site. The best use of synthetic testing is to help identify larger issues, or catch problems with website performance in a situation where you don’t have a lot of traffic to your site yet to collect data from. Recommended tools: WebPageTest.org, Pingdom Load Testing: Like synthetic testing, load testing is a simulation meant to help you get an understanding of how your website will handle specific situations. Where it differs is that instead of simulating the experience of a single user accessing your website, it attempts to simulate how your website performs with different volumes of users hitting your website at the same time. Load testing can help identify issues that you might encounter when your site reaches higher volumes of concurrent users. The accuracy of the data it provides, however, is questionable. It is nearly impossible to simulate real user behavior, and website performance may be impacted with a smaller volume in real users than was revealed during a load test. Load testing can also be very expensive, and with it’s limited accuracy it might not be a worthwhile exercise for every business. Real User Monitoring: In many ways, Real User Monitoring (RUM) is the holy grail of accurate website performance measuring tools. RUM tools measure your website’s performance for actual users and aggregates the data into the metrics you care about. You can see how your website performed during different levels of volume, from different geographic locations, how performance varies between different paths through the website, or the difference in page load speed for new visitors compared to repeat visitors. Advanced RUM tools will also break down metrics into front- and back-end load times to quickly show you what areas are in need of improvement. The major drawback to RUM is that is requires actual data from users on your site. If you are launching a new site, or have a small number of users, RUM might not be very useful because it will take too long to start producing meaningful information. In these cases, synthetic testing might be more valuable. Unfortunately, not every RUM tool is created equal. Google Analytics, for example, has a RUM tool that can be a useful free option, but it samples a very small number of sessions on your site, making the data susceptible to large margins of error. This tool also only shows page load time, and doesn’t break it down further. Other RUM tools are expensive and provide an extraordinary amount of data that may be useful for sophisticated developers but would be overkill for the average marketer. For those looking for these advanced tools, we recommend New Relic, which will give you detailed information on front-end and back-end performance in addition to scalability analytics and more. Improve Your Website Performance Today To improve your website performance, measure your page speeds using the tools above and then consider implementing a caching strategy such as Varnish Cache to immediately improve page load times. Section makes it easy for you to install and configure Varnish Cache on your website, plus we provided added website speed benefits through our CDN. Sign up for a free CDN trial today and you’ll get 14 days of free service, no credit card needed. For ecommerce sites looking for detailed information on measuring and improving website performance, we also have an eBook available to download for free:"
"93","2016-10-18","2023-03-24","https://www.section.io/blog/measuring-website-performance-speed/","As more and more of our interactions with companies move from the real world to digital spaces, website performance has become a primary concern for business of all shapes and sizes. Website performance, however, could mean a lot of different things to different people depending on what type of good or service they are offering. Some businesses that handle sensitive information, like banks or government agencies, could be most concerned with security and reliability and consider that to be an element of website performance. A business focused on user engagement, like online gaming or education, might interpret website performance as how effective their site is at engaging their users. While website performance can be interpreted differently, the central aspect of performance that should be critical to every business with an online presence is speed. Why does website speed matter so much? First and foremost, it is a critical factor to achieving a positive user experience. It’s been a long time since dial up modems were how most people accessed the internet, and users have grown accustomed to accessing web pages within just a few seconds. When those pages don’t load within the expected time frame, users are quick to bail. It is reported that bounce rate increases by 50% with every 2 seconds of additional page load time. A website that is quick to load also eases any anxiety a user might have with the quality of the site and builds trust with your brand. If you can deliver a rich online experience quickly, you are much more likely to achieve the desired outcome from your users. It’s also important to consider the improvements to scalability that come with a faster site. A website that has the ability to serve content quickly to users is less likely to overload its servers and cause outages. When you improve speed through caching, your site becomes more scalable and able to handle spikes in traffic volume driven by holiday seasonality or a successful marketing campaign. Furthermore, a fast website (especially one with a globally distributed caching strategy) has a better chance of standing up to a DDOS attack, which are becoming more and more common. So now that we know that the speed is the cornerstone of website performance, how do we know how fast a website actually is? The rest of this post will look at what metrics to measure when assessing website performance, and our next post will give you some tools to use to find these metrics. How to Measure Website Performance and Page Speed There are dozens of different website performance metrics that can be measured, and can be valuable for diagnosing problems and optimizing website speed. There are several metrics, however, that should be tracked accurately and frequently to make sure website performance isn’t hurting your business. Time to First Byte (or HTML Load Time): Between the time a user attempts to load a page on your site to the time the page is fully loaded, dozens, sometimes hundreds, of different operations will take place. Content needs to be accessed from the server, images need to be retrieved, and style sheets need to be loaded. Before any of these operations can even start, your server must send an HTML document to your users browser to provide the instructions on what actions it needs to take and what files need to be retrieved. The time it takes to deliver the HTML document from your server to your user’s browser is the Time to First Byte (TTFB). An ideal TTFB is around 200 milliseconds which can be achieved when your HTML document is served from a cache. Time to Title: The title is the name of a page which appears in the browser above the web page address. While this metric is sometimes overlooked as it is only one of many pieces of content that are loaded and displayed to the user, it can go a long way to creating a positive perception in the eyes of your users. Most users have experienced sites that fail to load, have been shuttered, or attempted to access a site with the wrong address. The moment a title is displayed for the page within the browser, anxiety for the user that they may be in the wrong spot is reduced. The longer it takes before the title is displayed, the less patience a user is likely to have while waiting for the rest of the page to load. Start Render Time: The start render time is an important measure because it indicates when the viewer first sees the page appear in their browser. While the web page may still need to make dozens of additional server request to gather all the content needed, the faster a website begins to render the more likely the user will hang out long enough to load the entire page. Page load time: The page load time is probably the most common metric used to assess web speed, as it is the easiest to understand. This metric references the time in seconds it takes from the moment a user attempts to access your website until all of the content has been loaded in your browser. The importance of page load time, however, can be overstated, as a user might perceive the page to be completely loaded well before page load is complete. If you have have large images below the fold, or javascript elements loading in the background, your total page load time might be inflated but your user’s experience isn’t impacted. Ideal page load time is under 2 seconds to prevent users from bouncing and increasing engagement across your site. Improve Your Website Performance Today To improve your website performance today, measure your page speeds using the metrics above and then consider implementing a caching strategy such as Varnish Cache to immediately improve page load times. Section makes it easy for you to install and configure Varnish Cache on your website, plus we provided added website speed benefits through our CDN. Sign up for a free CDN trial today and you’ll get 14 days of free service, no credit card needed. For ecommerce sites looking for detailed information on measuring and improving website performance, we also have an eBook available to download for free:"
"94","2016-10-24","2023-03-24","https://www.section.io/blog/website-speed-seo-benefits/","Search Engine Optimization or SEO is an area filled with ever-changing recommendations from a wide range of sources. At it’s core, SEO is the practice of websites improving their rank in search engines by adhering to best practices including providing relevant content, using keywords and meta-tags, having good on-page user experience, and being linked to by other websites. Google’s PageRank is a famously mysterious algorithm which is constantly changing, making it difficult for websites to know how to appear on the coveted first page of search results. Here is an example of what the first page of Google looks like for the high traffic keyword ‘Content Delivery Network’: Website speed and SEO Despite the questions surrounding SEO and how to appear high on organic search results, there are some things we know about SEO and specifically what Google factors into their PageRank. One of these factors is website speed. Website speed is an increasingly important element of SEO, and it should be something all marketers look into when performing a SEO analysis of their website. Since 2010, Google has included website speed as an element of the PageRank algorithm, and in 2016 Google announced that the speed at which mobile pages load would be a major factor in their mobile search rankings. Facebook has also announced that content which has not fully loaded will be down-ranked in the newsfeed, demonstrating the increasing impatience of users to wait for content to be loaded. A fast website has additional SEO benefits as it decreases bounce rates and improves general on-site user experience. But what exactly should you be looking for in terms of website speed when it comes to SEO? There are many different measures of website speed, from the first moment a browser receives data from the website server to the time it takes to load a full page and everything in between. Websites should try and optimize for all of these metrics, but there is evidence that suggests the main element Google takes into account is the Time to First Byte, which is the time it takes a browser to connect to a website server and receive the first part of the HTML document that generates a page. The Time to First Byte is a crucial element of website speed because it indicates the time at which the browser can begin to build a web page. A long TTFB would mean a user looking at a blank web browser, which often leads to them navigating off the page. A good TTFB to aim for is around 200 milliseconds. How to improve the Time to First Byte for SEO TTFB is all about your server response time, and with inferior hardware or servers which cannot handle an influx of users at the same time, your TTFB will slow down for everyone. Importantly, this is a back-end metric, so no matter how well-optimized your images, CSS, and other front-end files are, a poor TTFB would still harm your website speed and SEO. The best way to improve your TTFB is to install a globally distributed Content Delivery Network, which brings the content closer to end-users around the world, with a flexible caching solution. By caching content either locally or through a CDN, your content will be served from the cache so the browser does not need to travel back to your website server. This is by far the most efficient way to improve TTFB. To ensure your website is the fastest it can be, you should cache full HMTL documents using a solution such as Section. To learn more about caching, download our guide to improving website speed, or sign up for a free trial of Section’s CDN."
"95","2017-08-15","2023-03-24","https://www.section.io/blog/scalability-performance-ecommerce-holiday-season/","Each year the holiday season seems to roll around more quickly, with Thanksgiving decorations popping up before Halloween has concluded and winter-themed decorations and merchandise appearing in stores mid-September. For retailers, holiday sales dominate promotions beginning in mid-November and go all the way through post-season blowouts in January. In the US, Black Friday has given way to a full week of sales that start on Thanksgiving Day itself and go well into the following week. In 2005, Cyber Monday was introduced as a holiday to focus on online sales, but increasingly customers are turning to ecommerce for their shopping throughout the holiday season. In 2016 the number of people who shopped online on Black Friday was 108 million, up from 2015, while the number of people who went to stores was 99 million, down from the 2015 figure. The 2016 online sales on Thanksgiving Day and Black Friday were $5.27 billion, a 17.7% increase from 2015. Over the full 2016 holiday season non-store sales (made up largely by ecommerce) reached $122.9 billion, a 12.6% increase from the previous year. Mobile sales were also up and hit the $1 billion mark on Black Friday for the first time ever. From these statistics it is clear ecommerce isn’t going anywhere in 2017, and retailers need to be more prepared than ever for mobile and omnichannel shoppers, and several weeks of sustained sales and promotions. Improving Website Performance and Scalability for the Holiday Season Is there a downside to this growth in ecommerce during the holidays? Not if your website is prepared, but each year some stores lose customers and revenue because they haven’t properly prepared their site for the increase in visitors. Even large websites are not immune to this issue: In 2016, the Macy’s website held back some visitors from accessing the homepage due to an influx of shoppers, and some shoppers reported issues with adding products to cart once they were allowed in. Victoria’s Secret, Walmart, and some Gap Inc websites also experienced outages due to high traffic, and Williams Sonoma suffered from slow load times, likely resulting in abandoned transactions. Website security is also even more crucial than usual during the holiday season, when cyber criminals are waiting to take advantage of the increased number of online transactions and Distributed Denial of Service attacks can be triggered easily due to the high number of visitors already on websites. While not conducted online, the Target hack that resulted in an $18.5 million settlement first took place around Black Friday. Websites of all sizes see spikes in traffic, consistently higher visitor counts than usual, and increased hacking attempts as more transactions are being processed. While ecommerce sites spend months designing holiday merchandise and coming up with marketing and promotional plans, all of that work is rendered useless if your website goes down in the middle of a traffic surge. Even if your website stays up, a slow down in page load time will result in fewer pages viewed and less revenue. This guide on How to Prepare Your Ecommerce Site for the Holiday Season goes through what you need to know about website speed, scalability, and security as your ecommerce site heads into its busiest season. The preparation you do in the months, weeks, and days leading up to Black Friday and the month of December will prove their ROI when shoppers come around and are greeted with a fast website and seamless shopping experience. Click here to download the free guide."
"96","2016-09-30","2023-03-24","https://www.section.io/blog/scalability-for-ecommerce-site-holiday-sales/","Improve user experience on your ecommerce site during the Holiday Season We’ve spoken a lot about the importance of speed for your website and why it’s especially crucial for ecommerce sites going into the holiday season. Many studies have shown that improved web performance leads to more pages viewed, more products viewed, and most importantly, increased cart conversions and revenue. A new study by Google found that over 50% of mobile users will abandon a website if it doesn’t load within 3 seconds, which is increasingly important as over 30% of ecommerce sales are now via mobile commerce. Less has been discussed around web performance’s cousin, website scalability, which is equally important, especially during a time of increased visits to your ecommerce site. Website scalability is the ability for a website to handle both steadily increasing amounts of traffic (such as when running an ongoing ad campaign) and sudden bursts of traffic that may occur during sales or the holiday rush. Having a website which doesn’t scale well would mean all those potential customers encounter a slow website, or even worse, can’t connect to your site at all, at exactly the time when you’re trying to take advantage of the increased traffic. If you’re investing time and money in bringing customers to your ecommerce site, you need to ensure the site stays up and fast so they don’t immediately leave the site or abandon their cart before completing a purchase. During the holidays, there’s a chance for you to see a sizable uptick in customers and conversions, and a website that doesn’t scale to meet the demands of additional shoppers will result in lost revenue. In addition, if you invest now in a well-performing, scalable website for the holidays, your users will have a good experience on your site are more likely to become regular shoppers down the line. Website scalability is closely linked to these factors: Effective use of caching : Having a properly installed and configured caching solution is key to a scalable website. Magento, a leading ecommerce platform, now strongly recommends merchants utilize Varnish Cache, an open-source caching solution that can be installed on any website. Varnish Cache or another similar tool will reduce the number of requests that go back to your web server by serving users with copies of your website directly from the cache server. This both increases scalability and reduces your server costs at the same time, what we like to consider a win-win situation in the land of ecommerce sites! Effective use of a CDN : Utilizing a Content Delivery Network that includes a caching solution will further improve your website scalability, by hosting a caching solution on a network of globally distributed servers. This means that an increase in website traffic can be absorbed by a combination of cached content and servers that are closer to your customers. Hosting and server options : If you manage your own hosting we recommend you use the “N+1” hosting method, which means you have N number of servers to handle your regular workload plus one extra server in case you see a drastic increase in traffic or one of your servers goes down. Looking for a quick way to improve website scalability before an influx of shoppers? Section is a website performance, scalability, and security solution that is easy to install and configure, meaning you can sign up now and take full advantage of improved user experience for the holiday shopping season. In addition, we have a no-commitment plan which allows you to try our solution just for a few months. Try a free 14 day free trial of our website enhancement tools, or with more questions about how to improve user experience on your ecommerce site."
"97","2016-11-30","2023-03-24","https://www.section.io/blog/mobile-website-performance-speed/","One of the most common questions we get at Section is “How will your solution help my mobile site or application performance?” With more people visiting websites, downloading apps, and making purchases through their mobile phones than ever before, it is crucial for mobile websites and mobile applications to be fast, reliable, and secure. Around 50% of ecommerce transactions are already made on mobile, and mobile applications are predicted to be a $77 billion industry in 2017. In addition, mobile visitors are even more impatient than their desktop counterparts, with over 50% of mobile users saying they will leave a site that takes over 3 seconds to load. Types of Mobile Optimized Content Before we get into how Section can improve website performance for mobile, there are some important distinctions that need to be made around the various ways mobile content can be presented. Mobile Website: A mobile website is built specifically for the mobile phone, and will have a different URL from the main desktop site, such as m.yoursite.com. To get users to this version of the site, the website will determine what type of browser they are using and direct them to the mobile site URL. Mobile sites could have completely different content or (more likely) similar content with less images and less functionality than the full website. Responsive Website: A more popular option recently due to the proliferation of large mobile phones and tablets, a responsive website is one that adjusts its contents to fit the screen size. While the URL will be the same across all browsers, the content will adjust - boxes may stack on top of each other on mobile sites, menus can collapse, and websites can even include different image sizes to be served depending on the screen size. Responsive sites involve more development work to create, but are usually easier to manage on an ongoing basis as changes don’t need to be made on two separate sites. While they may be slower to load (especially if you aren’t using a tool like Section), responsive sites provide good user experience across phones, “phablets” (phones with screens over 5.5 inches), tablets, and desktop. Mobile Application: Mobile applications are software applications that can be downloaded from an app store and live on the homescreen of your mobile device, so users can access content separately from the mobile browser. They usually are designed for a specific purpose, such as to play a game or browse an ecommerce store, however as application development has gotten more advanced their functionality has increased. Mobile apps are unique from mobile or responsive websites in that their look and feel can be fully controlled, and many take advantage of phone features, such as the phone’s touchscreen or accelerometer. Improving Mobile Performance While these three types of mobile content delivery are unique, they all share one important commonality in that they serve traffic over HyperText Transfer Protocol or HTTP. This means that no matter what type of content you are looking to speed up, a solution like Section will work across all of your applications and websites. Section improves the speed of both mobile and desktop sites by bringing the content closer to your end-user through use of a global Content Delivery Network, and by increasing the cachability of your content using Varnish Cache, an HTTP accelerator. The more content that is cached through Section, the faster it will be served to users regardless of how they are accessing that content. Because poor network connections can slow down mobile browsers, websites should ensure that they are serving content at lightning speed back to the mobile browser to reduce any additional wait time. This can be achieved through a smart caching strategy such as the one Section provides. Section delivers all of the required performance optimizations for website and mobile websites out-of-the-box, so you will see improvements in page speed and load time immediately after turning on Section. To try it for yourself, start a free 14-day trial today or contact us with any questions."
"98","2015-09-30","2023-03-24","https://www.section.io/blog/using-section.io-as-a-ssl-reverse-proxy/","Using a reverse proxy for SSL can improve site load speed and free up resources on your servers. People often set up nginx as a SSL terminating reverse proxy. Deciding which protocols and ciphers to use can be a challenge. Nginx leverages OpenSSL which provides an excellent range of TLS protocol versions and ciphers options. Hosting SSL on Section’s reverse proxy SSL termination Using Section, you can rely on the system’s TLS protocol and cipher arrangements. We keep these up to date. Section continually tests our SSL configuration to maintain SSL best practices. If you test SSL certificates on a Section site, you’ll see a Qualys A+ grade."
"99","2016-09-12","2023-03-24","https://www.section.io/blog/what-is-a-reverse-proxy/","When I first started to tell people about my new job with Section, I often was asked “What is a reverse proxy”? To be honest, I had actually googled the same question. I was familiar with proxies, but wasn’t entirely sure what the difference was between a proxy and a reverse proxy. This confusion comes from people dropping the term “forward” from forward proxy. There are forward proxies and reverse proxies, both of which are different types of proxies. By dropping “forward” from forward proxy, people ask what is the difference between a proxy and a reverse proxy, when really (using correct terminology) a reverse proxy is a type of proxy and the intended question is what is the difference between a forward proxy and a reverse proxy. Let’s briefly discuss these three concepts. What is a proxy A proxy, in the context of the internet, is a server that your web requests go through. This makes sense as the definition of a proxy from the dictionary is one who acts for another. The difference between a reverse proxy and a forward proxy is who or what they act on behalf of. What is a reverse proxy A reverse proxy acts on behalf of your web server, taking requests from externally (ie. your customers) and determining how they should be handled. A WAF (web application firewall) which as Modsecurity will handle a request by auditing or blocking the request. A web accelerator such as Varnish Cache will handle a request by trying to answer with cached information. What is a forward proxy A forward proxy acts on behalf of clients. A common use case is taking requests from internally (ie. a local network in an office) and determining how they should be handled. For example, the devices within an office may not directly connect to the internet, they may first connect to a forward proxy who then connects the requests to the internet. This allows for a system administrator to decide which content should be allowed into the network, which provides both content filtering and security. At its core, Section is a reverse proxy management platform that allows our users to easily add globally distributed reverse proxies and gain visibility into the impact they have on their web servers. If you are interested in seeing how a reverse proxy might work for your website, trial Section free for 14 days."
"100","2016-11-08","2023-03-24","https://www.section.io/blog/is-website-performance-achievable-with-cdn/","For years, companies have been turning to Content Delivery Networks to address all their website performance needs, from page speed and scalability to security. Caching content globally, which originally was the main purpose of CDNs, has undoubtedly helped create a faster and more scalable internet, but are CDNs living up to their lofty promises? Is it possible to achieve excellent website performance by simply using a CDN? Does the performance improvement offered by most CDNs justify the thousands of dollars they cost a month? It is difficult to answer this question broadly, as every website has different needs. Depending on the distribution of your audience, frequency of front-end deployments, and technical ability to spend time configuring your caching strategy, a CDN might be your best option for improving website performance. Truly great website performance, however, is going to be unachievable with most of the CDN solutions offered in the marketplace today. This is largely due to the way CDNs keep reverse proxies, software which is at the core of most CDN features, locked in black boxes in fixed networks. CDNs and Reverse Proxies If globally distributed PoPs are the backbone of Content Delivery Networks, reverse proxies are the meat of the CDN: they are powerful tools which are deployed on all of a CDN’s PoPs to perform the functions CDNs are most known for, including caching content, adding security through a web application firewall, or blocking bots. A CDN is really just a network of reverse proxies that allow you to perform these website-enhancing actions on a global network of servers. The global network reduces latency by keepings some of your content closer to the end user and reducing the strain on your origin server, but it is the reverse proxies on top of that network that provide the most performance benefits. CDNs use a variety of reverse proxies, including proprietary reverse proxies which they have built themselves and modified versions of open-source reverse proxies such as Varnish Cache. CDNs compete with each other by promising that their reverse proxies are faster, their network of servers is larger, or their servers are built on better hardware. The truth is, the differences in performance CDNs offer their customers are bound to be insignificant as long as their reverse proxies are hidden in fixed networks. Reverse Proxy Configuration and Testing By limiting transparency into how their reverse proxies are coded and configured, CDNs make superior website performance unachievable as it becomes impossible for developers to configure the reverse proxies to truly optimize for performance. Without knowing how the reverse proxy is built, making and testing configuration changes is difficult and risky to deploy. As a result of this “CDN Black Box”, websites are often limited to caching static assets, like JavaScript files that don’t change, style sheets, and images. They are prevented from caching the full HTML document or maximizing hit rate through customized configurations that would drastically improve page speed and scalability. Because most CDNs keep their reverse proxy software locked in fixed networks, developers are unable to test website changes locally before deploying to production. This can drastically increase development cycles, create bugs in production, and often leads developers to turn off CDN features rather than attempt to solve the problem in the dark. Ultimately, without exposing the reverse proxy software to the developers for customizable configuration or allowing them to test those configurations locally, the promises offered by CDNs are destined to fall flat. Section is unique in that it uses fully open reverse proxies and makes it easy for developers to configure them on a globally distributed network, and has the added feature of a local development environment. For more information about how to achieve great website performance learn about what content to cache or download our guide to optimizing ecommerce performance."
"101","2016-11-04","2023-03-24","https://www.section.io/blog/cache-is-king-magento-live/","The Section Australian contingent is excited to be attending MagentoLive Australia in Sydney from November 7-8th. As an Australian-founded company with offices in both Australia and the US and a recent Magento Select Technology Partner, we love catching up with Magento clients and hearing about trends in the ecommerce industry at Magento events. This year, we’re also launching a campaign to educate ecommerce sites on why we consider caching the #1 way for businesses to see quick improvements in website speed, page views, and even conversion rates and revenue. As part of this campaign, if you tweet #CacheisKing, even if you’re not a Magento platform or at Magento Live, you’ll be entered to win a Cache is King t-shirt and one of these BB-8 robots from Sphero. You can also enter to win at this page. Caching Immediatly Improves Website Performance Here are the top 5 reasons we think #CacheisKing: Browser caching stores copies of files on a website visitor’s own computer, reducing page load time when they next visit your site. Server caching stories copies of files up to and including full HTML documents on a caching server (or globally distributed caches such as a CDN delivers), and a properly configured cache solution will improve website speed more than almost any other website enhancement. Caching speeds up pages and it also reduces the back-end load on your server, saving you hosting costs. Using a flexible cache solution such as Varnish Cache websites can set rules so that some pages are cached and others are not, or parts of pages that include un-cacheable content such as personalized information is not cached. Caching improves both speed and scalability, the ability for your website to handle sudden influxes of traffic without slowing down or going offline. To learn more about caching, read our blogs on an overview of caching and what content to cache and why. Set up an easily configurable Varnish Cache with Section Section provides an out-of-the-box basic Varnish Cache configuration and a fully editable file for websites to configure a more advanced caching strategy. Sign up for a free trial to start caching your website today, or contact us if you’d like to speak to one of our website performance and caching experts. next generation CDN or sign up for a free 14-day trial of the only website performance solution built for developers."
"102","2017-10-09","2023-03-24","https://www.section.io/blog/debugging-varnish-vcl/","Varnish Cache’s Configuration Language (VCL) can be very approachable to get started with basic caching behaviour for your website. However, once you start to leverage the power of VCL to achieve the best possible performance for your site, you can find yourself in a situation where some HTTP requests don’t appear to be behaving how you expect. Varnish Cache has some great tools to help you troubleshoot. Varnishlog is my preferred tool for these scenarios. By default it allows you to see the HTTP request and response lines, the HTTP headers that are received, modified, and sent, and the VCL subroutines that are called as each request is handled by Varnish Cache. This capability alone is extremely useful but there are at least two other options you can enable to improve it even further. The first option is to change the Varnish Cache’s vsl_mask Run Time Parameter to log VCL_trace messages to Varnish Cache’s Shared-memory Log (VSL) from which varnishlog reads. In almost every version of Varnish Cache, you will then see in varnishlog an extra line labelled VCL_trace reporting the line number and character position within your VCL as each VCL statement is executed. This makes it easier to follow the path through your code and why particular headers are modified or particular subroutines get called. In Varnish Cache 5.2.0, the VCL_trace messages were extended to also include the VCL configuration name and the program source index to help when debugging code that leverages the VCL Labels feature first introduced in Varnish Cache 5.0. The second option is to change Varnish Cache’s vsl_reclen Run Time Parameter to increase the maximum size of an entry in the VSL. By default, the vsl_reclen is capped at 255 bytes which means that entries can be truncated, especially the ReqURL, ReqHeader, and RespHeader entries if your site uses particularly long query strings or many long cookies. When your VCL is making decisions based on these values, but the relevant portion has been omitted from the log, it can be very confusing trying to following the logic. The VCL_trace option is already enabled in vclFiddle, and both options can be controlled in Section’s Developer PoP via the proxy-features.json file. These options are typically left in their default states in Production Varnish Cache deployments however since they can have an impact on workspace memory usage and also on shared-memory lock contention during Production-level high-traffic situations."
"103","2017-04-05","2023-03-24","https://www.section.io/blog/true-value-of-dynamic-caching-html-caching/","Dynamic content caching is a buzz word in the Content Delivery Network industry: Everyone wants it, many CDNs claim to offer it, but only a few make it truly accessible. However, the real value of dynamic caching is something not often discussed: saving hosting and server costs. Here’s how it works. What is Dynamic Caching? The name “dynamic” implies that this is content that changes regularly or is unique for each website user. In reality dynamic caching usually refers to caching of the HTML document, which is the building block of the entire webpage and can change quite frequently. This is especially true for ecommerce sites or media sites who are updating content on pages regularly to reflect the latest articles, featured products, or prices. Dynamic content does not include content that is different for each user, such as the account information often displayed in the upper corner of a webpage: this content should not be cached. Reduce Server Costs with Dynamic Caching Here’s why caching of the HTML document is so valuable and why it results in higher speed than caching of static objects such as images: The HTML document is the first thing that needs to be generated and sent by the website origin server once it has connected to the browser. It includes instructions on how to draw the web page and where to find linked files such as images, CSS stylesheets, and JavaScript codes. If 1,000 visitors are going to a website within 1 minute, the HTML document will need to be generated by the origin server 1,000 times. Because of this stress on the origin servers, website managers need to plan for the maximum amount of traffic they believe they will have at any time. This means buying servers for peak traffic times, even if those peaks are only reached 1% of the time. When the HTML document is cached, the caching server (such as Varnish Cache) is the only one making a request to the origin server: If the HTML document is set to live for 1 minute, then the caching server would make one request back to the origin server per minute, whether there are 10 visitors or 1,000 visitors going to the website in that minute. This means the website servers are freed up for critical transactions such as the check-out process, increasing the number of visitors that can be served at once and at the same time reducing the number of servers that need to be bought in case of peak traffic. This saves businesses in hosting costs and ensures users still have a good experience. CDNs and Dynamic Caching Due to the high value of dynamic caching both in performance benefits and server costs, it seems that every website out there would want to take advantage of this practice. However, because the HTML document is the backbone of the entire webpage, many websites feel it is too risky to cache it. This is exasperated by the fact that the vast majority of Content Delivery Networks, which frequently manage caching for sites through a global server network, do not allow for proper testing of the cache configuration. Without being able to test that HTML caching is set up correctly, websites run the risk that they could take down their entire webpage when the configuration goes live if there are any issues. It was for this reason that Section created a development environment that pulls the CDN and cache configuration code into a developer’s local environment, so all caching can be tested before going live. By doing this, we empower Section users to cache their HTML without worrying about issues arising in production. So, while major CDNs claim to support dynamic caching, the reality is that the majority of their customers will not be caching HTML documents due to the inability to test before going live. Cache Dynamic Content with Section If you are interested in caching dynamic content and improving your website performance, please contact us for a consultation and demo. Get Started Today"
"104","2016-11-15","2023-03-24","https://www.section.io/blog/tips-to-improve-website-speed-whitepaper/","Speed is one of the most important elements of any website: slow websites lead to higher bounce rates and decreased conversions, and as more and more transactions are performed through websites and mobile applications, users expect fast load times and a seamless browsing experience. But website speed is often overlooked because it may involve several teams (marketing, web development, and back-end tech support), or is passed over for more obvious website revamps such as design. Website speed and user experience The reality is, website speed is a crucial aspect of user experience, and it should be treated as such. Maintaining an average or above-average website speed is an ongoing task. Although some of the most effective ways to improve website speed involve setting up solutions which require some technical knowledge, there are also smaller improvements that anyone with a basic understanding of how to edit their website can implement immediately. First, websites must perform a basic analysis of their current web speed. A synthetic test such as WebPageTest.org can be performed and read by anyone, and you can also perform speed tests for the websites of competitors to get an idea of where you stand compared to others in the same industry. Using the outcome of your speed test as a baseline, there are then several steps websites can take to improve their performance. Free tips to improve page speed In this free guide, Section provides the top tips used by leading websites to improve their page speed. These tips are ranked by difficulty, cost, and speed improvement. We’ve included a variety of website performance improvements ranging in technical difficulty and cost, so there’s something for everyone, whether or not you have the technical ability or money to invest in some of the larger speed enhancements we suggest. Before you embark on your speed-improvement mission, we suggest you also take stock of current marketing metrics such as bounce rate, conversion rate, and pages viewed. Once your website speed has improved, expect to see these go up! Download Section’s Tips To Improve Website Speed here."
"105","2017-05-08","2023-03-24","https://www.section.io/blog/website-performance-webinar-speed-conversions/","Join Section CEO Stewart McGrath on May 23rd at 1pm ET as he leads a webinar on the importance of website performance for ecommerce and media sites. This interactive session will give viewers solid takeaways on why website speed and scalability are important and how to start improving the performance of their own websites through both Website performance is a sometimes-overlooked element of user experience, and is becoming more and more crucial as website visitors are increasingly browsing on devices other than computers. In the ecommerce space, fast page load times can mean more products are added to a cart and the checkout conversion rate goes up since buyers can quickly move through the checkout process. For media websites, readers are encouraged to click to more articles when they have a good on-site experience, increasing page views and advertising revenue. Faster page load times also improve bounce rates and SEO for search engines like Google. How to measure and improve your website speed In this webinar we’ll unravel the different elements of website performance, show you how to measure your current website performance and what steps to take if you’d like to improve your performance. We’ll include both quick wins, things that can be easily fixed for small website improvements, and longer-term solutions which will have a larger impact on your website speed and scalability. In addition, you’ll get an overview of: What metrics to look at when measuring website performance and what your goal metrics should be. How web caching using a software such as Varnish Cache works to speed up page times and decrease server load. What Content Delivery Networks bring in terms of website performance and scalability. To sign up for the webinar, visit this page. If you can’t make it, sign up anyway - we’ll send a recording around afterwards. Get Started Today"
"106","2017-07-26","2023-03-24","https://www.section.io/blog/website-security-devops-webinar/","There are several challenges to deploying a robust security solution on your website or application. As we have discussed previously, hackers, bots, and attack types are becoming more and more advanced, and it can be difficult to block all known and unknown threats using older rules-based Web Application Firewalls. In addition website security solutions can be frustrating to integrate with modern developer workflows such as DevOps and Agile. Website Security and DevOps As more organizations shift to DevOps and Agile approaches where changes are deployed quickly and development and operation teams work closely, it has become clear that rules-based security systems cannot keep up. There are a few main reasons for this: Failure to provide full visibility to both development and operations teams Inability to quickly make changes to the WAF rules, which in a continuous integration/continuous delivery world means the WAF is not up to date with current code Traditional WAFs are built on a hardware and network-based approach when so much is moving to the cloud Failure to fully test the security solution to see how it will interact with code and block visitor As a result of the above challenges, many rules-based WAFs are deployed in detect-only mode and never transitioned to blocking mode. If they are switched to blocking mode, companies report a high number of false positives, where legitimate visitors are blocked based on overarching rules. For businesses this can mean a loss of revenue and decline in trust from real visitors who are unable to access a website. Modern Website Security Solutions Luckily there are several new solutions that have been created to solve the problems with older website security solutions. These include next-generation intelligent WAFs such as ThreatX, which block threats with no tuning needed, RASP or Runtime Application Self-Protection solutions, which detect and block threats to applications in real time, and DevOps-focused solutions like Signal Sciences, which offers a range of deployment options and gives development and operations teams full visibility into what traffic is being blocked. To learn more about how new solutions bringing DevOps and website security together and blocking more threats while protecting legitimate vistors, register for our webinar with Tyler Shields, VP of Marketing, Strategy, and Partnerships at Signal Sciences, and Daniel Bartholomew, CTO at Section. The webinar is Tuesday August 8th at 10am PT/11am MT/1pm ET and we’ll send a recording to those who sign up but can’t make the live webinar."
"107","2017-06-06","2023-03-24","https://www.section.io/blog/website-security-for-ecommerce-websites/","In the past few years it’s seemed like there has been a new widespread security breach every other week. High profile incidents such as Heartbleed and WannaCry and hacks of notable entities including Sony Pictures and the Democratic National Committee have brought cyber security to the front of people’s minds. The magnitude of Distributed Denial of Service (DDoS) attacks has risen with the increased number of devices connecting to the internet, and as more of the population engages with these devices the risk of sensitive information being taken advantage of continues to rise. Website Security and Ecommerce Bank accounts, credit card information, healthcare data, tax returns and personal identifying information are now regularly submitted online or stored in a network that could be vulnerable. Markets on the so-called “dark web” that sell stolen information can wreak havoc by enabling others to charge money to someone’s account or even steal their identity. Connected Internet of Things devices like fridges, home security systems and even cars can be taken over remotely, bringing severe consequences. Of course, some of these examples point to the worst case situations. But they demonstrate that all Internet users need to be aware of the security of websites they visit, and websites themselves need to be increasingly aware of the security lapses they could face. For the ecommerce industry, website security is a particularly important topic. Online retail stores are becoming more and more prevalent, with almost 400 billion dollars being spent online in the US alone. Pew Research found that 79% of American adults have used ecommerce sites to purchase books, clothes, makeup, and household essentials. Among people under the age of 30 that number is even higher: 90% have bought something online and 77% have used their mobile phone for an ecommerce purchase. Ecommerce sites of all sizes are susceptible to attack because they process credit card information, email addresses, and passwords for user accounts. If not properly secured, credit card numbers can be taken and email/password combinations can be tried on other websites. In the following sections we will go through what security issues ecommerce sites face - download our full Guide to Website Security for Ecommerce Sites for more information on threats and how to protect your website. Threats Posed to Ecommerce Websites Known Vulnerabilities to Ecommerce Platforms Any software you are using, including your ecommerce platform and extensions, will have certain vulnerabilities that are known to attackers. These could include ways to access your site through a backdoor, inject malicious JavaScript into a form to create new administrative accounts or takeover legitimate customer accounts, or inject other code to take over your database. Some of the most common vulnerabilities found in ecommerce sites include: Cross Site Scripting: In this form of attack, an attacker will insert a JavaScript snippet on a vulnerable web page that to a browser looks like a normal script and is therefore executed. This can then perform a number of harmful actions such as accessing a user’s cookie information to impersonate them. This technique can also give attackers access to other information on the user’s computer and leave them vulnerable to phishing attempts or malware installation. Although this form of attack may not be targeting the website itself, it is targeting your website’s users which can still impact your business. In 2016, one attack of this type impacted over 6,000 ecommerce websites by stealing customer credit card data. Even when those websites use a 3rd party payment processor or HTTPS encryption they were still vulnerable, and some did not patch the issue for months. SQL Injection: SQL injection can affect any website or web application using a SQL database, which includes ecommerce platforms such as Magento. In this type of attack a hacker can insert malicious SQL statements in a payload which will be included as part of a legitimate-seeming SQL inquiry. If the attacker gains access to the database they can create an administrative account for themselves, delete database entries, or view sensitive information. Phishing Attacks Phishing scams are often in the form of emails that look legitimate or like they come from someone you know, although phishing through phone calls also occurs. These scams usually include a link or direction to a page that if accessed will take over an email account or install malware on your computer that can steal personal information, access your microphone and camera, or log keystrokes. Targeted phishing attacks can be very convincing, and if a company employee falls for one they could inadvertently give an attacker access to their administrative account and other information that poses a risk to your website and company. Distributed Denial of Service or DDoS Attacks A Denial of Service (DOS) or Distributed Denial of Service (DDoS) attack aims to take down your site by overwhelming servers with requests. In its distributed form, the attack will come from hundreds or thousands of IP addresses which usually have been compromised themselves and tricked into requesting your website over and over again. This attack type overloads your servers, slowing them down significantly or taking your site temporarily offline, preventing legitimate users from accessing your site or completing orders. DDoS attacks are difficult to stop by simple IP blocking since they come from many sources, and those sources often look similar to your legitimate traffic. As more devices are connected to the Internet, DDoS attacks have grown both in prevalence and strength, meaning even websites with a large number of powerful servers are unable to withstand them. High-profile ecommerce sites are susceptible to this type of attack, and smaller ecommerce sites may also be vulnerable if their web host or DNS provider is targeted: For example, in October 2016 DNS provider Dyn was targeted by a DDoS attack and thousands of websites were taken offline as a result. Bad Bots Targeting Ecommerce Bots are prevalent all over the Internet, and can be both good and bad. “Good” bots are used by search engine sites such as Google and Bing to crawl and index your site for their search results. You want your site to be visible to these bots so that when someone searches for keywords related to your site it will show up in the results. However, there are also malicious bots which gather information from your website such as pricing data, hold products in carts without intending on buying them, buy up your inventory of a limited release to resell it at a higher price, or take over real accounts by guessing the passwords. Some bad bots can also access your database and gather a list of user account logins that can be resold later. A recent report by Distil networks found that 97% of sites are hit with some sort of bad bots. For ecommerce sites, bad bots account for an average of 15.6% of a website’s traffic, with good bots accounting for 9.3% of traffic. Bots can be programmed to perform a wide range of activities, but here are the most common for ecommerce sites: Price Scraping: If your site has unique pricing and product information, the chances are extremely high (around 97% according to Distil) that you will be hit by scraping bots. These bots collect pricing and product data and send it back to the bot-maker, who could be a competitor, so they can lower their prices and take sales away from you. Scraping can also hurt SEO and the likelihood that potential customers find your product, as the scrapers may create duplicate content which search engine then take into account when ranking websites. This type of bot can be extremely hurtful if you are selling the same product as other websites and trying to price competitively. Login Fraud: Bots can attempt to login using one of your real user’s credentials by guessing the password by rapidly going through a dictionary of words and number combinations (a brute-force approach), or by testing known credentials that have been leaked elsewhere. If bots are successful at logging in, they may not use the account information immediately, but sell the information to a third party. If a purchase is made using a stolen account and stored credit card information it will compromise the trust your users have in your site and result in a loss of money if an order ships and you need to refund the customer. If admin accounts are compromised using these same tactics, you could be unwittingly giving away a larger list of account logins. Bots can also create new accounts in order to test stolen credit card numbers. If bots are able to access an account by guessing the login, they can guess the expiration date and CVV number of stored credit cards and make a fraudulent purchase. Holding Items: Because bots can act more quickly than human browsers, they are able to refresh pages many times over to check for sales or limited-release products. Bots can add items to a cart, limiting inventory for actual users who came to your site looking for a specific product. If the item has a high resale value, bots may purchase it and resell it at a higher price on a third party website such as eBay. Even if bots do not ultimately purchase the product, your actual visitors may abandon your site if it appears an item is out of stock, and when the bot releases the product your cart abandonment rate will go up. Incorrect Analytics: A secondary effect of bad bot traffic is that it can significantly impact the analytics you track. Over 50% of bots can load JavaScript, which is the mechanism most analytics tools use to measure page views, bounce rate, conversion rate, and more. Since bots are imitating human behavior, they will be included in your analytics and can do harm to these important metrics, lowering your average conversion rate or convincing you to spend more money on advertising. Bots can also make it falsely appear that one advertising campaign is working better than another, or in other ways encourage you to target specific keywords or interests which are unlikely to have good a good click through rate. Man in the Middle Attacks A man in the middle attack is when an attacker listens in on a user’s communication with your website. This could happen because a user is connected to an unsecure public wifi network, has been tricked into connecting into a vulnerable network, or because a hacker has targeted a specific network and gained unauthorized access to it. If the connection between the user and website is not encrypted, a man in the middle attack could see all of the pages a user is visiting, view emails they are sending, and intercept usernames, passwords, and credit card numbers. Even if a website has a SSL/TLS certificate to encrypt data with the HTTPS protocol, there are a number of ways hackers can trick the user’s browser and gain access to unencrypted data. In addition, websites who only use HTTPS on certain pages (for example on the payment or login pages) are leaving their users more susceptible to this type of attack, as attackers could steal session cookies or other sensitive information when users browse an unsecured page on the same website after they have logged in. Malware Malware is the malicious software that attackers insert into your web files or pages once they have gained access to your site. Malware may be found on an individual’s computer if they have themselves fallen victim to a phishing attack or otherwise been compromised, or it may be inserted directly onto your website after a successful SQL injection or if administrative account access has been granted to a harmful entity. Malware can also be installed on your site if you are on a server with other compromised websites in a cross-site contamination incident. Popular ecommerce platforms like Magento are particularly susceptible to widespread malware infections due to their prevalence in the market. As with software, malware can perform an extremely wide range of activities, from turning your computer into a botnet that can be part of a DDoS attack, to stealing credit card and account information from your website users. One type of malware that targeted Magento sites was able to take credit card information and store it in images so that the attacker could easily access it without flags being raised. Malware can also perform spam activities by linking to websites selling pharmaceutical or other goods, redirecting pages to other sites, inserting pop-up ads onto your site, or adding tags into the metadata of your site. Protecting Your Ecommerce Website from Attack The above represent some of the most common threats that can significantly harm your ecommerce business. For a reference book of these threats and additional information on how to protect your website, please download our free Guide to Website Security for Ecommerce Websites. You’ll get detailed information on securing your website through measures including: SSL certificates and HTTPS encryption PCI compliance Security patches Vulnerability scanning Web Application Firewalls Bot blockers Content Delivery Networks Download the guide here. For more information on how Section can improve the security of your website, please contact us."
"108","2017-05-15","2023-03-24","https://www.section.io/blog/speedindex-for-website-performance-monitoring/","One of the trickiest parts of understanding how fast your website loads for users is knowing which metric to use. The natural inclination would be to measure how long it takes the entire page to load. Others would argue that you should instead look at how long before something appears visually on the page. Or should we instead focus on Time to First Byte because Google cares about it enough to factor it into your Search Engine Results Page? I would argue that all of these metrics may have specific use cases where they reign supreme, but the most important metric that people should be caring about is also the least understood: SpeedIndex. Never heard of it? SpeedIndex was originally developed by WebPageTest.org in 2012 and has grown in use over the years but still remains a relatively obscure metric. So what is SpeedIndex and what does it do that the other metrics don’t? Measuring What Matters What makes SpeedIndex special is that it attempts to measure what actually matters to you - the experience your visitor has with your site. Rather than count the seconds between two events that may correlate with a good or bad experience, SpeedIndex actually examines the the progress of how the visible page loads and derives a score for how quickly the content appeared. To help understand why this is so important, let’s imagine two identical web pages (Page A and Page B) with the exact same full page load time of 5 seconds. Page A takes about 1 second to load the first byte, but doesn’t show any visual content on the page until around 3.5 seconds. Page A is given a SpeedIndex score of 5000. Page B loads the first byte within 150ms, and loads 90% of the visual content of the page within 1 second. The final 4 seconds are spent loading third party Javascript that doesn’t impact the users experience. Page B is given a SpeedIndex score of 1000, 5 times better than Page A. You can see from this example how little the full page load time tells you in comparison to SpeedIndex. A real-world example of this can be seen below: Beauty sites Julep.com and Birchbox.com both have “load times” of around 6 seconds under synthetic conditions from WebPageTest. However, Birchbox has a fully loaded time of 13.7 seconds and Julep has a fully loaded time of 9.0 seconds. In addition, Julep’s start render time is faster and their content loads more quickly. As a result, Julep’s SpeedIndex is around 6,000 while Birchbox’s is around 13,000. You can see the visual representation of these differences below. Clearly, by looking at only the “load time” shown by WebPageTest, you would not get the full picture of how these competitor sites are performing. Calculating SpeedIndex Unlike most metrics, SpeedIndex isn’t simply a timing measurement between the initiation and completion of a process. It instead measures how long it takes to reach visual completeness of the page by analyzing a film strip of the page load. This sounds complicated because it is! A more in depth explanation of how SpeedIndex is calculated is covered at WebPageTest.org but we will walk through the basics here. SpeedIndex is measured by taking snapshots of the page while loading, and identifying how complete the visual content on the page is for each snapshot. For example, 0% would be recorded for a blank screen and 100% for a complete page above the fold. Once a score for each snapshot is taken, the following formula is calculated: (time between snapshots)*(1 – visual complete % / 100) This calculation would be repeated until the page is finished loading. For a page that loads in 3 seconds with 500ms intervals between snapshots, the resulting data could look something like this: 500 * ((1-0)/100) = 500 500 * ((1-0)/100) = 500 500 * ((1-.25)/100) = 375 500 * ((1-.60)/100) = 200 500 * ((1-.75)/100) = 125 500 * ((1-1)/100) = 0 To arrive at the final SpeedIndex score, you would add up the scores for each snapshot: 500 + 500 + 375 + 200 + 125 + 0 = 1700 By breaking down how the score is calculated, you can see that by completing more of the page earlier during the loading process, your resulting SpeedIndex score will be lower. That’s why site owners should optimize to lower their SpeedIndex scores as much as possible. It’s also important to not think of SpeedIndex in terms of time. While the time it takes to load the visual content of a page is a crucial element in deriving a score, SpeedIndex is more abstract and should be used within a context of optimizing your site for a better user experience. How to Use SpeedIndex SpeedIndex is best employed as an optimization tool to focus on improving the experience of your visitors. By taking steps to reduce the SpeedIndex time, you can be confident that you will be improving the experience of your site’s visitors rather than spending time on something that will have little to no business impact. SpeedIndex is highly effective at focusing your optimization efforts on problems that actually impact your business. Let’s quickly walk through the best ways to get your SpeedIndex number, and some easy wins to start bringing your score down. Getting your SpeedIndex score: Because SpeedIndex is relatively complicated to calculate, it’s not included in every site monitoring tool. It is also highly subject to the methodology of the tool being used, so it is best to test within a controlled environment. Fortunately there are many free synthetic monitoring tools out there that will easily provide you a SpeedIndex score for any web page. (WebPageTest.org, Pingdom, and Section’s Synth to name a few). The benefit of synthetic testing as opposed to Real User Monitoring for optimizing your SpeedIndex score is that you can avoid the messy outlier data that can cloud your results. Think of synthetic testing as running an experiment in a lab rather than collecting data in the wild. It is far easier to isolate problems and measure your progress in fixing them while measuring results in a controlled environment. Quick SpeedIndex Wins: Truly optimizing your site can involve a lot of long term projects that may include infrastructure considerations such as your hosting provider, code base, and caching strategy. These are all very important, but there are some things you can do immediately to help bring down your SpeedIndex score and start improving the experience of your visitors. Let’s start by looking at some content optimizations that require little effort but can make a huge difference. Image Optimization: It goes without saying that the larger your image files are, the slower they will load. Many people think there is a necessary trade off between image quality and site speed. There are, however, free tools to drastically reduce your image sizes without noticeably impacting their quality (even on hi-definition screens). Tools like TinyJPG and Optimizilla allow you to upload your images and resize them easily. You can also resize your images on the fly with a reverse proxy like Google PageSpeed (now available on Section’s Edge Compute Platform) which ensures images are optimized for performance. You also get added benefits like lazy loading, css minification, inline images, and javascript deferral. Gzip Compression: You can do more than just compress your images to reduce the size of the files on your webpage. By enabling Gzip compression, you can reduce HTML, CSS, PHP, and Javascript files by as much as 30%. This will allow your users to download a much smaller file and then decompress it in the browser after the download is complete. Check out this site for how to enable Gzip compression using an .htacess file. Critical Render Path Optimization: SpeedIndex cares less about how long it takes to complete the full page load and more about how quickly content appears to the user. Therefore, optimizing the render path can provide big improvements to your SpeedIndex score. There are a lot of ways to improve the critical render path (Catchpoint has a great deep dive here), but in essence you want to reduce any render-blocking scripts, defer your javascript files, and reduce the number of css files that could prevent visual content from loading quickly. Moving to HTTP2 can also optimize your critical render path optimization. SpeedIndex Problems to Consider While SpeedIndex is a fantastic metric, it is not without it’s drawbacks. Some sites may not be a good fit for what it is trying to measure. For example, single page apps and dynamic sites are likely to have superficially lower scores since they don’t need to refresh the page once it’s loaded. Sites that have a hero with rotating images may be penalized as content continues to change after the page loads. While it might not be a universally applicable metric, when it comes to site speed optimization, SpeedIndex remains the most crucial metric to pay attention to as it is the most likely to impact your customer’s experience and ultimately your business."
"109","2017-04-11","2023-03-24","https://www.section.io/blog/containers-are-inevitable/","Containerization breaks applications into components which can easily be deployed and scaled, bringing many benefits to software development. The flexibility containerization brings fits well with modern application development practices and DevOps principles, and has become popular with the growth of platforms including Docker and Kubernetes. The Continued Rise of Containers Earlier this week the popularity of containerization and services built around containers was again demonstrated with the acquisition of Deis by Microsoft. Deis provides tools for managing applications built on Kubernetes. This was another aggressive statement by Microsoft highlighting the emerging dominance of containers as they change the way application engineers build, ship, run and manage applications. We have watched container technology bring fundamental change to application architecture and management across many industries for several years, and clearly the trend is catching on: Datadog updated their Docker Adoption Report last year and showed that adoption rates increased 30% year over year. What’s more containerization is showing no sign of slowing and nor should it. Using Google search as an indicator of interest in Docker and VMware as a measuring stick, we can see that as a percentage, Docker is still on a steady rise. Containers and Content Delivery Section is committed to making content delivery easier and more accessible for developers using Agile and DevOps methods. Because of this, we chose Docker containers as the building block of our content delivery solution over 30 months ago, and are the first and only enterprise-grade global content delivery system built using container technology. Our entire solution now runs on Docker and a combination of Kubernetes and some home-grown container orchestration. Below we go through what we run in containers and how Section users benefit from our leveraging of this technology. What do we run in Containers? In containers we have our: Global Delivery Platform Running a discrete and unique container chain for every single one of our customers’ web applications on; Major clouds including AWS and Azure Specialized bare metal hosting services Metrics platform Running a unique and discrete stack of Graphite and Grafana for each customer Logs Platform Again, a unique and discrete Elasticsearch, Logstash and Kibana stack for each customer application Git Repositories Providing our agile application development for configuration of Section User Portal Web portal hosted in Docker containers hitting our API endpoints 5 Benefits of Containers for our Customers 1. Portable PoPs Our points of presence consist of a multi-tenant Docker host built to run under our smart load balancing and DNS layers. Because our PoPs can run anywhere you can run a virtual machine (or a Kubernetes cluster), Section’s PoP options are practically limitless and can leverage: The capacity and peering of the public clouds like AWS, Google and Azure Specific benefits of boutique hosting companies like Packet.net who provide bare metal as a service or regional specialties Behind the firewall PoPs for extension of the content delivery solution into the enterprise 2. Developer Workflow A special case for PoP delivery is into the developer workflow. Section customers can git clone a Docker repository onto the developer workstation. This “Developer PoP,” not offered by any other CDN, is powered by the flexibility of container technology. By running a Developer PoP, engineers can ensure their application works seamlessly with the production PoP configuration before pushing application code to test, staging or production. Conversely, configuration changes to the content delivery solution can be made (and tested) at the developer workstation where feedback loops are fastest; allowing engineers to drive the configuration harder to achieve optimal caching, security and application optimization outcomes. 3. Choice of Reverse Proxy Software By publishing reverse proxy software such as Varnish Cache in containers and being able to run those containers in a multi-tenant PoP, Section can deliver a different set of proxy software for different customers even though they are running on the same network. Thanks to the flexibility of containers, when using the Section platform, customers can chose from a library of containerized reverse proxies and then chain those containers together in the order which suits each individual application. A customer may choose to run Varnish Cache and/or PageSpeed for performance and scalability and add ModSecurity or Threat X for a web application firewall. Different versions of the software can be run in the same PoP for different applications or customers. 4. Independent Upgrade Paths Given that customers can run different versions of the same software subject to their needs at any time, customers can also choose when they would like to upgrade. They can simply swap out one container for another when ready. For example, customers can choose to upgrade from Varnish Cache 3 to 4 or 5 whenever suits them. Given the syntax changes between versions, not all customers are ready to upgrade at once so forcing them to do so does not make sense (nor does leaving every customer languishing on outdated versions software because the delivery platform cannot upgrade in a timely fashion). Noting the above developer workflow, customers may also choose to create a local git branch to upgrade versions of software and test locally before promoting and merging the new version of software (and respective configuration changes) into the production branch. 5. Process Isolation One of the primary benefits of containerization in a shared delivery platform is the isolation of the processes which execute inside the containers. This is quite a benefit from a security perspective as it prevents leakage of process execution which is important especially in light of some recent security problems on other global CDNs. Try out containerized Content Delivery To try Section’s content delivery solution and developer workflow please get in touch and we’d be happy to give you a demo and a free 14 day trial of our system and reverse proxies such as Varnish Cache, Pagespeed FEO, and Threat X. Get Started Today"
"110","2017-04-26","2023-03-24","https://www.section.io/blog/sectionio-open-source-manifesto/","In 2005 Linus Torvalds, the creator of Linux, began working on a version control system that would soon revolutionize the way developers collaborated. Realizing that proprietary source control management systems were not sufficient to achieve what Torvalds needed, he decided to design his own that would meet his needs. Torvalds also implemented a design criteria to look at Concurrent Version System, a client-server revision control system, as an example of what not to do, and when in doubt, make the exact opposite decision (as explained in a 2007 Google Tech Talk). This project became known as Git, and eventually became the most widely used source code management tool. In my opinion, Linus Torvalds is a true champion of humankind, his impact on the “information age” has been tremendous, giving us both Linux and Git. It is also hard to overstate his impact on the world of open-source software which has played a major role in accelerating the technological advancements over the last several decades. Embrace Open Source and a Choice of Software at the Edge When we set out to build Section, I took inspiration from the design criteria Torvalds laid out while developing Git. Following Torvalds’ philosophy when he built Git, we looked at legacy Content Delivery Networks as an example of what not to do as we built Section. Doing the opposite of legacy CDNs became a sort of internal manifesto, and led to the following decisions. Don’t hide your software. Legacy CDNs lock their edge software in black boxes making it impossible for developers to understand what is happening. Hiding edge modules leads to difficult or impossible troubleshooting and slow deployments, while slowing innovation in the CDN industry. Don’t pretend that your edge software structure is the right structure. Every website and application has different priorities, and truly great performance and security cannot be achieved with a one-size fits all approach. Giving developers the flexibility to customize their edge container stack will lead to better performance and security outcomes. Don’t deploy hardware Rather than building a closed network, aggregate existing infrastructure and make it flexible enough to meet the performance, security, and scalability needs of all websites. Don’t treat it as “black-box” infrastructure The best performance comes from putting the developers in the driver’s seat. Give them a platform rather than a network to push the limits of what’s possible in edge compute and achieve truly great experiences for their customers. Don’t hard-code the stack A modular approach to the edge container stack that incorporates both open-source and proprietary software spurs innovation and allows websites to take advantage of cutting edge technology without needing to change vendors. Don’t leave diagnostics to the end user - build them in. Comprehensive diagnostic tools need to be in the hands of developers in order for them to achieve great results. The ability to analyze logs, metrics, synthetic testing, and real user monitoring need to be a standard feature set in a performance and scalability solution. Don’t hide open source. Don’t leverage open-source software to create a proprietary edge module. Give developers access to unaltered open-source software and leverage the open-source communities to help them achieve the best outcomes. Legacy CDNs will not survive in a world created by Linus Torvalds. We built Section from the same values that led to the creation of Linux and Git. Transparency, collaboration, and community."
"111","2017-01-24","2023-03-24","https://www.section.io/blog/cdns-are-dying/","For nearly 20 years, use of Content Delivery Networks (CDNs) has been a smart way to deliver web traffic. CDNs promise faster, more secure and more scalable web applications. However, modern software development practices and the definition of networks and infrastructure using a software-centric approach are making legacy CDNs increasingly less relevant. “Software is eating the world” and CDNs have forgotten to come to the table. The problem is that the CDN network and software structure is the product of a waterfall development world. CDNs can neither embrace modern software development speeds themselves nor support applications which use modern software development practices. CDN Software Can’t Change Fast Enough Aside from DNS and routing technologies CDNs consist of two key components; Computers distributed around the Internet (Points of Presence or PoPs) which run the CDN software CDN Software - also known as Reverse Proxy Software (Used for Caching, Web Application Firewall, Bot Blocking, Image Optimization, WAN Optimization, Front End Optimization, API Authentication etc) Akamai was the first the build out a network using DNS, distributed computers and reverse proxy software. As was the custom at the time, they coupled the software tightly to the network and compute. Following the same network build approach of Akamai, the reverse proxy software has been so tightly coupled to the network infrastructure by all CDNs that making fundamental changes to the software itself is proving complex for CDNs. Unfortunately for CDNs, advancements in reverse proxy software are moving fast. Dramatic new approaches to handling web traffic for the purposes of improving security, scalability and performance are being launched more frequently than ever before. CDNs cannot move quickly enough to take advantage of these software developments. By way of example; Level 3 has taken over three years to migrate from its Squid based caching proxy to Nginx. We believe this project may still be ongoing. Fastly’s caching reverse proxy is still based on Varnish Cache 2.1 whereas Varnish Cache itself has now moved on to version 5. The Modsecurity based firewalls being run by the likes of Akamai and Cloudflare are being left way behind in features, functionality and ease of use by the newer “learning” WAF products like Threat X and Signal Sciences. Newer and smarter Bot Blocking and Anti Scraping proxies like Perimeter X and Distil Networks have hit the market leaving the Bot Blocking capabilities of the CDNs behind. Fastly have been promising the release of a WAF for years but have not yet delivered. The only thing we can be sure of with respect to proxy software for the delivery and security of web traffic is that there will be more coming onto the market and that software provided by niche players and specialists will become significantly smarter than what CDNs currently provide. Due to the old school architectural constraints, CDNs cannot move to deploy these newer reverse proxy technologies. Their features and functionality will be overrun. A software-based approach provides customer level flexibility on which software is run for each customer and even what version of software is run per customer. Upgrade paths can become independently driven on a per customer basis; as compared with an upgrade of the amorphous mass of proprietary proxy software legacy CDNs run. What’s more, customers of a software-defined proxy network can become future proofed against changes and upgrades to proxy software. When new software arrives they can test and deploy when ready. CDNs Were Not Built to Support Modern Agile Developers CDNs will not keep pace with the agile movement. As Continuous Delivery and Continuous Integration are more widely adopted, it is becoming more challenging for CDNs to keep in sync with the application being delivered, meaning CDNs are becoming generally less effective. As we have noted in previous posts, CDNs live only in production. CDN proxy software, and hence the impact it has on the application it serves, remains totally separate from the core application development process. This breaks the fundamental principles of agile software development. Again, due to the old school architectural constraints (relating to the locking up of the proprietary proxy software with the network) CDNs cannot be pulled into the software development lifecycles for modern web application development. More sophisticated customers are looking for CDN integration with their development lifecycle right now. Take for example the following hacks that have been created to deal with the lack of developer tools supporting agile teams in today’s CDNs: From REA Group to statistically test Akamai config in production or staging, well after development cycles: https://github.com/realestate-com-au/akamai-rspec/ Fastly emulator built by Reddit: https://github.com/reddit/varnish-fastly Others quietly (or sometimes not so quietly) suffer the consequences of missing out on the ability to modify, test and deploy the CDN software in their development environments. For example, we wrote about a Steam Store incident in which the website needed to rapidly update caching configuration after a DDoS attack. We have also heard many examples where customers have turned off the Web Application Firewall or caching features of a CDN for which they have paid simply because they cannot run a sensible development and test cycle against these valuable features and hence, cannot make them work safely with their application. A Software-Driven, Agile Content Delivery Solution Reverse proxy software can provide a vast range of tremendous benefits for web application performance, availability and security. The challenges of the modern Internet for web content providers are increasing. It’s harder now for content providers to deliver larger amounts of more complicated web content to a wider and more complicated range of devices with more demanding consumers of that content. Against these challenges, reverse proxies should play an even larger part of the content delivery solution. We need a new, software-driven approach to providing web engineers with access to and control over reverse proxy software; regardless of whether the software is running at the origin or in a CDN. Engineers should be able to safely exploit the tremendous benefits of reverse proxies. Engineers should not feel locked into any one proxy software stack at any one time but be able to pick and choose the tools that work best for their website. By decoupling proxy software from the networks and taking a software-driven approach to configuration, management and deployment of reverse proxies, Section’s Edge Compute Platform is the future of a web application delivery platform. The Edge Compute Platform gives developers full control over reverse proxy configuration, regularly adds reverse proxies to its offerings, and provides a testing environment so changes can be tested locally. To learn more about how Section’s Edge Compute Platform can work for your website, please contact a member of our team today."
"112","2016-01-04","2023-03-24","https://www.section.io/blog/reverse-proxies-for-agile-development-vs-waterfall/","We are fans of agile development practices. We value the ability to push application changes to production quickly, safely and often. We also love the incredible benefits which reverse proxies can provide in the delivery chain for websites. If you want your website to load faster, be more cost effective to deliver, stay up longer when traffic spikes, be more secure or generally delivery a richer browsing experience for your customers, then reverse proxy servers should be in your website delivery chain. What we don’t enjoy… …is seeing the way in which reverse proxy servers have been smashed into modern web delivery chains whilst ignoring the basic principles of agile development practices. We all know agile development environments should be the same (or as close to) production development environments as possible. We know reverse proxy servers have direct effects on the HTML and objects being delivered into users’ browsers; they are an extension of the application code base. However, up until now, reverse proxy servers have not been considered 1st class citizens of the development environment. An interesting production example of the implications of this was the recent unfortunate incident experienced by store.steampowered.com. According to Valve’s post mortem blog, the Steam Store was the subject of a DDOS attack and in response, some new HTML caching configuration was rapidly deployed into the production reverse proxy servers. A quick DNS dig on steampowered.com indicates that the reverse proxies in question in this instance are Akamai’s Content Delivery Network (CDN). I wonder if the HTML caching rules deployed went through Steam’s normal agile development cycle working from the developers’ machines all the way through staging and test environments before hitting production? I will the guess the answer is “no way”? If it takes hours for a config change on the Akamai network to propagate and the developers don’t have the Akamai reverse proxies on their development machines, there is little chance the developers would have enjoyed the luxury of a fast feedback loop on their local machines. They would not have had the opportunity to thoroughly test and tweak the effect of the new HTML caching rules prior to release nor test application code changes against any HTML caching ruleset. I am guessing there was a pro services engagement to implement the caching rules by Akamai folks who (through no fault of their own) just can’t have an intimate knowledge of the Steam Store application in the same way the Steam Store developers do. In the midst of a DDoS attack there would have been pressure on to move fast (like agile fast!) but unfortunately, modern CDNs were built to support waterfall development models and not agile CI/CD workflows. Modern CDNs were built to go through a once off pro services engagement to get “set up” and then left alone. This approach is not workable when application code is being changed constantly and the application outcomes for users is so dependent on the interplay between application code and the reverse proxy configuration. Broad and deep HTML caching correctly implemented is an excellent part of a website delivery chain. Unfortunately, many websites to date have not been able to avail themselves of the benefit precisely due to the “session leak” issues which Steam Store experienced on their CDN. Without the opportunity to configure and test the HTML caching properly in their development environments or then continue to test ongoing application code changes against the configured HTML caching rules, many websites choose not to take advantage of the tremendous benefits of HTML caching. “When Dev and Prod conflict Production features are removed” We see the same issues with Front End Optimisation reverse proxies such as Google’s ModPagespeed, Load Balancers like HA Proxy, Web Application Firewalls like ModSecurity and all the proprietary reverse proxies powering commercial CDNs. Often we have seen customers buy the comfort of a WAF deployed on their chosen CDN. They will take a waterfall approach to the configuration of the WAF, spending thousands on a once off project to set up or configure the WAF. Then, as application code is changed in an agile fashion and deployed weekly, daily, or hourly, the WAF settings are slowly turned down (or off) as the WAF configuration conflicts with the application code being deployed. Reverse Proxies should absolutely be used to improve users’ experiences of web applications in the way in which Steam Store attempted with Akamai. They reduce the cost to serve, they improve security, they improve user experience and indeed give developers a whole new toolset in their armoury with which to bring innovation to web applications. Hey, if you want to go green, reverse proxies can even reduce the carbon footprint through overall reduction of compute! However, unfortunate issues such as those experienced by the Valve team with Steam Store or the experience of many websites who have purchased WAFs and then turned them off, will continue to be a part of reverse proxy life (whether deployed as CDNs, or even Application Delivery Controllers (ADCs)) where those reverse proxies are not part of the agile development workflows. Get more from your Reverse Proxies We believe in empowering agile developers and operations teams to take full advantage of the awesome benefits of reverse proxies servers for their web applications. Developers and Ops teams should use reverse proxies in their development environment and have full and open access to all the metrics, logs and alerts they need in all environments to configure and manage those reverse proxies."
"113","2018-07-26","2023-03-24","https://www.section.io/blog/elasticsearch-and-kibana/","Elasticsearch and Kibana are open source tools run by Elastic. Elasticsearch, at its simplest, is a document search storage engine and Kibana is the user interface for Elasticsearch. Elasticsearch Elasticsearch is a search engine based on the Lucene search engine. It is as scalable and flexible as its name suggests. Elasticsearch stores data centrally and its documents can be searched in near real-time. It enables advanced search queries for detailed analysis. You can perform and combine multiple types of search, including structured, unstructured, by geography, by metrics, etc. Advanced search operations like paging, sorting, filtering, scripting, aggregation are all also available. Elasticsearch offers a comprehensive REST API that can be used to interact with your Elasticsearch cluster. In addition to executing search operations, you can also use your API to check your cluster, node, and index health, status and statistics; administer your cluster, node, and index data and metadata; and perform CRUD* (Create, Read, Update, and Delete) and search operations against your indexes. According to the DB-Engines ranking, Elasticsearch is the most popular enterprise search engine and one of the ten most popular database management systems. Most websites that store large amounts of data use Elasticsearch for their search engines as it is so fast; for instance, Wikipedia, Ebay, Yelp and Netflix are all users. *NB You can’t use the CRUD operations with the elasticsearch index provided by Section Elastic Elastic, the company behind Elasticsearch, has always harbored ambitions beyond just search technology. Its Elastic Stack, in which Elasticsearch remains the key component, also includes Logstash for data collection and Kibana for analytics and data visualization. These three products are designed for use as a single integrated log management tool, previously known as the ELK Stack. However, the company then added a fourth product, Beats, to its stack (a platform for single-purpose data shippers) and continues to add to its portfolio - largely by acquiring startups in different fields, including most recently Swiftype. All its products are open source. The Elastic Stack can be deployed on premises or made available as Software as a Service (SaaS). Kibana Kibana is Elastic’s data visualization plug-in. It offers visualization tools on top of the content indexed by an Elasticsearch cluster. Kibana’s diagrams can visualize complex queries executed through ElasticSearch, as well as geospatial data and timelines that show how different services are performing over time. Custom graphs that fit the needs of specific applications can be generated and saved. Through visualizing the information stored in Elasticsearch, Kibana gives developers’ rapid insights into the documents stored and how their system/s are operating. Lucene Search Query Syntax in Kibana There are multiple ways in which you can search Kibana, for instance: Enter a text string to perform a free text search e.g. enter ‘chrome’ to search all fields for the term ‘chrome’ To search for a value in a certain field, prefix the value with the field name e.g. enter ‘status: 100’ to find every entry that contains the value ‘100’ in the ‘status’ field To search for a range of values, use the bracketed range syntax as in Lucerne: [START_VALUE TO END_VALUE] e.g. to find entries that have 2xx status codes, enter ‘status: [200 to 299] Use the Boolean operators AND, OR and NOT to indicate more complex search criteria e.g. to find entries that have 2xx status codes and an extension of ‘php’’, enter ‘status: [200 to 299] AND (extension: php) How Section Works with Elasticsearch and Kibana At Section we provide a hosted Elasticsearch and Kibana instance for each customer, which provides transparency and visibility for developers. We use Elasticsearch and Kibana to gain visibility into our customers’ HTTP traffic, taking the logs from each proxy in your stack and putting them into Elasticsearch to be able to track requests, identify problematic patterns and solve any issues that may need fixing. Indexes, Types, Documents, Proxies, Fields Each Section application has its own Elasticsearch index, which acts as an identifier or tag. This index is used to track the customer application through Elasticsearch and/or Kibana. Inside each index, every document is further broken down by a _type field, which specifies the type of log for each proxy. Every application stack at Section has an edge proxy and a last proxy, plus the proxies that relate to each module in each specific application stack e.g. a Varnish proxy, an OpenResty proxy, etc. Each _type has different types of field, for example geo info path, URL, user agent, etc. Some of these are shared across logs for the different proxies; others are specific to certain proxies. Edge Proxy / _type: edge-access-log Every application proxy stack within Section starts with the edge proxy. When a customer makes a request, it always hits the edge first then goes through the proxies for the other modules until it hits the last proxy. The edge proxy acts as the endpoint through which the user’s web browser connects. It performs several critical actions: Performs the TLS handshake for HTTPS connections Routes requests to the correct application proxy stack Implements the HTTP/2 protocol Requests correlation These functions ensure that your experience on the Section platform is consistent irrespective of the other proxies in your application’s specific stack, or the order in which they may appear. IP Geolocation As each request comes in, the edge proxy aims to resolve the connecting IP address to a specific geography. These results are then shared with the other proxies in the application’s proxy stack and with the origin web server as HTTP request headers. These include the country code e.g. US for the United States; the country name; the region; the city (if applicable); the latitude, longitude, postal code and for U.S. IPs only, the Nielsen Designated Market area ID as used by DoubleClick. These results, along with the geo-fields, are always logged in the edge-access-log. Client IP Detection The edge proxy adds a True-Client-IP request header to every request that comes in. This provides the IP address of the client that connected to us, and is the same IP address used for IP geolocation. The request header can then be used for fraud detection, IP whitelist/blacklisting, rate limiting and logging client usage. We log this field in the edge log as “remote_addr”. Request Correlation As part of the edge proxy’s handling of each incoming request, it generates a unique identifier, which is added to the request via a section-io-id HTTP request header. NB The format can change without notice, so it should always be treated as an opaque string. This request header then travels through each proxy in your stack in addition to your application’s origin web server. When the user agent receives the final response via the proxy stack, the edge proxy inserts the same identifier as a section-io-id HTTP response header. This enables easy correlation of log entries across all the proxies in the application stack. The section-io-id request header can also be logged on your origin server to help with diagnostics. Every log from the edge proxy - through your custom stack of proxies - to the last proxy contain a section-io-id for traceability. For more information on the Edge Proxy, see our related guide. Last Proxy The last proxy communicates with the customer’s origin server, and is the final proxy in the application stack sequence of proxies. These are the two types of logs we capture from the last proxy: _type: last_proxy-access-log The last proxy access log contains fields related to the upstream response we receive from the origin server. This is a really good place to look when determining whether an issue is originating from Section, or the origin server itself. Here are examples of the types of upstream fields we receive: upstream_bytes_received upstream_http_cache_control upstream_http_content_type upstream_request_host upstream_response_time_seconds upstream_status _type: last_proxy-error-log The last proxy error log contains any errors generated by nginx when communicating with the origin server; for example, it would log an entry if our last proxy was unable to establish a connection with the origin server. Use Cases for Elasticsearch and Kibana Debugging & Visibility The primary use case for Elasticsearch and Kibana at Section is debugging and the next most common is visibility. When a customer reports a 500 error, something has gone wrong on the website’s server that needs to be further investigated. If one of our customers asks for our assistance working out what the exact problem is behind the 500 error and supplies a correlated section-io-id from the response headers, we can easily see where that error originated from and the time that it occurred. By searching via the section-io-id, Kibana will parse each document logged, no matter the type and pull all related docs. By doing this, we can determine what in the stack generated the error message. This will tell us if the problem is originating from the customer’s server or if the error is being served from one of the proxies in the customer’s Section proxy stack. If we identify a pattern, the logs stored in Elasticsearch will help us identify when the problem started, what the server did in response and what needs to be done as a fix. We will always help our customers diagnose any problem they report; however, when we get back to them with our response, we like to include the logs that we’ve been looking at to encourage our customers to use Elasticsearch and Kibana themselves - with the ultimate goal of self reliance."
"114","2018-08-13","2023-03-24","https://www.section.io/blog/slow-websites-lose-customers/","New Study from Eggplant on Why Web Speed Matters A new global study from website optimization company Eggplant found that just under three quarters (73%) of consumers will move to a rival site if the website they are on is too slow to load. Eggplant polled 3,200 people within the US and UK about their attitudes towards website speed and performance. They found that website speed is important to a significant majority of people in both countries: 85% of Americans said it was important to them and 88% of Brits said the same. Slow websites in fact frustrate users more than a website being temporarily down or not working. In the US, an overwhelming 79% of American adults found slow websites more irritating to use than one that is unavailable. In the UK, this number rose a couple of percentage points to 81%. Over half of Americans said that they feel ‘much more negative’ towards a brand if the site is slow to load every time they go to it; less than a quarter (23%) said they felt this way about a site that was unavailable. 7 in 10 (70%) of adults in the UK rate website speed as important; the American number was lower at 41%; however, only a tiny proportion in both countries (1%) said speed was irrelevant. The study’s results point to the fact that retailers stand to benefit as much from focusing on optimizing website speed as on ensuring website availability. Brands that focus on commoditizing their offerings based on prices, such as hotel, travel and ticket sites, particularly stand to lose customers to rival websites if they are experiencing poor website speed. And for e-Commerce companies preparing for the peak retail period, website speed should also be a significant focus. Even small delays in page load times can lead to significant losses in conversions and revenue. Dr. John Bates, CEO of Eggplant, warned, “If brands were not aware before, they are now — website speed is critical to business success. With retailers already preparing for the holiday shopping season, we urge them to make sure they analyze and optimize every element of their online presence to make sure it is running as efficiently as possible. As the research has shown, a slow website will drive consumers away, negatively impacting the health of the business.” The online study for Eggplant was conducted by digital market research and online analytics company YouGov who polled 2,000 adults in the UK and 1,200 in the US online between July 3rd-4th, 2018. The Eggplant study is yet more confirmation that slower website speeds can kill your business. Further Research into Latency Multiple studies have been run on website speed and the impact of page load time across multiple types of sectors. Back in 2012, a range of studies came out dramatically illustrating this. Amazon revealed the staggering statistic that one second of delay of page load time could lose them $1.6 billion in sales per year. Presumably that figure is even higher today. Google revealed at the time that if its search results were slowed by just one four tenths of a second, they could lose 8 million searches daily, leading to significantly reduced advertising revenue. Indeed a widely quoted study run by the Aberdeen Group asserted, “A 1-second delay in page load time equals 11% fewer page views, a 16% decrease in customer satisfaction, and 7% loss in conversions.” In dollar terms, if your website earns $10,000 daily, you would lose $250,000 in sales each year because of the one second delay. In 2018 and indeed over the last several years, studies about website speed as a killer of business have been largely focused on mobile. Google released research last year, in which the search giant concluded, “The average time it takes to fully load the average mobile landing page is 22 seconds. However, research also indicates 53% of people will leave a mobile page if it takes longer than 3 seconds to load”. No doubt this research fueled Google’s recent “Speed Update” in which (since last month) slower websites now rank lower in Google’s search results for mobile. The Financial Times In-Depth Study – “A Faster FT.com” In 2016, the Financial Times ran an in-depth study, which we analyzed at the time that looked at why all this matters in relation to its specific business model. As part of the build of its new website, the FT commissioned research to investigate how much the speed of its site impacted user engagement, in particular session depth i.e. the number of articles read - one of the newspaper’s key indicators of success. The FT ran two tests; each lasting four weeks. In the first, subscribers were divided into two groups: a control group which saw the normal website and the alternate group who were subjected to a five-second delay on every page load. A blocking CSS call was inserted within each HTML page, which was configured to wait to respond for five seconds each time. The newspaper’s research team saw “a significant drop in engagement” from the slower group. The FT then ran a second test in which subscribers were divided into four separate groups: the control group saw the FT website at standard speed, the second experienced a one-second delay on each page load, the third group a two-second delay, and the fourth a three-second delay. This time, the researchers looked at the impact of a delay across three different axes: By device class (and network speed) – on mobile devices, tablets and desktops By historical engagement on ft.com – low, medium, high Time – 7 day, 28 day windows Minimal impact on session depth was found for very short visits of two pages or less, but for visits of three pages or higher, there was a gradual decline across all three test variants; and the longer the journey the more significant the drop-off in engagement. A similar decline was tracked in longer-term engagement – another key indicator of user engagement for the paper. Over 28 days of testing, users read fewer articles each day when they experienced delays loading web pages. Mobile users were more patient with delays than tablet and desktop users; the FT hypothesized this was because “mobile users are receptive to small delays as they may blame the network connection, but [including on mobile] as delay becomes unreasonable and consistent the engagement drops off at higher rates”. Ultimately, significant detrimental impact on the FT’s revenue rate was seen both in terms of the subscription renewal rate (when user engagement drops, so do subscription rates); and in relation to its advertising revenue. Advertising inventory is partly based on page views and how long each advert is seen for, thus if a user spends less time on the FT website, they are exposed to fewer advertisements. The newspaper’s health was impacted “over the short term, to the tune of hundreds of thousands of pounds, and in the long-term millions”. Section Research into Web Speeds How Do Slow Web Speeds Impact Magento Users? A couple of years ago, we conducted an in-depth study of 324 websites (selected at random) that ran on Magento Enterprise, the leading e-commerce platform, which many of our customers use as we are a Magento Select Technology Partner. We wanted to find out “just how crucial and imperative optimal website performance is to the success of one’s company or organization”. We mainly used the free tool WebPageTest for testing because of the wide range of information that is revealed from a performance test, from page speed optimization to resource loading waterfall charts. We looked at five measures in particular: Redirect time to the website’s domain HTML load time – also known as the Time to First Byte (TTFB), meaning the time it takes for the HTML document to be delivered to the browser Start render time – The moment in time in which the first non-white content is made visible First view load time – the time from initial navigation to a fully loaded web page Speed index – a metric that calculates how fast all the contents of a page are visually populated We compared the top 10 performing websites from Magento with the bottom 10 websites and then looked at the averages for each individual metric. He noticed a considerable difference in all areas for the top 10 and the worst 10. The average speed index for all the websites surveyed was 5,142 milliseconds. The top 10 performing sites far outweighed this – all of them came in at under 1,156 milliseconds. Key Takeaway: “Although there are a lot of factors that impact the page load time of your website, the most important thing to take away is that milliseconds ultimately do matter when it comes to optimal website performance. There are an abundance of studies that have proven this notion”. Website Speed and SEO As we’ve written about previously on the blog, website speed is an increasingly important element of SEO. Slower speeds generally equal a poorer user experience, and Google has included website speed as a component of its PageRank algorithm since 2010, and as discussed above, its Speed Update rule has just come into effect, meaning that slower websites rank lower in its mobile rankings. The same is true for Facebook where content is down-ranked in the newsfeed when it has not fully loaded. There are multiple ways in which you can improve your SEO; however, probably the main one to focus on is Time to First Byte (TTFB), which Google appears to focus on in its search rankings. TFFB is crucial because it tells you the time at which the browser can start to build a web page. A long TTFB means a user spends too much time staring at a blank web page, which frequently leads to them navigating away from the slow site. A good average for TTFB is around 200 milliseconds. Work with Section to Optimize Your Site Just a few options to consider….. Improve Your TFFB The most significant way in which you can improve your Time to First Byte is via your server response time. No matter how well you optimize your images, compress video, etc., substandard servers or hardware that can’t handle an influx of users at once will have the most detrimental impact on your TFFB. You should cache full HTML documents in order to guarantee your website is the fastest speed possible and your origin infrastructure is operating as efficiently as possible. Improve Image Load Time Serving optimized images for the particular browsing requesting can provide significant load improvements. Utilizing an Edge solution such as those offered by Section can provide quick and easy wins for your customer experience. Magento Optimization One of our specialisms is working with e-commerce sites. We are a Magento Select Technology Partner and as such, the Section website performance and security offerings integrate seamlessly with the popular ecommerce platform. Our Magento 2 extension allows you to achieve performance improvements with a handful of simple steps, or Magento 1 account holders can sign up to their own account. We offer a free 14 day trial for both options. Real User Monitoring Our Real User Monitoring (RUM) now includes business metrics that directly show how speed impacts user behavior on your website, revealing the average number of pages viewed by page speed and average bounce rate by page speed. Our RUM JavasScript snippet can be inserted onto your website in minutes, allowing you to view full visitor data without making alterations to any part of your website or changing your DNS. Not Sure Where to Start? Download our free guide to optimizing the speed of your website. or Contact our team to discuss your needs and concerns. We can help you find the right solution for your particular web appplication."
"115","2018-07-16","2023-03-24","https://www.section.io/blog/google-mobile-update-seo/","Back in February, Google announced an update that takes page load times into consideration as one of its signals in Google’s mobile search rankings. The change, which Google is calling the “Speed Update” began in March and will go into effect across July. While factors like the relevance of content will still be considered, very slow websites on mobile devices will begin to rank lower in Google’s search results. It’s important to be prepared for these changes by optimizing your site content now. Google Research 2017 Findings In its original announcement, Google emphasized how important page speed and optimized content is to good business, writing, “If there’s too much friction, they’ll [visitors] abandon the site and move on”. In a set of studies run in collaboration with SOASTA Research last year that focused on ‘The State of Online Retail Performance’, Google found that 53% of mobile site visitors will leave a page that takes over three seconds to load. On average, mobile conversion rates are lower than desktop (even though over half of web traffic comes from mobile traffic). Google linked this to the fact that the average landing page on mobile devices takes 15 seconds to load. “In short, speed equals revenue”, Google cautioned. Their research found that the automotive, retail and technology sectors had the slowest loading pages. Loading a web page is more resource intensive than ever across all industries, however. Google/SOASTA noted that 70% of all the mobile landing pages they analyzed took over five seconds for the visual content above the fold to display on the screen, and it took over seven seconds for visual content both above and below the fold to fully load. The other major piece of analysis, stemming from a 90% accurate deep neural network the research team trained to model the human brain and nervous system, led to the prediction that as page load time moves from one to ten seconds, the likelihood of a mobile site visitor bouncing goes up by 123%. Likewise, as the total number of elements on a page rises – specifically from 400 to 6,000, the likelihood of conversion decreases by 95%. Getting Up to Speed Digital marketing expert Jim Stewart told Internetretailing.com recently that businesses have limited time to get their websites up to speed before they see their Google rankings drop. “It’s all about user experience. You’ve got a week to go through and check your mobile site”, Stewart warned, “make sure it’s as fast as possible – because this is a speed update.” The need to prioritize speed is particularly important for less well-known brands (brands with high relevance, but slow load speeds will still rank highly) and those with mobile-first websites (as Google will deploy mobile-first indexing if a site primarily has mobile users). Irrespective of the Google Update, increased mobile performance drives conversion rates, which equals increased revenue; hence there is really no time like the present to seek out performance optimization tools and techniques. Performance Optimization Tools Available at Section There are multiple actions that developers can take to prioritize and increase the speed of mobile sites. Features that are key to bettering site speed include HTML dynamic caching, static content optimization and front end optimization, all of which we offer at Section through our modular Edge Compute Platform. HTML dynamic caching is a highly beneficial technology for making websites faster. When dynamic caching is utilized for the entire HTML document, both back end load time and Time to First Byte (TTFB) are dramatically improved. However, dynamic caching is not commonly deployed. Many websites don’t do it either because they cannot obtain visibility of the HTML document performance or because of the risk of caching dynamic content without being able to test cache configuration before going live. At Section, we provide code-level configuration and a local development environment for testing. Section offers developers full control over the ways in which you can cache dynamic content and the option to instantly clear cache once changes have been applied. We also offer static caching for both objects and images - this can significantly reduce page load speeds, specifically by serving static objects directly from the cache back to the request instead of retrieving them from your web server, thereby reducing overhead. Another of our offerings is Google’s PageSpeed for front-end optimization. The majority of websites spend 80% or more of their load time on front end elements. PageSpeed improves web page latency by implementing best practices for front end optimizations into one tool using a set of configurable filters. The filters improve performance in a number of ways, including via optimizing caching (via local storage cache or outlining JavaScript, for instance), minimizing round trip times and reducing the payload size. We also offer Varnish Cache (3, 4, 5 or 6), a reverse proxy that deploys rules you can determine in VCL (Varnish Configuration Language) to accelerate websites and applications by overseeing HTTP requests to cache objects or redirect users. Other performance optimization tools in our library include anonymous page caching, image optimization with Kraken Pro, reducing unnecessary traffic with bot management and mitigation, re-ordering JS execution and HTML streaming. Mobile Website Optimization Section also offers various technologies specifically focused on Mobile Website Optimization. These include: Uplifting Websites to HTTP/2 Automatically uplifting your website to HTTP/2: the new version of HTTP. This will help shrink the number of roundtrips and connections between the browser and server and improve parallel asset loading. Minification and Optimization of JS and CSS Files Minimizing and optimizing JS and CSS files shrinks the amount of data being sent to the mobile device, thus increasing the rate of delivery and image and objects rendering speed. Website Image Optimization As most of a website’s data tends to be made up of images, image optimization is critical in reducing load times and shrinking the burden on network resources, including data usage. Image optimization works by scaling the image to the specific mobile browser (its type and size) and lessening the amount of compute time a browser needs to resize each image as the images arrive. Lazyloading is also available on our platform, allowing users to always receive the above the fold information first. Determining the Right Compression Levels Use Section to determine the right compression levels for your site - balancing quality, detail and size to ensure that only the smallest amount of data is sent to the browser. Learn More For sites looking for detailed information on measuring and improving website performance, we also have an eBook available to download for free:"
"116","2016-07-01","2023-03-24","https://www.section.io/blog/milliseconds-matter-for-website-revenue/","The evidence is conclusive; website page load time matters. The faster your website the more revenue your website will generate. Amazon and Google have studied this phenomenon. We have studied it and recently FT.com produced a great summary of their findings. ![FT.com’s findings re pages viewed per session versus page load speed](/assets/images/pages viewed subject to load speed.png) We all conclude that a faster website means very good things; Reduced bounce rates Increased number of pages per session Increase number of check outs Increased size of checkouts Higher levels of customer satisfaction Increased likelihood of customer returning But Why? Half a second of page load time seems inconsequential. When you view a page loading in isolation it is actually hard to feel or see that extra 500ms. But evidence shows conclusively that even 500ms matters on page load time. But, why does it matter? Here is my Segue to an Answer; A week or so ago I was climbing up a rock wall here in Boulder Colorado. Most of the time, I was focussed only on the wall and the moves I was making. But at times, I slowed down, lost rhythm and maybe started chatting with the folks on the ground. The climbing experience was the best and most fluid when immersed in the task at hand. Chatting with a climbing partner after the session we started discussing this sensation and its applicability in other sporting scenarios; bombing down a hill through the trees on a mountain bike, hitting smooth and rhythmic turns through the bumps on skis. We agreed that the immersive and fluid sensation of being buried in these (or any) activities, was consuming and truly special. Flow This sensation is one which has been referred to in a number of different ways by life coaches, psychologists, and various writers but for the purposes of today, lets refer to it using Mihály Csíkszentmihályi’s term; “Flow” “the mental state of operation in which a person performing an activity is fully immersed in a feeling of energized focus, full involvement, and enjoyment in the process of the activity” The state of flow is not just restricted to sporting endeavours. It can be achieved when pursuing musical interests, programming, gaming, learning etc. Csikszentmihályi’s proposes that the human mind can process “110 bits of information per second”. When in a state of flow, a person’s mind will be wholly focussed on the task at hand. That 100bits of information per second will be wholly consumed by and within the task at hand. Whilst performing one activity in state of flow, if we give the human mind an opportunity to receive and process information from an external activity then we give that mind an opportunity to become distracted and move out of a flow state. Websites and Flow State So perhaps, users of your website are more likely to stay on your website, learning about your product range, your pricing (or whatever it is that interests them) if they are not presented with an opportunity, be it ever so small, to receive and process information from some external source. Clicking a hyperlink to load a web page is a natural opportunity for a break point in the flow state of a user on your website. The extent to which that break point impacts their flow state will depend on how quickly that next page loads. Being in a flow state is a desirable state of being. When we reach it we associate the task at hand with a pleasurable experience. I believe the longer a website can keep a user in a flow state on that website, the more pleasurable the user will believe that website is to spend time on; both during that session as a repeat visitor. Flow state users on your website will be not only good news for immediate revenue but also for brand association and longer term revenue. While a 500ms improvement in page speed may not be noticeable for most users to the naked eye in a one off page load event, such a reduction reduces the period where your user has the opportunity to soak up information (55 bits in fact) from some source other than your website. Regardless of the speed of your website, you should be pursuing performance improvement as a feature. It is a UX must have, not an option if you want to increase the chance for your users to view more pages, transact on your website and return for repeat visits."
"117","2017-09-18","2023-03-24","https://www.section.io/blog/varnish-5-2-now-available-on-sectionio/","Varnish Cache released their latest 5.2 version late last week and it is already available on Section’s globally distributed Edge PaaS. Section provides you with Varnish Cache 5.2 on Section’s 60 Global PoPs right now, with: Instant global cache clear; Instant global configuration change; Comprehensive real time logs and metrics; and A git backed management workflow for complete control. Section’s modular, transparent Edge PaaS means you will never again be stuck with old software at the edge to improve your web application performance, availabilty and security. According to Varnish Cache most of the changes in 5.2 are transparent to users, however there are some VCL changes and new default VMODs. Hyphens now allowed in subroutines & VMOD object names This is just for language consistency, but subroutines and objects can now have hyphens in their names. So if you’ve been really wanting your subroutine names to use kebab-case rather than snake_case, your dreams are now a reality. req.hash & bereq.hash variables and the blob_vmod The hash that Varnish Cache uses to lookup the current request in the cache is now available as a variable, so it can be sent to the client or the backend server using the new blob vmod to encode it. import blob;

sub vcl_deliver {
    set resp.http.Hash = blob.encode(BASE64, blob=req.hash);
}
 req.hash & bereq.hash are a BLOB data type. This represents “opaque data of any length” according to the docs, and previously this data type was only available in vmods. Because of this, Varnish Cache has also included the blob vmod to work with this type. It can be used to create, manipulate, compare & encode blobs. See the blob vmod docs for more information on what it can do. bereq.is_bgfetch variable The is_bgfetch has been added to the backend request object. If true, this request to the backend is happening as a background fetch, which happens if a request comes in for which there is an stale cached object (ie its TTL has expired) but grace mode is enabled. The requesting client will be served the stale cached object and a request will be sent to the backend server to refresh the cache. You can now change the behaviour of the request to the backend or how to process the response it sends if the request was a background fetch. vmod_purge Back in the misty past of Varnish Cache 3, there was a purge function you could call inside vcl_hit & vcl_miss which would remove the object corresponding to the current request’s hash from the cache. This was changed in Varnish Cache 4 to a return (purge) called in vcl_recv. The purge vmod adds 2 functions purge.hard and purge.soft. purge.hard does the same thing as calling return (purge), but you can call it from vcl_hit & vcl_miss. It also returns the number of purged objects. You can see an example of how this can be used on the vmod docs. purge.soft(DURATION ttl=0, DURATION grace=-1, DURATION keep=-1) allows you to update the ttl, grace and keep of a cached object. If you’re not familiar with how those 3 variables impact how long an object is retained in cache, have a look at this diagram in the Varnish Cache Book. If you don’t pass in any parameters, the object’s ttl will be immediately set to 0, but the grace and keep will be unchanged. This means that, if grace is enabled, subsequent requests for the object will be served the now stale object and a background fetch will be done to refresh it. If you wanted to expire the object, but have the object removed from the cache at the end of the existing grace period you would do this purge.soft(0s, -1s, 0s);
 Other changes and additions There are a number of other updates, including a new default vmod to help with testing, a few other new VCL variables and some subtle changes to how varnishd, varnishstat and varnishlog work. If you would like all the details check out the changes page and upgrading notes on the Varnish Cache docs site. Try it now You can spin up Varnish Cache 5.2 for your site right now by signing up with Section, you can try it risk-free using our Developer POP. Alternatively, if you just want to check out some of the new language features, we’ve added Varnish Cache 5.2 to vclFiddle"
"118","2018-10-01","2023-03-24","https://www.section.io/blog/containers-revolutionizing-cdn-market/","The CDN market is in the midst of a revolution, and containers are at the forefront of the sea change. Over the last several years, the containerization of services has become an increasingly popular mode of building and managing applications, which shows no sign of abating. The flexibility that containerization offers through breaking applications into different components offers many benefits to software development. Container management services like Docker and Kubernetes have seen a great deal of momentum in their usage by development and IT ops teams. Datadog’s latest Docker Adoption Report showed that adoption rates of Docker, for instance, have increased by 20% year over year. What are the Benefits of Containerization? The primary reason behind the popularity of containerization is that containers offer a solution to the longstanding challenge of how to get software to continue to run reliably when it is moved between different computing environments. When the supporting software environment (or other elements, such as network topology) isn’t identical, problems can quickly arise for developers. A container is essentially a complete runtime environment, consisting of an application and all its dependencies, libraries and other binaries, along with the configuration files required to run it, bundled into one package. When the application platform and its dependencies are containerized, the differences between OS distributions and its foundational infrastructure are abstracted away. By abstracting systems from the physical platform, you are able to move them from platform to platform (or cloud to cloud). In addition to consistency, portability and flexibility, other benefits to containerization include: Increased scalability – As a container is much smaller in size than a virtual machine (which they are often compared to as both solutions allow you to package your application together with libraries and other dependencies), a single server can host many more containers than virtual machines. Greater modularity – The application is divided into modules (e.g. the database can be separated from the application front end, etc.), creating isolated environments for running your software services. Separation of roles – Developers are better able to focus on application logic and dependencies; meanwhile IT teams can concentrate on deployment and management, improving company-wide cost and performance efficiencies. Reduction in complexity – The modular nature of containers makes them easier to manage than virtual machines as each module is lightweight, and changes can be made to each module without having to rebuild the entire application. Security - Containers offer a further layer of security as your applications are not running directly on the host operating system; furthermore, security services are platform-specific, not application-specific. Lower costs – As containerized applications can be instantiated in a “just in time” manner when needed and similarly disappear when no longer required, they free up resources on their hosts and accordingly lead to lowered costs. Section Container-Enabled Benefits Section is the first and only enterprise-grade global content delivery system to be built entirely using container technology. Our Edge Compute Platform platform runs wholly on a combination of Docker, Kubernetes and home-grown container orchestration. This choice was made as part of Section’s steadfast commitment to making content delivery more straightforward for developers using Agile and DevOps methods. Benefits to our use of containerization include the ability to: Leverage Our Portable PoPs Our PoPs are capable of running anywhere that a physical or virtual machine can run, allowing us to take advantage of a wide variety of options - from leveraging the peering and capacity of the big public cloud providers to taking advantage of the particular benefits of boutique hosting companies such as the bare metal as a service offered by Packet.net. Utilize the “Developer PoP” A unique service to Section is the “Developer PoP” (driven by the flexibility of our container-based technology), which allows our customers to git clone their configuration, pull Section’s Docker images onto the developer workstation and test out changes in the developer PoP before pushing out configuration changes to production. Customize your Reverse Proxy Software Another benefit to the flexibility provided by containers is our ability to deliver a different group of proxy software for each of our customers despite the fact that they run on the same network. This in turn allows you to customize your reverse proxies to build a particular reverse proxy chain to suit each application individually. Flexibility with Upgrading Software You choose to upgrade software when it suits you. Each customer is different as to when they are ready to upgrade; when you are, you simply switch out one container version for another. Benefit from Increased Security Leakage of process execution is prevented as a result of the additional layer of isolation provided by containerization. Given the security issues that other CDNs have experienced in the past, this is a particularly important benefit stemming from our longstanding containerized approach. Two Approaches to Containerization There are two approaches to containerization: the first is to build a customized system to manage and run containers, automatically launching new container instances as needed with increased processing; and the other is to work with one of the container platforms like Kubernetes, Cloudify or Docker Swarm. These orchestration, scheduling and clustering technologies are usually the more attractive option as they can provide the essential mechanisms that enable scalability. Kubernetes (originally designed by Google), for instance, is capable of scheduling an unlimited number of container replicas over a group of node instances, which allows most large container-based applications to scale up or down as needed. Docker Swarm, the newest tool out of the three, offers clustering, scheduling and integration capabilities, allowing developers the potential to generate and ship multi-container/multi-host distributed applications, which include scaling and management for container-based systems. If you do go for the second option, it is worth trying out a proof of concept with each technology, using automated technology and real-world workloads to see which might best suit your needs. Remember to consider security and governance as part of this process as these also need to be able to scale as required. Why Containers are Here to Stay The problem that containers solve: how to truly provide application portability from platform to platform (or cloud to cloud), has long been a challenging one for developers. Certainly in the context of cloud computing moving from simple to complex architectures, the use of containerization (because of their lightweight nature) is a preferable solution to placing workloads in virtual machines. The relatively small size of containers makes the creation of workload bundles that can be moved from cloud to cloud straightforward. Thereby, containers enable workload migration within hybrid or multi-cloud environments crucially without the need to modify much or any of the application. We can expect the growing rate of adoption of container-based technologies to only continue to grow."
"119","2018-12-08","2023-03-24","https://www.section.io/blog/l8ist-sh9y-daniel-bartholomew/","Section Co-Founder and CTO Daniel Bartholomew was recently interviewed on the L8ist Sh9y podcast. Hosts Stephen Spector and Rob Hirschfeld chatted with Daniel about the critical role that the developer experience plays in realizing the potential of edge computing. Listen to the episode ***Daniel Bartholomew on developer applications running at the edge today*** for insights on how we can empower developers to build and scale in the new age of edge compute."
"120","2020-04-13","2023-03-24","https://www.section.io/blog/content-security-policy-csp-edge-module/","In today’s dynamic threat landscape, Section’s Content Security Policy (CSP) Module adds another line of defense in web application security. The CSP Module, backed by computer security standards recommended by the W3C working group, allows system administrators to easily configure, enforce, and track activity around security policies to ensure that browsers only fetch or execute resources from valid sources. These policies help protect against Cross-Site Scripting (XSS) attacks, clickjacking, and other data injection threats. Why should application developers use CSP? Although there are some built-in protections offered by the same-origin policy (SOP), whereby a web browser permits scripts contained in a first web page to access data in a second web page if they share the same origin, the modern web is more demanding. Web applications today use scripts and other resources from external sources. A few examples include analytics and font scripts, and resources served from content delivery networks (CDNs). Without additional security measures in place, browsers may confuse legitimate sources with illegitimate sources attempting to inject malicious code into web applications. How does Section’s CSP module work? The Content Security Policy (CSP) helps protect against these threats by explicitly telling browsers which sources to trust content from. Based on the defined protocols around HTTP, Section’s CSP module makes it easy to quickly configure and deploy security policies for a given application’s unique requirements. The CSP Module accepts user-defined security policies for browsers to determine which origins to accept content from. These policies are passed through using a Content-Security-Policy HTTP header, which will block assets requested from origins not in the list. You can also configure settings for the browser to send reports to a designated url using the report-uri directive of the Content-Security-Policy header. A few examples of common security policies include: All content must come from the site’s own origin (excluding subdomains). Allow content from a trust domain and all its subdomains. All content must be loaded using TLS. Allow HTML in email, as well as images loaded from anywhere, but restrict JavaScript or other potentially dangerous content. If you’d like more information on how to leverage the CSP module or if you’d like some assistance in getting it set up, contact our team of solution engineers."
"121","2018-09-21","2023-03-24","https://www.section.io/blog/protect-ecommerce-website-security-risks/","eCommerce websites are at the forefront of cyber attacks that can impact websites, breach customer confidentiality and damage a business’ reputation in the short and long-term. As more and more of our private data is stored online - from credit card information we upload ourselves to healthcare data that our healthcare providers store for us - cybersecurity risks are likewise increasing. The dark web is often the ultimate repository for stolen information with cyber-attackers selling people’s confidential data on to willing buyers who then go on to perpetrate further attacks, including those of the most serious kind such as stealing another’s identity. As the Internet of Things (IoT) proliferates further from smart fridges to home alarms - these ‘things’ are also vulnerable to being taken over remotely and manipulated to damaging effect. Website owners and users need to be aware of these risks, both the worst case scenarios and the more everyday security challenges. For eCommerce sites, website security is a particularly relevant topic. Online stores are becoming increasingly popular: the U.S. Commerce Department reported that e-Commerce represented 13% of total retail sales last year with over $450 billion spent on retail purchases made on the web. Amongst people under the age of 30, 90% have made an online purchase. eCommerce sites (from small to large businesses) are attractive to hackers because they process personal information, including credit card details and passwords often used across multiple accounts. If websites are not secured properly and successfully attacked, hackers can benefit from not only the use of credit card numbers, but also email/password combinations that can be attempted on other websites for use in further attacks. Cybersecurity Threats to Be Aware Of Some of the most significant security threats to eCommerce sites include: Exploiting Known Vulnerabilities All software, including your eCommerce platform itself and any extensions built thereon, has certain vulnerabilities known to attackers, such as how to access your site via a back door, inject malicious SQL statements in what appears to be a legitimate-seeming SQL inquiry or cross-site scripting, in which an attacker will insert a JavaScript snippet on a vulnerable web page that looks like a normal script to a browser and is therefore executed, enabling for example, the theft of customer credit card data. Even though this type of attack doesn’t directly target the website, it is still targeting your website’s users and could negatively impact your business. Security Hacks & Phishing Attacks Security hacks have made high-profile news for multiple years from the potentially election-swaying hack into the Democratic National Committee last year by suspected Russian agents to the Sony Pictures hack in 2014, which led the Obama administration to impose economic sanctions against North Korea for its suspected role in the attack. Businesses have been just as much under attack, however. In September 2017, Equifax disclosed probably the worst corporate data breach in history. The hack exposed the personal information of 145.5 million U.S. users (including birth dates, addresses and Social Security numbers), meaning that almost half the entire population of the U.S. had their crucial identifying data exposed to cyber-attackers. Small businesses are often even more susceptible to hacks because they have lower defenses. Hacks are often perpetrated via phishing scams, which typically take the form of emails that are made to look legitimate so that the recipient will open them. They usually include a link or direction to a page that if followed can then take over an email account and/or install malware on your computer without your knowledge. If an employee falls victim to a phishing scam, they could unintentionally provide access to not only their computer, but your entire network. Ransomware and Malware Recent ransomware attacks that have stolen the headlines include Magecart, WannaCry, Petya, NotPetya and BadRabbit, all of which used exploits explicitly aimed at compromising corporate networks. According to Kaspersky Lab, 26.2% of ransomware targets last year were business users, up by 4% from the previous year. Fedor Sinitsyn, senior malware analyst at Kaspersky Lab hypothesized why, saying, “business victims are remarkably vulnerable, can be charged a higher ransom than individuals and are often willing to pay up in order to keep the business operating”. Malware is malicious software that attackers will insert into your web pages or files after they have gained access. Popular eCommerce platforms such as Magento are especially vulnerable to widespread malware infections due to their widespread popularity. Malware can perform a wide range of activities from making your computer into a botnet that can be unwittingly used as part of a DDOs attack to stealing your website user’s account information. Malware can also perform spam activities such as the creation of false links or inserting pop-up ads onto your site. Distributed Denial of Service (DDoS) Attacks DDoS attacks also continue to rise, both the volumetric kind such as Mirai and the more sophisticated that target the application layer. Both need to be vigilantly protected against as they are equally damaging in terms of potentially slowing down site speeds and/or knocking websites offline altogether, which can lead to a significant loss of profits in both the short and long-term. How to Protect Your eCommerce Site There are multiple ways in which you can protect your eCommerce site and website security is never a one-size-fits-all approach. Some large eCommerce sites may require the presence of an internal security team to monitor and manage potential threats whilst smaller to mid-sized sites can handle cybersecurity without a dedicated internal team. Bigger sites tend to require more security as the larger a site, the more attacks it will face. However, as smaller sites typically have fewer defenses they also represent an attractive target to attackers. Everyday ways in which you can protect your website from vulnerabilities include the following activities: Regular audits for potential vulnerabilities: using tools that will scan your site for malware and other likely vulnerabilities to particular platforms Examine your logs for potential threats to see where suspicious or unusual requests come from and potentially then set up traffic blocking measures for particular regions Deploy SSL/TLS encryption on all website pages Update security patches regularly - make a maintenance schedule Always use strong passwords and 2-factor authentication for administrator accounts, which are especially vulnerable to hacking attempts by both individual attackers and bots Abide by PCI-compliant regulations on all elements of your platform, including third-party payment gateways, such as Stripe or Authorize.net Only use trusted extensions, platforms and themes - According to WP Scan, a black box WordPress vulnerability scanner, 52% of known Wordpress vulnerabilities last year came from plugins and 11% from themes. Download directly from your platform’s marketplace and be particularly cautious of free tools. Larger security measures to take include: Setting up a Web Application Firewall (WAF) to protect against application-specific attacks, including cross-site scripting, SQL injections, known platform vulnerabilities, and others. You can either use a classic rules-based WAF such as ModSecurity or a next-gen firewall such as Threat X and Signal Sciences that deploy heuristics and AI to detect attacker patterns and automatically block threats. At Section, we offer all three as options. Deploying a bot blocking solution such as ShieldSquare to protect your site from bad blocks through blacklisting, providing alternate content, using CAPTCHA forms, and other customized actions. Working with a CDN such as Section, which can provide two layers of protection and website optimization to your website: a DNS layer, which routes your traffic to the nearest global server in the CDN’s network, and a reverse proxy layer, which intercepts traffic and blocks threats, in addition to speeding up performance. Reverse proxy software typically performs many different security and optimization functions including blocking bots, acting as a WAF, caching content and performing image optimization."
"122","2018-07-09","2023-03-24","https://www.section.io/blog/varnish-cache-versus-nginx/","There are a range of web caching software options designed to help speed up your website or app. In this post, we focus on two of the major ones. Varnish Cache Varnish Cache is a powerful front end accelerator or caching HTTP reverse proxy that lets users cache both static and dynamic content leading to much faster page load times (300-1000x times). It is mainly used to cache content in front of a web server. In addition to its speed, Varnish Cache is popular for the flexibility of its programming language, Varnish Cache Configuration Language (VCL). VCL allows developers to specify request handling rules and set specific caching policies giving them a lot of control over what and how they cache. Varnish was designed for content-heavy dynamic websites as well as APIs, and is used accordingly by sites such as Wikipedia, online newspapers including The New York Times, and social media sites like Facebook and Twitter. Nginx Caching Nginx is an older caching option; its first iteration was released in 2004. It is an open source web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. Nginx has a reputation for speed and efficiency, particularly in relation to handling concurrent applications, making it popular with some of the world’s most popular websites, including one-third of the Fortune 50 companies and 5 of the 10 largest tech firms. Compared to Varnish Cache, it is a standalone solution, allowing the running of an entire application. Its commercial version, Nginx Plus, offers various features that the OSS version does not. How do they Compare? Varnish Cache supports ESI while Nginx doesn’t; Nginx supports SSL where Varnish Cache doesn’t Varnish Cache is a pure web cache that has more advanced cache-specific features than Nginx; however Nginx can act as a “true” cache server when placed in front of an application server/s Varnish Cache has a lot of flexibility, allowing developers to create a more complex caching structure than Nginx Varnish Cache has a built-in mechanism that lets you purge content while Nginx OSS does not natively support this (however, Nginx Plus does) Nginx is known for its highly efficient serving of static content, particularly when the static files are on the same server as Nginx If you want to avoid additional overhead by introducing new technologies, Nginx might be a better fit Other Alternatives Squid Squid is a caching and forwarding HTTP web proxy that began life as a client-side cache. The most significant difference to Varnish Cache is that Squid is a forward proxy that can be configured as a reverse proxy rather than having been built as one. Squid is older than Varnish Cache and offers more extensive features; however, it offers less flexibility to run policies. Squid is primarily used for HTTP and FTP, but does offer support for several other protocols, including SSL and TLS. nuster nuster is a new high performance HTTP proxy cache server based on HAProxy to primarily be used to cache and proxy requests to web servers. Its dynamic caching abilities are highly configurable. It offers cache purging. nuster also supports HTTP2. Some benchmark tests (NB run by nuster) have concluded that nuster is almost three times faster than Nginx when both are using a single core, and almost two times as fast as Nginx and three times as fast as Varnish Cache when using all cores. Which is the Best? There is no one right answer to the web caching software right for you. Every use-case is different. You have to choose the right one for your website. Run them all, or any of them on Section. For more information on the our modular Edge Compute Platform, check out our documentation!"
"123","2018-06-22","2023-03-24","https://www.section.io/blog/sectionio-partners-shieldsquare/","We are pleased to announce today that we have added ShieldSquare Bot Mitigation and Management to Section’s modular web optimization platform. Both Section and Shieldsquare focus on improving the security of web applications and thier content. This addition to Section’s suite of reverse proxies for security and performance allows our customers to utilize this advanced bot management and mitigation software within Section. To learn more or add Shieldsquare to your Section setup, please contact us. The full press release is below: Section Joins Forces with ShieldSquare to detect and eliminate undesirable non-human web traffic Boulder, CO, June 26, 2018 - Section today announced the expansion of its website security offerings with the addition of ShieldSquare bot mitigation and management. From Shieldsquare “Specialized in delivering best-of-breed non-human traffic detection and management solution, ShieldSquare is a pioneer in bot management and protects over 80,000 internet properties spread across 70 countries. The solution secures online businesses against automated threats that are difficult to stop using conventional security measures. Sophisticated automated attacks are often massively distributed or adequately low and slow to operate under the permissible limits of conventional rule-based security measures. ShieldSquare uses proprietary Intent-based Deep Behavior Analysis (IDBA) to understand the objective behind highly sophisticated automated attacks. IDBA performs behavior analysis at a higher level of abstraction of intent, unlike the interaction based behavior analysis. Capturing the objective behind automated attacks allows IDBA to provide higher levels of accuracy in bot detection.” Section’s Modular Platform Section’s modular web traffic optimization is built using container technology, which allows it to offer a choice of technologies for website performance and security. With the addition of ShieldSquare, Section’s offerings now includes multiple Web protection tools (ShieldSquare, open-source WAF ModSecurity, intelligent WAF Threat X, Signal Sciences Protection Platform), multiple unmodified Varnish Cache versions, Kraken for Image Optimization, Nginx with LUA for powerful Edge Side Rewrites, and Google’s PageSpeed for front end optimizations. “The ability for Section users to choose the web security solution that works for them, rather than being locked into black-boxed tools that legacy CDNs offer, is changing how people think of content delivery. We’re pleased to partner with ShieldSquare to bring our customers the most advanced website security platform on the market today,” said Stewart McGrath, CEO and co-founder of Section. Section’s core features include robust metrics, real time detailed logs, real user and synthetic monitoring, instant cache clear and configuration propagation, a global PoP network, and a Developer PoP for building and testing configurations locally. Over the coming months Section will deploy additional solutions for website performance, security, and scalability on its platform. About ShieldSquare ShieldSquare offers a non-intrusive API based approach for detecting and eliminating non-human traffic from web applications, mobile apps, and APIs in real-time. ShieldSquare protects online businesses against automated threats such as account takeover, application DoS, price scraping, content scraping, skewed analytics, and ad fraud. The company safeguards over 80,000 internet properties of global online brands spread across 70 countries. ShieldSquare operates through its offices in New York (the US), Bengaluru (India) and Chennai (India). Get Started with ShieldSquare and Section To get started with ShieldSquare and Section, please contact us. Get Started Today"
"124","2018-07-02","2023-03-24","https://www.section.io/blog/sectionio-series-a-announcement/","Today we are pleased to announce we have closed our Series A funding round lead by Foundry Group. Nearly three years ago, when we first started the process to move our HQ from Sydney Australia to the USA in search of talent, capital and a larger market for our vision, we met with Ryan McIntyre from the Foundry team in Boulder CO. We have a product focussed on improving the output of web software engineers, both Dev and Ops. There have been few VCs who have truly grokked our vision. Aside from the truly straightforward approach of the Foundry team in total, when Ryan was able to appreciate where we are heading and immediately playback value add suggestions, we knew there was a good fit for us with Ryan and Foundry. As our vision has been coming together over the past three years, we have have been checking in with Foundry from time to time, have had the pleasure of spending time with the partners in various settings and we are looking forward to the future working relationship with the Foundry team. Next Frontier Capital (NFC) are also playing a large role in the Series A. Steve Souders, who pioneered much of the work in the world of web performance, helped NFC perform their technical due diligence for the round and had the following to say about Section: “Section has created a platform which from the ground up is truly a step change to existing Content Delivery solutions. Their container based approach means Section has developed a solution which is incredibly flexible and lets their customers pick the stack that works best for them. The modern developer focus with git backed workflows, real-time diagnostics and API first mentality is a very strong platform core and is ideal for modern web engineering teams following DevOps principles."" “In addition to their flexible deployment options and security modules, I’m especially looking forward to seeing what the Section team does in the area of web performance. Their platform architecture allows Section to build very interesting edge compute and intelligent, data-driven decision-making elements that give customers a lot of power without doing all the heavy lifting.” Throughout the last three years we have had great support from a really strong group of seed investors and before that from a number of tremendous Angel investors in Australia. We are excited to bring Foundry and NFC onto our journey to improve the flexibility, transparency and control modern web engineers have over the software running at their Edge Compute Platform. We have a highly capable team based both in Sydney and Denver / Boulder and we are securing and speeding up web activity for customers such as Thrifty Car Rental and Universal Music Group. The Section platform is being used by engineers to power apps for major North American sporting teams and events. This Series A funding will allow us to continue to bring incredibly talented engineers into the Section team to code, ship and run amazing software, to work with our customers to help them use our software and to work with our partners so they can easily and effectively integrate with our software. If you are interested in learning more about extending your infrastructure to the edge, or you’re an engineer willing to take on the big challenge of defining the next decade of application and content delivery, we’d love to speak with you; please contact us. At Section we say there is no such thing as work life balance, it’s just life. We are loving this part of our life and looking forward to the next."
"125","2017-12-13","2023-03-24","https://www.section.io/blog/secure-website-6-quick-fixes/","Website security is a hot topic these days, with a new hack of a major institution making it in the headlines every few weeks. You know security is important, and you want to protect your site, but what measures should you take to keep your site secure? The truth is website security will be slightly different for every site. Some large ecommerce sites may need an internal security team in place to juggle the various security tools being utilized, while smaller to mid-size sites can manage without a dedicated security team. This is due to the complexity of larger sites, but also because the bigger a site the more attacks it will face: Distil’s 2017 Bad Bot Report found that large sites (defined as Alexa rank 1-10,000) get 57.9% bad bots compared to 42.1% good bots, whereas the smallest sites (defined as Alexa rank 150,000+) get a ratio of 28.6% bad bots to 71.4% good bots. A similar trend is likely seen with other types of attacks, as the bigger the site, the more sensitive information they have access to. Here are 6 quick fixes that can cover your bases when it comes to getting started with website security Quick Fixes to Start Improving Security These security tips are standards that any site should be adhering to. While they may not protect your site from particularly large or sophisticated attacks, these will ensure you are taking regular steps to protect your site and your customers. Scan your website for Vulnerabilities: The first thing you should do when examining your website security is to do an audit of where you currently stand. There are many tools that will scan your website for malware and known vulnerability from platforms including Wordpress, Magento, Joomla, and Drupal. Some popular free tools include Sucuri and Quttera. Once you know what your vulnerabilities are, you can start patching them and evaluating what additional tools your site needs to block threats. Another way you can examine your site for potential threats is to look at your logs. If you use a log management tool or ELK stack logs (a combination of ElasticSearch, LogStash, and Kibana) you can search logs to see where requests come from and identify if your site is getting unusual requests. For example, if you sell exclusively in the US and get a lot of suspicious traffic from other countries, you could see that and try to block that traffic from accessing your site. Use SSL/TLS Encryption for All Pages The majority of ecommerce sites use the HTTPS encryption protocol on their payment pages through the payment gateway they use, however having HTTPS only on some pages of your website could still leave you vulnerable to attack and your users’ browsing information open to be taken. Browsers including Google Chrome (which has a 59% market share on desktop) will label your site as insecure in the URL bar if it is not on HTTPS throughout the site. In addition, Google search has started ranking HTTPS-only sites higher in search results, and having HTTPS implemented on all pages will allow you to use the newer HTTP/2 protocol, which offers better website performance and can also improve SEO. We also highly recommend using the Qualys SSL Labs tool to evaluate the quality of your SSL configuration. You should aim for an A+ rating which indicates the certificate itself is valid, and that the protocol support, key exchange, and cipher strength are also strong. Just having an SSL/TLS certificate isn’t enough, as there are weaknesses in the way some SSL certificates are deployed and if your certificate is expired it could also expose you to attacks and harm your reputation with customers. Stay on Top of Security Patches: 44% of attacks are because of known vulnerabilities in the platforms websites us. Some bots scan your website regularly for vulnerabilities so that an attacker can take advantage of those found without manually searching. Always stay up to date on patches for these issues, which will be in a developer or security section on the platform’s website or in their portal. Doing regular scans of your website yourself will also help pick up these security risks. If you use an open-source Web Application Firewall, as discussed in more detail below, it’s also crucial that you are regularly updating that against new security risks. Use Strong Passwords and 2-Factor Authentication for Admin Access Administrator accounts are particularly vulnerable to hacking attempts by bots or individual attackers. You should regularly audit the people who have administrator access to your website or database to check that no one has created an unauthorized administrator account, and make sure that your authorized users are strongly protected against hacking attempts. Requiring them to use a strong, randomly generated password that is unique from any other logins is important. A password manager such as LastPass is useful in generating and storing strong passwords. If you can enable 2-factor authentication for logins that will go a step further in protecting your administrator accounts. In addtion, there may be platform-specific steps you can take to protect yourself from login fraud. Wordpress by default does not limit login attempts, meaning bots continue to try login combinations. To protect yourself from this, you can enable brute-force protection. Be PCI Compliant Ecommerce websites of all sizes that accept credit card payments are required to PCI compliant. The Payment Card Industry Data Security Standard (PCI DSS) are security standards to protect customers when they are submitting payment details online. There are several levels of verification depending on the number of transactions you process each year, ranging from a full network-level assessment to a self-assessment for smaller merchants. Using an ecommerce platform such as Magento or Shopify will not directly make you PCI compliant because they don’t directly process transactions. You will need to make sure your server network, Content Delivery Network, and payment gateway (such as Stripe or Authorize.net) are PCI compliant before performing your assessment. Use Trusted Extensions, Platforms, and Themes As mentioned previously, it is crucial that you stay on top of updates for the platforms that you are using. In addition, you should use trusted platforms, extensions, and themes as these can open you up to vulnerabilities: Last year ecommerce platform Magento found that several third party extensions were at risk of SQL injection attacks. Wordpress has also found vulnerabilities in their numerous plugins and themes, with 52% of known Wordpress vulnerabilities coming from plugins and 11% coming from themes. To find trusted themes and plugins that are less likely to have vulnerabilities, download directly from the platform’s marketplace and be wary of free tools which seem to good to be true. You can also check how many other extensions a company has created, the number of downloads or reviews each extension has, and the length of time they have been creating extensions as a good indicator of if they are a trustworthy business. To learn more about how to get started with Website Security please download the full Website Security Guide. For more information on how Section can improve the security of your website, please contact us."
"126","2017-02-24","2023-03-24","https://www.section.io/blog/in-light-of-cloudbleed/","Summary of security issue raised In the past few days, vulnerability researchers at Google discovered Cloudflare’s reverse proxies were dumping uninitialized memory into their outputs, opening up websites that use Cloudflare to data leaks. This data included cookies, passwords, encryption keys, and even user’s private data from large sites that use Cloudflare. This is similar to the Heartbleed incident a couple years ago, but this incident is Cloudflare specific (hence the term Cloudbleed). Cloudflare’s reverse proxy configurations are shared between their customers, so many of them could have been affected. Why is Section not affected? In light of these events we wanted to share how Section is built differently in order to avoid such widescale incidents. Section applies true multi-tenancy based on the concept of process isolation by being built with Docker, a software development platform that runs on the principle of containers. Each one of Section’s customers runs its own processes and Docker containers that contain a reverse proxy stack specific to that customer. When customers sign up for a Section account, they choose what technologies they want to run and a suite of Docker containers unique to them is created. Later, they are able to add additional reverse proxies to their own stack. We run multi-tenancy systems which run each process in its own container because each container needs its own secure and virtual computing environment - this ensures incidents like Cloudbleed, which could impact many customers running the same reverse proxy configurations, do not ripple out beyond a customer’s environment. What should you do? Websites using Cloudflare may have been affected. If your website content is tranmitted through Cloudflare, you should read Cloudflare’s Incident report on memory leak caused by Cloudflare parser bug. Consideration should also be given to advising users of your website that a compromise may have occurred and, if you do not force password reset, your users should consider resetting their own passwords. For more information on Cloudbleed: Cloudflare Reverse Proxies are Dumping Uninitialized Memory Incident report on memory leak caused by Cloudflare parser bug What you need to know about Cloudbleed"
"127","2017-09-13","2023-03-24","https://www.section.io/blog/early-tls-deprecation/","Since before PCI DSS 3.2 was published, Section has provided delivery infrastructure with early TLS protocol versions disabled to allow customers to adopt the PCI requirements ahead of the June 2018 deadline. For many other customers though, disabling early TLS would have prevented too many visitors from being able to access, or complete transactions, on their websites. In the past 18 months, the usage of TLS 1.0 and 1.1 has declined, partly due to browsers and operating systems being updated to prefer modern TLS, and partly due to pressure to abandon older devices and operating systems as popular sites have stopped supporting these older TLS versions. As the 30th June 2018 approaches, Section will be adopting the PCI TLS protocol recommendations for all delivery infrastructure platform-wide, thereby discontinuing TLS 1.0 and 1.1 for all sites. If you’re concerned about the impact this will have on your site, you can see which requests are using early TLS via the “tls_protocol” field in the Section edge access logs. If you have systems that are dependent upon TLS 1.0 or 1.1, please contact us for advice about alternative options. At the time of writing, TLS 1.0 usage represents less than 1% of all requests through the Section platform and the vast majority of that traffic appears to come from automated bots, not real users. TLS 1.1 usage is negligible."
"128","2016-11-28","2023-03-24","https://www.section.io/blog/debugging-a-secure-website/","How to Debug an HTTPS Website As the Internet evolves with an ever increasing demand for security, websites are taking a full-HTTPS approach combined with additional features like Strict Transport Security (HSTS) and Public Key Pinning (HPKP). Websites adopting HTTP/2 for its performance benefits are also required to use HTTPS everywhere. The growing ubiquity of using TLS (the protocol that provides the security of HTTPS, which previously was provided by SSL) is great for our security and privacy but it can hinder our ability to troubleshoot problems that require us to inspect the traffic going over the wire. The Developer Tools in modern browsers are making this much easier with each release but sometimes you need to see the network packets and HTTPS makes this tricky. Tools like Telerik Fiddler and Charles help a lot but sometimes they change a site’s behavior and if HSTS and HPKP are in use on your site then even these tools may be insufficient. Forward Secrecy It used to be the case that you could capture the packets on your network using a tool like Wireshark and as long as you had the private key for your site’s HTTPS certificate, you could decrypt the traffic to see the plain-text requests and responses. However, modern browsers and web server TLS configurations are preferring to use ciphers with Forward Secrecy, e.g. ciphers using the Diffie-Hellman key exchange (DHE), which means this technique no longer functions. Thankfully, the Google Chrome and Mozilla Firefox browsers support logging the TLS session keys specific to your requests in a file that Wireshark can then use to decrypt captured traffic between your browser and HTTPS websites. IT Security Professional Jim Shaver published an excellent article about how to do this. Too easy In short, the process involves defining a SSLKEYLOGFILE environment variable specifying a file in which to record the keys before you launch your web browser, then configuring the SSL protocol preferences in Wireshark to read this file. It is remarkable easy compared to many other aspects of working with TLS and has been extremely useful at Section for troubleshooting our own Section Console. Have a read of Jim’s article, try it yourself, and relax knowing that next time your need to debug HTTPS requests on your site you have the tools you need."
"129","2016-09-27","2023-03-24","https://www.section.io/blog/blog-chrome-56-http-insecure/","In case you missed it: Chrome to mark non-HTTPS pages insecure Google recently announced that in version 56 of their Chrome browser (expected in about 3 months) will change the address bar to clearly label websites served without HTTPS as “not secure” if the page contains a password or credit card input field. Furthermore, this is just one step in Google’s plan to have Chrome display a warning on all websites served without HTTPS regardless of the page content. What this means is that if you don’t currently have a plan in action to migrate your entire website to operate over HTTPS all the time, then you better start putting that plan together very soon. Website Security Benefits Aside from all the security benefits, moving to a fully HTTPS website also enables performance improvements through HTTP/2 (provided by default on Section) and more effective use of browser caches. There are also some scenarios that become simpler to handle when your site only uses HTTPS, such as Cross-Origin Resource Sharing and protocol-relative URLs. Website Security and HTTPS Challenges Based on our experience helping our existing customers, the main challenges you should look for include: Pages that force a redirect back to HTTP if requested over HTTPS Third parties that do not provide their resources over HTTPS Absolute HTTP-specific URLs stored in a Content Management System Some of these can be address by a CDN solution like Section that provides free SSL certificates and we’d be happy to discuss them with you. If however you have found that your challenges include: Certificate issuing and renewal Achieving a secure web server TLS configuration Adding additional headers for, eg, Content Security Policies or Strict Transport Security Then you definitely should talk to us, as these problems are easily addressed by the Section platform. Please contact us with any questions about HTTPS or sign up now for a trial which includes a free SSL certificate."
"130","2016-09-14","2023-03-24","https://www.section.io/blog/multiple-ssl-certificates-for-multiple-domains/","We’re pleased to annouce that Section now supports multiple free SSL certificates if your website application uses multiple domains. The Section platform can now create, manage, and renew a free SSL certificate for all the domains that use one Section configuration, so you don’t have to worry about your SSL certificate expiring and your website becoming more vulnerable to attacks. What is a SSL certificate? SSL, or more recently, TLS, is an authentication protocol that encrypts web data. This leads to the HTTPS that is seen in your web browser accompanied by a green lock indicating that the webpage you are visiting is using secure communication methods, and any data you enter on that page is encrypted. To use SSL and have your webpages be delivered over HTTPS, you must get a digital SSL certificate and install it on your web server. There are many services which provide you with a SSL certificate for a small fee, which you’d then need to manually install and manage on your web server. Section provides you with a free SSL certificate, and in addition we manage the certificate for you, including renewing it when it expires so your website stays continually secure. Do I need SSL certificates for each of my domains? Agencies or sites that have many local domains need a SSL certificate for each of their domains so they all show HTTPS. For example, if a website manages both www.shopnyc.com and www.shopchicago.com, they would need two SSL certificates. This new feature enables those managing multiple domains with the same Section configuartion to get a free, automated SSL certificate for each of their domains. For those managing tens or hundreds of domains, Section now takes away the hassle of getting, managing, and renewing all of those individual certificates. How do I get started with multiple, free SSL certificates? Simply sign up at Section for free SSL certificates for all of your domains. We offer a free 14 day trial for all users, in addition to valuable features including a development environment for testing CDN configuration, real-time logs and metrics, easy setup, and the ability to customize your solution using powerful reverse proxies like Varnish Cache and ModSecurity."
"131","2016-08-30","2023-03-24","https://www.section.io/blog/free-ssl-certificate-management-for-websites/","A Study of how Magento Websites use SSL for HTTPS Recently, Section studied a random sample of around 330 websites running on Magento Enterprise, the leading eCommerce solution for large businesses. One of the areas we investigated was security use and use of SSL or TLS certificates per domain. SSL/TLS is an authentication protocol that encrypts web data, leading to the more commonly known HTTPS that is seen in your web browser accompanied by a green lock: this indicates that the webpage you are visiting is using secure communication methods so any data you enter on that page (such as your name or credit card number) is protected. This is important to understand as incorrectly deployed SSL certificates can leave a website open to attack, and the traffic between a browser and the website open to interception and exploitation. SSL Labs provide a handy online scanning tool (Qualys) which can investigate the certificate quality on a website and report a rating based on the following: From the SSL Labs Methodology Overview: We first look at a certificate to verify that it is valid and trusted. We inspect server configuration in three categories: Protocol support Key exchange support Cipher support We combine the category scores into an overall score (expressed as a number between 0 and 100). A zero in any category will push the overall score to zero. We then apply a series of rules (documented in the Changes section) to handle some aspects of server configuration that cannot be expressed via numerical scoring. Most rules will reduce the grade (to A-, B, C, D, E, or F) if they encounter an unwanted feature. Some rules will increase the grade (to A+), to reward exceptional configurations.* An example of the output SSL Labs gives is as follows. Magento SSL Results; Ouch! Only 1% of the Magento sites we tested scored an A+ which is quite staggering. Even more interesting is that 25% of sites failed and a further 22% recorded “Trust Issues” initially, meaning their certificate was not trusted due to it being expired, not matching the domain name, or another issue. If the trust issues are ignored, then scores can be applied to those sites and were generally around a “B”. Managing SSL Certificates for HTTPS Unfortunately, managing an SSL certificate for any website is a regular (and rather mundane) task but one which is absolutely necessary. As a website owner or engineer, you should be seeking to make sure your certificate is up to date with the latest protocols supported, has the strongest cipher possible, and has a strong key exchange procedure. Our recommendation is to deploy your SSL certs in a manner which provides for ease of ongoing management. Your cert should be updated and renewed automatically so you don’t have to think about maintaining a high score and keeping your website traffic safe and secure. Achieve a Higher SSL rating and Get Free SSL Certificates SSL Labs also provide a useful guide to SSL/TLS deployment best practices which engineers can use to improve SSL certificate management and ensure they are following all the steps to a secure website. Alternatively, sign up to Section and use our A+ rated certificate management functionality. We provide all customers with free SSL certificates and an automated system that manges the renewal and upkeep of your SSL certificate so you never have to worry. It doesn’t get much easier than that!"
"132","2015-10-23","2023-03-24","https://www.section.io/blog/architecture-of-section.io/","At Section we wanted to enable people to use popular reverse proxies like Varnish Cache and ModSecurity to improve the performance and security of their websites but remove the hassle of dealing with deployments, high-availability, patching, TLS configuration, or instrumentation. As such there are a set of components that the Section platform provides so you can focus on tailoring the caching and security rules that work best for your website. The Edge The first component is the Edge proxy. The Edge is what your users’ browsers connect to. It is responsible for the TLS handshake, HTTP protocol negotiation, and routing requests to your chosen Varnish Cache or ModSecurity proxy. The Section team ensure that the TLS configuration in front of your website continues to receive a Qualys SSL Labs Grade A+ rating as the vulnerability landscape changes. The Edge also enables your users’ browsers to connect using the modern HTTP/2.0 protocol even if your origin does not implement it. Your Proxies Immediately following the Edge proxy will be your chosen reverse proxy. This might be a particular version of Varnish Cache, or ModSecurity, or both in a chain. The Section platform ensures that there are always at least two instances of your proxy running in different locations to provide fault tolerance and high-availability. The platform can also provision more instances on demand to handle increases in traffic to your website. Origin Proxy After your chosen reverse proxy comes the Section Origin proxy. The role of the Origin proxy is to establish a secure HTTPS connection back to your origin webserver(s), and to ensure the time-to-live of your origin DNS records is honoured if your origin IP addresses are not static (e.g. if using an Amazon Web Services Elastic Load Balancer). The Origin proxy is necessary because some reverse proxies do not implement HTTPS natively or do not acknowledge DNS TTLs but even when your chosen reverse proxy can support this, the Origin proxy provides a consistent mechanism connecting to your origin, regardless of which reverse proxy you use. Logging and Messaging Outside of the delivery pipeline, there are two other Section platform components worth mentioning: the logging system, and the messaging system. The logging system ensures that all the web access logs from the Edge proxy, the Origin proxy, and your chosen reverse proxies are shipped to a central location for processing and made available to you for investigating and debugging (via Kibana) and for long-term trend analysis (via Graphite). This log aggregation also serves as a data feed upon which to configure monitoring and alerting (via Umpire). Lastly, the messaging system provides the means by which the platform is able to quickly distribute your configuration changes, cache ban requests, and trace requests to all your running proxy instances in the Section global network, and report back any results. While each of these individual components each performs a simple task, together they represent critical pieces of functionality needed to effectively operate a reverse proxy in front of your website and with Section taking care of this, you’re free to focus on what’s important: making your website faster, more resilient to high-traffic, and more secure."
"133","2015-04-30","2023-03-24","https://www.section.io/blog/varnish-install-quick-and-detailed/","How To Install Varnish Cache In this article we will work through the components required for a successful Varnish Cache implementation. The quick version: Ubuntu, Debian: apt-get update
apt-get install Varnish Cache
 RHEL, CentOS and Fedora: yum update
yum install Varnish Cache
 That’s it. Varnish Cache is now installed… Or is it?… Is it working?… Can you use it effectively? The quick version Section style: Get Started and get all of the below setup in seconds. The longer (and closer to reality) version: All high performance Varnish Cache implementations need to start with a quick architectural plan in order to understand how the solution fits into your environment. Varnish is an HTTP accelerator (also called a reverse proxy) and hence needs to sit in front of your existing web site as close as possible to your users. A successful implementation may look like: We will now work through each component required for a successful setup and provide links to specific code/tools where needed SSL/TLS: Varnish does not support HTTPS/SSL traffic. Period. As the majority of the worlds websites are moving to be all HTTPS these days this presents an obstacle that needs to be overcome for just about any implementation. The standard practice to support HTTPS is to implement a layer infront of your Varnish Cache servers that performs HTTPS negotiation and then traffic can be passed through your Varnish Cache layer (Commonly with the addition of an X-Forwarded-Proto request header set to “https”). A common solution to resolve the lack of SSL is to deploy an nginx tier infront of your Varnish Cache server. Here is some sample nginx/SSL configuration{:target=”_blank”} Load Balancing: Installing Varnish Cache on a new server is great but this implements a single point of failure in your infrastructure. Options available to resolve this are: Deploy Varnish Cache on each of your web applications servers - This can be a short term win as it doesn’t require additional hardware and along as you have multiple application servers you then have redundancy across your Varnish Cache implementation. This approach falls short when you have more than two web servers as your cache is split between each web server and cache hit rate will fall off dramatically as the web server count increases Implement a load balancer infront of your redundant Varnish Cache servers. This adds some complexity but is a scalable way to implement traffic balancing. Here is an example architecture link with code samples to deploy a load balanced environment{:target=”_blank”} HTTPS / SSL to the Origin application: It’s important to consider how traffic from your Varnish Cache tier passes back to your origin servers. Compliance and general security best practice mean that you should pass HTTPS traffic back to the origin application over HTTPS. The best approach to resolving this is to have your front end HTTPS/SSL termination add an X-Forwarded-Proto request header containing the value “https”. You can then add another nginx tier as a Varnish Cache backend which will have a proxy pass statement{:target=”_blank”} that will use the X-Forwarded-Proto value to determine whether to connect to the origin servers via HTTP or HTTPS Metrics: Most Varnish Cache implementations focus on getting the solution installed into the environment. Tackling all of the above activities takes effort and architectural planning. An area where little time/focus is commonly spent is understanding how well your implementation performs once it is serving traffic, This is the critical step that delivers results over the short and long term. There are multiple areas of focus: Individual requests In order to understand how Varnish Cache is performing for specific requests there are several tools available. These tools provide information on the flow of execution through your VCL and whether each request was served from Varnish Cache or required the origin server to respond. The best request specific debugger that’s quickly available on all Varnish Cache implementations is varnishlog. This tool allows you to send individual requests and analyse how each request has been handled. Reference for: varnishlog{:target=”_blank”} varnishtop is a second command available that shows continuously updated counters of values that you specify Reference for: varnishtop{:target=”_blank”} Summarised information on cache hit rates overall Tracking Varnish Cache performance on an ongoing basis is critical to understanding whether the solution is performing effectively. This is an area that often requires investment in additional logging/metric platforms. As data over time is hard to gather/parse from built in Varnish Cache tools. varnishstat is an option for short term review as its run at the command line. Here is an article describing how to use varnishstat{:target=”_blank”} The most robust method for metric management is to send Varnish Cache access log data to a metric/log management product. This is generally a requirement for an enterprise level deployment of Varnish Cache and can have its own cost to implement if nothing else is already available. In order to output access logs in an Apache style format you need to use the varnishncsa command, Here is a guide to setting up the varnishncsa command{:target=”_blank”} Summary On the surface a Varnish Cache install is a single command. Underneath this however there is actually a great deal of work that goes into a Varnish Cache deployment that is effective and secure. That’s why we made www.section.io - All of the above comes out of the box, Is setup in seconds and is free until you start to use a serious amount of data. Check it out if you want a quick Varnish Cache win - www.section.io"
"134","2017-01-11","2023-03-24","https://www.section.io/blog/private-public-key-encryption/","You may have noticed that over the past few months more and more websites have started to use HTTPS, the secure version of the communications protocol HTTP, for all of their pages. In Google’s Chrome browser, sites not using HTTPS now include an “information” icon next to address which states “Your connection to this site is not secure. You should not enter any sensitive information on this site (for example, passwords or credit cards), because it could be stolen by attackers.” Sites with HTTPS implemented have a green padlock with the word “Secure” next to it, and banks and other websites that use an extended validation certificate have a large green block with the name of their website in it. What does HTTPS mean? The main purpose of HTTPS is to encrypt the data sent between your browser and the website origin server, meaning that while hackers could see your connection with the website, they would not be able to see what pages you were visiting or read what information you were entering on the website (such as passwords or credit card numbers). HTTPS also ensures that visitors are going to the correct website by validating its SSL certificate, and are not visiting a site that is falsely identifying itself in order to steal sensitive information or, in the case of media sites, spread false information. How Public Key Encryption Works So how does HTTPS actually work to encrypt your information? It relies on a layer of SSL/TLS encryption (TLS being the newer and more secure technology), which uses authenticity certificates and public and private keys to establish a secure session between the browser and web server. This public-private key encryption uses unique keys to encrypt and decrypt data passing between a website server and browser, meaning that any content sent between a browser and the web server cannot be read by a third party that intercepts that data without the appropriate key. With modern TLS cipher suites that exhibit forward-secrecy, even a 3rd party that obtains the private key after the conversation is complete is unable to decrypt the data. In public key encryption, the web server holds two paired keys that can be used to encrypt and decrypt information. The server keeps the private key and does not share it, while the public key is sent to every user that attempts to securely connect with them through the SSL/TLS certificate. Through the use of a “SSL handshake” the server sends the user the public key, opening up a secure channel by which the user can encrypt messages that can only be decrypted using the paired private key. This paired key system means that even if a malicious third party intercepts the key exchange process, since the private key is never shared they would be unable to decrypt messages. With modern TLS cipher suites that exhibit forward-secrecy, even a 3rd party that obtains the private key after the conversation is complete still cannot decrypt it. The SSL/TLS certificate also authenticates that the browser and server are who they say they are: If a user is attempting to access the New York Times website, the certificate validates that the server’s SSL/TLS certificate has the New York Times as its owner. Extended Validation Certificates take additional steps to ensure the validity of the certificate owner and are used by entities such as banks, however the encryption process used once the handshake is complete is identical to those with standard SSL/TLS certificates. Get included SSL/TLS Certificates with Section Section provides all users with a SSL/TLS certificate for no additional cost, and we manage that certificate to ensure it never expires leaving websites vulnerable. To learn more about how Section improves website security for all users, please contact us."
"135","2017-03-21","2023-03-24","https://www.section.io/blog/cdn-saas-content-delivery/","Software as a Service or SAAS platforms are all over the Internet – they are pre-built services for companies that don’t want to create their own software for a specific need, and include job boards, online support systems, internal portals, and more. Many of these SAAS companies offer custom domains to their customers. This feature allows brands to use these services while maintaining their own brand identity. By “white-labelling” URLs, end-customers feel comfortable that they are staying within the ecosystem of the website they are on. While users of these SAAS platforms choose the domain name and can typically include some custom styling on the service, the SAAS companies are the ones who manage the backend of the system. This includes hosting, security, and website performance. SAAS platforms want to be fast and secure to remain competitive, but turning to Content Delivery Networks to improve website performance and security can be difficult in these use cases. Here’s why SAAS platforms have in the past been unable to use CDNs or have incurred great cost for doing so, and how Section approaches SAAS performance and security differently: Improving Content Delivery for SAAS platforms Configuring Multiple Domains: Since SAAS platforms manage thousands of domains, configuring each of these for caching and security individually can be extremely time-consuming. While many CDNs force every domain to be set up separately, Section allows one account to manage unlimited domains under the same configuration. This means when changes need to be made, they only have to be made once to be applied across every domain. SSL Certificate Management: Many SAAS platforms are unable to offer HTTPS due to the hassle of managing so many SSL certificates, which need to be created, uploaded, and renewed when they expire. Most CDNs charge for SSL certificates, and even then may only provide shared certificates. Section offers unique SSL certificates at no extra cost, and allows one account to have multiple SSL certificates for each of its domains. In addition, Section manages the renewal of these certificates so they never expire and leave websites vulnerable. Prohibitive Costs: Content Delivery Networks often have cost structures which include a base price for each domain and costly additional charges for advanced configurations, SSL certificates, and more. Section only charges for total traffic from all domains, so SAAS platforms can manage all of their domains in one account and only pay for total traffic, with no consequences for adding additional domains. As security and website performance become more and more important, brands looking to SAAS platforms expect them to be encrypted by HTTPS and load quickly. By using Section’s SAAS-friendly content delivery solution, SAAS platforms can bring their customers best-in-breed website performance and security. To learn more, contact us today and a Section representative will get in touch with you shortly!"
"136","2017-02-08","2023-03-24","https://www.section.io/blog/ecommerce-bots-seo-bad-bots/","Have you ever gone to an ecommerce site right as a highly anticipated launch was occurring and found yourself locked out or inventory immediately gone? Flash sales or new product launches can be both a blessing and curse for ecommerce sites: While they bring motivated visitors who are ready to purchase quickly, they can also be plagued by scalability issues, bots holding product up in shopping carts, and malicious traffic that aims to re-sell items on a third-party site such as eBay. At times, selling out quickly can boost a brand’s reputation: Kylie Jenner’s online makeup launches have led to an “economy of scarcity” where items sell out in minutes and are quickly listed on auction sites for 10 or 20x the original price, and limited edition sneakers backed by celebrity athletes have been in high demand ever since the famous Air Jordans were first released. However, for less high-profile product launches, customers become frustrated with slow websites or products that sell out quickly and abandon the site, resulting in a lost sale and poor user experience. There are a few different culprits for this occurring: Poor Scalability: If a website doesn’t scale well, it will not be able to handle a large influx of traffic. Websites can mitigate this by caching as much content as possible and using a globally distributed content delivery solution such as Section. By caching content, the majority of customers will be served from the cache and the load on your origin server will be dramatically reduced. Scraping Bots: Many bots are constantly scraping prices and product information from ecommerce websites in order to match prices on other websites. This high volume of scraping bots can have detrimental effects if the website is not built to handle enough traffic (see point 1). Bots Who Abandon Carts: Malicious bots may be programmed to place hundreds of items in a cart and then abandon it without completing the purchase. This action will tie up inventory for real visitors, showing them an item is out-of-stock when it is not, and also negatively impacts conversion rates and ROI for ecommerce sites. Bots Who Purchase Product: Bots can also be programmed to actually complete a product purchase by being pre-loaded with credit card and shipping information. These are most often used for products which have a high resale value: Some websites even sell access to pre-built bots that claim to work on a number of ecommerce websites. Protect your site from bad bots with Section section.io’s Edge Compute Platform provides protection to ecommerce sites experiencing heavy traffic by increasing scalability, decreasing load on website origin servers, and blocking bots via smart solutions such as the Threat X WAF. If you’re having trouble with bad bots or scalability, speak with one of our representatives today. Contact Us to Get Started"
"137","2016-08-26","2023-03-24","https://www.section.io/blog/cost-of-404-not-found/","Reducing 404 error pages on your website Do you know how many resources are consumed by your origin web servers to determine that a requested URL is non-existent and render the Not Found response page? Many site owners cannot answer this question, and most of the time this is fine because a well-managed site will rarely need to serve a 404 under normal conditions. With the effort that goes into marketing and SEO to bring traffic to your site, good site managers will ensure that old pages that are no longer appropriate will not simply be deleted but will instead redirect to a new URL with relevant content. Serving a 404 would be a lost opportunity. Abnormal conditions Unfortunately, pages that once existed and no longer do are not the only source of Not Found responses. Every now and then a site will get scanned. That is, it will receive a barrage of requests for many different URLs to see which serve an interesting response. This is much like a search engine crawler but potentially malicious or, at the very least, suspicious. Sometimes the URLs requested will take the form of possible administrator login URLs for different combination of web platforms and configurations. Other times there will be requests will be for URLs that look like common names of data export files or website backups. Often the number of requests will easily be in the thousands. That’s a lot of HTTP 404s From our experience improving the websites of our customers, we see many 404 pages that have a significant impact on origin web server resources. Sites today are now typically more dynamic than they are static. Determining that a URL exists can no longer just test for the presence of a file on disk but must instead execute web application code. This is somewhat worsened by sites preferring “clean” URLs, ie omitting the traditional .html, .php, or .aspx suffixes. I’m not suggesting that friendly URLs should be avoided, merely highlighting that has become a contributing factor to 404 processing costs. Once the request is being handled by the application code we then see additional costs. The most common is the establishing of server-side session state. This server state is often never used again as the scanning clients rarely honour session cookies. Eventually this can lead to all the available session state storage being consumed and along the way potentially slowing access to the state for legitimate users. Another pattern, that regularly appears, is a Not Found response that queries the database to find a list of categories or products that may be relevant to then provide helpful links on the resulting 404 page. These database queries can be expensive and compete for database resources needed to serve requests for other users. For occasional requests that result in a 404, performing this work is a minor concern. But when many of these requests happen quickly in a short time frame, this workload can quickly overwhelm the web servers and leave them unable to efficiently handle legitimate requests from users trying to purchase your goods or services. Mitigation Sadly, the solution is not trivial. Caching at the CDN is often ineffective in this scenario for two reasons: Every requested URL is different, the scanner does not try the same URL twice, so every request is a cache miss, and is proxied to the origin. Understandably people are often reluctant to cache 404 responses in general in case it impacts the deployment of a new page, or the cache space consumed by 404 responses competes with other resources needed for real responses. It is possible to get protection from a Web Application Firewall (WAF), depending on the nature of the scan. If there are reliable patterns in the requests a WAF can be configured to reject them before reaching the origin. Such patterns might be: A common User-Agent header present in all requests that is different from any browsers your real users use. A common client IP address or range (if the scan is not distributed). Request headers that are sent by modern browsers but are absent in requests from the scanner. There are two caveats though: You need to be wary of false positives, ie using a pattern that blocks requests from your real users too. Access to your website can be disrupted before you are able to identify the patterns and configure the WAF to block them. Often the best solution is to implement 404 handling in your web application efficiently. If possible, serve static content for 404 responses. If that’s not feasible, consider caching the results of the database queries in the application to avoid the cost of querying the category list every time, for example. Also, consider changing the server session state logic so that it doesn’t create or update a session in response to a request for a resource that is not found. If you’re really feeling adventurous you could try pre-generating a white list of all possible URLs that are valid on your site. This list would be similar in concept to a sitemap.xml which you may already have. This list could then be used to create some Varnish Cache VCL that would reject anything not on that list before it even reaches your origin web servers. Are you interested in trying out Section’s CDN and easily implementing Varnish Cache? Click below to get started with our 14 day free trial, no credit card required, or read our documentation to learn more. Try Section for free"
"138","2016-08-09","2023-03-24","https://www.section.io/blog/abs-census-fail/","As we know, millions of Australians have not been able to lodge their Census online as the ABS Census site is offline. We have been reviewing the nature of this outage and it appears the ABS took precautions against the volume of traffic it was expecting. But did it do enough? The ABS has stated that deliberate attacks from overseas were to blame for this attack. DoS was to blame. Here is webopedia.com’s definition of a DoS attack; Short for denial-of-service attack, a type of attack on a network that is designed to bring the network to its knees by flooding it with useless traffic. While not useless, traffic from the many real users (census lodgers) hitting the ABS site could also appear to be a DoS attack. However, that may not be what happened in this instance given the ABS states the traffic originated from overseas. The question remains, could or should the ABS have been able to prevent such an attack? Findings We have done a little bit of digging into the performance of the ABS Census site and whilst it is hard to diagnose these issues from the outside, especially when the site is completely offline, here are a few observations: The ABS expected significant traffic and took steps to be prepared; They have their application architecture set up to split across what appears to be around 10 isolated units within the hosting infrastructure. They have static objects serving from a Content Delivery Network They may have performed a large range of other activities which we cannot see from the front end – particularly with the site currently offline The site has been flooded with network level requests. The graph above, showing extended time to execute an SSL connection indicates packets are not getting through. While most tests conducted did not generate any connection, this one which has, indicates network type congestion. The ABS could have done better The HTML request for the site is not serving from a CDN The site does not appear to have network level DDoS protection It appears the architecture of the site means it must not be portable. The ABS could have moved the site to serve from alternate infrastructure The system was not build for graceful degradation. There is not even an “outage page” being served at present – Could the ABS repoint the DNS to alternate infrastructure and serve some level of friendly outage page now? Modern hosting infrastructure prevents these types of network layer attacks. Did the government procurement system prevent the ABS from having access to these global organisations or settle for a government endorsed supplier?"
"139","2016-12-19","2023-03-24","https://www.section.io/blog/levels-of-ddos-protection-cdn/","A DDoS attack is a distributed denial of service attack. This means that an attack is coming from multiple places with a high volume of requests with the intent to bring your site down. With Section we offer a few different levels of DDoS protection to our customers in the event of an attack. Limit DDoS Attack with Varnish Cache For customers with Varnish Cache reverse proxy on their platform, we are able to limit the impact of a DDoS attack. Varnish Cache will not block any IP addresses in the event of an attack. All requests made by a malicious attacker will be answered, however, the protection level comes from how they are answered. With Varnish Cache, your website has a cache that saves copy of webpage and assets. Once a copy of these files are saved in the cache, the rest of the requests can be served from the cache. So if 1,000 malicious requests come in at the same time with the intention of taking your site offline, the first request will be served and the other 999 can be served from cache, protecting the server. Varnish Cache limits the impact of a DDoS attack by only having a fraction of requests go to the server, reducing the ability for the attack to take your website offline. Manual Block with IP Blocking The next level of protection is independent of the reverse proxy being used on the platform. Section allows you to manually add IP addresses or blocks of addresses you’d like to block. Any IP addresses you add to to the “Restrictions” tab in the Section platform will not be able to access your website. In order to identify which IP addresses you want to to block, we recommend using Kibana within our platform to view your logs. During an attack you will see a high volume of requests coming from one or more IP addresses. To find IP addresses with a high volume of requests, you can look to “http_x_forwarded_for” in Kibana. This is a header that creates a chain of IP addresses representing all the IPs that handled the request in order including the original client (which will be the leftmost value). Kibana will show the percentage of records that have the same value for “http_x_forwarded_for” and therefore the IP addresses with the most requests will rise to the top. You will want to ensure that total traffic is up in Kibana (which you can see if there is an uptick in logs via the default view which show total number of logs) to confirm you are in the middle of an attack. Automatically Block with Web Application Firewall For customers with with Modsecurity reverse proxy on their platform, we can automatically block requests. Modsecurity works by having a set of rules that it uses to determine malicious traffic. Out of the box, we have the core OWASP rule set but you can also define additional rules. By default Modsecurity is in Detection Only mode which means that it will look at traffic based on rules and log its findings. This lets you review the rules and see how it would impact your site if it was actively blocking requests. By changing Modsecurity from DetectOnly to On to enable protection mode, we will start blocking traffic based on the rules defined. Protect your site with Varnish Cache, Modsecurity and our CDN today Section allows developers to choose the level of protection their site needs, whether it is basic DDoS protection through our CDN and Varnish Cache or more advanced protection through ModSecurity’s Web Application Firewall. Test out your security configurations by signing up for a free trial account, or contact us with any website security questions."
"140","2016-12-07","2023-03-24","https://www.section.io/blog/is-your-site-ready-for-peak-traffic/","It’s a marketer’s dream. You come up with a new idea for a campaign to drive attention to a sale on your site. You get approval from the bosses, build a carefully targeted email list, put together some display ads, and get ready to scream about it on social media. Finally, when you pull the trigger all your hard work, creativity, and ambition is rewarded with a flood of traffic ready to throw money at your site. And then the dream becomes a nightmare. Your site speed takes a hit, and bounce rates start spiking. Browsers start timing out before items can be added to a cart. Payments aren’t processed. Finally, your site goes offline entirely and you find yourself sending panicked emails to anyone you can think of that can help get it running again. You sweat each passing minute feeling the loss of potential revenue and praying that all of these would-be customers don’t associate your brand with the incompetence you’ve displayed and the frustration they are feeling. By the time the site is back up, the moment has passed. Your wildly successful marketing campaign has turned into a disaster. While not malicious, a great marketing campaign that drives overwhelming interest to your site can be just as dangerous as a DDOS attack. It doesn’t look any different to your web server attempting to respond to so many simultaneous requests. In some ways it is even worse, because you’ve taken down your site at a moment of peak interest while leaving a bad taste in the mouth of potential repeat customers. This is why website scalability is so important, regardless of what your average traffic levels look like. In order to protect your website from going dark at the worst possible time, you need a plan to make sure your website is fast at any scale. Here are some action items to consider to make sure your site stands up during your next traffic spike. Prepare your site for an increase in visitors Cache static objects - Caching the static objects on your website can drastically reduce the load on your server by offloading files like images and javascript to reverse proxy servers. This not only makes your website more scalable, but improves user experience by speeding up page load time. Front end optimization - Taking a look at the performance of your website and finding areas to improve is a crucial step to creating a scalable website. Find large images that can be compressed or combined with other images to reduce the number of calls to the server. If you have old third party JavaScript still running on your site but providing no value, remove them to improve page load time. Also consider your options when it comes to hosting providers and make sure they meet your needs in terms of website performance. Use an overload protection service - Getting your site in scalable shape is your first priority, but it is always important to have a backup plan for huge traffic spikes. Overload protection services allow you to place a cap on the number of concurrent visitors your site will service at a time. This keeps your site running for existing customers and displays a message to new visitors to try again later. Learn more about overload protection from Section. When it comes down to it, website performance should be measured by how your website performs under pressure. A great website performs not only under ideal circumstances but under duress as well. Make sure you’ve taken steps to prepare for the most demanding conditions and stop letting great marketing go to waste. To improve your website performance today, sign up for a free trial account of Section."
"141","2016-06-08","2023-03-24","https://www.section.io/blog/3rd-party-javascripts-reduce-impact-on-page-load-speed/","3rd party JavaScripts can be great. Scripts such as conversion trackers like Google Analytics are great for the marketing team and management. There are scripts out there that conduct AB testing, such as Optimizely. Other scripts help users navigate the site such as auto complete input for search bars, or promoting pages via social networking like Facebook or Pinterest. How do they affect performance? All of these scripts add features to a site, but consider the performance impact of using these scripts. They are not free. At what point does the benefit of these scripts become outweighed by their negative impact on performance? We often see beautiful, feature rich sites bogged down by having too much of a good thing. The image above shows a snippet from a WebPageTest of a site that is heavy on 3rd party JavaScripts. You can clearly see heavy CPU usage before page is fully loaded. See this article about speed and its relationship with conversions. Does speed improve every website? Some may think that using the async attribute is the answer but it only means that the script doesn’t block the browser from parsing the rest of the DOM. This doesn’t mean you have unlimited bandwidth or CPU cycles to load everything concurrently. So scripts will compete for available resources along with all other elements on the page. What can be done? For each script, consider the following: Is the script visible to the user or does it modify visible elements? If it is visible, is the script providing a function that is required at the beginning of page rendering? If the script isn’t visible or has a feature which isn’t likely to be useful during early stages of page load, then it should be loaded after visible content. Doing so can significantly decrease page load times which in turn improves the user’s experience on the site. Consider wrapping the script to be loaded once the page load event is triggered by the browser: <script type=""text/javascript"">
(function(w){
    function fix(){
        var d = document, g = d.createElement(""script"");
        g.type = ""text/javascript"";
        g.src = ""SCRIPT URL"";
        var s = d.getElementsByTagName(""script"")[0];
        s.parentNode.insertBefore(g, s);
    }
    w.addEventListener ? w.addEventListener(""load"", fix, false) : w.attachEvent(""onload"", fix);
})(window);
</script>
 Different scripts may require different ways to be deferred. But in most cases the payoff is well worth the effort. The images above shows a WebPageTest of the same URL as previous after many scripts were deferred. We see a speed improvement of around 5.5 seconds to page load event. Note the shift in CPU usage to after page load. Conclusion You can have to benefits of 3rd party JavaScripts without impacting the overall experience of the website by your users. For best practise, defer scripts that do not impact visible content to load after visible elements. Section’s platform can insert these optimisations on the fly as the HTML document passes through our platform. Contact us to find out about this feature and other ways we can improve the speed of your site"
"142","2016-06-06","2023-03-24","https://www.section.io/blog/product-manager-kpi-page-load/","As a digital product manager, a key responsibility is to prioritize engineering projects based on business metrics or Key Performance Indicators (KPIs). These KPIs are typically conversion or engagement related, such as activation rate, monthly active users, or new paying customers. A product manager can then prioritize projects based on a cost-benefit analysis - ie. the project’s impact to these KPIs vs the project’s level of effort required. So when prioritizing projects, it’s important for a product manager to know which KPIs matter. In a recent discussion, someone asked if a product manager should have page load time as one of their KPIs? There are clearly engineering projects to improve page load speed, but is it something a product manager should care about and factor into their planning? If you are a digital product manager, you should care about page load times, and here’s why: As page load time decreases, conversions increase. By working on this metric a product manager can see gains on most other conversion metrics. Let’s go through two examples. Web App Onboarding Example For web app, you may have analyzed the steps in your onboarding funnel and identified a few steps where drop off is higher than desired. Some of that drop off could be caused by users leaving the app or website before the next step has loaded. That means that even if you spend the resources to optimize the call to action (CTA) on that page, via development time and a/b testing, your users may have left before they even had the chance to see it. In fact, if your page takes 4 seconds or longer to load, page load could be responsible for 25% of your drop off! Mobile Ecommerce Example Another example is for ecommerce product managers. Let’s say you are testing a new layout for mobile browsing galleries. You want to ensure great usability for your customers, so you explore changing the navigation of your galleries and how you display each of your items, optimized for mobile viewing. Depending on the size of your business, any increase you see in click through rates and add to cart conversion would make this project successful. But did you know that if your mobile commerce site doesn’t load in 3 seconds or less, 40% of your users will drop off! So by focusing on improving mobile page load you could see even bigger wins to your bottom line. These are both staggering numbers, considering how important each percentage conversion rate is. Now after making the case that page load is important, I want to take a step back and acknowledge that there are many important metrics to consider as a product manager. If we could move the needle on everyone, wouldn’t the world be wonderful? Of course, choosing to focus on page load improvements is a tradeoff of course, as there may be feature work that could be more impactful to those conversion metrics. Time Allocation Suggestion As any product manager will tell you, tradeoffs are a huge part of the job. One way to ensure things don’t get overlooked it to budget for different types of work. Perhaps 75% of the engineering team’s time should be spent on new features and 25% of improvements to page load times. In this case, you could see the wins on your feature work (let’s say successfully a/b testing the 2nd step of your onboarding flow) compounded by not losing customers to slow page load times. Celebrations all around! Another approach, for larger companies, is to give different development squads different metrics to focus 100% of their energy on, where one squad is 100% focused on page load time as their KPI. This helps to ensure that each KPI is consistently getting development time devoted to it. It should also be noted that even while working on feature work, teams should avoid introducing new code that slows down page load time as the decrease in speed may remove any wins the new feature could have been introduced. A minimum measure of success should be for new feature work is to increase KPIs without decreasing page load times. Marketing ROI In this post, I’ve focused on how a product manager should think about page load time as their KPI, however, there is another group of managers that should be concerned with page load times - marketing managers. As a marketing manager, the page load time improvements I’ve mentioned above for conversions can increase the effectiveness and ROI of your marketing spend. Whereas a product manager may need to prioritize development resources to one project or another, marketing managers can advocate that their product managers and development managers spend time on page load improvements. As described above, you will see increases in conversion rates as fewer people abandon at each stage of the funnel making every dollar spent more effective. Making sure your product manager knows about the benefits could help change the cost-benefit analysis in the favor of projects focused on page load time which will help your marketing budget go further. Infographic from Mashable"
"143","2016-06-01","2023-03-24","https://www.section.io/blog/pagespeed-insight-scores/","One of the current go-to tools for people when trying to make their sites fast is Google’s PageSpeed Insights. https://developers.google.com/speed/pagespeed/insights/ Enter an URL and it will generate a “score” along with a list of suggested fixes for any issues it thinks relevant. This score is out of 100 and is also split between mobile and desktop devices. What does it mean? It is important to note that Insight Score does NOT measure real world speed. What it actually measures, is how well your site is inline with Google’s principles of what a fast performing site SHOULD be, in terms of structure and delivery. Now granted that these principles are generally correct, but sometimes they are not realistic for what you wish to achieve. More on this later. Insight scores are independent of each other. What this means is that site A with a score of 100/100 can have a load time of 5 seconds while site B with a score of 50/100 can have a load time of 2 seconds. These scores are indicators of potential performance, not actual performance. The Insight score simply indicates that Google believes site B can potentially be faster if it made changes to be inline with recommendations. False Positives One thing to keep in mind is that the score is based on generic principles that should improve site speed. But sometimes a feature is placed on the site that contradicts the general principles, but for good reason. One of the most frequent issues we see has to do with render blocking JS and CSS files. Render blocking is something that Insight often indicates as a serious issue that needs to be fixed. But there are occasions where you should do the opposite of the recommendation. A good example of this is Optimizely scripts which are used widely. Their script should render block, because otherwise it will cause unintended “flicker” during page load. But Insight will flag this script as “blocking” unless you load it asynchronously. See: https://help.optimizely.com/hc/en-us/articles/202666094-Synchronous-and-Asynchronous-snippet-loading Real World Performace Insight is good at giving an overview of where performance could be extracted, but it is no substitute for real data. As can been seen for the two following sites. Site A on the left has a much higher Insight score, but site B on the right is faster for all metrics. Conclusion From the above results, it is clear that Insight scores are not a good indicator of speed. Insight should be used as one of many tools to help make your site faster, but not as an actual measure of performance. Use its recommendations as simply that, recommendations and not hard and fast rules. Then use real world data to measure the performance of changes."
"144","2016-05-09","2023-03-24","https://www.section.io/blog/shark-tank-preparation-monitoring-results/","Scenario A couple of weeks ago we were asked by several clients to bring their sites onto our platform to handle large spikes of traffic generated by Shark Tank the TV show. Section were engaged last year to help a number of sites stay online through the Shark Tank experience. We saw first hand that a well tuned cache will keep the site performing super faster with 100% availability under the extreme traffic spikes, while it was reported to us that sites without caching struggled and even went offline. This not only impacted their sales but set a poor first impression for potential investors. Our challenge was to keep the sites not only operational but performing fast to provide a great user experience. This meant we needed to minimise traffic hitting the origin through the use of Varnish Cache caching, as well as offer methods for our clients to monitor and manage traffic. Preparation We examined the sites and placed pages into different buckets, cacheable and non-cacheable. Luckily none of the sites had visitor-individualised data inside the HTML, allowing most pages to be cached. A Varnish Cache configuration for each site was then written, taking special care around pitfalls that broke caching such as session management, as well as forms, cart and checkout. We also setup features in our managed platform such as: Overflow management - Only allow a certain number of active users on the site, users over that number is sent to a holding area for a few minutes with a nice Overflow page to look at. Multi-CDN for static assets - Give users the best page load speeds by serving static assets from the closest CDN POP. Active user monitoring - See number of active users on the site, which pages they are viewing, which region they are viewing from, source of traffic, etc. Site speed monitoring - Tracks page load speeds with breakdowns for Homepage, Category, Checkout etc. Origin response monitoring - Logs the HTTP responses from the origin to be able to easily spot issues. And more On the day Our engineers monitored our dashboards with the aforementioned monitors to help the clients if needed. As each business was featured, we saw a huge volume of traffic hit the sites, over 900% increase for every site as compared with previous maximums. You can see in the graph below the sudden surge in HTTP responses served by our platform. Results All customer sites performed well throughout the event. For all sites we saw HTML hit rates of at least 70%, substantially decreasing working load on the origin. Here is one of the better performing sites with a HTML hit rate of 85% Keeping in mind that a large portion of the misses are intentional as some pages were not able to be cached, for example Cart or Checkout. Static asset offload was also a huge win. Our front line multi-CDN cache served over 80% of the static requests. And of the misses, up to 93% were served from our secondary caching layer, which means very minimal number of static requests had to be handled by the origin. From a user experience perspective, the sites maintained their median page load speeds throughout the event. You will see in the graph below, that when the throughput increased, the median and mean page load speeds became more consistent and lower due to the cache being well populated. The Overflow feature was not required at any stage due to the high amount of offload provided by our caching layers. It was reported by our clients that even during that largest spikes, their origin did not exceed 50% usage for any resource, up from 20% during normal operations. This was a great result as resource usage on the origin only increased by 30% while actual traffic increased by orders of magnitude. If you’d like to know more about how Section can help your site, then sign up, join a weekly webinar, or contact our engineering team."
"145","2016-11-22","2023-03-24","https://www.section.io/blog/traffic-overload-prevention/","A top concern for websites is staying online during spikes in traffic. Businesses spend lots of money from their marketing budget to attract customers to their website, however, if the website cannot handle the amount of traffic it can go offline during these traffic peaks and lose all of the potential revenue from customers in the middle of transactions. Protect Your Website Section now allows for you to set a limit on the number of visitors your site can safely handle to manage your online web traffic more effectively. When your website reaches this limit, an overload page will be displayed to new customers so that they cannot access the website. This allows the existing customers who are actively browsing and transacting on your website to complete their session uninterrupted. When those customers leave the site, additional customers can enter, creating a virtual waiting room or a virtual queue for customers during your biggest spikes in traffic. You can customize the language on your overload page so customers know that they are in line to get access to your website. Setting Up Virtual Waiting Room Section’s Virtual Waiting Room uses real-time analytics to determine the number of users currently on your website. The application will then use Varnish Cache to provide the overload page when your maximum number of users has been set. To add Virtual Waiting Room to your Section application, check out our docs for instructions. Determine Your Maximum Users The next question you may have is, how many users can my website safely serve before going offline? If you have ever been taken offline by extreme traffic before, you should see the tipping point in your metrics. Choosing a maximum number slightly below that will give you a number that you’ve seen your website be able to handle. If you have not been taken offline before, we recommend analyzing your scalability (ie. how well your website handles increases in traffic) using a tool such as New Relic’s Scalability Analysis to determine how well your website scales. If you see an upward trend in CPU as the number of requests increases, that means your website has trouble handling increased volumes of traffic. You can approximate at what point the number of users would max out your available CPU and set your maximum number of users just below that. Making Your Website More Scalable If you feel your maximum number of users is lower than you would like it to be, there is work you can do to improve the scalability of your website. By having Varnish Cache on your website, you can cache more objects which will reduce the work required by your servers. By storing objects in the cache, the server is only hit by a request when the cache needs to be updated. That means users can get their responses handled by the cache, whether there is one user or one thousand, and keeping the work required by your servers low. Section offers Varnish Cache on all of our plans and Section’s consulting services can help you identify the best places to optimize the cache to improve your scalability."
"146","2017-09-18","2023-03-24","https://www.section.io/blog/virtual-waiting-room/","Section is pleased to announce another feature to help websites stay online when extreme traffic peaks arrive. A Virtual Waiting Room can ensure that your website stays up and serving customers when an extreme traffic event would otherwise have caused a complete outage. To ensure your website is always online and always fast, a Virtual Waiting Room is a great addition to Section’s other performance and scalability features such as Dynamic Content Caching, HTML Streaming, Static Object Caching and Image Optimization. Protect Your Website with Virtual Waiting Room The Section Virtual Waiting Room means you can set a limit on the number of visitors your site can safely handle. When your website reaches this limit, Section’s Virtual Waiting Room will push excess users to an “overload page” of your choice. This allows the existing customers who are actively browsing and transacting on your website to complete their session uninterrupted. When those customers leave the site, additional customers can enter, creating a virtual waiting room or a virtual queue for customers during your biggest spikes in traffic. You can customize the language on your overload page so customers know that they are in line to get access to your website. Setting Up Virtual Waiting Room To add a Virtual Waiting Room to your Section application, check out our docs for instructions. Making Your Website More Scalable If you feel your maximum number of users is lower than you would like it to be, there is work you can do to improve the underlying scalability of your website. For example, by deploying Varnish Cache on Section for your website, you can cache more content which will reduce the work required by your servers. By storing objects in the cache, the server is only hit by a request when the cache needs to be updated. That means users can get their responses handled by the cache, whether there is one user or one thousand, and keeping the work required by your servers low. Section offers Varnish Cache and Section’s performance engineers can help you identify the best places to optimize the cache to improve your scalability. Find Out More Contact our engineers to find out more about implementing Section’s Virtual Waiting Room or any of our web application performance, security and scalability features. Contact Us"
"147","2016-12-05","2023-03-24","https://www.section.io/blog/prepare-website-holiday-sales-ecommerce/","The holiday shopping season starts earlier and earlier each year, with stores putting up decorations and websites running holiday features as soon as November begins. However, when big sales come around some websites always seem to struggle. Luckily, even in the middle of the holiday season there are several steps websites can take to ensure they are smoothly running through December and into the January sales. Set up robust monitoring and reporting One of the most important things to understand as you go through the peak traffic period is how your users are experiencing your website; Are all your users still having an acceptable browsing experience? Are they able to view products and go through the checkout process quickly? Where is your traffic coming from? How many more visitors can your site handle without crashing? While synthetic tests can estimate user behavior, it’s extremely difficult to guess what the real user experience will be when your site is at record traffic levels. So the best solution is to use a real user monitoring (RUM) system to measure what your users are experiencing in real time. Monitoring the performance of your webpages from the browsers of your users can provide you with invaluable assistance to help you make informed decisions on the fly. What’s more, quality monitoring and reporting will also allow you to have a meaningful post-December breakdown and help with the preparation for the next high traffic event. Deploying real user monitoring can be very simple; taking just minutes of the IT department’s time to engage the monitoring tooling. There are a number of different vendors in the market today and each has their strengths and weaknesses in this space. We recommend New Relic, and Section customers get discounts on New Relic tools. Deploy Infrastructure Wisely When setting up your website for peak traffic periods, having banks of servers prepped and ready to go might sound attractive but can be very costly (in terms of hosting and software license costs) in addition to unwieldy to manage. Making use of cloud-based elastic infrastructure can provide “burst” capacity so you only pay for the capacity during the period in which you need it. Be aware that truly burstable hosting infrastructure can require you to architect your site in specific ways and not all hosting providers can accommodate server scale increasing immediately on demand. Using correctly deployed Content Delivery Networks can be a great way to offload traffic from your infrastructure. Content Delivery Networks can prevent up to 98% of user requests even reaching your application infrastructure. This delivers a tremendous increase in your application capacity (and the speed of your website pages) without making any changes in your hosting or application, and allows your servers to focus on the critical checkout transaction processes. We should note that incorrectly deployed Content Delivery Networks can slow your website down. However, It is possible to have a correctly deployed “whole website” Content Delivery Network up and providing offload and acceleration for your website in hours. Prepare for the Best (Worst) Situations The best possible situation you can have through the festive season is a website flooding with quality traffic and the worst thing that can happen at that time is a website crash. This happens to every website at some time and the trick is not to panic. You should always have a graceful way to inform a user your site is down or not functioning fully rather than throwing them to an ugly “Server 500” error page. You could serve the static pages for your site from alternate infrastructure (such as the Content Delivery Networks); this may include a custom error page which informs the user that the site is offline in a user friendly manner. Thus, even when transactions aren’t being captured at that moment, at least the user has a more pleasant experience and may be more inclined to return later. Correctly structured “outage pages” can also give users a sense that this site and its offer is so hot everyone wants some and they have to come back. There are also several services which offer features that prevent websites from going offline by capping visitors at a certain number and telling any subsequent visitors they need to wait until someone leaves the site to be let on. Section offers this in our Virtual Waiting Room, which uses visitor traffic data to set a reasonable visitor limit and prevent site crashes. Get time on your side Sending out the marketing emails to your user base advertising the latest deal will undoubtedly bring quality traffic to your site. But if you already have good quality traffic hitting your site, sending out that a holiday email with great deals can be the straw which breaks the camel’s’ back. Ideally, you should be able to send out your marketing emails at the time which generates the best click through rate for your particular user base. However, you do not want to drive traffic to an under-performing site and create a poor experience for all users. Subject the level of traffic on your site, it may be prudent to break your list down into chunks so you can email groups of users at different times and control the timing of traffic being driven to your site. Managing the timing of the marketing campaigns well can be enabled by understanding what is happening on your site in real time. (See Monitoring and Reporting above). Avoid the “Hail Marys” This is always a tough call. It’s easy to say avoid panic in a high traffic period but when the site is down and everyone wants to know what you are going to do about it, the temptation is to reach for any option. Rarely do the Hail Marys come off. In fact what they can do is complicate things both in trying to bring the site up immediately and then later when in the cold light of late January, you and the team conduct a post-mortem. That said, if you have reached the point of watching over a digital corpse in December as traffic floods your website, then a Hail Mary option from the tech team may be a sound decision. Try and do things which have been done before and keep the monitoring turned on. And next year, get started on steps 1-4 as early as possible. Section is a website performance and security tool that has worked with many ecommerce sites through record traffic times. Our platform allows developers to configure a smart caching solution on our global CDN in just a few hours, just in time for peak traffic times. Sign up for a free trial here or contact us with any questions about preparing for peak load times."
"148","2017-02-01","2023-03-24","https://www.section.io/blog/threatx-intelligent-web-application-firewall/","Today we’re excited to announce that we have added Threat X, an intelligent Web Application Firewall, to Section’s choice of reverse proxies. At Section we’re committed to bringing you a choice of the best-in-class solutions for website performance and security. That’s why we let you choose which unmodified, open-source version of Varnish Cache works with your application, and now you have the choice between two WAFs: Threat X, which is a next-generation intelligent WAF backed by a team of security experts, and ModSecurity, a leading open-source WAF. Threat X is unique because it learns your website’s visitor profile and detects and blocks threats with no configuration needed from you. By using smart sensors, Threat X can analyze your site’s traffic patterns and identify malicious entities based on their combined behavior, rather than on individual security events. This means that Threat X detects more malicious traffic while reducing false positives that block legitimate traffic. Threat X’s intelligent WAF is also backed by a team of security expert who constantly monitor your website and the latest hacker trends so you are always one step ahead of the game. When Threat X blocks an attack, you will immediately be alerted and see all the information about the attack, what was impacted, and how it was blocked on a user-friendly dashboard. By combining Threat X and Section, users will be getting the best, most modern solutions available for performance, scalability, and security. Section’s global Edge Compute Platform and Varnish Cache provide your site with lightning-fast speeds, while Threat X works seamlessly behind the scenes to ensure your site is not taken down due to an attack and visitor data stays protected. Since Section doesn’t lock you in to a long term contract or specific reverse proxy selection, you can always adjust your proxy setup as we add additional functionalities for website speed and security. Learn more about Threat X and contact us to add it to your Section application or start a new account with Section. Contact Us to Get Started"
"149","2016-06-22","2023-03-24","https://www.section.io/blog/announcing-free-automated-https-certificates/","We have recently released two new features that make it easier to utilize Section’s CDN for your website. There are two requirements to start serving traffic over Section: Changing your DNS Uploading your HTTPS certificate With new Section features, we’ve streamlined the process so it’s easier to get setup and easier to manage once you are setup. For more information on our new feature for hosted DNS, read our announcement. Announcing Automated HTTPS certificates Until now, you’ve needed to provide your own certificate to allow Section to make your site HTTPS capable. This required you to purchase a certificate, manage the keys and installation of those keys, and ensure you stay on top of renewals so the certificate doesn’t expire. With our automated HTTPS certificate, we can now generate a certificate for you with no additional cost to you. In addition to cost savings, you will see time savings as you no longer have to manage keys or worry about renewing your certificate. We still allow you to bring over your own certificate if you prefer. Why should I use Section’s Automated HTTPS certificates? Certificates allow your website to encrypt your customers data to protect their information from security threats. A critical component of a digital security strategy, certificates can be time consuming and costly. With Section’s certificate offering, your website receives a free certificate automatically without any action required from you. Fully Managed Experience With our fully managed certificate experience, we get your website up and running on HTTPS without any effort required by you. We continue to manage the certificate so you never have to worry about it now or in the future. And with our free offering, we save you both time and money. Procurement: Before you can run your website on HTTPS, you need to get a certificate. This means you need to research vendors, compare prices, and spend money. With Section, we take care of certificate procurement for free. Installation: Once you have purchased a certificate, you then need to install the certificate on your website by finding the appropriate keys. You then need to safely store the keys in case you ever need to install the certificate again. On Section’s platform, you don’t need to do anything. We will install the certificate and store the information so you don’t have to worry about it. Renewal: After your website is running on HTTPS, it’s easy to forget about your certificate management. With Section, we take care of certificate renewal for you, so you never have to worry about an expiration date again. Automated renewals ensure your site will offer uninterrupted HTTPS for your customers without any action required by you. Reliable Data Encryption When your customers come to your website, you want to ensure that their private data is safe from security threats. Poor management of customer data not only creates a negative experience for customers, but also lost revenue. With Section security offering, we immediately provide your website with end-to-end encryption to ensure your customer’s data is protected. Rigorously Tested: Section performs regular automated testing to ensure your encryption always meets or exceeds industry testing. Our testing ensures a Grade A or better on Qualys SSL Labs, a comprehensive HTTPS server test with results you can check yourself at anytime. Enhanced Performance: Our end-to-end data encryption wraps your reverse proxy configuration with secure protocols to allow you the benefits of performance to your website that reserve proxies offer while ensuring your customers are safe from threats. Immediate Website Benefits Certificates provide your website with data encryption, but they also provide other benefits to your business. These benefits are seen immediately with Section, as the entire process of setting up your site on HTTPS happens automatically and without any action required by you. Improved Brand Perception: By having your site on HTTPS, your url will display a green padlock next to your domain. This signals to potential customers that your website is secure and their data will be safe. Increased Revenue: With a more secure brand perception, customers feel that their information is protected. This makes customers more comfortable making purchases online which leads to an increase to your bottom line. Reputation Management: Unlike other services who make you share your certificate with other websites, our certificates are provided for your site only. This means you don’t need to worry about your reputation being impacted by things that are out of your control. SEO Improvements Search engines review your website for many things, including if your website is secure. Having your website be HTTPS capable means that your website is more attractive to search engines and can improve your SEO and rankings. To start improving the performance of your website or get started with automated HTTPS certificates, contact us today."
"150","2016-06-22","2023-03-24","https://www.section.io/blog/announcing-free-hosted-dns/","We have recently released two new features that make it easier to utilize Section’s CDN for your website. There are two requirements to start serving traffic over Section: Changing your DNS Uploading your HTTPS certificate With new Section features, we’ve streamlined the process so it’s easier to get setup and easier to manage once you are setup. For more information on our new feature for automated HTTPS certificates, read our announcement. Announcing Hosted DNS Until now, you’ve needed to go to your current DNS provider and create a CNAME record to point to Section. If your provider did not allow you to point at bare domain, or required you to point at an IP address, this presented problems for you. In the past, we’ve resolved these problems by created redirects which can add time to your page load and also hurt SEO for your website. With our new hosted DNS you can now host your site directly on Section. This allows your bare domain to point directly to Section without the need for redirects, reducing overall page load times. You can also now manage your DNS records and TTL from directly within your Section portal, removing the need for one more website to visit to manage your website. Why should I use Section’s Hosted DNS? Section offers a globally distributed hosted DNS service leveraging a highly scalable Anycast network. Section Hosted DNS is fast to setup, easy to manage, and offers performance and security benefits to your website. This service is included as part of all Section accounts. Fast to Setup Section provides you with an easy DNS hosting option that gives you the confidence your website is in good hands, without having to dedicate unnecessary time and effort to getting started. With Section, we make it easy to setup and offer a reliable and globally distributed Anycasted DNS network so you can spend time running your business. Immediate Access: With hosted DNS, Section can get your website up and running quickly on our CDN so you can start seeing performance benefits immediately. Avoid spending time creating and managing CNAME records at a separate DNS hosting provider by hosting directly with Section. Simply update your nameservers at your domain registrar where you purchased your domain and we’ll take care of the rest. Globally Distributed DNS Network: By setting up your website on our hosted DNS service, you’ll be set up on a globally distributed Anycasted network. With 24 PoPs located across 6 continents, we ensure your website will be available to your customers no matter where they are. By routing your DNS queries to the closest data center, we also make the experience fast. Easy to Manage We’ve streamlined the process of managing your DNS so you can easily manage your account using our user-friendly interface. All routing and configurations for your DNS and CDN will now be accessible in one place, giving you full control and transparency into the backbone of your website. Simple Record Management: Within our user-friendly interface, you can add, edit, and remove multiple record types as the needs of your business change. Supported record types include A, CNAME, MX, TXT, and ALIAS/ANAME. Easily view your existing records from directly within the platform without having to log in to another system. TTL Control: Easily change the time to live (TTL) for each of your DNS records within your account, giving you full control of how your DNS changes propagate across our Anycasted DNS network. With TTL control access in Section’s platform, now all your website caching configurations, including DNS, browser, and edge proxies, can be managed in one place. Website Benefits Not only is Section’s hosted DNS service fast to setup and easy to manage, it provides critical benefits to your website. Fast Performance: Section is known for providing performance benefits to your website, and hosted DNS is one more way we can make your website faster. By hosting your DNS with us, you eliminate creating redirects needed with a CNAME record. That means your website visitors are directed directly to your website without a redirect and therefore see faster page load times. Optimized for Search Engines: Another benefit of removing a redirect needed with a CNAME, is that your traffic is directly accessing your website without www. in front, so you won’t lose any SEO earned if you’ve chosen to send your traffic there. This means you no longer need to choose between marketing and performance, you can have both. Secure Service: Using Section hosted DNS allows us to easily validate your site and provide a SSL/TLS certificate to encrypt your application data over HTTPS. This means you won’t need to purchase your own certificate or manually provide the key information to ensure your site is secure. To start improving the performance of your website or get started with DNS hosting, contact our team of experienced engineers today. If you already have an account, checkout out documentation to get setup."
"151","2016-06-21","2023-03-24","https://www.section.io/blog/bare-domain-cookies/","Suppose you bought the domain name “example.com” for your website. Will your visitors enter www.example.com or just the bare domain example.com in their browser’s address bar to access your site? The general advice is to ensure that both addresses work but to choose one to be your canonical domain. That is, one address will be preferred and the other one will just redirect the user to the preferred one. Typically the redirect is achieved by serving a HTTP 301 response code with the correct Location response header. Which one should you choose? The “www” domain is traditional but the bare domain (aka zone apex) is quicker and easier to both say and type. There are numerous arguments one way or the other but there is one reason that I recommend choosing “www” to be the primary. No, it’s not related to the DNS limitation of bare domains. You may have heard that using a bare domain limits you to using an A record for DNS and you cannot use the CNAME record type required by many service providers, including Section. I don’t worry about this because all modern DNS providers, including Section’s own Hosted DNS, support a DNS record pseudo-type known as an ALIAS or ANAME record which essentially enables CNAME-like behaviour for bare domains. The reason I recommend “www” is due to cookies and sub-domains and their combined impact on both web security and caching. Caching When you prefer the “www” domain, you can ensure your web server specifies that cookies are only applicable to the “www” domain. The browser will then only send the cookies for requests to the “www” domain. If you have other sub-domains, eg images.example.com, blog..., or mail..., the browser will not send the cookies that apply only to “www” in any requests to these sub-domains. However, if the bare domain is chosen as the canonical site, the cookies should also be scoped to the bare domain. The problem here is that different browsers have different opinions on whether a cookie set at the bare domain should also be sent for sub-domain requests and it varies further depending on whether the domain attribute was included in the Set-Cookie response header or not. Cookies are a signal to a cache that content may be user-specific and that it might be safest to bypass caching. While Varnish Cache, Section’s caching solution, can be configured to ignore cookies that are known to be cache-safe, it is easier to manage if you can be sure the browser will never send the cookies to the sub-domains at all. Security All too often, sub-domains, especially those not considered part of your main website (like web mail or a blog), can be missed in regular patching cycles or security reviews. This means that security vulnerabilities, such as Cross-Site Scripting (XSS) and others, may be present. It is also common that those vulnerabilities are assumed to be less important because the sites on these sub-domains are less critical to the business. When combined with a main website using the bare domain, vulnerable sub-domains can provide a potential attack vector into the main website itself. If the cookies intended for the main website are unintentionally included in requests to sub-domains (due to the end-user’s browser choice or Set-Cookie header values) then an XSS vulnerability could allow an attacker to steal cookie values intended for the main website and then use those to masquerade as other users on the main website. Of course, vulnerabilities on sub-domains are not limited to impacting the main website on the bare domain. There are other classes of attacks that can impact the main website even if it uses “www” but by choosing “www” to be preferred you are at least limiting the types of attacks that are possible."
"152","2016-06-13","2023-03-24","https://www.section.io/blog/csrf-and-caching/","As summarised by OWASP, Cross-Site Request Forgery (CSRF) is “an attack that forces an end user to execute unwanted actions on a web application in which they’re currently authenticated”. There are several recommended ways to defend against CSRF and the strongest, and very popular, mechanism is the Synchronizer Token. This technique requires a secure, random token to be included as an extra hidden field to be sent with requests. When the server receives the request it compares the token with a value stored in the server-side session state corresponding to the user’s session cookie also sent in the request. If the comparison is a match the request is processed, otherwise the request is rejected as fraudulent. You cannot cache it The Synchronizer Token is very effective but it also happens to be the only recommended CSRF defense that is dependent on server-side state and session cookies. This dependence has a negative impact on cacheability. Take a common ecommerce site example where the home page lists the most popular products and a convenient Add to cart button beside each product. When the button is clicked, a request is made that needs to include the anti-CSRF Synchronizer Token specific to the active user in order for the request to succeed. Typically the token would be embedded in the HTML form’s hidden input fields which means that this high-traffic page would need to be rendered by the origin web server individually for each user. This means it cannot be served from a shared CDN cache. Furthermore, every user who loads the home page - including an unauthenticated, first-time visitor - needs to be assigned a server-side session. That is potentially a large quantity of traffic and workload hitting the web servers and increasing page load times for the end users. This impact isn’t only limited to a home page but also category pages, search results pages, and the individual product pages - wherever the add-to-cart functionality is protected against cross-site request forgery. How did we get here? Web security is a complex field and it is this complexity that often leads to misunderstanding and mistakes and ultimately to producing vulnerable software. One way this is tackled is by trying to remove to need to consider the complexity at all. Programming languages and frameworks have tried to pre-empt common security vulnerabilities by implementing many recommended mitigations in the core behaviour and in the default project scaffolding. This is certainly a great step toward reducing the number of security problems in software in general but it can interfere when secure-by-default is trading off other important criteria - like scalability or performance. The industry has also built automated security scanning tools that will flag almost any HTTP POST method as requiring CSRF mitigation, and every cookie as needing the “httponly” attribute, partly because a tool cannot easily determine the real purpose of each request or cookie and errs toward a conservative view. On top of existing pressures to deliver value, people in the IT business often lack the knowledge, or the confidence, or even the appetite to argue each case that the tools have highlighted in a security report, and instead comply by implementing a blanket mitigation. In the context of CSRF, it is important to revisit the intent of the requests you’re trying to protect. Continuing the original example, it is debatable whether there is any significant impact if an unwanted product could be added to a visitor’s cart, assuming that the contents of the cart are displayed to the user for review before beginning the checkout process. In many situations forging this request would have negligible consequences. However, allowing an unintended add-to-cart action could be a problem for products of a sensitive nature, or it could be a problem if adding an item to a cart is sufficient to alter a user’s list of “other products you may like” and future marketing campaigns. This is unfortunately where the complexity appears and the details of your specific site will need to be considered to make the right call. Solutions In some cases, if the platform your website is built upon only has limited controls, you may need to disable the CSRF protection on certain actions to improve the cachability of your pages and give your users a faster experience and to enable your origin to cope with greater volumes of traffic. Alternatively you may be able to use Edge Side Includes (ESI) or AJAX to update the CSRF token in the forms to match the user’s session. This can enable most of the page to be served from cache but will still require (hopefully simpler) requests back to the origin to fetch the token. This is how Turpentine solves the problem for Magento 1.x. With a programmable caching solution, like Varnish Cache and its VCL, you could substitute Synchronizer Tokens with another CSRF protection mechanism which does not depend on session state. As a start, performing Referer and Origin request header checks is easily done in Varnish Cache and it is a recommended practice anyway. Most of the Double Submit Cookie technique can be implemented in Varnish Cache with the origin performing the final check. If changing the origin is infeasible, Varnish Cache Modules (VMODs) can provide the missing pieces to implement a Double Submit Cookie or even the Encrypted Token Pattern completely in VCL. While all CSRF prevention techniques are vulnerable (or irrelevant) to cross-site scripting (XSS) and man-in-the-middle (MITM) attacks, the stateless mechanisms can also be vulnerable to sub-domain attack vectors. So if your www.example.com site also has a blog.example.com site, any XSS vulnerabilities on the blog could be used bypass CSRF protection on “www”. Most POST actions on your website, like login, checkout, and change-password can retain the stronger Synchronizer Tokens, and the alternative protection types could be limited for use with lower-risk actions like add-to-cart. It is also important to ensure you have other security mechanisms in place to mitigate more than just CSRF attacks. This includes serving an appropriate X-Frame-Options or Content-Security-Policy response header to mitigate Clickjacking and ensuring that HTTPS is used for all subsequent requests once a user has authenticated, or better yet use HTTPS-only for the entire site."
"153","2016-05-09","2023-03-24","https://www.section.io/blog/improving-your-websites-transport-security/","Ivan Ristic of Qualys blogged last year about a TLS Maturity Model, citing that “over time, … deploying TLS securely is getting more complicated, rather than less”. The article goes on to describe five levels of maturity for a TLS implementation: Chaos Configuration Application Security Commitment Robust Security At Section we have reached level 4 for our own website and our management portal (Section Console) and we are working toward achieving level 5. The work required for us to achieve this now means our platform can help you improve the TLS maturity of your websites too. By using Section for your website, you immediately achieve Level 2 because our platform is handling the TLS protocol configuration which we maintain at a Qualys “Grade A” level. This frees you to focus on the application-level changes required to achieve Level 3, eg ensuring you don’t have mixed-mode resources on your pages, that cookies use the httponly and secure attributes appropriately, and that your origin validates anti-CSRF tokens. When you’re ready to switch your website to HTTPS-only and enable Strict Transport Security, Section’s Varnish Cache proxy can help you redirect all insecure HTTP requests to their HTTPS alternative and also inject the necessary HSTS response headers so browsers will use HTTPS for your site by default. This brings you to level 4. Finally, to make level 5 just a little bit easier, the HTTPS configuration page in our Section Console displays the public key fingerprint of your HTTPS certificate that you will need when adding Public Key Pinning response headers."
"154","2016-04-24","2023-03-24","https://www.section.io/blog/modescurity-for-ddos-attacks/","DDOS attacks come in many styles and target various layers of a system. It is increasingly common to see attacks targeted at Layer 7, where a surge of HTTP requests is made to your site. There are various ways to block these attacks, from rate limiting techniques, CAPTCHA, and Javascript detection injection, through to deep HTTP inspection. Section allows you to use ModSecurity to block malicious HTTP traffic. Here we will talk about using ModSecurity to provide deep inspection to filter out bad traffic and allow good traffic through. ModSecurity Setup On Section, you choose the HTTP proxies that form the HTTP pipeline for requests coming to your application. You can choose from a library of proxies that we make available to you, which you can change at any time. Adding the proxy to your site is based on our configuration-as-code principles, so that means you’ll commit to your Section git repository to declare that our traffic should pass through ModSecurity. Our ModSecurity container comes preconfigured with the OWASP Core Ruleset (OWASP CRS), which is a set of rules designed by experts in the developer and security community. Out of the box, Section runs the ModSecurity Apache Module in Detect-Only mode. This allows you to start running the proxy and collecting data, which feeds into our HTTP analytics toolchain. In order to change the mode of the proxy you can follow the normal process for ModSecurity, by changing the SecRuleEngine option in modsecurity.conf to On. {% gist 721981ee1462fad435ee2d303390f491 %} Now, when the ModSecurity rules determine that the request should be blocked, the user agent will receive a HTTP 403 response code. Measuring and Monitoring ModSecurity It is important to understand exactly what ModSecurity is doing and to do this yourself you’d probably fall back to log file analysis. You’d need to log onto all your ModSecurity servers, possibly aggregate the logs and transform this data into meaningful information. With Section, this happens by default. Our log processing based on logstash makes this easy for a high-availability ModSecurity installation. ModSecurity Metrics Section parses the ModSecurity audit logs using Logstash, statsd and Graphite. Each minute, we provide up to date metrics on which rules were tripped. This lets you see the volume of data traffic coming to your site, and which rules are blocking the most attacks. Here’s an example of looking at which rules from the OWASP CRS were being tripped on a recent event. I’ve removed the scale and the legend here, you’ll see these in your own instance of Section. There are actually six ModSecurity OWASP CRS rules being tripped during the five waves of malicious HTTP traffic. One rule stands out - allowing ModSecurity to block traffic that is tripping this rule will force the source of the traffic to redesign their system. Usually, they move onto greener pastures. It is also interesting to note the size of the attack waves, and their timing. In the example above, the site started with ModSecurity in DetectionOnly mode and was switched to On at around 16:10. This caused the HTTP 200 responses to be changed to HTTP 403 response codes - the green line indicates blocked traffic. These are the tools you need to understand and take action on a wave of undesirable traffic. ModSecurity Log Analysis with ElasticSearch and Kibana Because Section also parses the ModSecurity audit log into ElasticSearch via Logstash, and provides you with full Kibana usability to query your logs, you’re able to see some very interesting insights into the attack. Section’s Kibana tool provides some really useful and neat query and visualization capabilities for these kinds of problems. Let’s have a look at Kibana showing a Tile Map visualisation of this traffic. Wow, this traffic originated from all over the world, with a hotspot in Asia. Let’s ask Kibana to get more information. Yes, the main focus is traffic coming from Malaysia, then Indonesia. Have a look at the IP distribution of this traffic. Geo-blocking techniques may have worked, and even IP blocking may have worked, but a better choice is using a Web Application Firewall, like ModSecurity. ModSecurity Alerts Hooked up to Pingdom and New Relic How do you know when these things are happening? Any operations team needs alerts hooked into their existing framework. Alerts on Section are based on synthetic checks. You can configure synthetic check tools like Pingdom or New Relic to periodically check any graphite metric we expose. Then, you can get an alert forwarded to your operations team that will give them early warning that there’s some unwanted traffic. Handling ModSecurity False Positives With any WAF solution, false positives can be a problem. A false positive is where ModSecurity blocks a request that it shouldn’t. This happens when there’s a clash between the application design and the firewall rules. Because Section is the only CDN built for continuous integration, verifying in your local developer environment and in your CI systems before you go to production is the normal way to ensure that your WAF doesn’t break your site. Change Management for ModSecurity As Section is the only configuration-as-code based CDN, all the changes you would make to set this up are fully managed in git. Results with ModSecurity As you can see, using Section to run ModSecurity gives you full control of ModSecurity in a CDN. Also, the tools we provide to support the development experience, and the operational tools we provide to support runtime give you everything you need to effectively run ModSecurity with so much heavy lifting taken care of. This lets you focus on the outcome, not the implementation."
"155","2016-04-15","2023-03-24","https://www.section.io/blog/vanity-tracker-domain-https-pardot/","Most astute companies are using marketing automation software to help streamline their marketing message and increase sales results. Security and performance typically aren’t your first thoughts when setting up such systems and configuring vanity tracker domains. Nowadays people expect to see the padlock in the address bar as more and more websites switch to HTTPS by default. It gives users confidence that their data won’t be seen by prying eyes and that the destination server is trusted onwards increasing conversions. In the last 12 months Electronic Frontier Foundation & LetsEncrypt have help spread the message securing your web presence isn’t difficult and given time will be the status quo every user will expect. What is the deal with X-Forwarded-Proto Oops, by adding a reverse proxy in front of Pardot it causes browser errors due to mixed content issues. Why does this happen? Good question; ask Pardot, it’s how they have configured their platform. Lets go and edit the form template and change the affected script to use https prefix, nope not going to happen it’s dynamically injected at runtime. Which resource is causing the page to break due to mixed content errors? piUtils.js in the head tag. When the Pardot servers see a request with a X-Forwarded-Proto header they act like the connection is over unsecure HTTP connection and return assets script & styles prefixed with http:// instead of https://. Browsers behave by blocking unsecure resources over secure connections to ensure top security. Removing the X-Forwarded-Proto header from the request before it hits pardot’s servers corrects the problem. Using Section Over the past few weeks we have been in the process of onboarding marketing automation software Pardot. We are sticklers for security and ensure Section web properties are HTTPS enabled and default; Pardot wasn’t a straight forward setup as they don’t support vanity tracker domains with HTTPS enabled. This post was inspired by Roman Derevianko who blogged on how to use setup and configure ngnix. We regularly dogfood our delivery platform and leverage the flexibility it provides. Create Section account . __Setup vanity domain in Pardot__ In Pardot navigate to User > Settings > Edit Account Set tracker domain to https://go.yourdomain.com (prefix https:// is important) Save changes __Create a new website in our portal__ With stack details: Hostname: go.yourcustomdomain.com Origin: go.pardot.com Stack: Varnish Cache (latest) __Once your application has been setup edit section.config.json__ Navigate to Configuration > Repository > Open section.config.json > Edit Add remove_request_headers and add X-Forwarded-Proto & save {% gist section-io-gists/3b7b3b92ca77a52c122047875b7ed4a9 %} __Setup DNS to point go.yourcustomdomain.com at Section__ Wait for deployment to complete and you should have a running application Section is a self service platform, sign up the first 14 days are free."
"156","2015-10-23","2023-03-24","https://www.section.io/blog/finding-the-ip-address-of-your-visitors/","When you create an account on Section we give you lots of powerful tools to improve your website’s performance and security. There are great charts showing how many requests are being served and what is being done to them. You can dig into the logs and see how each request flows through Section and what happens to it. However, it is important to remember that at our core, we are providing a reverse-proxy service, that is, we handle HTTP requests on behalf of your website. This also means that any request that doesn’t get cached, redirected or blocked will eventually be passed back to your website. AKA The Origin. When your first setup your site on Section we resolve the DNS of the hostname you enter and set that as the origin address. This origin address is stored in a JSON file in the root of the git repository for your application. We have documentation on how to view and change the origin address that we point at. Who is connecting to my site? Often you want to know the IP address of who is viewing your site. Possibly to know where they are in the world using Geo-IP or to do fraud detection. Once you change your website’s DNS to point at Section every request for your website will pass through our servers before going to your site. This means that from your website’s perspective all requests will be coming from us. This means that the usual way of finding the IP address of the visitors to your site won’t work. Usually you would get the requesting IP address. In PHP this would be $_SERVER['REMOTE_ADDR'] or in ruby it’s request.env['REMOTE_ADDR']. in .NET it’s HttpContext.Current.Request.ServerVariables[""REMOTE_ADDR""]. All of these values are the IP address of the computer that is connecting to your site. Once you put Section in front of your site, that IP address will be one of our servers, which is not very helpful in this case. So what to do? x-forwarded-for to the rescue! When a HTTP request comes from Section to your website it will have an additional request header: x-forwarded-for. The value of this request header will be the IP address of the computer that connected to Section. This is the address that would have been in $_SERVER['REMOTE_ADDR'] previously. This means that if you’re writing code on your site to check the IP address of your visitors, always check the x-forwarded-for header before looking at the remote address value. If you’ve got a Magento site that has a plugin doing geo-ip features, make sure it uses the x-forwarded-for value. And if your payment gateway does fraud detection based on an IP address, check that the gateway will use the x-forwarded-for header . The x-forwarded-for header is a useful tool to allow you to still have visibility of who is viewing your site while still getting all the benefits of a powerful reverse-proxy service like Section."
"157","2017-05-01","2023-03-24","https://www.section.io/blog/what-is-cache-warming/","At Section we are occasionally asked if users should implement cache warming to increase the likelihood that their visitors will be served content from the cache and as a result speed up page load times. In this blog we want to go over the concept of a warm cache vs a cold cache and the benefits and downsides of warming your cache for visitors. Warm Cache vs Cold Cache As explained in our post on how web caching works, a cache, whether deployed on one local server or through a Content Delivery Network, stores a copy of the files that make up a webpage so they can be delivered to visitors more quickly. By caching copies of image files, CSS, and HTML documents, the origin server does not have to generate these files each time a new visitor comes to the website. This both improves page load time and decreases stress on the origin server, meaning a website can serve more visitors at once. Because modern websites are constantly being updated - whether it’s a media site updating the articles on their homepage or an ecommerce site updating inventory of a certain product - files are set to expire after a set period of time, which may be a minute or an hour. Each time a file in the cache expires, it needs to be re-collected from the origin server. The first visitor to visit a website after a cache is initially set up or a cache expires will go through an empty or cold cache and experience a “cache miss.” The cache will visit the origin server to retrieve the file, deliver it to the visitor and keep the file in the cache so it is then a full or warm cache. Each subsequent user that visits before the cache expires again will be served from cache - a “cache hit” for all files that have been stored. To summarize, a cold cache is one that does not have any files stored in it, and a warm cache has files stored already and is prepared to serve visitors. Cache Warming: Benefits and Drawbacks Generally websites want visitors to encounter a warm cache so that they are served more quickly. However, unless sites engage in active cache warming, some visitors will encounter a cold cache after content expires or the cache is cleared. So what is cache warming and how does it benefit websites? Cache warming is when websites artificially fill the cache so that real visitors will always get a cache hit. Essentially, sites that engage in cache warming are preparing the cache for visitors (hence the term “warming” as in the warm engine of a car), rather than allowing the first visitor to get a cache miss. This ensures every visitor has the same experience. While it seems logical that websites would want all of their visitors to have a similarly fast page load time, the drawbacks of the practice can often outweigh its benefits. For one thing, the majority of websites will have more than one cache server or cache content through a globally distributed content delivery network, which could have hundreds of caches. To have a completely warm cache, websites need to fill all of those caches. They can do this by using a crawler, but that crawler would have to hit the website multiple times and from multiple locations to ensure every cache is filled. This becomes increasingly complicated as the number of caching servers increases, and may unnecessarily increase load on the website since the caches must be filled regularly in anticipation of visitors coming to the site, even if actual visitors are not hitting the website before the cache expires. Another issue arises when looking at websites with many pages, articles, or products. To fully warm a cache, these all would need to be cached regularly, even though some pages may be requested very infrequently. A website with 1000 products may only get visitors to 20 product pages, and by using real visitors to warm the cache rather than a crawler the visitors will dictate which items need to get cached often. While cache warmers for systems including Magento and Wordpress are quite popular, we recommend sites that use Varnish Cache or another caching solution examine both the benefits and downfalls of using a cache warmer. For high-traffic sites, cache warming is not necessary as enough visitors are coming to fill the cache regularly, and for sites with many products cache warming can significantly increase server load. In addition, websites should always aim to improve their performance using several methods rather than only through caching - with a fast connection time and front end optimizations in place, users will have a good on-site experience even if they reach a cold cache. Get Started Today"
"158","2017-06-13","2023-03-24","https://www.section.io/blog/varnish-cache-tutorial-varnish-logs/","Varnish Cache is a modern, flexible HTTP accelerator that speeds up websites by caching both static and dynamic content. Varnish Cache is an open source tool that can also be deployed on content delivery solutions such as Section, and it has grown in popularity in the past several years because it is faster and more configurable than older solutions like Nginx and Squid. Varnish uses a unique language, Varnish Cache Configuration Langauge or VCL, that gives developers a high level of control over what they cache and how they cache it. This level of control allows users to cache HTML documents (the first element of a web page that is loaded and the most important for back end load time), hole-punch pages so that only certain content is cached, and much more. Varnish Cache can also be configured to be used as a load balancer, and there are several other uses that make it an extremely versatile tool. The options in caching when it comes to Varnish Cache are almost limitless, but to use it effectively developers must have a good understanding of not only where Varnish Cache sits in relation to their servers and load balancers, but also how to use VCL and how to measure Varnish Cache’s success using metrics and logs. Learn How to Configure Varnish Cache In this Varnish Cache tutorial, Section’s CTO Daniel Bartholomew will review the fundamentals of Varnish Cache, give an introduction to VCL and demonstrate how to set up basic caching with VCL. He will also show viewers how to use Varnish Cache logs and metrics to improve configurations, and how to test and troubleshoot Varnish Cache setups. Join us on Tuesday, June 27th to learn how to get started with Varnish Cache - whether it’s on your own servers or through a content delivery solution. This interactive session will include demos of VCL and Varnish Cache testing, and cover frequently asked Varnish Cache questions such as how to use Varnish Cache with HTTPS. At the end of the session Daniel will take questions to help viewers with their own specific Varnish Cache needs."
"159","2018-03-26","2023-03-24","https://www.section.io/blog/advantages-and-disadvantages-of-cache-warming/","What is Cache Warming? Cache warming is a process that some engineers follow to try and improve the performance of their website. Many websites depend on caches. A cache is a system that stores portions of the website in high-performance storage to help avoid reading from systems that have poor performance or to reduce pressure on bottlenecks in the system. Caches exist in many places in your website setup. For example, there are caches within your CPU, built into your database, and even within specific applications like Redis or Memcached. How do I warm my Varnish Cache? In order for Varnish Cache to store an asset, it has to receive a response marked as cacheable from the origin server to an incoming request for that asset. Every time the TTL (time-to-live) for a cached asset expires, Varnish Cache removes it. In order for an updated copy of that asset to be reintroduced to the cache, Varnish Cache needs another cacheable origin response to a request for that asset. Practically, this means that every now and then one of your users will receive a slow response for an asset that Varnish Cache should be serving quickly because the asset has expired. and their request is going all the way to the origin and back. This one slow request refreshes the asset in cache and all additional users receive a quick, cached response, but our first user nevertheless has a bad experience. Warming a Varnish Cache is a technique designed to shield users from this inconvenience by making those necessary but slow cache-refreshing requests yourself. You make a series of requests to your server for cacheable assets and you get the slow responses needed to refresh the cache instead of your users. Manually cache warming your web site in a browser, however, isn’t a very repeatable process. You won’t want to sit at your computer and click through every page on your website. What tool can I use for cache warming? If you look around the Internet for cache warming tools you’ll find a bunch of tools of varying quality. When our users ask us to help them with cache warming, we generally do not recommend these tools, because there’s a good chance you already have a great tool at your disposal: WGET. Wget has a command line argument to tell it to be “recursive”. This means that you can point Wget at a single page on your website and it will download the content. Then, it will scan the page’s HTML for any hyperlinks and fetch those pages as well until it has run out of links. This approach will cache all of your websites’ assets, but it has some significant trade-offs that we’ll explore below. Here’s an example that you can reference to cache warm your website with Wget: https://gist.github.com/thomasfr/7926314 What are the advantages of cache warming? The advantages of cache warming are that you can try and get content into your cache ready for user traffic without making them experience slow, non-cached delivery times. This means that your users will put less traffic on your backend servers — delivering a better UX experience for your customers and protecting your backend infrastructure from excessive traffic. What are the disadvantages of cache warming? In a cache, web traffic is not generally evenly spread across all the objects. For example, your home page will be requested much more frequently than your privacy policy page. Generally speaking, it is unlikely that you’ll have a cache large enough to cover all the content that your website holds. If you write a cache warming system that starts at your home page and then crawls your website by following links (like the wget example above) then you’ll make one HTTP request for the home page, and one for your logo file, one for the CSS files and JavaScripts. Varnish Cache will store all the cacheable responses. Then the crawler will move onto the next link it finds on the page. It will download that, Varnish Cache will store the cacheable assets, then the next link, and so on. Because of Varnish Cache’s typical memory constraints, if you indiscriminately crawl your site as described above, it’s possible that you will run out of memory and begin to overwrite important assets stored at the beginning of the crawl, like your home page, with trivial assets found at the end of the crawl, like an image from your privacy page. This severely limits the speed benefits of Varnish Cache. Next, you need to think about how your cache is distributed. If you are trying to cache warm a CDN, which Point of Presence (PoP) is your cache warming script talking to? If you run the cache warming script from a central location, it is expected that your cache warming agent will connect to the PoP in the CDN network that is geographically optimal for the server doing the cache warming. So when you run your cache warming script you may only be warming the cache in a single location. Finally, you need to consider the configuration of the cache. Do you have rules configured in your cache for different countries? Different browser types? Your cache warming script would also need to be aware of the way you have configured vcl_hash. If you have different versions of your pages for desktop and mobile devices, be sure to configure your crawler to fill the cache for both device types. Similarly, if you have different versions of content in your cache for different countries (pricing and currency details, perhaps), or for different languages, be sure to consider that in your cache warming design. Let’s Recap Cache warming may benefit your users by improving site performance or preparing the origin application for initial user traffic. Getting it right, however, isn’t straightforward — you’ll need to think about your specific circumstances: What are the most important assets to have in cache? What is the size of my cache’s memory? Are my caches distributed across a Content Delivery Network? If so, which cache am I trying to warm? Are there configurations in my cache that I need to be aware of? One recommendation With all these things in mind, we generally recommend not worrying about cache warming efforts. When natural traffic is used to prepare the cache there may be minor performance degradations when content is refreshed. However, the caching platform is generally best optimized when primed via natural cache rules. You don’t need to consider the above items, the cache will organize itself to best suit the traffic that your users are generating. To learn more about how to get started with Varnish Cache, including writing Varnish Cache Configuration Language to cache content for your application please download the full Varnish Cache Guide. If you have specific questions about Varnish Cache and VCL check out our community forum or contact us at [info@section.io](mailto: info@section.io) and one of our Varnish Cache experts would be happy to help you."
"160","2016-08-10","2023-03-24","https://www.section.io/blog/five-top-cdn-myths/","Here are my top five CDN myths (for today) and a quick discussion of why they are not exactly true. Myth 1: CDN Means Static Object Caching When we discuss CDNs and their function, many engineers consider caching and serving static objects (image, CSS, JS files) as the only function of a CDN. This is an important function of a CDN. Static objects are the easiest to cache on a CDN and certainly help with respect to reducing bandwidth restrictions at your origin servers and reducing computational load at your origin servers. Serving static objects from some location nearer to the user (in terms of Network latency) is also helpful for increasing the page speed. However, CDNs can (and should) be so much more than just a static object serving network. CDNs are distributed PoPs containing reverse proxy servers. Those PoPs run under some DNS smarts to choose which PoP to send traffic. The type of reverse proxy server contained in the PoP will determine what you can do with the CDN. Some CDNs are only good for static object caching. However, when you run additional Reverse proxy servers in the PoP, then you can start performing some awesome functions at the CDN including: HTML Caching- Massively improving page performance and computational offload from the origin servers Blocking Spurious Requests and DDoS attacks – Using Web Application Firewall to screen and block HTTP requests where appropriate Blocking Bots and preventing Scraping – Using JS detection Proxies to prevent bots scraping your site or bringing it offline. Modifying images as they pass through to reduce weight or resize Streaming large files like video And many, many more website augmentation options to improve performance security and scalability. CDN means so much more than static object caching on a network of distributed servers. This is most easily appreciated when we accept that CDNs are comprised of reverse proxy software and that there are a good number of awesome reverse proxy servers which can be run in a CDN PoP. Aside from Section, CDNs don’t generally disclose what reverse proxies they are running so you won’t really see the “magic” in the black box. But rest assured there are reverse proxies in there driving the website augmentation. The trick is to choose the right CDN which lets you have access to the reverse proxy servers you need. Myth 2: More PoPs is Better Back in the old days when we were using dial up modems and 3 and a half inch floppy disks, more PoPs was better. In those days CDNs were the right way to navigate the problems of low bandwidth into and out of hosting centres and between network hops. These days, it makes sense to run PoPs as close as possible to the Internet backbone to take advantage of the reduction in time required to make the multiple hops between network devices. Fewer PoPs also means fewer PoPs to fill with your content. Therefore, you will experience a higher cache hit rate for your content and ultimately more offload and faster retrieval of the content; which is what you were hoping for in the first place. There is of course a balance to be struck between the number of PoPs and the global spread. Given the average number of requests to build a webpage these days is over 100, it makes sense to reduce the roundtrip time by serving content from as close as possible to the users. Note that closeness in this case should be measured in terms of Network Latency, not proximity (although proximity is a factor contributing to latency) More modern CDNs provide fewer PoPs closer to the Internet backbone. The next generation of CDN provides you the opportunity to select the number (and location) of PoPs which is right for your application and audience. Myth 3: You Should use a CDN Domain Previously in the days of HTTP/1, using multiple domains to serve static content was seen as a means of delivering a page into a users browser more expediently. “Sharding” content away from the main www.website.com domain onto say cdn1.website.com or static.website.com allowed a users browser to bring the content into the browser more efficiently by opening up multiple TCP connections and parallelising the content download across those connections. This is not the case any more. HTTP/2 means content can be downloaded in a parallelised fashion without opening up more TCP connections. So, opening up those additional TCP connections is actually forcing a cost which is not necessary. The use of multiple domains or sharding, is now a performance anti-pattern. Some CDNs still force their customers to use static object domains because it is easy for the CDN to deploy in this fashion. This is robbing the website of the opportunity to make use of our modern protocol – HTTP/2. Myth 4: You do not need to use SSL to Origin When you set up your website and install an SSL certificate without a CDN in play, the traffic for your website travels all the way to your origin infrastructure in an encrypted fashion. You may terminate at the load balancer and decrypt there or perhaps directly at the web server. However, when you bring a CDN into play, your content will be terminated and decrypted at the CDN edge so that the CDN can serve the traffic per the CDN configuration and then, where required, relay requests back to the origin severs for the origin servers to complete. The process of terminating and decrypting at the CDN is why you need to install or provision an SSL certificate into a CDN if you want the CDN to serve SSL traffic for you. But, not every CDN will re encrypt the data on the way back to your origin. This means traffic could be intercepted on the way back the origin from the CDN. You should have a certificate installed at your origin and your CDN should re-encrypt the requests before sending to your origin servers so that your CDN can maintain a secure conversation for your customers all the way from their browser back to the origin. Myth 5: CDNs can/should only be a “production” network All legacy CDNs have been built from a networking perspective. CDN companies have dropped physical infrastructure in data centres and dropped a “black box” of reverse proxy software onto that hardware. Unfortunately for these legacy CDNs, they are now hamstrung by the network structure of their CDNs as the software is difficult to update and certainly hard to share. CDNs (well actually the reverse proxy servers which run on them) are a part of a website delivery stack in the same way as a web server or a load balancer. Modern website engineers working in an agile fashion want to have web servers, load balancers and databases running on their local development environment. They also should have the reverse proxy servers running on their development environment. A software driven approach to the management of reverse proxy servers means that developers can now have (if they use Section) the CDN running in their local development environment (and test and staging or wherever they want)."
"161","2017-04-24","2023-03-24","https://www.section.io/blog/instant-purge-content-delivery/","One of the most common questions we are asked at Section is “how long does it take to (select your favourite term) “clear”, “purge”, “evict”, or “ban” an item the cache?” Our answer has been “near real time” as we built Section to instantly purge cache globally. We monitor every cache purge on our platform and I thought it might be interesting to review the results and put some hard numbers against the time to clear cache on Section. How to Clear Cache on Section As a bit of background, users of Section can clear their cache in a number of ways to achieve any cache clearing outcome they may desire, from very granular clearing to a broad dump of the Section cache globally: 1. Purge particular URLs You can remove particular URLs from the cache by entering the URL and clicking Purge URL. This form accepts wildcards in the form of * so entering /assets/images followed by a star will purge all URLs starting with /assets/images. 2. Empty the entire cache This will remove everything we have cached for your site. Use this if the cache is completely out of date and it needs to be refreshed. NOTE: This may cause increased load on your origin server until we are able to refill the cache. 3. Custom Varnish Cache ban expression Our cache is an instance of Varnish Cache so you can use ban expressions to prevent objects from being served from the cache. To learn how to use bans, see the Varnish Cache documentation. 4. Clearing Pagespeed Cache For users of Google’s Pagespeed module on Section, clearing the Pagespeed cache is simple as there is only one option: empty the entire cache. To do this, all you need to do is click the red button labeled Empty the entire cache. 5. Magento plugins Users of Section’s Magento 2 plugin will have their Magento 2 website already hooked up to Section’s platform so, for example, if you update a product on your Magento 2 install it will clear the cache on Section of that product image automatically. Magento 2 users can also clear different parts of the Section cache within the extension. Magento 1 users can launch cache evicitions from the Magento 1 application by installing our Varnish version which supports the Turpentine extension. Time to Clear Cache on Section Section’s platform is a globally distributed Docker container platform. A cache evicition instruction can be launched from our console as described above, directly against our API gateway or from a Magento instance. Regardless of the trigger point, a cache clear instruction will hit our API gateway and then be distributed out to your Docker containers running whatever version of Varnish Cache or Pagespeed (or both) that you have running on Section. How long does this take? End to end, a cache purge on Section takes 180 milliseconds. I just grabbed the following data describing the median and 95th percentile cache clear times from the Section platform globally over the last hour: I have heard other content delivery solutions talk about cache clearing in minutes or seconds. It’s not fast enough unless we are talking milliseconds. And the time to apply a ban on the Section platform? 45 milliseconds (median obsevation from following graph): How did we measure? We are a little bit fanatical about measuring everything which happens on the Section platform. The graphs you see above are a Grafana representation of Graphite data rolled up from our ELK stack. So we log the ban time for every ban (see following representation of the log in Kibana) and roll them up from there. This particular example was a Magento 2 initiated ban: Try out containerized Content Delivery To try Section’s content delivery solution and developer workflow please get in touch and we’d be happy to give you a demo and a free 14 day trial of our system and reverse proxies such as Varnish Cache, Pagespeed FEO, and Threat X. Get Started Today"
"162","2018-11-12","2023-03-24","https://www.section.io/blog/preventing-long-tail-latency/","We recently had a client approach us who was struggling with long tails in latency times with their previous CDN provider. They had a small portion of customers who were experiencing up to 30 second load times, which in their terms was “completely unacceptable”. For this particular client, reduction in overall average latency was important, but more important to them was getting the outliers under control. What Is Long Tail Latency and What Contributes To It? Tail latencies are always expressed in a percentile term; long tail latencies refer to the higher percentiles (such as 98th, 99th) of latency in comparison to the average latency time. When looking at your analytics dashboard, you may notice it says something like “per request, 1% of your users are experiencing an average delay of a second”. For service providers, it can be a challenge to keep the tails of latency distribution short, particularly for interactive services at scale. In a study led by Luiz André Barroso, distinguished Engineer at Google, it was found that the larger a system grew in scale, the wider the degree of latency variability. When the system’s size and complexity scales up, it is more difficult for service providers to provide consistency. It is not simply a question of scaling everything up as overall use increases. When a request is processed in parallel, long tail distribution of the parallel operation can immediately become an issue and dominate the overall response time. Each response must have a consistent and low latency otherwise the overall operation response time will be very slow. High performance leads to high tolerances, meaning your entire system needs to be designed to exacting standards. Other causes of long tail latency include: Shared resources – If machines are shared by different applications all vying for the same shared resources (e.g. memory or network bandwidth, processor caches, CPU cores) within the same application, different requests might end up competing for those same resources. Global resource sharing – Applications that are running on separate machines might compete for global resources (e.g. shared file systems, network switches). Background daemons – Background daemons may use a specific amount of resources on a typical basis; if scheduled, however, they can generate multi-millisecond hiccups. Even if these are few and far between, they can affect a significant number of all requests in large-scale distributed systems. The Impacts of Tail Latency Why is it that the tails are more important than the average? Gil Tene put it succinctly in a podcast on tail latency for SE-Radio: It is “because humans do not perceive the commonplace; they do not forgive you because your average is good… pain and bad experience is what you remember”. Systems that are able to respond quickly to user actions (within 100ms) feel more natural to users than those which take longer. Responsiveness is key. Good averages are not enough as web users respond to speed. In a study run by Amazon involving delaying pages in increments of 100 milliseconds in A/B testing, even tiny delays led to substantial and costly drops in revenue. Similarly, in a study Google ran, it was shown that half a second’s delay in loading time led to a 20% drop in traffic. In the Barroso study, he uses an example to illustrate how long tail latencies can occur: Imagine a scenario in which a client makes a request to a single web server. Ninety-nine times out of a hundred their request will be returned within a reasonable duration of time. However, one time out of hundred it might be slow. If you examine the distribution of latencies, most are small, however there might be one out on the tail end that is large. This doesn’t have a major impact. It only means that one customer receives a slightly slower response every so often. However, imagine you have millions of requests to multiple servers. Now instead of only one customer receiving a slow response rate, 10K are affected, significantly changing the impact of the tail latencies. Using the same components and scaling them leads to an unexpected outcome. This is a fundamental property of scaling systems: high performance equals high tolerances. At scale, you can’t afford to ignore tail latency. How Section Addresses Long Tail Latency Completely eliminating all the sources of latency variability issues is not practical at scale or in shared environments; however, tail-tolerant software techniques can be implemented to help form a predictable whole out of less predictable parts. Section practices multiple tail-tolerant techniques to combat issues of long tail latency, including: Isolation - (via containers) for different environments Memory limits on all containers - prevents putting a strain on the entire system Kubernetes - manages the deployment of multiple replicas of data items and provides a high availability architecture AnyCast DNS resolution - allows DNS queries to be routed to the closest data center for fast DNS connection times and improved website performance A Higher Level Solution A unique solution at a higher level that Section uses involves caching HTML in a way that gets the time to first byte down. Our competitors do not use the same method, tending to focus more on the static assets side of things. Caching HTML is an essential step when trying to eliminate long tail latency. We have found that getting the HTML delivered as fast as possible to the user is the best way to reduce page load times. Another method for reducing the time to first byte is our ability to spin up our PoPs anywhere via our Composable Edge Cloud. Our network currently includes over 60 PoPs across North and South America, Europe, Asia and Australasia, and we are able to create new PoPs on demand. Third, slow load times can be attributed to the resources of a third party. At Section, we actively work on either providing suggestions on how to defer these resources after the page load event, or we bring third party resources onto the client domain and cache them, which also leads to faster delivery. Finally, we consistently provide excellent visibility on long tail latency through Section’s comprehensive RUM data, which provides our customers with the insight they need to help get those long tail outliers under control."
"163","2018-10-19","2023-03-24","https://www.section.io/blog/load-balancing-web-performance/","The two most important demands on any online service provider are availability and resiliency. It takes a server a certain amount of time to respond to any given request, depending on its current capacity. If during this process, a single component fails or the server is overwhelmed by requests, both the user and the business will suffer. Load balancing aims to solve this issue by sharing workloads across multiple components rather than using a single server, thereby ensuring consistently fast website performance at any scale. Service providers usually build their networks through the use of front-end Internet-facing servers, which move information to and from back-end servers. The front-end servers typically contain load-balancing software that decides which requests to forward to which back-end server, determined by resource availability and a combination of internal rules and logic. Key Benefits of Load Balancing Avoid service outages, providing users with fast, uninterrupted service even during traffic spikes Scale your application efficiently by distributing traffic across your origin servers Gain flexibility and control with your technical infrastructure Access real-time performance monitoring through metrics, logs and alerts Local and Global Load Balancing Local Load Balancing refers to load balancing within a single data center. There are two main reasons for deploying local load balancing: (i) To achieve high availability – for which you need at least two backend servers in case of failure; (ii) To gain access to a control point over your services – allowing you to configure filtering rules, change backends during deployments and manage the overall flow of your traffic. Global Load Balancing refers to load balancing between multiple data centers. Global load balancing is necessary once a business reaches a certain scale of operations. The two main reasons for a business transitioning to a global load balancing solution are: (i) To avoid failure through the housing of all digital operations in a single building or region; (ii) For regulatory or jurisdictional reasons, for example, the need to keep European data within Europe, or to retain Asian traffic within an Asian data center. The Section Load Balancer Most existing industry load balancing solutions involve a mixture of appliance-based application delivery controllers (ADCs) and cloud-based solutions. ADCs have evolved out of the earliest load balancer designs and are still the dominant model in use today despite their challenges in elastically scaling in real-time, and high costs for maintenance and support. Cloud-based load balancers can offer both cost savings and better performance; however, their main issue is that they are built on top of DNS, which means that they can only route traffic through use of the IP address. As they can’t see anything more within the request, they are unable to offer a single unified service for microservices. Cloud-based load balancers also rely on time to live (TTL), which involves caching responses from a DNS lookup and thereby limits immediacy and control. With the Section load balancer, however, decisions are made at Layer 7 instead of the DNS layer, which enables application-specific decisions on every single request. Additionally, failover decisions are made on every request, not just if the DNS cache expires. When your primary server is not available, automated failover to a different backend server kicks in meaning your users will never receive a 502 error. With the Section layer 7 load balancer, you are also able to gain a granular level of control and immediate scalability. You can programmatically determine your own custom routing rules, including request location, device or browser type, and cookies or headers, in order to immediately route HTTP and HTTPS requests. Our application layer load balancing service means that you can serve unique content to different geographic regions and unique content to different user browsers, as required. Furthermore, the Section load balancer can be utilized to randomly distribute requests to your origin servers to ensure that no single server gets overloaded, enabling immediate scalability and avoiding performance degradation or availability issues. Section offers git-backed version control for straightforward configuration, along with in-depth metrics and logs to transparently show you how each request was handled so that you can quickly diagnose any issues. We have also created a set of common load balancing scenarios in VCL, which you can edit via your Section portal. You can easily add other Section services to guarantee a unified architecture across your complete application, deploying Varnish Cache and the ELK stack logs alongside the Load Balancer, for instance, to guarantee the easy handling of any traffic spikes and to optimize user experience."
"164","2018-09-27","2023-03-24","https://www.section.io/blog/6-key-business-uses-for-logs/","There are multiple ways for businesses to use logs - from tracking user engagement, to following compliance guidelines, to monitoring suspicious activity. Frequently, however, businesses treat logs and metrics as an afterthought rather than as an integral part of their corporate strategy and development process. Furthermore, the volume of data available from logging can be overwhelming, particularly if you don’t have a clear goal in mind and established processes set up for its collection and analysis. Similarly, important data can be missing if the right procedures haven’t been set up to collect it. If logging is given careful thought as part of a larger plan, however, it can prove a highly valuable tool. The following six use cases are largely for web traffic logs. There are, of course, a wide range of other use case scenarios. What You Can Learn From Web Traffic Logs Track user engagement with your product: By analyzing data captured in your logs, you can watch how users interact with your application at a detailed level, noticing, for example, when they abandon transactions or using the logs to track aggregate use to work out the most popular features. These insights can then be exploited to tweak different aspects of the application to improve user engagement and increase revenue. Troubleshoot problems: Logs can be helpful when troubleshooting application problems as you can use historic logs to compare real-time findings to baseline data. This can be useful in working out contributing factors and efficiently resolving them. Ongoing monitoring can also help you spot unusual patterns and identify problems before they have the chance to affect users. Offer improved user support: When users approach companies for help on an issue, they are not always articulate or eloquent about exactly what the problem is or the steps they took that led up to it. By looking at the associated logs (if they are set up to track user clicks and requests), your help desk team can more rapidly diagnose what the problem is and take steps to resolve it. Enable effective security monitoring: For effective use of logging for security purposes, it is important to regularly review your logs to both monitor real-time security concerns and track larger patterns and trends over longer durations. Make sure you have the appropriate processes in place to fully utilize logs to their maximum advantage. Meet compliance requirements: Logs can provide a useful record of the kind of data that compliance standards and legal requirements demand you maintain. They can also show who has accessed that data and when. Support data integration: By creating a central area in which all of an organization’s data is readily accessible, well-managed logs can be used strategically to support data integration. Section Logs As part of our goal to be the most DevOps-integrated Edge Compute Platform solution in the market, the Section platform supplies numerous detailed logs and metrics, giving valuable insight into how your websites and proxies are running. Every Section account includes these tools, letting developers gather baseline data before issuing any improvements, easily assess the impact of any changes made, and troubleshoot issues as they happen. At Section, we work with three different types of logs: Grafana Metric Visualization Graphite Monitoring HTTP Logs The Section Console Overview Page Each environment within Section is provided with a distinct overview page, which provides a range of information, including: General information about the environment’s proxy stack - you can view all the proxies running in your environment, from the edge proxy to the LastProxy, as well as their current order; HTTP traffic summary - this is presented as an overview graph which offers views into three different metrics: (i) the number of HTTP responses served per minute, offering insight into the overall volume of traffic hitting your site (ii) the breakdown of error requests for the last hour, helping you detect any recent increases in errors served (iii) the downstream response bandwidth served per minute, showing you the total amount of bandwidth sent to your customers Data from Varnish Cache (cache hit, pass and misses for the last hour and upstream request bandwidth served per minute) or Modsecurity (intercepts, audits and passes % for the last hour and upstream request bandwidth served per minute), depending on your choice of vendor; Errors that may have occurred between Section and the origin server in recent history; Links to access a more granular level of information, including HTTP logs and DNS records. Grafana Dashboards and Metrics Visualization The Grafana view offers a holistic way to visualize Graphite metrics in straightforward-to-understand visualizations. Section offers several different ways to view the metrics, starting with the Grafana view, which is an overall holistic level visualization tool using queries from Graphite. We also provide four separate dashboards, which can be viewed individually to gain different types of insight into your logs: Traffic Summary - This view will help you understand your website’s traffic from an overall perspective. It includes the number of HTTP requests served per minute and the corresponding bytes served per minute, in addition to a break down of all client side and server side errors. Varnish Cache Overview - This view will only be available if you have a Varnish Cache proxy set up. If you do, then these charts will be helpful in providing details about your application’s hit, miss and pass ratios. This will help you understand how much content you’re serving from the cache, allowing you to adjust it as needed. Varnish Cache Request Performance - This is a more detailed dashboard that supplies metrics on time to serve and bandwidth by hit type, content type and response time. It offers a good starting indication of the type of content to cache and provides an overview of how your website is responding to cache requests. Varnish Cache Hit Rates - This dashboard further breaks down the overall hit rates by their content type i.e. Images, HTML, CSS, JavaScript. Graphite Monitoring You can easily change the view from Grafana Dashboards to Graphite Metrics, which allows you to customize how you pull data using the same data source. You can then create your own customized dashboards for a more flexible working environment. You can view data within all the different environments, including your production environment, your development environment and any others. You can then put this data to use to create your own graphs, allowing you to easily change what is displayed and the look of each graph so that you can view statistics in the way that best suits you and your team, and fine-tune your configurations accordingly. HTTP Logs HTTP Logs give you a view of Kibana, which lets you dig deeper into your data and the trends you have identified in Graphite or Grafana. HTTP Logs lets you search through your logs for details within specific fields, for instance you can search using ‘scheme’ to determine whether a request was HTTP or HTTPS or ‘status’ to show the response code (200, 3XX, 4XX, etc.). These searches can then be turned into different graphs in the “Visualization” view, and saved under the “Dashboard” view for ease of use."
"165","2018-09-24","2023-03-24","https://www.section.io/blog/website-testing-site-speed-impact/","Website testing tools can be very useful in helping improve user experience, allowing you to compare different iterations of a web page or application against each other, exploring a user’s experience of different variations of a particular page, or more extensive full stack testing in which backend elements are also tested for how they impact customer experience. However, an unfortunate side effect of using testing tools is that they can lead to slower site speeds. Many in the industry believe that a diminishment of page speed is an inevitable side effect to using website testing tools. You can overcome this, however, while still taking advantage of the benefits of website testing through using a different type of testing tool. The Section platform enables users to plug and play with best of breed solutions so you can benefit from the most efficient and effective tools for your stack. One such tool is SiteSpect, which offers server side multivariate, A/B and end-to-end testing designed to optimize any and all functionality while not impacting on your site speed. Types of Website Testing and Optimization Tools A/B Testing – Rigorous A/B testing allows you to compare two iterations of a web page or app against one another to find out which factors increase conversions and improve revenue. A/B testing involves two or more variants of a page being shown to users at random, then statistical analysis helps determine which variation has performed better in the context of your specified conversion goals. SiteSpect offers an easy to use, interactive Visual Editor for optimization and testing across digital properties. Multivariate Testing – Multivariate testing involves the measuring of interaction between the various independent elements on a given page (e.g. headlines, product imagery, copy text, etc.) and the opportunity to test out multiple different combinations, including a control version. This type of testing can be particularly useful for optimizing and refining an existing page that has many components, such as a landing or homepage to make better use of existing design work and digital assets. End-to-End Testing – SiteSpect’s end-to-end optimization tool (known as SiteSpect Origin Experiments) is the industry’s only such optimization offering, which involves advanced full stack testing to optimize the complete customer experience. Its server side testing allows you to explore the impact of multiple features, functionalities, algorithms and infrastructure components, which impact the customer, even if they are not directly aware of their use. This solution also involves front end and client side testing. Do Website Testing Tools Slow Your Website Down? It has been proven that various types of website testing can indeed negatively impact site speed; for instance, after Dutch telco Simyo implemented an A/B testing tool on its site, the Google Page Speed score for its homepage dropped from 99/100 to 87/100, negatively impacting its competitiveness in terms of SEO and conversion rates. This 12-point dip occurred despite the company having followed best practices recommended by the vendor. In fact, most A/B testing software uses a client-side script based on JavaScript, which not only causes a delay on loading, but also slows the render execution of other parts of each page. How to Conduct Website Testing Without Affecting Site Speed SiteSpect’s solution is different. In a recent blog post, Eric J. Hansen, SiteSpect’s CTO, explains why this is the case. “Every other testing solution works by using JavaScript tags which require pages to either load slower or flicker. SiteSpect doesn’t use tags at all, so your sites often even speed up after implementing SiteSpect. This is how tags impact your site speed, and why it’s important to consider this impact in your search for a testing solution.” In partnering with SiteSpect, we were excited to find a testing service that not only won’t slow down your website, but which in fact could improve site speed and have a knock-on impact on improving SEO ranking, conversion rate and overall user experience. There is no need to rely on a third-party testing tool that will slow down your website while you are actively trying to improve it. As we wrote about recently, the negative consequences are too serious to risk. In Eggplant’s recent global study into why web speed matters, the website optimization company found that just under three quarters of consumers (73%) will transfer their business to a rival site if the website they are visiting is too slow to load. Want to learn more about how to run A/B, Multivariate and End to End Testing in Section? Contact Us"
"166","2018-09-20","2023-03-24","https://www.section.io/blog/virtual-waiting-room-for-website-traffic-spikes/","Do you manage a website that experiences traffic spikes, whether occasional or frequent? If so, do you have a safety net in place to prevent user experience degradation and potential revenue loss? How Virtual Waiting Rooms Optimize User Experience When your website experiences a peak traffic event, an online queueing system can route a designated proportion of users to a Virtual Waiting Room. Those excess users are placed in a queue to wait for access. They are kept on hold in the Virtual Waiting Room while other users continue to effectively browse, engage and transact with your website. Rather than all users experiencing site delays and/or outages as a result of your servers slowing down or crashing, the users already on your site will continue to have a high quality experience and those kept waiting are reassured that they will in turn gain access to your site. You know how many visitors your site can safely handle, so you determine the threshold over which excess users will be routed to the Virtual Waiting Room. You also have the option of customizing the overload page with branded content and the language of your choice to keep users engaged and show them their place in the virtual queue so that they know how many users are ahead of them. As soon as current users exit the website or complete their transaction, waiting room users will be sent on to the site to complete their transactions. Use Cases for Virtual Waiting Rooms Common use case scenarios include: Ticketing eCommerce Media Education (at peak times, such as enrollment) Public sector (for example, visa applications, tax filings, etc.) If you need to guarantee 100% uptime during these types of peak events, the Virtual Waiting Room solution is an essential feature for your website. Improve your Site’s Scalability If you are looking for a more permanent way of increasing your maximum number of users, you might want to work with Section to improve your site’s underlying scalability. Section’s performance engineers can help you determine the optimum ways in which you can most effectively and efficiently serve the maximum number of users at any given time with the lowest possible hosting cost. Combine the Virtual Waiting Room solution with other Section performance and scalability features, such as image and full page caching. By deploying Varnish Cache on your website, you can cache more content locally and reduce the amount of work your servers have to do. This way, the server will only receive requests when the cache needs updating. User responses are entirely handled by the cache, no matter the number, keeping the work required by your servers to a minimum and optimizing performance on your site. Easy Set Up Getting started with the Section Virtual Waiting Room is easy. No code changes or deployments to your infrastructure are required. Let us help you get started today."
"167","2018-09-19","2023-03-24","https://www.section.io/blog/varnish-cache-6.1-release/","The latest release of Varnish Cache (6.1.0) is now available on Section, bringing about several new features and stability improvements, albeit minor in the words of Poul-Henning Kamp. Section made this version of Varnish Cache available within our platform 2hrs after the source code release, further highlighting the speed and flexibility of the Section Edge Platform. Update: 6.1.1 Since this article was published, there has been a new bug fix release, Varnish 6.1.1, introduced on October 26, 2018. This release is recommended for anyone running 6.1.0. Packages are available from the official repositories. There are no new features in this release, and no reconfiguration is necessary. Notable Varnish Cache 6.1.0 Updates Since new users often forget to vcl.discard their old VCLs, they have added a warning when you have more than 100 VCLs loaded. There are parameters to set the threshold and decide what happens when it is exceeded (ignore/warn/error). req.http.Host is now mandatory and requests are handled without it on the fast DoS avoidance path. req.grace had been previously removed, but has been reintroduced, since there are use cases that cannot be solved without it. Similarly, req.ttl used to be deprecated and is now fully supported again. The evaluation of the beresp.keep timer has changed a bit. keep sets a lifetime in the cache in addition to TTL for objects that can be validated by a 304 “Not Modified” response from the backend to a conditional request (with If-None-Match or If-Modified-Since). If an expired object is also out of grace time, then vcl_hit will no longer be called, so it is impossible to deliver the “keep” object in this case. The beresp.filters variable is readable and writable in vcl_backend_response. This is a space-separated list of modules called VFPs, for “Varnish fetch processors”, that may be applied to a backend response body as it is being fetched. obj.hits has been fixed to return the correct value in vcl_hit (it had been 0 in vcl_hit). The Host header in client requests is mandatory for HTTP/1.1, as proscribed by the HTTP standard. If it is missing, then builtin.vcl causes a synthetic 400 “Bad request” response to be returned. View a complete list of changes and detailed upgrade documentation. Next Scheduled Varnish Cache Release According to Poul-Henning, the next Varnish Cache release will take place March 15th 2019, and later this year they will decide whether that version will become 6.2 or 7.0. If you need assistance upgrading to Varnish Cache 6.1 in your Section account, we’re here to help. Get in touch"
"168","2018-09-14","2023-03-24","https://www.section.io/blog/magento-website-hosting-solution/","When running your Magento website, one of the foundational decisions you will need to make is which hosting provider to choose. Your web host, or web hosting provider, will supply the technologies and services necessary for your Magento site to be viewed online. How your Magento application code, files and data are hosted is critical to your website’s performance and availability. Website speed matters more to many customers than most other website features, so choosing the right host is critical to avoid user abandonment and loss of revenue. When considering which type of hosting provider to select, it’s important to consider it as part of the overall picture, for instance, appraise such elements as your caching strategy, expected throughput, current traffic volume and anticipated growth rate. Hosting Options Managed Hosting A Managed Hosting option will handle the hardware components (from patching to server configuration) to keep your server running at peak performance and ensure it is kept secure. Ideally think about working with a Magento specialist as they will be able to fine-tune the server environment to tailor it to the running of Magento code and databases. Self-Managed Hosting If you self-manage your hosting, you will be responsible for configuring, patching and updating the server/s yourself. If you want flexibility and need to keep hosting costs to a minimum, this will likely be the best option for you. If, however, you want specialist overhead support, a Managed Hosting option should be carefully considered. Server Options Shared Server A shared server (only available with a Managed Hosting) effectively means you are renting only part of a server. This is usually the cheapest option as you will share resources with other customers, but the downsides can be severe. Your website performance can be seriously impacted by other activity happening on the shared server; for instance, malicious activity occurring to your neighbors could threaten your own Magento store. Dedicated Server Although this is a more expensive option, opting for a dedicated server will offer you greater control and tighter security. Today with cloud infrastructure options, a dedicated server is typically offered on shared infrastructure - your cloud provider will dedicate a specific amount of its server resources for your application needs, which are then scaled up or down on a demand basis. Through a cloud-dedicated server, you can still gain some of the cost-effectiveness of a shared server. Number of Servers We recommend for everyone except the smallest Magento stores that you run in a High Availability (HA) hosting environment, which includes two application servers and a load balancer in front to give you maximum availability and scalability. If one server fails, the other can pick up the slack. Recovery from server faults can also be taken care of more carefully in a HA environment. Staging It is wise to run a staging server that shares its configuration with a production server to allow you to test impending configuration changes or code and database deploys. Other Considerations You should also consider the kind of security provided by your host solution, and the level of support offered if any server problems arise. Selected Magento Hosting Options to Consider AWS Byet Cogeco Peer 1 ChinaNetCloud Hostway Nexcess Rackspace Simple Helix Tenzing Zerolag"
"169","2018-09-06","2023-03-24","https://www.section.io/blog/caching-stale-content/","What is Caching Stale-While-Revalidate or Extending Grace? Caching makes pages load more quickly by drawing on a previous copy of a response instead of re-fetching it over the network. Section offers developers total control over their Varnish Cache configuration, enabling you to determine the features you need to maximize web speed by caching static and dynamic content, including full HTML documents. Certain useful caching features are less well-known than others and in this post, we are going to concentrate on what is known in other parts of the caching world as stale-while-revalidate, a HTTP cache-control extension. Varnish by contrast, calls this same feature, ‘extending grace when the backend is down’. Two independent HTTP cache-control extensions for stale content were proposed as an industry standard back in 2010 to “allow control over the use of stale responses by caches”, yet the concept itself remains a relatively little-known cache feature. It can be used to help improve cache hit rates, particularly on items that are only requested infrequently. Additionally in instances in which there is a problem with your origin server or when it is taking a long time to fetch new content, using this feature will mean that Varnish Cache continues to serve the previous (i.e. stale) cached content as users request it, leading to lower latency and a better user experience. How do you Cache Stale Content with Varnish Cache? In earlier versions of Varnish, stale-while-revalidate was only possible using an “evil backend hack” - achieved by permanently having a backend which was down, setting it as the backend for the request, allowing grace to apply itself and finally restarting the transaction. However, since Varnish 4.0 was released in 2014 (we are now on Varnish 6.0), such hacks are no longer necessary. In Varnish 4.0 on, Varnish Cache will always prefer the fresh object, but in instances where it can’t be found, it will look for the stale one. Once detected, the stale object will then be delivered allowing Varnish to initiate the asynchronous request. Essentially, Varnish serves the request with a stale object while simultaneously refreshing it. The VCL that has been added as standard (in builtin.vcl) since Varnish 4.0 looks like this: sub vcl_hit {
    if (obj.ttl >= 0s) {
       return (deliver);
    }
    if (obj.ttl + obj.grace > 0s) {
        return (deliver);
    }
    return (fetch);
}
 Each object must be made a candidate for grace by setting beresp.grace in vcl_backend_response: sub vcl_backend_response {
  set beresp.grace = 2m;
}
 What are the benefits? The critical benefit of this design pattern is that all users are always served from cache, including the first user. The first user to attempt access to certain new pieces of content often pays a penalty in terms of page loading speed. When using this VCL, even though the first user gets an older (stale) version of the content, they will receive it quickly while the new content is fetched in the background. Indeed, the benefits are three-fold: (i) users receive faster pages, (ii) cached resources are kept up-to-date, and (iii) the update process is minimized by its staying in the background, therefore user experience is not negatively impacted. Site performance and cache hit rates can be significantly improved, particularly on sites that have a very large number of pages such as an online bookstore or on sites that receive a relatively low volume of traffic. Section users will experience the benefits at both the browser and edge. The edge benefits from the caching of stale content as users are not slowed down while a response cached at the edge is updated. To implement this solution within Section using Varnish 6.0, you just need to set a grace period on the object in vcl_backend_response."
"170","2018-07-02","2023-03-24","https://www.section.io/blog/varnish-cache-404-errors/","What is a 404 error, how does Varnish Cache handle them by default and how can it handle them? We also explore the cost of 404 Errors in this article: Cost of 404 Not Found. What is a 404 Error? The 404 error in Varnish Cache can be a frustrating one as the status-line 404 means ‘File Not Found’. Cached data takes up space on your device so the 404 response being cached on your computer, smartphone or tablet can be an irritation as it essentially represents a waste of space. A 404 error occurs when data hasn’t been updated in a timely manner. By default, Varnish Cache holds onto 404 responses along with other cached content. This is problematic since 404 responses are firstly trivial in relation to their file size and secondly, typically occur because of a mistake, which will usually be quickly resolved. While the systems in place are updating, for instance, there may be two backend servers both serving the same data. However, one server may be slower than the other. When a request comes in to the server that is slow, it will throw a 404 Not Found. Default Varnish Cache Behaviour The following status codes will be cached by default in Varnish Cache: 200: OK 203: Non-Authoritative Information 300: Multiple Choices 301: Moved Permanently 302: Moved Temporarily 304: Not modified 307: Temporary Redirect 410: Gone 404: Not Found Source: http://book.varnish-software.com/4.0/chapters/VCL_Basics.html What Should Happen when Varnish Cache sees a 404 Error? As seen above, Varnish Cache holds onto 404 responses along with other cached content by default. However, you can modify the settings to prevent 404 responses being cached at all, or change them so that they are only cached for a short window of time. If you stop Varnish Cache from caching 404 errors, this means that all 404 requests then get handled by the backend where they can still be logged and analyzed, if needed. What do you need to do to ensure Varnish Cache does not Cache a 404 Error? To prevent Varnish Cache from caching 404 responses, the following code can be applied (to version 4 or later) at, or near, the top of your vcl_backend_response: sub vcl_backend_response {
	# Don't cache 404 responses
	if ( beresp.status == 404 ) {
		set beresp.ttl = 120s;
		set beresp.uncacheable = true;
		return (deliver);
	}
}
 Thanks to Nigel Peck. This code tells Varnish Cache not to cache any backend response with a 404 status and to store a “hit-for-pass” object instead in the cache, so that requests across the next two minutes (120s) get sent straight through to the backend. Similarly to other cached objects, hit-for-pass objects have a TTL (time-to-live). After the object’s TTL has expired, the object is removed from the cache. The vcl_backend_response built-in subroutine is designed precisely to avoid caching in conditions that are likely undesired, such as this one. Peck also offers an option for caching 404 responses for a short time: sub vcl_backend_response {
	# Don't cache 404 responses
	if ( beresp.status == 404 ) {
		set beresp.ttl = 30s;
	}
}
 The 30 seconds TTL amount can obviously be altered to whatever duration you require the 404 responses to be cached for. Learn More To learn more about how to get started with Varnish Cache, including writing Varnish Cache Configuration Language to cache content for your application, please download the full Varnish Cache Guide. If you have specific questions about Varnish Cache and VCL, check out our community forum or contact us at our community Slack, and one of our Varnish Cache experts will be happy to help you."
"171","2017-04-20","2023-03-24","https://www.section.io/blog/varnish-cache-5-content-delivery-network/","Section is pleased to announce we have added several additional Varnish Cache versions to our Edge Compute Platform. When setting up with Section, users can choose the reverse proxies they run and the versions of those proxies. Section is committed to offering unmodified open-source software and not “locking in” users to a specific reverse proxy version. That’s why we now offer 7 Varnish Cache versions and additional options with VCL for static caching and Magento-specific capabilities. These include the most recent Varnish Cache version 5.1.2, released April 7, in addition to options with VCL for Magento 1.x and 2.x. Section is the only content delivery solution which offers the most up-to-date Varnish Cache release to ensure our users are getting all the latest features and bug fixes. We’ll also continue to offer previous versions and allow our users to upgrade when they want or remain on an older version. Varnish Cache is a powerful caching reverse proxy which excels at caching dynamic content or the HTML document of a web page. By caching the HTML document, websites can see a huge performance improvement over the caching of only static objects. Other CDNs offer modified earlier versions of Varnish Cache, but by forcing users to stay on older, unsupported versions of the software they are not able to take advantage of all of Varnish Cache’s capabilities and may be vulnerable to security risks. Get the latest Varnish Cache version in your CDN To get started with Varnish Cache 5.1.2 on Section’s globally distributed Edge Compute Platform, contact us for a demo or sign up for an account yourself and choose Varnish Cache v 5.1 basic. Contact Us Sign Up Today"
"172","2018-06-21","2023-03-24","https://www.section.io/blog/varnish-cache-https/","One hurdle of Varnish Cache is that it is designed to accelerate HTTP, not the secure HTTPS protocol. As more and more websites are moving all of their pages to HTTPS for better protection against attacks, this has become something many Varnish Cache users have to work around. To enforce HTTPS with Varnish Cache you will need to put an SSL/TLS terminator in front of Varnish Cache to convert HTTPS to HTTP. One way to do this is by using Nginx as the SSL/TLS terminator. Nginx is another reverse proxy that is sometimes used to cache content, but Varnish Cache is much faster. Because Nginx allows for HTTPS traffic, you can install Nginx in front of Varnish Cache to perform the HTTPS to HTTP conversion. You should also install Nginx behind Varnish Cache to fetch content from your origin over HTTPS. In the picture above, the TLS/SSL terminator (such as Nginx) is sitting both in front of Varnish Cache to intercept HTTPS traffic before it gets to Varnish Cache, and behind Varnish Cache so that requests are converted back to HTTPS before going to your origin. As shown by steps 7 and 8, if Varnish Cache already has an item or full page in its cache it will serve the content directly through the first Nginx instance and will not need to request via HTTPS back to the origin. For detailed instructions on setting up Varnish Cache with HTTPS read this handy Digital Ocean tutorial. If you are deploying Varnish Cache via a paid service or content delivery solution they may be able to handle this for you. Section provides free SSL/TLS certificates for users and handles the SSL/TLS termination so users do not need to configure it separately. Please contact us if you would like to know more about Section and how we provide full automated TLS/SSL for our users. Get Started Today"
"173","2017-08-24","2023-03-24","https://www.section.io/blog/what-is-an-ajax-call-javascript/","AJAX calls, which stand for Asynchronous JavaScript And XML, are becoming more popular as web applications look to bring in personalized items dynamically. AJAX calls are one method to load personalized content separately from the rest of the HTML document, which allows for the full HTML document to be cached, improving back end load time. AJAX calls have a wide range of applications but are particularly useful for websites who have a large amount of personal information which can prevent full HTML document caching. Ecommerce websites are among those that can use AJAX calls to load cart contents, account information, and product recommendations without embedding that information direclty into the HTML of a page. Platforms such as Magento have embraced AJAX calls, with Magento 2 utilizing AJAX for all personalized information. At Section we regularly use AJAX calls combined with Varnish Cache to allow for the caching of dynamic content without sharing personalized information between users. How AJAX Calls Work AJAX uses both a browser built-in XMLHttpRequest object to get data from the web server and JavaScript and HTML DOM to display that content to the user. Despite the name “AJAX” these calls can also transport data as plain text or JSON instead of XML. AJAX calls use a JavaScript snippet to load dynamic content. As a basic example you could configure a page counter that changed each time a page is reloaded by programming a snippet that is loaded after the main content: < script > $.getJSON(‘/pagecount’, function(data) {console.log(data); $(‘#p’).html(data.pagecount);});}); < /script > Because this is called following the rest of the page, you can cache the entire HTML document of the page without worrying that the dynamic content will be broken. AJAX calls are beneficial for several reasons. Unlike Edge Side Includes, another method of adding personalized content to a webpage, AJAX do not depend on an ESI processor to run, so you could build and test an AJAX call locally without having to run Varnish Cache or another ESI processor locally. Since JavaScript and AJAX are common in development it can also be easier to get started with AJAX than with ESI. AJAX is also a good solution for handling failures because you can program custom error messages to send if, for example, a user’s account or cart information. You can find more information on setting up your own AJAX calls here. AJAX vs Edge Side Includes Another method of adding personalized information to a page without loading it directly in the HTML document is by using Edge Side Includes. Edge Side Includes can tell Varnish Cache to cache a page but fill in the blank dynamic content by fetching that content from the origin server. This is done by adding an ESI line such as < esi:include src=’/pagecount-esi’/ > for page count. By using ESI you can cache the full HTML document with the dynamic content and do not need to use JavaScript. A key point of ESI is that Varnish Cache will start to send the HTML document immediately even while it is fetching the dynamic content from the origin - this will keep your total HTML document load time around the same but dramatically improve the time to first byte which is an important metric for both search engine ranking and perception of load time. With ESI you need an ESI processor such as Varnish Cache. Some benefits of ESI are that it allows you to program specifically within the Varnish Cache layer and does not require the use of JavaScript. This can be beneficial if your users’ devices/browsers do not accept JavaScript. ESI is also a good solution for API calls which typically do not execute JavaScript. One downside of ESI is that they it be can be difficult to test properly if you do not have a specific testing solution like Section’s Developer PoP installed. Download Section’s Guide to Varnish Cache Section uses Varnish Cache as a caching reverse proxy so that users can easily cache both static and dynamic content. To learn more about how Varnish Cache can be used to speed up your website please visit our docs."
"174","2017-07-24","2023-03-24","https://www.section.io/blog/chrome-developer-tools-tutorial-network/","The Chrome Developer Tools are a set of debugging tools built right into Google’s Chrome browser that show developers how the browser is interacting with their website or application. Anyone can use the DevTools to see how a particular website is built, where it content is being served from, how quickly the page is loading, and much more. These tools are extremely useful for those assessing the performance of their website or troubleshooting certain aspects of the page load. Video: Chrome Developer Tools Tutorial This video goes through how to use the Chrome developer tools network tab to examine request and response headers being sent by your web application to the browser. First, you’ll need to open the DevTools. To do this you can click the option menu (three vertical dots) in the upper right hand corner of Chrome, navigate to “More Tools” and “Developer Tools,” right-click any part of the page and click “Inspect,” or use a keyboard shortcut (Ctrl + Shift + I or Cmd + Opt + I for Mac) to bring the DevTools into view in the lower half of your web page. Next you’ll want to navigate to the tab you’re interested in. Tabs include Elements, Console, Sources, Network, Performance and Security among others. For the purpose of this DevTools tutorial we are looking at the Network tab, which provides detailed information on how the origin server and browser interact to load the page into the browser. When you first open the network tab you will not see anything - you’ll need to refresh the page with DevTools still open so it can record data. We also recommend checking the “disable cache” button which will disable the browser cache while DevTools is open. This is important if you are trying to examine responses from your web server or Content Delivery Network, as otherwise you will likely be served content from the browser cache. When you reload the tab you’ll see individual requests coming in to the browser in real time. Each of these is an HTTP request, and the first will be the HTML document. By clicking on each individual request you can access the headers, cookies, timing, and content of the response. Response and Request Headers The response and request headers will give you information on where the request is being served from and additional data such as if it was served from a cache or Content Delivery Network. The response headers are what each HTTP request is returning to the browser from the web server or CDN. The header for each request will be unique even if they are being served from the same server. For example, Section assigns each request a unique id under section-io-id and Varnish Cache will also give each request an identifier under x-varnish. The headers will also tell you if this request was a cache hit or cache miss. For Varnish Cache, a cache hit will be marked x-cache: Hit. Response headers can also tell you the cached age of the requested item, when the item was last modified, and the content type such ash text/html or image/png. The request headers are the headers sent by the browser when it makes a connection to your remote address. These give information on the type of browser and operating system making the request, the connection type, the IP address of the computer making the request, and more. By looking at these headers you can identify if your web application is having issues on certain operating systems, perform redirects based on user location or device type, and set caching rules. Stay tuned for more Google developer tools tutorials in the near future, and if you have any furthur questions please feel free to contact us. Contact Us"
"175","2017-07-12","2023-03-24","https://www.section.io/blog/improve-time-to-first-byte-html-streaming/","Section is pleased to introduce HTML Streaming, a new feature which allows for dramatic improvement of the Time to First Byte and Start Render time without needing to cache the full HTML document. While Section already gives users the power to cache full HTML documents and provides a Developer PoP for testing before going to production, we understand some websites don’t want to change their source code to allow for the full HTML Document to be cached. That’s where HTML Streaming comes in. Faster Website Performance with Varnish Cache and Lua HTML Streaming works by combining the power of Varnish Cache and Lua to cache the < head > of the HTML document while dynamically pulling in the rest of the content through edge side includes. This results in a fast Time to First Byte without risk of caching personalized content or needing to make any changes to your source code. Section’s experienced engineers will help implement HTML Streaming for you, and once it’s in place it automatically pulls the most up-to-date < body > content from your origin server. This feature is an excellent solution for websites with personalized elements looking for quick speed wins without making AJAX calls or other code changes. For Magento 1.x users who want the speed of Varnish Cache without using the more complex Turpentine extension, this solution is quick to implement and delivers immediate speed wins. All users of HTML Streaming also get the added caching power of Varnish Cache for static and other content, and all of Section’s DevOps-friendly tools like real time logs and metrics for debugging. To learn more about HTML Streaming and how it can work for your website please contact us. Contact Us"
"176","2017-07-05","2023-03-24","https://www.section.io/blog/front-end-optimization-feo/","When it comes to page load time, there are two distinct parts that any website manager or developer should consider: The back end load time and the front end load time. Back end load time is important because it is the time a website visitor is waiting before any information from your website is sent to their browser. If your website has a long back end load time, users will be waiting in front of a blank screen without even seeing the name of your site populate in their window, and may navigate away from the page. Back end load time consists of the time it takes the browser to look up your website’s IP address, connect to your server, do any SSL negotiations for HTTPS, and for your web server to generate and serve the HTML document to the browser. FEO for Faster Website Performance Once the HTML document is sent to the browser (also when the time to first byte is measured), the front end load time begins. This is when the browser begins to fetch and load all the items called for in the HTML document - including CSS files, fonts, images, JavaScript snippets and more. While improving the back end time is important for optimal UX, it is the front end load time which takes up the majority of the total load time - usually around 80% or more of the time spent loading a page is in the front end. This is particularly true for ecommerce and media websites who rely heavily on quality images to sell products, advertise sales, or illustrate articles. Ecommerce sites often utilize large banners in addition to individual product images on the homepage, and product pages can include upwards of 25 images. Ecommerce websites also often have many JavaScript calls in place that track the browsing habits of users, add personalization elements such as “recommended for you” sections, and more. These elements mean that this type of website is often very heavy on the front end. While a majority of modern websites include over 100 requests per page, we often see ecommerce pages with 300 or more requests. Because of this it is crucial that ecommerce websites use a variety of front end optimization techniques to improve their page load time and more importantly, how users perceive their page load time. As shown by WebPageTest’s Speed Perception challenge and our explanation of SpeedIndex, the full page load time is often not a good indicator of how visitors actually experience your website’s speed. This is because total page load time includes elements that load in the background of a website and may continue to load for several seconds after all visible elements are completed. What is really important is that a user can interact with your page - scroll through it, view images, and click on links to navigate around. To improve user experience in this way, websites must focus on smart front end optimizations that improve not only image load time but also ensure that important elements load first. Below are several tips to improve front end load time. Set Browser Caching Caching content in your visitors’ browsers will mean that images, CSS files and more will be stored on each visitor’s computer so if they visit your website or the same page more than once the page can partially load from elements they already have saved. This is especially useful for websites which have many images, such as ecommerce websites which visitors often browse several times before making a purchase. Utilizing browser caching will both improve user experience for your visitors and reduce server load as your web server will not have to re-generate cached files. To enable browser caching you will need to set the request headers of your files to use caching and specify how long each file or file type can be saved for. Minify CSS and Reduce Requests CSS files for fonts and styling help make your website unique and consistent across pages, but having large CSS files or multiple CSS files that need to be loaded can slow down your front end load time. There are a few ways you can improve how CSS loads. Minify CSS: By minifying CSS you are essentially flattening the CSS document to make it smaller, reducing unnecessary tabs, spaces, and code that make it easier for you to read and edit files but are not actually needed. There are many free tools that will minify CSS for you. Combine CSS Files: If you are using several CSS files and fetching some CSS from external sources, you can com,bine these into one file to reduce the number of requests a browser has to make. While HTTP/2 allows for multiple requests to be made at once, overall load time will still be reduced if you have fewer and more condensed requests. Inline CSS: Small CSS files can be added directly to the HTML document rather than requested as a separate file. This will again reduce the number of requests a browser makes, although if you are not caching the HTML document you will need to consider if caching the CSS file separately would result in a greater speed improvement. This may be the case with larger CSS files which is why we recommend inline CSS mainly for small CSS files. Lazy Load Images So called “lazy loading” of images will load images based on where they are located on the page: this ensures that images above the fold are loaded first and that images below the fold do not delay the perceived completeness of the page. This trick is useful for websites with a large number of images that are not immediately viewable as it will prevent those images from blocking elements of the website that are viewable and necessary for interaction. Lazy loading will also reduce the transfer size if a user navigates off the page or onto another page before scrolling to the bottom. Lazy loading is usually accomplished by utilizing JavaScript that requests and inserts images as they become viewable. You can set up Lazy Loading yourself following these instructions or automatically by using Google’s PageSpeed on Section. Optimize Images Image optimization is crucial to a good front end load time because images can easily make up 50% or more of your website. By compressing image files or changing the file type you can dramatically reduce their size, and there are several techniques which do this without any visible loss in image quality. For example, JPG files may contain metadata that is unnecessary when displaying on web, and PNG files that do not contain any transparent elements will be smaller if they are converted to JPG. WebP is a more modern image file type that supports transparency but gives images that are 25% smaller with no or minimal quality loss. There are many other ways that images can be optimized to render quickly. PageSpeed can initially load images of reduced quality so that the page fully renders before those images are replaced with higher quality versions, and PageSpeed can also reduce image sizes to the exact dimensions needed. Optimus.io is another tool for automatic lossless or lossly image compression, and there are other free tools like Optimizilla and TinyPNG that can give you a compressed version of an uploaded image. Using a tool like Gulp can also optimize and minify images for you. Manage 3rd Party Javascript Many websites are overloaded with 3rd Party JavaScript requests that do everything from track users’ behavior as they move through a website to giving them personalized product recommendations. These JavaScript calls can dramatically slow down front end load time and result in a poor user experience if invisible elements are preventing the page from fully loading. Regularly check what JavaScript elements are included in your site to ensure they are all still in use - many web managers add analytics using JavaScript and neglect to remove the snippet after they have stopped checking the analytics data. Performing a waterfall test is also a useful way to see what JavaScript elements are blocking the load of the rest of your page. There are several steps you can take to prevent JavaScript from slowing down front end load time. You can minify JavaScript in the same way that you can reduce CSS file size, and should also place most JavaScript calls near the bottom of your HTML document so they do not block the loading of other website elements. You can also defer JavaScript so it does not execute until after the page has loaded, although this can be risky if not properly implemented. Cache Pages at the Server Caching images and CSS files using a content delivery solution like Section will dramatically improve your front end load time. Content Delivery Networks utilize caching software like Varnish Cache, Nginx, and Squid (here’s a full list of what software each CDN utilizes for caching) to store files in a cache that sits in front of your origin server. CDNs have global networks of caching servers which deliver content to users from the server closest to them, further reducing the round-trip time and resulting in faster page load times. Websites can also install a caching solution themselves - although it will not have the benefits of the global network a CDN brings, installing your own caching server will speed up load times as not all elements will have to be fetched from your origin server. Front End Optimizations with Section At Section we offer several versions of Varnish Cache for caching, which can improve both front end and back end load times. We also offer Google’s PageSpeed module which automatically performs many of the front end optimizations mentioned in this post, including image optimization, CSS and JavaScript minification and more. To get started with Section get started for free. Get Started Today"
"177","2017-03-03","2023-03-24","https://www.section.io/blog/how-to-improve-back-end-load-time/","There are several factors to look at when considering the total page load time for your website. Metrics such as Time to First Byte, Start Render Time, Visually Complete, and Full Page Load Time are important for different reasons and each have an impact on your user experience. If a user is waiting a long time before anything appears in their browser, that will negatively impact their browsing experience. However, they will also have a poor experience if a site begins to draw in the browser quickly but then slows considerably before all of the above-the-fold images are loaded. The work your website does to produce a fully loaded, interactive page can very generally be split into two areas: Back end load time, and front end load time. Both are equally important and either can have a huge impact on your total load time, but the processes to improve backend vs frondend speeds are very different. Back end load time Back end load time includes all of the processes that take part in the background as soon as a URL is searched for in a browser. This includes looking up the DNS host of the website, which tells the browser that www.mysite.com can be found at a certain IP address, opening up a connection to the website server at that IP address, successfully connecting to that web server, and exchanging any necessary security keys so that content sent between the browser and server can be encrypted. This occurs before any information is sent back to the browser. All of the instructions on where to fetch images, files, and CSS is located in the HTML document, which is the first thing the server sends back to the browser. Once the HTML document is received, the loading switches over to front end load time. Front end load time Front end load time includes everything that is loaded in the browser once the browser and server connect. This includes text, images, JavaScript snippets, fonts, layouts, and more. Because modern websites are quite content and image-heavy, there can be hundreds of roundtrips between the browser and server before a page is fully loaded. This is why many websites focus their speed optimizations on front end measurements by optimizing images, enabling lazy-loading, minifying CSS and performing other tricks that reduce the front end load time. Google’s PageSpeed module, offered by Section, is an example of a Front End Optimization reverse proxy. Why you should improve back end load time Although many websites examine their metrics and aim to improve total page load time, a surprising number ignore the importance of backend load time. Because front end load time can take several seconds if not optimized well, websites aim to improve those parts of their website and don’t worry too much about the additional few hundred milliseconds it can take to load backend elements. But a slow back end time can be crippling, as the user will be waiting in front of a blank screen before back end tasks are completed. In addition to bad user experience, studies have shown that the Time to First Byte, which is recorded when the HTML document is received by the browser, is one of the main speed metrics used for search engine rankings. A good rule of thumb is that back end load time should take no more than 20% of your total load time. Studies show that the top 50K websites do even better than this, averaging about 13% to 87% in back end vs front end load time. A good back end load time to aim for is 200ms or less. So if you want to improve back end load time, how do you go about doing it? While you can look into upgrading your origin server hardware, the best way to dramatically improve your back end load time is to cache your HTML document. By caching this vital piece of information, your cache server or CDN will immediately send it to the browser without having to fetch it from the origin. This not only speeds up the TTFB, it also significantly reduces load on your origin server therefore reducing hosting costs. While this solution is effective, many websites will not cache their HTML documents because they believe it to be too risky. The HTML document is so vital to the building of a webpage that, without being able to test if caching works as expected, they cannot risk caching it. At Section we have built a solution that enables all users to easily cache HTML documents. Section’s platform includes a local testing environment and all the logs and metrics needed to ensure HTML caching is working as expected before and after website changes go live. To learn more about how to improve your back end load time with Section, contact us today."
"178","2017-06-29","2023-03-24","https://www.section.io/blog/cache-static-varnish-cache/","Varnish Cache is a popular tool due to how quickly it delivers content from the cache and how flexible it can be. Using Varnish Cache’s domain-specific language, Varnish Cache Configuration Language (VCL), users can cache both static and so-called “dynamic” content, also known as the HTML document. While truly dynamic content which is unique to each user should not be cached (for example a CAPTCHA image that should be different each time or personal account information), using VCL and configuring your website correctly will allow for the pages around personalized content to be cached. If you are interested in learning more about advanced VCL configurations sign up for our tutorial on July 12th. But before you learn advanced Varnish Cache configuration you’ll need to know the basics, namely, how to cache static objects such as images and css files. Varnish Cache Default VCL and Varnish Cache BuiltIn VCL One of the main misconceptions about Varnish Cache is that when a user sets up the open-source version on one of their web servers it will immediately start caching content. Unfortunately this is not the case. When a user gets the open-source Varnish Cache they will get two files - default.vcl and builtin.vcl. The default VCL file is blank and only has explanations of each subsection in it. This is the file you will edit to configure the VCL for your specific application. If the default VCL is not edited, Varnish Cache will go to the builtin VCL. The builtin VCL does have some instructions to cache objects, however because it does things like ignore any URL which has cookies in it and does not override headers set from the server, it often will not cache anything for modern websites. Because of this, when getting started with Varnish Cache users must edit the default VCL to work for their application. If you have been caching static content with a basic system like Amazon’s CloudFront you could actually see a performance decrease if you switch to Varnish Cache without configuring anything. Although Varnish Cache is a faster and more sophisticated tool which will ultimately provide much better performance results, it requires some configuring to cache content for most visitors. Because all applications are configured differently at the server, every VCL file will be slightly different. However, below we give one example of a common issue which websites run into when trying to cache static objects. Setting Cache-Control: Max-Age One common setting that can prevent Varnish Cache from caching items by default is when the website server settings add headers such as “Cache-Control: Max-age=0” which tell Varnish Cache that the maximum amount of time it can cache that object is 0 seconds, meaning it will not try and cache it at all. Luckily it is quite simple to configure Varnish Cache to cache static objects using basic commands in VCL. VCL is useful because users can easily override server-side settings without touching the origin server, which is often risky or could require several levels of approval. Because traffic is flowing through Varnish Cache, it can be instructed to ignore or change certain headers that have been set at the website server. Below we get into how to use Varnish Cache to cache static objects by stripping cookies from requests and changing the max-age. Web servers will often insert headings on objects that say they are uncachable, for example by setting the Max-age at 0 seconds. The below video and instructions will tell you how to change this action in VCL so Varnish Cache will cache static objects. In this demonstration we are using Section’s Developer PoP to build and test Varnish Cache configurations, which allows us to immediately see the impact of VCL changes on page load time in a testing environment. Example VCL To override the max-age in VCL you will need to navigate to the backend response section of your VCL, called “sub vcl_backend_response” which takes action after Varnish Cache has seen the headers sent by your website server. If you want to cache all jpg files on your website, you can insert the below code into the backend response section to cache JPG for 3600 seconds or 1 hour: if (bereq.url ~ “jpg”) {
    set beresp.ttl = 3600s; }
 This statement is telling Varnish Cache “If you see a backend request URL containing “jpg,” then set the backend response time to live to 3600 seconds,” which will then cache all jpg files. You can tell the VCL to cache anything with a filename ending in JPG, PNG, CSS, etc, however there are some security vulnerabilities to setting your VCL in this way, so we recommend using a specific folder to store all your static assets in. You can then tell Varnish Cache to cache anything stored in this folder using code such as the below, which assumes statics are saved in a folder called “assets”: sub_recv section:
    if (req.url ~ ""/assets"") {
    return (hash);
}

sub_backend_response section:
# Set the cache control headers for statics here.
# The cache-control header is for the browser cache.
# The TTL is the cache control header for Varnish Cache.
# The default is 3 days.

    if (bereq.url ~ ""/assets"") {
        unset beresp.http.set-cookie;
        set beresp.http.cache-control = ""public, max-age=259200"";
        set beresp.ttl = 3d;
        return (deliver);
    }
 As shown in the above video, simply by adding instructions to cache static objects you can dramatically and quickly improve load time. Images that previously took 1 second or longer to load can load in 20ms, giving your user an extremely fast page load time even before you move to more advanced Varnish Cache configurations. To learn more about Varnish Cache, view our Varnish Cache Fundamentals Tutorial or sign up for our next tutorial on July 12 - Advanced Varnish Cache Configurations: Hole Punching for Personalization. As always feel free to reach out to us with any VCL specific questions via our community forum or visit the Section documentation for more help with Varnish Cache."
"179","2017-05-31","2023-03-24","https://www.section.io/blog/varnish-cache-hit-for-pass/","When using Varnish Cache one of the most important things you need to understand is how and why various requests get labelled as they do. A “cache hit” and “cache miss” are easily understood - a cache hit is a request which is successfully served from the cache, and a cache miss is a request that goes through the cache but finds an empty cache and therefore has to be fetched from the origin. A cache pass is a request which is labelled as uncacheable, meaning it will bypass the cache and go straight to the origin. This might be an item that is unique to each visitor or one that hasn’t been cached When examining your Varnish Cache metrics and logs you should aim for a high cache hit rate, which means you are successfully serving content from the cache to the majority of your visitors. This both speeds up response times for visitors and reduces load on your origin server, as it only has to generate a file when the cache file expires. If you are caching the majority of your static content and your HTML documents, you should be able to achieve a cache hit rate of 95% or higher. Varnish Cache Hit-for-Pass So, if we know what a hit, miss, and pass for, what is a Varnish Cache hit-for-pass? This is a response you may occasionally see and it can be somewhat confusing. A Varnish Cache hit-for-pass is related to the way Varnish Cache reacts when several visitors come to your website at the same time and encounter a cold cache, for example one that has just expired and thus needs to re-fetch and re-cache files from the origin. As Varnish Software explains “When Varnish Cache is expecting a cacheable object and an uncacheable arrives it creates a hit-for-pass object.” Because Varnish Cache hasn’t been told to pass this object in vcl_recv and thinks that the files fetched will be cachable, it will only send one request back to the origin even if you have five people who have encountered a cold cache. Those requests are then put in a line so that once the first request comes back and fills the cache, the other requests can then be filled directly from the cache. This prevents your origin server from becoming overloaded with cachable requests. The hit-for-pass comes in when Varnish Cache realizes that one of the objects it has requested is uncachable and will result in a pass. Rather than making all the requests wait in line only to discover they need to go to the backend for this request, Varnish Cache will mark this request as a “hit for pass,” essentially caching the fact that this is an uncacheable object. The next time someone requests this object, they will be sent straight to the origin and if multiple people request it at once they will all be sent to the origin rather than putting them in line. The downside of a hit-for-pass is that it means Varnish Cache will not try to cache the object while the hit-for-pass note is on it. Because of this you should ensure a hit-for-pass does not have a long Time To Live (TTL), which could overwhelm your servers. The Varnish Software blog has a good explanation of how to avoid this and what VCL is used. You should also be aware that hit-for-pass is a read-only, internal note to Varnish Cache and it can cause problems if used in cases where you expect a cache pass. Varnish Cache and Content Delivery Section gives users a choice of several Varnish Cache versions on our Edge Compute Platform along with detailed Varnish metrics and logs to help you troubleshoot your Varnish Cache configuration. The Section team includes experts in Varnish Cache and VCL and we are always happy to help. Check out our community forum for common Varnish Cache questions or contact us if you’d like to get started with Section’s Edge Compute Platform and Varnish Cache. Get Started Today"
"180","2017-05-24","2023-03-24","https://www.section.io/blog/load-balancing-content-delivery/","We’re pleased to release a new feature on the Section platform: Load Balancing for your origin servers. Even with a content delivery solution such as Section and caching in place, websites still need to make sure their origin servers are available for crucial tasks such as generating user-specific content and going through a checkout process. Websites with global audiences want their users to be served content from the closest origin server, but to be routed to another server if that one is having issues. They may also want to serve unique content to visitors from different geographical locations (such as pages in another language or region-specific sale information). Load balancing allows websites to automatically route traffic to a different origin server if one server is down, and to serve unique content to visitors from different geo locations, browsers, or device types. In addition, Section can send requests to different servers based on various algorithms to ensure one server is not getting all of the traffic from a traffic spike such as a sale or electronic direct mail campaign. We do this by providing all of our users with Varnish Cache Configuration Language for load balancing and the ability to set unlimited load balancing rules within the Section platform. Since Section gives users code-level control over unmodified Varnish Cache, Section users are able to configure many different load balancing scenarios. To get started, we have provided the VCL and tutorials for common load balancing configurations. In the future, we will add more configuration options to this feature. We encourage users to look at these sources for additional use cases. Find out more about Section load balancing here and contact us if you’re interested in learning more. Get Started Today"
"181","2017-05-15","2023-03-24","https://www.section.io/blog/synth-website-performance-monitoring/","Section is committed to giving our users a full suite of DevOps-friendly monitoring and measurement tools that enables them to easily tune their content delivery without relying on external tools. Now we’re excited to announce we’ve launched Synth, an ongoing monitoring tool available to anyone. The best part? It’s free for everyone, and will stay that way forever. Free Website Performance Monitoring Synth gives websites access to ongoing synthetic test data. Similarly to tools like WebPageTest or Pingdom, Synth runs synthetic tests on your website or the websites of your competitors. Unlike these other tools, Synth’s tests are automatically run at regular intervals every day, providing granular and historical data on how your site performs under high traffic or after website changes have been made. This data is also extremely useful for setting a performance baseline before making updates aimed at speeding up your site. Synth by Section gives all users a set of metrics that are easy to understand. These are broken down into several dashboards: Page Summary (individual page metrics), Site Summary (average metrics for all pages), and Page Timing metrics (a linear representation of metrics over time). Individual data points include: First visual change Last visual change SpeedIndex Fully loaded time Front End and Back End times Number of requests and type of requests (JS, images, CSS) Response codes Anyone can sign up for Synth’s free website performance monitoring here. We will send you an invite to Synth shortly after. We hope everyone finds this data useful in measuring and improving their website performance!"
"182","2017-03-24","2023-03-24","https://www.section.io/blog/magento-imagine-website-performance-contest/","Section is excited to be attending and speaking at Magento Imagine in Las Vegas from April 2-5th. We’ll be at booth #29 throughout the conference, and our CEO Stewart McGrath is speaking alongside client Kate Morris on how her company Adore Beauty has turned conversion optimization and website performance into revenue gains. We will also be running a contest to see which Magento websites come out on top - and on bottom - in terms of website speed. Have you been optimizing your Magento website or one of your clients’ websites for optimal performance? If so, enter our contest and if your site is the fastest you’ll win a Sphero Robot from Section to race around town. If you think your website isn’t performing so well, you could win too! We’re rewarding the slowest website we test as well, and in addition to the Sphero Section will give you a report on how to improve your website performance so you can move up in the ranks. We’ll be using the Speed Index measure to test websites. This looks at not only back and front end load time, but also how a user sees content loading on your page and thus how they truly experience your page performance. For example: If your site has a fast Time to First Byte but then only loads the header image and it takes another 2 seconds for anything else to load, the end-user experiences what feels like a slow website. Website performance is an extremely important measure for any website, and Magento ecommerce websites in particular should always be looking to improve performance as this can lead to increased conversions and revenue. We’re committed to helping Magento websites achieve the best performance possible, so we hope you’ll stop by our booth at Magento Imagine to chat about how your ideal website performance can be achieved."
"183","2017-03-13","2023-03-24","https://www.section.io/blog/hosted-google-pagespeed-front-end-optimization/","We’re excited to announce the addition of Google’s PageSpeed module to Section’s Edge Compute Platform. PageSpeed provides a large number of front end optimizations that speed up websites by modifying files and applying filters on-the-fly as traffic passes through Section. By adjusting the content that your customers receive, Section delivers a better user experience as customers will see pages earlier, browsers will draw the pages faster and the user will be able to interact with your site more freely. PageSpeed Front End Optimizations We talk about backend load time at Section a lot, because it is often slow compared to frontend load time. While we recommend only 20% of total load time is spent in the backend, many leading sites spend 40% or over connecting, fetching the HTML document, and serving it to browsers. This can be dramatically reduced by caching HTML documents, which Section enables users to do. However, front end load time is equally important. Front end load time includes everything after the HTML document has been served to the browser: images loading, links becoming enabled, and other content appearing. If an image on the homepage takes several seconds to load, users may take that as a sign that a website doesn’t care about user experience or will continue to be slow throughout their browsing experience. The optimizations PageSpeed offers range from image optimization and compression to CSS minification, lazy loading, and JavaScript optimization. PageSpeed breaks its filters into several general categories, including caching optimizations, minimizing round trip times, minimizing request overhead, minimizing payload size, and optimizing payload size. Image Optimization with PageSpeed Here’s an example of what PageSpeed can do without any work needed from you: If you choose to optimize images PageSpeed can extend the cache of images, resize them to the size given in HTML, and recompress to a smaller file type to reduce your total load size. This will improve page load time and provide ecommerce and other sites with additional pageviews, lower bounce rates, and more revenue as a result. Section users can now run PageSpeed by adding it as a reverse proxy to their Section configuration and deciding which specific filters they want to run on their website. To turn filters on, users simply need to access the PageSpeed file in their config and edit one line of code that includes all the filters you want turned on. PageSpeed will immediately begin to optimize images and enact any filters you have turned on. To get PageSpeed on Section, contact us or sign up for an account and follow the instructions to add a proxy."
"184","2016-05-06","2023-03-24","https://www.section.io/blog/integrating-magento-with-section.io-for-performance-metrics/","Magento loves Varnish Cache, and Magento 2 especially loves Varnish Cache. Also, Magento developers love agile, and continuous integration. Section’s new model for Content Delivery Networks is the only CDN that supports continuous integration, one of the backbone technical processes that support high speed Magento development. Section’s CDN facilitates this because we allow every developer to launch an instance of the CDN point of presence, or POP, directly on their own computer. This means you can work with Magento like you normally do, and when the time is right, test your Magento setup with the CDN in place for you commit your code to git. Not only does Section give you the fastest feedback loop during the development cycle, but our belief that all things should be easy, open and controllable goes beyond our reverse proxies that run in the CDN point of presence. It extends into our real time CDN logs, real time CDN log analytics, and our real-time CDN metrics. We have been working with our friends at Customer Paradigm to develop Magento CDN integration for CDN metrics. Today, I want to share with you how the open-source, DevOps friendly approach to Graphite metrics enables operational visibility for Magento teams. Why? Content Delivery Networks, especially those like Section that goes beyond static file caching into whole-site acceleration, are an important piece of a mission critical Magento implementation. That said, CDN’s are very techie tools for developers and operations teams. Knowing that they are doing their job is critical, and development and operations teams need to work together to make that happen. But there are other people that are interested in knowing that the CDN is working well, and they probably don’t want to be logging into a CDN console to check up on what’s going on. So, we worked with Customer Paradigm to build a Magento Extension that brings the essential data, metrics and information into the Magento admin reports, so that everyone can understand that the CDN is working well. Now, if you are in a development or operations role, you don’t need to spend any time explaining to a stakeholder that the CDN investment is living up to its duties. Those stakeholders can directly look in the Magento admin reports getting up-to-the-minute information on the CDN performance. How? Section provides unprecedented access to real-time CDN behaviour because we extensively log every HTTP request as it passes through our network. We leverage a bunch of tools to get that done, and the data arrives in a few places. We provide all CDN logs into ElasticSearch, and ever site we run gets their own Dockerized Kibana container. This isn’t a topic for today, but if you’d like to know more you might be interested in learning about that here. We also push all HTTP logs into statsd, and then into graphite. We love Etsy’s engineering principles, “if it moves, measure it”. As we do with Kibana, we also provide Dockerized graphite instances to all sites. We don’t just run Graphite and hide all the details from you, we provide direct HTTP access to the Graphite HTTP endpoints. This means you can consume graphite data in your own dashboards, continuous integration systems, and continuous delivery/deployment pipelines. In order to show real time CDN metrics in the Magento admin, Customer Paradigm helped us to build a Magento Extension for Magento 2 that shows these Graphite charts directly in Magento. Really, How? Once the Magento Extension for CDN metrics is installed, Magento queries the Section API to determine what website you want to show CDN metrics for. Once Magento has that information, we are able to construct a clean URL that lets Magento query directly into Graphite. Because we don’t want to make any Magento admin log into the Section Console to see the reports, Magento then proxies the images generated by Graphite and shows them to the user. I don’t use Magento, but this is cool. Of course, because Section is easy, open and controlled, you can use these techniques with any operations system you have in place. For example, if you have a Grafana instance that you use for operations dashboards, you can add your Graphite endpoint into your Grafana data sources. Then you can develop all the beautiful charts that you need for proper, real-time visibility into your CDN, without needing to reprocess laggy log files or dive into two operations consoles. I want this, show me more. We will be releasing the extension into the Magento Marketplace shortly. In the meantime, you can get going on Section."
"185","2020-03-04","2023-03-24","https://www.section.io/blog/from-human-to-machine-speeds/","State of the Edge 2020 highlights the transition we need to make from human to machine speeds in order to enable the Third Act of the Internet. More and more machines are coming online to support the exponential growth of real-time applications and smart devices, from drones to smart home technologies. These applications and devices typically need to communicate with one another and in order to work at their optimal performance level, they require machine speeds. The original Internet was designed to move at human speed: humans loading web pages, humans reading email, humans watching movies. “This is why today’s Internet—while fast enough for most humans—appears glacial when machines talk to machines.” State of the Edge 2020 The explosion of the Internet of Things (IoT) According to IDC’s most recent forecast, there will be 46.1 billion connected IoT devices, or “things”, by 2025, generating 79.4 zettabytes (ZB) of data. As the volume of connected IoT devices grows, the amount of data they generate will also grow. Some of the data will be “small and bursty”, indicating just one metric of a machine’s health. This is in contrast to the very large amounts of data being generated by other “things”, for instance, surveillance cameras using computer vision to analyze crowd behavior. Machine to machine communication (M2M) What is machine to machine communication? According to Wikipedia, machine to machine communication is “direct communication between devices using any communications channel, including wired and wireless.” M2M technology is crucial for developing connections across different aspects of the physical world. It can be used to actuate industrial processes, reduce costs by minimizing maintenance and downtime, and proactively monitor organizational assets to automate business processes and improve customer service. M2M use cases M2M was first adopted in manufacturing and industrial settings in order to help remotely control data from equipment. M2M has since found use in a range of sectors, including healthcare, insurance and business. It is also at the foundation of the Internet of Things and used in a range of real world settings, including: Industrial instrumentation: remote sensors sharing real time information with application software that uses it to take action - e.g. to share a drop in temperature to adjust an industrial process; to place an order to replenish out-of-stock inventory; to communicate fuel levels in oil drilling machines; System monitoring: the use of wireless technology to gain precise data about a system and act as a quality control - e.g. by monitoring a utility meter, the owner can find out if certain components have been tampered with while utility companies can use the information to bill customers the exact amount of resource they have consumed; Advertising: the use of wireless networks to update digital billboards, enabling advertisers to display a different message according to the time of day or day of the week, or to respond to a global event, such as a drop in the price of gas; Traffic control: sensors being used to monitor variables such as speed and volume of traffic, which is used by devices that control variable traffic information signs and traffic lights. Latency sensitive vs. latency critical applications There is an important difference between latency sensitive and latency critical applications. A latency sensitive application is one in which latency clearly improves performance, but which can still function at higher latencies than optimal, for instance, image processing or bulk data transfer. By contrast, a latency critical application, such as an autonomous vehicle or the controlling of a critical M2M process, is one that will function destructively or fail to function entirely if latency is higher than a predefined threshold. To illustrate the importance of machine speeds to latency critical applications, State of the Edge cites the example of a robotic drone flying at 60 miles-per-hour, allowing it to traverse the length of a football field in four seconds. Avoiding collisions in a situation like this may require decision-support from an edge server because a delay of 100ms could lead the drone to crash into something only 10 feet away. The critical imperative for machine speeds and demand for ultra low latency in leisure activities as well, such as VR or multiplayer online gaming, is driving the demand for edge computing and 5G. The necessity for an edge-enabled Internet More and more devices creating more and more data, which demand processing with ultra low latency requires a different kind of Internet to the one we have today. Centralized cloud data centers are simply unable to deliver the low latency necessary to keep up with demand from latency sensitive and latency critical applications. Data can’t move faster than the speed of light; requests to servers in centralized data centers hundreds or thousands of miles away inevitably take tens to hundreds of milliseconds to fulfil. If you’re simply scrolling through Twitter feeds or sending email, the difference can be unnoticeable. However, for a gamer in virtual reality or a surgeon operating remotely, those milliseconds matter. Using an edge data center for processing of information to reduce latency and jitter on the round-trip can make all the difference. Furthermore, working with an edge compute platform can help businesses significantly reduce latency, deliver consistent performance, and leverage developer workflows to make edge programming a reality today. At the recent Edge Computing World conference, there was much discussion from analysts, investors and technologists around the need for an edge-enabled Internet. “In the next few years I think we’ll have a massive switch to the edge. AI is potentially going to dominate all other forms of programming, and AI needs the edge. It’s difficult to build an intelligent system without an edge presence.” Steve Jurvetson, Future Ventures Founder and chip engineer who was an early investor in Tesla and SpaceX"
"186","2019-06-25","2023-03-24","https://www.section.io/blog/5g-edge-computing-for-gaming/","An optimized network is essential to making your gaming experience as smooth as possible. If the signal connecting you to the Internet is acting up, there is only so far you can go with hardware. Wireless networks are on their way to faster speeds and near instant connections with 5G networks. The combination of 5G and edge computing promises to radically improve gaming experiences by enabling lower latency, more advanced features, and greater immersion. 2019: “The Year That Mobile Will Change Forever” Verizon has announced its plan to deploy 5G in 30 U.S. cities by the end of this year, meaning that the new technology will likely appear in iPhones in 2020. “It’s just gonna be a total different experience in speed and throughput than you have ever seen before,” CEO Hans Vestberg said during an investor meeting, quoted by The Verge. The announcement was lacking detail around which cities would be involved and how extensively 5G would be implemented across those cities, but Verizon did say that each launch would involve deployment of incredibly fast millimeter wave radios. Verizon also said that this deployment would be real, standards-based 5G as opposed to the off-brand 5G it uses for its wireless home service. T-Mobile has also promised it will launch 5G in 30 cities across 2019; its 5G network is already on trial in a dozen cities. AT&T is also working on its own 5G deployments. Expectations for achievements by the end of 2019 for all of these carriers are relatively low due to lags in equipment. There are also only a handful of 5G-ready devices available on the market today. Nonetheless, there is a reason that people are calling 2019, “the year that mobile will change forever”. The first 5G launch and the first 5G phones going on sale denotes the next wave of latency. Much faster access on your phone will lead to much easier connections for all kinds of IoT devices and all kinds of mobile-based online gaming. The mobile gaming market is huge and growing daily - in both the developed and developing world. Once seen as the poor relation in the video game world, mobile games today make up just over half of global revenues in the gaming industry. By the end of 2018, an estimated $137 billion will have been spent on mobile games (a 13% rise from the year previous). This is expected to increase to close to $100 billion by 2021. Edge computing can open up a world in which gamers don’t need to worry about regularly updating their expensive, physical consoles. The combination of edge computing and 5G can enable the possibility for gamers to hold a high-end console in the palm of their hands. The increased deployment of 5G networks will only bolster these figures as it will enable lower latency and improved gameplay on mobile devices. Qualcomm claims that its x50 modem, the first ever 5G-ready mobile modem to be launched will deliver up to 5Gbps download speeds. Its latest successor, the x55 modem, promises an even faster bandwidth downstream of up to 7Gbps. While these speeds are achieved under test conditions, not in the more challenging real world, Qualcomm has released impressive figures for consumer activity. In a simulation in San Francisco, median browsing speeds of 1.4Gbps and file download speeds of 442Mbps were reported. 5G and Edge Computing: Hand-in-Hand 5G will not be able to meet its performance goals of extremely low latency and massive broadband without edge computing due to the time it takes data to travel across the fiber networks that connect the radios on towers to the network core. Once the application or content has been moved closer to the radio at the edge of the network, network latency decreases. With high-performance computer hardware and radios that prioritize traffic, 5G latency goals will be more easily met. Edge computing exists in many environments already, and as we’ve looked at, is expected to grow its reach exponentially over the next several years. However, as Data Economy recently wrote, “In short, 5G needs edge computing”. One of the major opportunities for edge computing and 5G to meet can be found in putting edge compute solutions at the edge of mobile networks, for instance, by deploying edge compute for applications and content at the base of cell towers, as close to the radio as possible. This allows the mobile operator to host or offer a range of applications and services that can leverage low latency, in addition to minimizing the amount of traffic that is sent back to the core network. Edge computing can also reduce network transport costs for the mobile operator due to money being saved via processing data at the edge instead of transporting it to centralized data centers. For consumers, the benefit is seen in an increase in the response of the mobile network, due to the lowering of latency. 5G needs edge computing to succeed while edge computing while not having to wait for 5G, can certainly benefit from it. The Future of Gaming Working together, edge computing and 5G looks set to reduce the workload and battery drain on mobile devices while enabling a high-end user experience. The low-latency network access and geographic proximity enabled by access to a 5G connection will mean an improved mobile gaming experience without the lag. AT&T describes edge computing as “an essential element” to its 5G strategy, and believes that “beyond mobile gaming, edge computing will help transform the gaming console industry at large. Players will worry less about buying the latest, expensive console or gaming PC – and will focus more on playing their favorite games when and where they want.” In a demo at the annual E3 gaming convention in Los Angeles, AT&T demonstrated its ability to offer high speeds and “near-instant feedback and constant interactivity”. AR, VR & Cloud-Driven Gaming: Where 5G and Edge Computing Meet AT&T has been especially focused on testing mixed reality and cloud-driven gaming in its desire to see where 5G and edge computing meet. At its research center, the AT&T Foundry, in Palo Alto, California, AT&T has launched an edge computing test zone. “We launched it because we wanted to create a real-world environment and a collaborative ecosystem that can tackle some of the business and technical challenges that this technology represents,” said Igal Elbaz, Senior Vice President of Wireless Technology, during another panel discussion at Spark. “We’re rolling out 5G as the first wireless system that was born in the cloud, and now we can take advantage of what I consider to be one of the most disruptive technologies to come along in years: edge computing. “Now you can think about placing low-latency, complex application and computation power closer to the users. By improving the functionality and the user experience, we can actually unleash some of the new business models that we all talk and hear about like AR, VR, self-driving cars, drones, mobile gaming, and others.” In its first phase of testing, the telco giant focused on how improved network performance metrics like jitter and delay could track across to improvements in application performance metrics, such as frame loss and motion-to-photon latency. One of the most interesting findings was that one of edge computing’s biggest benefits stemmed from delay predictability as opposed to the amount of delay itself. “We are excited about how AR and VR gaming can run on edge platforms,” Elbaz said. “As you probably know, latency is a real challenge for immersive mobile cloud gaming. We also learned and discovered the importance of the balance between low latency and how you build your AR development pipeline as it relates to encoding and decoding of the content that runs through the network.” Its next phase will see the rolling out of its edge test zone footprint to cover the entire San Francisco Bay area. Such experiments in edge computing by the big telecom players deftly illustrate the potential the framework has to work hand in hand with 5G to enable a new era of compute for gaming and many other industries alike."
"187","2019-12-30","2023-03-24","https://www.section.io/blog/edge-computing-2019-year-in-review/","2019 saw edge computing begin to emerge from its hype cycle as tech giants and leading enterprises, alike, began forming and publicly announcing their edge strategies. As we look ahead to 2020 and the coming decade, the future of edge computing looks bright. Here, we reflect some of the key developments in 2019 contributing to increased momentum at the edge. Leading research firms continue to increase edge computing’s rank among top trending transformational technologies Edge computing maintains a top-of-mind presence in industry reports and technology trend lists from research firms including Gartner, Forrester, and IDC, to name a few. While many cite IoT as the driving force behind edge computing adoption, analysts are beginning to emphasize the broader and nearer-term use cases for distributing compute power closer to end users. “Edge computing will become a dominant factor across virtually all industries and use cases as the edge is empowered with increasingly more sophisticated and specialized compute resources and more data storage,” said Brian Burke, Research Vice President at Gartner. Edge computing market projections reach $4 trillion Projections for the edge computing market have experienced significant gains over the past year. A report from Chetan Sharma Consulting estimates the emerging edge economy to reach $4 trillion by 2030. While the numbers still vary quite dramatically across reports, one constant holds true - many are placing large bets on the potential value of the edge computing market. Fastly and Cloudflare go public with edge stories In 2019, Fastly and Cloudflare both IPO-ed. As two technology providers who have emerged out of the content delivery network (CDN) space, both are racing to grab the edge computing market. Alongside CDN industry giant Akamai, these three providers serve much of the Internet’s video and large object traffic. While they jockey for positioning, emerging providers offering pure edge technology are beginning to gain momentum and attention as disruptive forces in the CDN world. (For further reading on the evolution of the CDN market, check out CDNs Were a Prototype for Edge Compute.) Industry alliances are formed to help advance edge computing adoption In order to realize its full potential, edge computing will require interoperability and collaboration among a complex network of technologies and players. As such, several organizations have formed to quicken the pace of innovation across a diverse set of disciplines, including (but not limited to): LF Edge: An umbrella organization under The Linux Foundation established to create an open framework for edge computing independent of hardware, silicon, cloud, or operating system. Kinetic Edge Alliance (KEA): Founded by Vapor IO and made up of members that span software, hardware, networking and integration partners, this industry group seeks to ease edge computing adoption by connecting stakeholders with end users and customers. Open Edge Computing Initiative (OEC): A collective effort by member organizations including Intel, Microsoft, Crown Castle, Carnegie Mellon University, and others to drive the convergence of edge computing platforms and services on a global scale. AWS stakes a firm claim in edge computing infrastructure At its annual re:Invent conference in December, AWS announced bold product shifts towards distributed cloud computing through Outposts, Local Zone, and Wavelength. For many, these edge-centric infrastructure product launches solidify the importance that edge computing will play in the future of the Internet. Kubernetes emerges as key technology powering the future of edge computing Kubernetes has emerged as the clear winner in the container orchestration race, with leading enterprises adopting the foundational technology to manage heterogeneous resources across compute, storage and networking. Furthermore, its flexibility and scalability make Kubernetes a natural fit for the edge, as it allows for developers or DevOps engineers to deploy applications and services in a standardized way at the edge. Edge compute platforms built on Kubernetes are quickly becoming the preferred technology for edge workload management. Edge computing going mainstream? Walmart develops edge strategy to remain competitive In its ongoing struggle to compete with Amazon, Walmart is turning to edge computing as a key differentiator. In their recently revealed, forward-looking strategy, Walmart called out their intent to add edge computing to its supercenters, renting out data processing capacity for more localized compute requirements. Strategic partnerships form between 5G operators and edge computing technology providers Perhaps the biggest buzz when it comes to edge computing is in the 5G realm. As telecommunications operators fulfill the promise of 5G infrastructure, we’re seeing partnerships form to fill the compute void across this expanding network of towers. Verizon and AWS made waves when they announced their 5G edge computing partnership in December. Similarly, Microsoft and AT&T have partnered up, and Orange and Dell Technologies are combining efforts. Edge Computing World debuts to gather key players in edge ecosystem In its first year, Edge Computing World managed to convene a who’s who of industry players in the diverse edge computing ecosystem, perhaps solidifying its place as the preeminent edge event. Over 350 Edge executives, users, and developers from around the world gathered to examine, debate, and showcase use cases, technical solutions, and innovations across the industry. Another key highlight of the event was the inaugural Edge Woman of the Year Award, which was awarded to Farah Papaioannou, Co-Founder and President of Edgeworx."
"188","2018-09-10","2023-03-24","https://www.section.io/blog/cdn-prototype-edge-compute/","CDNs - An Early Evolution of Edge Compute The financial and business logic of moving compute to the edge is a powerful incentive. Gartner predicts that by 2022, 50% of computing will be happening at the edge. Currently, around 10% of enterprise-data is created and processed outside a traditional data center or centralized cloud. Santhosh Rao, principal research analyst at Gartner, noted, “Organizations that have embarked on a digital business journey have realized that a more decentralized approach is required to address digital business infrastructure requirements.” CDNs have been a cache-close-by concept from their inception; rather than delivering millions of copies of content to users from a central location, they have always cached popular content in areas where it will likely be consumed. It could be said that when Akamai was founded in 1998 by then MIT graduate student Daniel M. Lewin and MIT applied math professor Tom Leighton, the duo invented not only the first content delivery network, but also a prototype for edge compute. By developing the algorithms necessary for intelligently routing and replicating content over an extended network of distributed servers, they placed content and caching servers closer to end users than ever before. This lessened network congestion and increased the speed of static content delivery. Legacy CDNS: Talking the Talk, But Not Walking the Walk In our recent article What and Where is the Edge we discussed the fact that there is no “one edge” to the Internet, While CDNs deliver a more distributed infrastructure, they do not hold ownership of the Edge. CDNs provide an Edge, but they do not provide the one and only Edge. Further, as we discussed in our article “CDNs Will not Remain Relevant in a Software World” the monolithic and inflexible nature of all CDNs means they cannot provide the software centric tooling engineers need to deploy and manage edge centric software. Increasingly, legacy CDNs are rebranding themselves as edge platforms. Fastly is now calling itself a “powerful edge cloud platform”; whether or not they have actually changed what they are doing at a fundamental level is more questionable. Even Akamai has rebranded itself to describe itself as an “intelligent edge platform”; however, they haven’t fundamentally changed their infrastructure either. Both are essentially still legacy CDNs operating in the same way as before. As Tom Nolle, President of CIMI Corp., pointed out in a recent blog post on CDNs and edge compute, “We have plenty of announcements about how Vendor X or Vendor Y are moving closer to the edge, but not very many are specific about what they plan on doing there or how they plan on justifying their deployment.” The Emergence of An Edge Platform We don’t think the old CDNs are true edge platforms. Rebranding alone cannot turn a CDN into an edge platform. Instead, it is more accurate to say they were an early evolution of Edge Compute . To support the emerging Edge Compute Industry, an edge platform should be able to: Service and perform compute activity at a range of network locations (or edges); from behind the firewall, through the cloud, within the telcos and even into target networks; Provide flexibility in choice of software and framework to run at the edge whether proprietary or custom developed; and Provide engineers with a development lifecycle and operational framework which fits with their Agile practices, CI/CD workflows and DevOps processes. Using an agile platform like Section offers you the flexibility to customize your network and choose the number and precise locations of PoPs for your application and your needs. Our global PoPs are built in partnership with a range of the world’s largest hosting providers and our users have the advantage of being able to select their own edge network topology. Section also offers the capability to deploy our Edge Compute Platform Platform on-premises or with smaller hosting providers. We offer you genuine flexibility in terms of leveraging both our global network reach and its attendant edge proximity to your end users with a secondary edge nearer the origin, providing maximum offload and scalability. The modular nature of the Section edge platform allows engineers to drop in their choice of compute activity, including reverse proxy and serverless functions. As importantly, Section’s git backed application development workflow for the edge offers developers and engineers the opportunity to test out code and make changes within their local dev test and staging environments before deploying to production. Plus, real time, searchable diagnostics (logs and metrics) means Ops Engineering teams have full visibility of edge activity regardless of location. It is these kinds of innovations that are truly making edge native and edge enhanced applications feasible."
"189","2019-10-28","2023-03-24","https://www.section.io/blog/headless-commerce-drives-edge-computing-adoption/","In e-commerce today, the challenge to meet and exceed customer expectations is driving innovation. The demand for frictionless shopping, 24/7 availability, superior product and impeccable service quality is ever increasing, putting pressure on retailers to deliver goods as seamlessly, quickly and engagingly as possible. Brands need to frequently update content to create attractive, compelling digital experiences while still delivering a seamless purchasing process. e-Commerce merchants that want to grab a competitive edge by delivering better user experiences are increasingly transitioning to headless commerce, an approach to building a digital store that enables both content and commerce to thrive. Headless commerce is a solution that decouples a website’s presentation layer (the front-end), from its business logic/function layer (the back-end). According to Adam Sturrock, Vice President of Customer Success at Moltin, “Headless commerce essentially brings two concepts together – a turnkey SaaS back-end with the core commerce functionality and utility APIs as the delivery mechanism to connect the head”. The Benefits of Headless Commerce There are numerous benefits to headless commerce, including: A flexible front-end and the ability to create more unique content, helping to attract more organic traffic, an imperative in today’s crowded field; Greater opportunity to customize your website or app through more advanced personalization, AI or AR capabilities; Sales and marketing teams can easily make changes and updates to the presentation layer without necessarily needing support from IT; As a result of the microservices architecture, you have the option to swap providers as needed, as opposed to having to migrate to an entirely new e-commerce platform; Ability to leverage best-of-breed providers for various back-end functions (e.g. the search or product content does not need to live in the same platform at the cart/checkout solution); Technical teams typically see headless as a way to be more agile and faster in relation to handling their workload; The same back-end can power numerous front-end experiences, helping enable different options for different regions or countries, or enable multi-site needs; Easily adaptable around consumer preference with the chance to offer seamless digital experiences across multiple channels and devices (from desktop and mobile browsers to social media to emerging channels such as voice assistants or AR); You can use a Content Management System (CMS) you may have already invested in and simply add commerce capabilities; A lightweight API controls data transmission between different systems - product, content, customer data, financials, and other systems exist separately, free from front-end related code or processes, enabling faster delivery of content; Improved security because of the decoupling of content and commerce. Finely tuned access control lists (ACLs) can be used to limit access to specific users and systems; Greater efficiency with often lower costs as each microservice scales to meet demand. Challenges with Headless Commerce Despite all the advantages, there are some challenges with headless commerce, one of which stems from limitations with SEO around single page apps. Google’s primary indexing only looks at the HTML of a page and not content rendered by JavaScript. To solve this, many are starting to leverage server-side rendering of single page apps at the edge. Another challenge is that routing to various API back-ends in a headless commerce setup can get complex, leading many to explore edge routing solutions. Edge Compute Platforms, such as Section, help simplify API routing by providing flexibility in how you choose to route traffic through a distributed application architecture. One final challenge with headless commerce is the fact that authentication/authorization and content can prevent effective caching. At the same time, moving authentication to the edge can deliver performance benefits and also simplify the overall architecture of an application. There are various edge authorization techniques that can be used ranging from direct authentication at the edge from a stored database, to tokens, to SSO and OAuth. (If you’re looking for some guidance on authentication at the edge, contact a Section Engineer.) Approaches to Headless Commerce The standard approach to headless commerce involves the use of a commerce platform, such as BigCommerce or Magento, without a customer-facing front-end. APIs act as the core to interface with separate business systems, and the “head” is run by an independent system, such as a Content Management System (CMS) or a native mobile application. The external CMS can deliver responsive content created independently of the platform architecture. As there isn’t code connected to the back-end database that stores content, front-end developers can easily update endpoint layout using their desired framework. A headless CMS has various distinguishable parts. The back-end database includes content, such as written copy and graphics. Headless commerce also uses a separate inventory management system that works in concert with the back-end database that stores the content. Other e-commerce back-end systems typically include customer relationship management systems (CRMs), multi-channel security systems, and payment processing platforms. The API is responsible for pulling information into the right system after specific actions are completed as opposed to simply providing content to different channels. Other related approaches to the e-commerce technology stack include: A hybrid approach in which the retailer’s commerce platform is used for delivery of web-based experiences and the APIs help enable new channels and experiences, such as IoT or AR. A decoupled CMS that has a separate back-end but typically integrates with a complete front-end that pushes content into channels, making content more agnostic. Edge Computing Enabling Greater Innovation Headless commerce and other innovative, flexible computing architectures are enabling e-commerce application engineers to leverage edge computing to meet their performance, security and scalability requirements. At Section, we are seeing many of our e-commerce clients increasingly utilize headless commerce in concert with Section’s Edge Compute Platform to deliver personalized shopping experiences and digital services that speed up delivery and maximize performance using techniques such as caching and image optimization. We are working, for instance, with Australia’s largest online beauty store, Adore Beauty, to deliver its Nuxt app closer to their end users, resulting in reduced latency and greater cost efficiencies. Specifically, the Adore Beauty team is utilizing the Section Node JS edge module - the front-end framework (i.e. Nuxt app) calls a Laravel API that manages information related to the product, category and price. Other requests are routed to its highly customized Magento back-end. Section’s modular approach allows e-commerce brands like Adore Beauty to leverage the combined advantages of edge computing and headless commerce through flexibility of integration and the opportunity to select the Section edge modules relevant to their needs, from caching to A/B Testing to image optimization. New software options and changing technology stacks allow businesses to select best-of-breed solutions, linking otherwise unrelated software tools and technologies. Looking Ahead There have been rapid advances in the retail space over the last decade as e-commerce has increasingly dominated the space over brick-and-mortar retail. Further innovation through continued advancements in edge computing looks set to continue apace. Small and big businesses alike need to take advantage of cutting-edge technology in order to compete in today’s highly competitive global retail industry. The gap between early adopters of headless commerce and edge computing and those still stuck in legacy systems looks only set to widen."
"190","2019-06-18","2023-03-24","https://www.section.io/blog/section-joins-kinetic-edge-alliance/","Section is excited to announce that it will be bringing its developer-led edge computing expertise to the Kinetic Edge Alliance (KEA). Vapor IO’s Kinetic Edge is the world’s fastest-growing colocation and interconnection platform at the edge of the wireless cellular network. “To meet the needs of the future, our industry must collaborate. We must bring together all of the products, platforms, and technologies, such as those offered by Section, in order to help make operators and developers successful with edge deployments.” - Cole Crawford, CEO, Vapor IO “Vapor IO’s Kinetic Edge will deliver the next wave of network and data center infrastructure, vital to support 5G, autonomous vehicles, and many other exciting technologies and applications,” said Vapor IO CEO Cole Crawford. “To meet the needs of the future, our industry must collaborate. We must bring together all of the products, platforms, and technologies, such as those offered by Section, in order to help make operators and developers successful with edge deployments.” The KEA brings together nearly two dozen industry leaders, including both deployment and technical partners, with the goal of fostering the wide-scale deployment of edge computing. Deployment partners, of which Section is one, are committed to bringing their edge technologies to the top 30 major metropolitan US markets, starting with the first six this year. Vapor IO’s other KEA partners, which include Alef, Pluribus Networks and Clear Sky, are supporting this goal through making their technology and expertise available to deployments. While infrastructure is a key component to edge computing, developers also need easy ways to run application logic on the underlying network of distributed hardware. Section’s developer tooling gives engineers the flexibility and control to run workloads in edge locations to meet performance, security and scalability objectives for their application. Section is working with the KEA to test real customer use cases by moving application logic to KEA-powered edge locations. “Edge computing requires a complex combination of software, hardware, networking and infrastructure. Section provides a platform that empowers developers to build applications that take advantage of distributed edge architectures,” said Section Co-Founder & CEO, Stewart McGrath. “By collaborating with fellow members of the KEA who bring expertise across each of their respective areas, we’re able to push towards more standardization that ultimately reduces barriers to expand edge computing adoption.” We are thrilled to be joining KEA, helping to lead the way in edge computing adoption with a particular focus on helping to remove friction for developers to implement an effective edge strategy. From the outset, our philosophy has been centered on open source principles. We believe that edge computing will only move forward through collaboration, transparency and giving control over to developers to run any workload, anywhere."
"191","2020-02-13","2023-03-24","https://www.section.io/blog/edge-data-center-expansion/","“The great land grab” for the edge was one of the main narratives emerging out of Edge Global Congress late last year. Several companies used it as a platform to launch their edge data center plans to the industry, and as we’ll see, several others have made similar announcements since, from startups to the Big Three cloud providers. As edge data centers increasingly become a real-world proposition, today’s communication networks and the nature of the Internet itself looks set to radically change. As an edge-forward company from the get-go, we see a lot to be excited about in the coming year. Sami Badri, an analyst at research firm Credit Suisse pointed out how noteworthy it was that data centers for edge computing were beginning to spring up before there is widespread demand for edge computing itself. Comparing the trend to “eggs showing up before chickens”, he highlighted how unusual this is in the tech world. Badri, however, predicts the trend will reverse itself in 2021 when, he says, demand for edge computing will begin to catch up to the increasing number of edge data centers. What is the compute edge and what are edge data centers? A compute edge can be constructed from existing telco infrastructure, cloud infrastructure and on-premise data centers. The term edge data center refers more specifically to the highly distributed micro data centers appearing to fill the compute void between on-prem and cloud. As demand grows for near-instantaneous computing, covering everything from virtual assistant responses to online video game streaming, the drawbacks of the centralized nature of current communications networks and Internet computing are becoming increasingly clear. Proximity to the end user allows edge data centers to support applications that need rapid response times, are latency-sensitive and/or require a large amount of bandwidth without the operational overhead of on-prem systems. Other potential benefits include lower data backhaul costs (due to the reduced amount of data being transmitted across networks), the potential for more location-specific or workload-specific security around how and where data is stored, and on a wider level, the way in which edge data centers lay the physical foundation for the creation of innovative, new services. The combination of edge data centers with cloud, on-prem, telco and device compute can be described as the edge compute continuum. Each of these locations has advantages for different application use cases, and the challenge for application architects is to find a deployment mechanism to efficiently run the right workload in the right location at the right time for their specific application stack. The growing rollout of edge data centers The inherent opportunity in laying the geographic footprint for the edge is garnering interest from a wide range of players, from data center companies like Equinix, to Internet providers like Verizon, to startups like Vapor IO and EdgeMicro. There is even speculation about Walmart joining the fray. According to a report in the Wall Street Journal, in a bid to compete with its biggest rival in retail, Amazon, Walmart is discussing the possibility of adding edge compute to its “secret weapon”, its supercenters. With 90% of Americans living within a 10-mile radius of a store, Walmart is exploring venturing beyond retail to use its stores as a base for technology infrastructure. In a significant potential shift discussed in a recent strategy meeting, Walmart is apparently considering doing this in two ways: to transform superstores into “edge computing centers” and to lease its roof space to telcos for installation of 5G antennas. Vapor IO to launch 36 edge data center sites across the US Austin-based startup Vapor IO recently announced a new raise, which brings its Series C funding to $90M and provides the means to “fund an accelerated coast-to-coast deployment of Vapor IO’s Kinetic Edge”. The company already has sites deployed in Atlanta, Chicago, Dallas and Pittsburgh and with another 16 cities underway, says this makes them “the nation’s largest provider of interconnected edge facilities”. Vapor IO plans to have a total of 36 multi-site markets coming online before the end of 2021. These include both City Scale Data Centers and Remotely Operable Data Centers. “The Third Act of the Internet requires that we build out edge computing infrastructure as quickly as possible. By deploying Vapor IO’s Kinetic Edge exchange, colocation, and networking services in the top U.S. markets, we provide a platform to deliver low-latency edge capabilities at the intersection of the wireline and wireless networks.” - Cole Crawford, Founder & CEO, Vapor IO EdgeMicro joins the startup charge Numerous startups are already in the process of deploying live edge data centers for commercial use. One such, EdgeMicro, took the occasion of Edge Congress to announce its first three commercial installations, located in Austin, TX, Raleigh, NC and Tampa, FL. The Denver-based startup said site selection and permitting of twenty further locations was also in progress. At twenty feet, EdgeMicro’s micro sites are tiny by comparison to the standard data center warehouses of Equinix, Flexential, etc. “This is about much more than three sites going live. We are starting with these initial sites and plan to quickly scale to hundreds of locations. People have wondered when the edge would take off. It’s here now.” - Jason Bourg, VP of Sales, EdgeMicro Undoubtedly, a large and growing number of vendors are getting involved in rolling out edge data centers, such as EdgePresence, DartPoints, EdgeConneX, Compass Data Centers, Axellio, MetroEdge, and Zellabox. However, few are giving out detailed information about what they are actually doing. EdgeMicro is unusual in disclosing the details of its first edge colocation centers, including their exact locations and capabilities. A new global phase for Packet? Packet is another key player in the edge data center ecosystem. In partnership with Vapor IO, the developer-focused bare metal cloud provider launched 15 new edge locations last year. The new sites are part of a plan by Packet to deploy edge locations almost anywhere you could imagine. “This is the digitization of real estate. Technology is going to get infused in all aspects of real estate, whether a tower or a mall or an office.” - Zachary Smith, CEO, Packet That plan is about to get much bigger. Earlier this month, Equinix, the world’s largest data center provider announced its plans to acquire Packet in order to accelerate the development and delivery of its interconnected edge services. Equinix’s intention is “to create a world class, enterprise-grade bare metal offering across Platform Equinix that allows customers to rapidly deploy digital infrastructure, within minutes, at global scale”. The bold move is an entry into a new market for Equinix, and shows how much momentum there is behind the edge computing space. On the transaction, Packet says it is continuing its plan to expand its number of new public cloud locations and expand current sites under new ownership by Equinix. Packet will continue to operate as before with the “same team, same platform” and “same vision”. The relationship with Equinix will allow it to scale much more substantially by connecting Packet’s customers with Equinix’s 200+ data centers in 55 markets and an ecosystem of 1,800 networks. Section and the Kinetic Edge Alliance Here at Section, we are working with both Vapor IO and Packet, along with other key industry leaders as part of the Kinetic Edge Alliance (KEA) to help foster the wide-spread deployment of edge computing, build a network of interoperable edge facilities, and test out customer use cases at the edge through the movement of application logic to KEA-powered edge locations. Packet and Vapor IO’s initial two edge locations in Chicago were in fact the first two KEA locations. Both sites follow a proprietary architectural design known as the Kinetic Edge, which combines several small facilities into a single virtual data center with multiple availability zones. Other KEA partners include Alef, Clear Sky and Pluribus Networks who are supporting the Alliance’s agenda by making their technology and expertise available to deployments. Section’s role within the KEA is focused on bridging the gaps between engineers and infrastructure by providing a developer-led platform to empower flexible and intelligent deployment at the Edge. “To meet the needs of the future, our industry must collaborate. We must bring together all of the products, platforms, and technologies, such as those offered by Section, in order to help make operators and developers successful with edge deployments.” - Cole Crawford, Founder & CEO, Vapor IO Run your edge workload anywhere Edge computing requires a complex combination of software, hardware, networking and infrastructure. As increasing numbers of edge data centers come into play and grab industry headlines, it’s just as crucial to pay attention to how developers can run application logic at those sites. As a vendor-neutral solution, Section’s developer-centric platform gives engineers the flexibility and control to run workloads in any edge location to meet their unique performance, security and scalability objectives."
"192","2018-10-08","2023-03-24","https://www.section.io/blog/edge-computing-beyond-iot/","The rise in the Internet of Things (IoT) is generally considered to be the leading driver for edge compute. By 2020, Gartner predicts there will be 20.4 billion ‘connected things’ in use around the world, up from 8.4 billion in 2017. There are many reasons why IoT is a natural fit for edge compute, ranging from the need for many IoT devices to be located in rural and remote areas that reside far away from traditional cloud data centers, to the challenges of meeting security and compliance requirements (especially in the GDPR era) when data is not processed on a local basis. Edge computing performs numerous business processes, including streamlining the flow of traffic from IoT devices, in addition to analyzing and stacking real time data generated by the IoT devices on a local basis rather than transmitting data across long distances to cloud services or data centers for processing. Edge Computing Use Cases Beyond IoT Although IoT has been the primary driver of edge computing, there are many other use cases, which are helping fuel its accelerating adoption rate, both current and potential. Industries outside of IoT from manufacturing to healthcare to finance likewise need to be thinking of and planning for edge compute now. “In most scenarios, the presumption that everything will be in the cloud with a strong and stable fat pipe between the cloud and the edge device – that’s just not realistic,” Helder Antunes, senior director of corporate strategic innovation at Cisco, told Network World. When you think about the fact that a “thing” is not just a sensor, but potentially also a vehicle, a drone or a smart phone; not to mention the rise in dynamic content leveraged by apps and websites to deliver superior end-user experiences, it becomes clear that a data avalanche is rapidly headed our way. Research group IDC recently predicted that the world will be generating 163 Zettabytes of data a year by 2025, a tenfold increase from the present day. This alone will lead to many different types of distributed, decentralized architectures, bringing compute, storage and networking closer to the end user from the mobile edge to the device, or IoT edge to the wireless edge. Moreover, increasing numbers of web users generally and a growing implementation of digital services are further fueling the growth of the market. Some of the real use cases discussed in the recently released State of the Edge report (sponsored by Arm, Ericsson UDN, Packet, Rafay Systems and Vapor IO) in addition to IoT include large-scale Industrial Internet of Things (IIoT) systems, AR/VR, autonomous vehicles, video games, machine learning, smart cities and edge content delivery. How Section Is Paving the Way for More Flexible Edge Computing As we’ve written about here recently, one of the areas of edge compute of particular interest to us at Section is the increasing need that application engineers have for complex application logic execution in real-time or near real-time at the edge. An edge platform, such as Section, is the last place in which application logic is executed before the application moves past the control of the application provider. Traditionally, this would be performed by a CDN, or an Application Delivery Controller, such as a load balancer; however, the IoT wave is challenging this definition of the edge, and the telcos and IoT devices are becoming elements of the edge in themselves. As logic executed at the edge becomes increasingly complex, there is a need for application logic functions that respond to real-time data ingestion, such as full page caching based on cookie set or user interaction type, image optimization based on type of browser, and request tracking or blocking based on real-time behavioral analysis. The legacy CDNs with their “network-first” approach are not providing the necessary transparency and control over the edge that modern application developers and operations engineers increasingly require. Another important key driver behind the demand for edge compute is that modern SaaS and the cloud are in urgent need of an improved infrastructure to better handle the increasingly distributed nature of modern computing frameworks. The rise in popularity of the cloud with enterprises and end users has been in part because the experience feels like that of a local compute resource while also being instantly available and provided on a “pay as you go” basis. However, this local compute framework is rapidly being broken down as apps are increasingly distributed over multiple regions and cloud providers. Furthermore, SaaS applications are increasingly becoming massively distributed, running microservices decoupled from the public cloud infrastructure that supports them. As distributed infrastructure increasingly becomes a part of the mainstream, developers are having to re-consider their approach to managing global systems, not only in relation to security strategy and high-availability across their global PoP, but also in terms of how to process the increasingly massive amounts of distributed data streams to create value. Configuration management has become a necessity of the infrastructure ecosystem, as has the need for infrastructure as code in managing global edge delivery networks. Our unique Edge Compute Platform offers developers and operations engineers a modular library of edge logic software, a flexible edge compute fabric and a DevOps-centric control layer. These services combine to provide a level of flexibility, visibility and control that no other edge solution is currently able to offer. To summarize, there are multiple reasons why the move to distributed edge computing is occurring, but we can simplify them into a few strategic areas: speed, capacity, efficiency, cost and responsiveness. And there is a growing recognition that edge computing is necessary to supplement the traditional cloud in order to provide the web of computing support that will be able to power the world’s growing demand for digital applications and services. Those companies that do get ahead of the edge compute curve have the opportunity to be market leaders, and capitalize on the business opportunity that edge compute represents. According to market research firm Million Insights, the Global Edge Computing Market size is predicted to grow at CAGR of 41% over the next seven years, with an anticipated value of $3.24 billion by the 2025."
"193","2020-12-14","2023-03-24","https://www.section.io/blog/cloud-native-solutions-saas-opportunities/","Cloud native is the next step in the digital transformation journey, and a critical path towards edge enablement for SaaS providers. As more and more developers adopt cloud native principles when building their applications, the demand for SaaS providers to flexibly fit into microservices-based architectures continues to grow. And while many SaaS providers today offer cloud-based deployment options, a large majority have yet to take the next step towards edge enablement that will allow them to create better digital experiences. Take a web application firewall (WAF) provider, for instance, who offers a variety of deployment options, including on-premise and cloud. While the cloud solution undoubtedly offers ease of deployment, users will still suffer latency penalties based on the geo-location of that cloud instance versus distributing the solution across a global edge network to sit closer to end users. Before edge computing can truly deliver on its promise, however, the challenge of distributed databases at the edge needs to be solved. To date, edge computing workloads have been mostly stateless, but changing edge workloads are driving the need for persistent data at the edge. Using cloud and on-premise databases is not the ideal solution. We need to figure out the most efficient way to process the tsunami of data at the edge. Section has helped multiple SaaS providers, such as next-gen WAF technology providers Wallarm and ThreatX, by providing turnkey edge deployment solutions. Looking Ahead to Cloud Native Tech in 2021 The demands of an all-remote workforce over the past year have overwhelmingly proven the benefits that public clouds offer for apps, development services, tools, and infrastructure. Without the cloud and the edge, these services would have largely stopped, unable to handle the overnight surge in usage of SaaS, PaaS, and the other cloud tools necessary to motor us through the overwhelmingly digital life required by the pandemic. Looking ahead to 2021, it doesn’t look as if workforce patterns will be returning to “normal” any time soon. Forrester is predicting that remote work will sustain a lasting increase of 300% compared to pre-COVID-19 levels. On top of that, emerging latency-sensitive and latency-critical applications, such as in IoT and gaming, are requiring application architects to rethink where they run their workloads. In its Cloud Computing-focused report, Forrester predicts that 2021 will see “a spike in global demand for both multi-cloud container development platforms and public cloud container/serverless services.” They add, “cloud native tech will continue to power digital transformation strategies: By the end of next year, 60% of companies will leverage containers on public cloud platforms and 25% of developers will leverage serverless.” By the end of next year, 60% of companies will leverage containers on public cloud platforms and 25% of developers will leverage serverless. The Benefits of Cloud Native at the Edge Applying and benefiting from cloud native principles is not confined to the cloud, however. It extends to the edge, indeed enabling an even greater array of benefits. The next step into the future is cloud native. We are seeing many companies choosing to move their application development entirely to the cloud or the edge using cloud native principles. SaaS providers that want to not only survive, but thrive, need to embrace cloud native. Cloud Hosted vs Cloud Native Many SaaS providers promote their products as “cloud hosted” or “cloud enabled”. However, this is known as “cloudwashing” because it markets existing software that exists on a cloud platform but doesn’t leverage the full benefits of cloud computing. Let’s look at an overview of the differences between cloud hosted and cloud native. Cloud Hosted Cloud hosted allows you to host your applications in the cloud. SaaS providers manage, monitor and maintain the majority of the tech stack necessary to operate the software. The opportunity to scale horizontally and control costs are reduced and flexibility is limited to the deployed environment. Cloud Native Instead of being about where applications are deployed, cloud native is about how apps are created and deployed. Cloud native impacts the design, implementation, deployment, and operation of your application. Cloud native can also be defined in terms of the tech it involves: serverless, containers, orchestration, microservices, etc., and the benefits that these technologies bring. Benefits include helping companies build, migrate and modernize customer-facing applications more easily, at scale, from the data center to the cloud to the edge. “Taking advantage of cloud services means using agile and scalable components like containers to deliver discrete and reusable features that integrate in well-described ways, even across technology boundaries like multicloud, which allows delivery teams to rapidly iterate using repeatable automation and orchestration.” - Andi Mann, Chief Technology Advocate, Splunk Benefits of cloud native solutions include: A more straightforward application lifecycle management Faster (and more flexible) deployment Greater and more efficient scalability There can be significant cost efficiencies in cloud native tech, reducing the monthly bill of your cloud provider. It is also worth considering that it can be challenging to integrate on-prem software with cloud environments and SaaS solutions. Cloud native applications, however, easily integrate with cloud vendor environments from the get go. Available Paths to Cloud Native To begin your cloud native journey, first you must construct a solid strategy. As part of this, lay out your objectives and the benefits of choosing a cloud native model. Determine which applications can be easily migrated and what can be refactored (adapting code without changing functionality i.e. enabling applications to run in containers). Next, decide whether you want to train your existing developers in cloud native, hire new developers experienced in cloud native, or leverage an edge platform partner. Upskill your existing developers or hire new developers experienced in cloud native. Constructing a journey toward cloud native can be quite complicated and involves several important steps, including: 1. Design your cloud native microservices architecture. Decide which cloud native services you will deploy and why: e.g. microservices, containers, service meshes, continuous integration/continuous deployment (CI/CD), edge platforms, etc. Determine the cloud native code based on your specific use cases. Developers have three main ways to package and deploy their code: Leverage containerization Deploy as a serverless function Deploy on a virtual machine. NB These approaches can be (and often) are combined. 2. Build cloud native-specific security considerations into your planning from the outset. The concept DevSecOps is built around the idea that security should be a key part of the planning at every step within the app development journey. 3. Ensure you have observability tooling. Visibility into your environment is essential before, during and after cloud migration Observability gives you the chance to catch problems early on in the process and benchmark performance. 4. Enthuse the wider company. Without buy-in across the org and an overall movement towards DevOps, a cloud native strategy will fail. Cross-team collaboration will be essential to successfully changing the operating model for the organization. Leverage an Edge Platform Partner An alternative approach to training your developers or hiring new developers is to leverage an edge platform partner. Section offers a fast, easy way for SaaS providers still in on-prem or cloud hosted solutions to benefit from a turnkey edge network that handles all the infrastructure provisioning, workload orchestration, traffic routing, and scaling to automate delivery of your SaaS solution. We can help you move your product to the edge without expensive rebuilds or ongoing management. That way, your IT team can benefit from improved performance, scalability and security, and focus instead on bullet-proofing your core competency. Our platform thrives on cloud native tech. Find out how yours can too."
"194","2020-11-20","2023-03-24","https://www.section.io/blog/applying-cloud-native-principles-to-edge/","Cloud-Native principles can help make edge computing business and operational models more viable. In this post, we will look at why cloud-native has become such a popular approach to building modern applications and the benefits of bringing cloud-native principles to the edge. Why Cloud-Native? Cloud-Native design principles are increasingly being used for designing modern applications built to run in public, private and hybrid cloud architectures. Cloud-native applications are designed to leverage the power of the cloud, take advantage of its ability to scale, and quickly recover in the event of server failure. The increased number of individual software modules and the demand to efficiently manage them in cloud-native environments has led to widespread adoption of container technology and dynamic orchestration of the containers to optimize resource utilization. This leads to a more flexible architecture and faster application delivery, helping to drive digital transformation. The Cloud Native Computing Foundation (CNCF) definition of cloud-native: Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach. These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil. There are many benefits to a cloud-native approach, notably: Gains in efficiency due to easier development and deployment than traditional and legacy models. Composability (i.e. the ability to build applications from component parts) and reusability (through ready-to-use infrastructure, reducing development complexity and offering control, visibility and self-service for developers). Scalability due to the microservices architecture (since each microservice handles a specific function within an organization, the application can be scaled by creating more instances of only the services that are necessary to handle demand). The potential for innovation due to the cultural movement away from segmented development practices to a more interconnected, agile approach. 6 Benefits to Bringing Cloud-Native Principles to the Edge “Cloud native can help organisations fully leverage edge computing by providing the same operational consistency at the edge as it does in the cloud. It offers high levels of interoperability and compatibility through the use of open standards and serves as a launchpad for innovation based on the flexible nature of its container orchestration engine. It also enables remote DevOps teams to work faster and more efficiently.” - Priyanka Sharma, General Manager, CNCF There are many advantages to bringing cloud-native principles to the edge. Here are six of the main benefits: 1. Using industrialized and proven capabilities Developers can utilize the momentum that CNCF (with applications like CloudEvents and NATS) and other organizations (such as The Reactive Foundation and Reactive Manifesto) have begun towards defining open standards, enabling greater consistency, accessibility, portability and productivity. The use of cloud-native open source projects (Kubernetes, Docker, etc.) similarly has many advantages, helping provide consistency and resiliency while preventing businesses from becoming locked-in to one supplier and legacy products. 2. Ease of deployment A cloud-native approach to edge extends the principles supporting a continuous integration/continuous deployment (CI/CD) model, which can streamline the delivery of code changes, enabling more frequent changes and updates. This includes completing faster rollbacks and being able to more quickly fix edge deployments that break or introduce bugs. Observability is essential for running edge workloads to maximize performance, security and efficiency. Kubernetes provides full visibility into production workloads through its in-built monitoring system, enabling real-time insights and the opportunity for optimization of performance and efficiency. You can also use a full metrics pipeline, such as Prometheus, to access a richer set of metrics. 3. Flexibility of the container orchestration engine Similarly to the cloud, Kubernetes enables organizations to efficiently run containers at the edge. There are three approaches to using Kubernetes in an edge-based architecture. The third approach, which is hierarchical cloud plus edge using a virtual kubelet as reference architecture, particularly allows for flexibility in resource consumption for edge-based architecture. Flexible tooling allows developers to interact with the edge how and where they need to, reducing the complexities linked to running compute across distributed edge locations. “Cloud native microservices provide an immensely flexible way of developing and delivering fine-grain service and control.” - William Fellows, Co-Founder and Research Director of 451 Research 4. Scalability In order for distributed edge systems to be economically viable, there needs to be a scalable way to manage them. Cloud-native principles present the opportunity for unified large-scale application delivery, maintenance and control on distributed edge devices. The entire deployment can be replicated to different sites using robust automation. The Kubernetes control plane, for instance, can handle tens of thousands of containers running across hundreds of nodes, which allows applications to scale as needed, ideally suiting the management of distributed edge workloads. Scaling through software as opposed to people leads to reduced costs and higher resiliency. 5. Efficiency gains At the edge, cost margins matter in making business models profitable. Dynamic scaling eliminates the need to consume resources that won’t be utilized, and extra resources can be switched off once spikes subside. A great example of this type of context-aware scaling and workload scheduling is Section’s Adaptive Edge Engine. Cloud-Native enables an OpEx model, allowing providers to leverage a demand-based approach, using flexible workflows and customized resource utilization to best meet the needs of each application’s unique requirements. 6. Performance Cloud-Native principles applied to the edge can further reduce latency, benefiting both from running workloads closer to end users and cloud-native tooling that boosts performance, acting as an essential enabler to edge-critical applications that depend on low latency. The Kubernetes Horizontal Pod Autoscaler, for instance, responds to latency and volume thresholds when deciding how to scale the number of pods up or down. Traffic can therefore be routed to the most optimal edge locations to reduce latency. Section’s Experience of Deploying Cloud-Native Principles at the Edge Cloud native edge solutions are still comparatively rare. IDC predicts this will change “as cloud-native edge solutions become more widely available and mature and we have more use cases take advantage of cloud as part of their design.” At Section, three of CNCF’s cornerstone open source projects - Kubernetes, Prometheus, and CoreDNS - are at the heart of our Edge Compute Platform. Kubernetes has allowed us to improve and scale our platform in ways that would have been impossible without it. With Kubernetes, we can also benefit from being infrastructure-agonistic and manage a diverse set of workloads running across a vendor-neutral global network of leading infrastructure providers. In addition, Prometheus acts as our monitoring solution, informing our ongoing decision-making, and CoreDNS is not only used inside our Kubernetes clusters but also used separately for our Section Hosted DNS offering. Other notable cloud native technologies that are on our radar include NATS, to replace our messaging system from pre-containerized days, and Jaegar/OpenTracing to augment our observability capabilities beyond simple logging and metrics. We continue to find that cloud-native solutions help power greater flexibility, improved resource efficiency, better scalability, lower downtime and improved performance across the entire production lifecycle."
"195","2020-05-12","2023-03-24","https://www.section.io/blog/kubernetes-enabling-edge-computing-adoption/","Since Kubernetes was released five years ago by Google, it has become the standard for container orchestration in the cloud and data center. Its popularity with developers stems from its flexibility, reliability, and scalability to schedule and run containers on clusters of physical or virtual machines (VMs) for a diverse range of workloads. When it comes to the Infrastructure (or Service Provider) Edge, Kubernetes is increasingly being adopted as a key component of edge computing. As in the cloud, Kubernetes allows organizations to efficiently run containers at the edge in a way that enables DevOps teams to move with greater dexterity and speed by maximizing resources (and spend less time integrating with heterogeneous operating environments), particularly important as organizations consume and analyze ever-increasing amounts of data. A Shared Operational Paradigm Edge nodes represent an additional layer of IT infrastructure available to enterprises and service providers alongside their cloud and on-premise data center architecture. It is important for admins to be able to manage workloads at the edge layer in the same dynamic and automated way as has become standard in the cloud environment. As defined by the Open Glossary, an “edge-native application” is one which is impractical or undesirable to operate in a centralized data center. In a perfect world, developers would be able to deploy containerized workloads anywhere along the cloud-to-edge continuum to balance the attributes of distributed and centralized computing in areas such as cost efficiencies, latency, security, and scalability. Ultimately, cloud and edge will work alongside one another, with workloads and applications at the edge being those that have low latency, high bandwidth, and strict privacy requirements. Other distributed workloads that benefit from edge acceleration include Augmented Reality (AR), Virtual Reality (VR), Massively Multiplayer Gaming (MMPG), etc. There is a need for a shared operational paradigm to automate processing and execution of instructions as operations and data flows back and forth between cloud and edge devices. Kubernetes offers this shared paradigm for all network deployments, allowing policies and rulesets to be applied to the entire infrastructure. Policies can also be made more specific for certain channels or edge nodes that require bespoke configuration. Kubernetes-Based Edge Architecture According to a presentation from the Kubernetes IoT Edge Working Group at KubeCon Europe 2019, there are three approaches to using Kubernetes in edge-based architecture to manage workloads and resource deployments. A Kubernetes cluster involves a master and nodes. The master exposes the API to developers and schedules the deployment of all clusters, including nodes. Nodes contain the container runtime environment (such as Docker), a Kubelet (which communicates with the master), and pods, which are a collection of one or multiple containers. Nodes can be a virtual machine in the cloud. The three approaches for edge-based scenarios can be summarized as follows: The whole Kubernetes cluster is deployed within edge nodes. This is useful for instances in which the edge node has low capacity resources or a single-server machine. K3s is the reference architecture for this solution. The next approach comes from KubeEdge, and involves the control plane residing in the cloud and managing the edge nodes containing containers and resources. This architecture enables optimization in edge resource utilization because it allows support for different hardware resources at the edge. The third approach is hierarchical cloud plus edge, using a virtual kubelet as reference architecture. Virtual kubelets live in the cloud and contain the abstract of nodes and pods deployed at the edge. This approach allows for flexibility in resource consumption for edge-based architecture. Section’s Migration to Kubernetes Section migrated to Kubernetes from a legacy homegrown scheduler last year. Instead of building our own fixed hardware network, Section distributes Kubernetes clusters across a vendor-neutral worldwide network of leading infrastructure providers, including AWS, Google Cloud, Microsoft Azure, Packet, DigitalOcean, CenturyLink, and RackCorp. Kubernetes allows us to be infrastructure-agnostic and seamlessly manage a diverse set of workloads. Our first-hand experience of the many benefits of Kubernetes at the edge include: Flexible tooling, allowing our developers to interact with the edge as they need to; Our users can run edge workloads anywhere along the edge continuum; Scaling up and out as needed through our vendor-neutral worldwide network; High availability of services; Fewer service interruptions during upgrades; Greater resource efficiency - in particular, we use the Kubernetes Horizontal Pod Autoscaler, which automatically scales the number of pods up or down according to defined latency or volume thresholds; Full visibility into production workloads through the built-in monitoring system; Improved performance. Summary As more organizations and operators continue to adopt and support Kubernetes-based cloud-edge patterns, the ecosystem will continue to mature. However, not every organization will have the resources and/or expertise to build these systems themselves. This is where edge platforms (like Section) bridge those gaps, offering DevOps teams familiar tooling to take advantage of the benefits that Kubernetes has to offer without the complexities that come along with it."
"196","2020-04-23","2023-03-24","https://www.section.io/blog/opex-vs-capex-edge-computing-model/","When Akamai was founded in 1998, co-founders Daniel M. Lewin and Tom Leighton, were not only inventing the first content delivery network (CDN), but also creating an early prototype for edge computing. Akamai’s network of servers distributed around the world has allowed customers to accelerate their digital content by distributing it from locations close to the end user. As edge delivery has grown beyond simple content, traditional CDNs face new challenges. Dynamic sites use more complex logic to display complex rendering that often can’t be cached, and devices are typically farther away from origin servers and are requesting content that is often not optimized for them. The CapEx Model Traditional CDNs have typically adopted a Capital Expenditure (CapEX) model as opposed to an Operational Expenditure (OpEx) one. The business model of the CDN has relied on an in-house approach to data centers, which historically has required large CapEx investments to purchase space, equipment, software, and support all of the resources required to maintain the assets. If a customer has very large data processing and storage needs, a traditional CDN would develop a strategy involving spinning up new data centers, buying and positioning new servers, and purchasing extra capacity at different physical locations around the globe as close to where the customer’s users are as possible. In that model, physical hardware is placed in a particular location or locations, and profits are made by optimizing the site and densely packing in as many users as possible. The OpEx Model In an OpEx model, providers choose not to invest in their own infrastructure and instead leverage existing hardware from other providers (i.e. cloud hosting, bare metal, telco, 5G). By not being tied to their own fixed networks, providers using an OpEx model are able to take a more demand-based approach, using flexible strategies and workflows to best serve each customer’s unique requirements and maximize cost efficiency. According to Gartner, global spending on IT slowed down last year, in particular with regards to device and data center equipment. John-David Lovelock, research VP, says software is expected to be the fastest growing market across 2020, reaching growth at 10.5%. The research agency notes that organizations with a high percentage of IT spending dedicated to cloud adoption “is where the next-generation, disruptive business models will emerge.” The Growing Edge Infrastructure Network There is a vastly growing global network of edge infrastructure - edge data centers, hardware and networks - from major cloud providers like AWS, Google, and Microsoft, to more niche players like Digital Ocean, Packet (a division of Equinix), Vapor IO, and RackCorp, not to mention everything going on with the 5G rollout. When CDNs and edge compute platforms talk about a multi-provider model, this is referring to the ability to target and access the benefits across different providers. Being locked into a single provider means that you’re only able to move and scale within a fixed network. Having access to a diverse set of edge infrastructure offers many advantages: Reliability: When a provider experiences downtime, you can failover to another provider. Scalability: Spin up/down infrastructure to meet real-time demands. Expansive edge footprint: Access to an aggregated network of infrastructure provides a global edge footprint beyond what any single provider can offer. Flexibility in moving workload: Making decisions about where to move workload is significantly easier than if you had to factor in the costs of building infrastructure from the ground-up. For many, the advantages of being able to freely move capacity where it makes the most sense to serve customers provides a more attractive model. This is the camp that we sit in at Section. We see the importance of giving developers access to infrastructure across a diverse set of providers so that the full potential of edge computing can be realized. When we talk about flexibility, what we mean is the ability for developers to deploy workloads onto any piece of hardware where we can run a Kubernetes cluster. That might include a standard cloud hosting provider, or, alternatively, it might be a cell phone tower location. As an edge compute platform, we want boundless ability to run workloads in the locations that are most suited to meet the demands of our customers’ applications. We like to refer to this as, “Any workload, anywhere.” The Role of Open Source and Industry Cooperation in Enabling OpEx Models Open source technologies and industry cooperation are further enabling the OpEx model for edge computing. Advancements in areas such as infrastructure-as-code and cloud-native technologies have allowed Section to build a platform that extends the benefits of the OpEx model to our customers. When Section joined the Kinetic Edge Alliance last year, Cole Crawford, CEO at Vapor IO, spoke to this. “Vapor IO’s Kinetic Edge will deliver the next wave of network and data center infrastructure, vital to support 5G, autonomous vehicles, and many other exciting technologies and applications. To meet the needs of the future, our industry must collaborate.” Cole Crawford, CEO at Vapor IO We must bring together all of the products, platforms, and technologies, such as those offered by Section, in order to help make operators and developers successful with edge deployments.”"
"197","2020-03-12","2023-03-24","https://www.section.io/blog/empowering-developers-to-build-at-the-edge/","As a larger, diverse set of stakeholders join forces to build the Edge, there remains a strong focus on edge infrastructure - edge data centers, hardware and networks. However, of equal importance is the focus on developing software that can run at scale across a massively distributed global network of edge infrastructure. As Ihab Tarazi, CTO of Dell Technologies (formerly of Packet/Equinix) pointed out in a blog last year on architecting for success at the edge, its development is “a complex undertaking that requires an enormous number of changes to occur.” He is right in saying, “This will be an evolution”; one in which we believe developers will take a front-row seat. “[Development of the edge] is a complex undertaking that requires an enormous number of changes to occur. This will be an evolution.” Ihab Tarazi, CTO of Dell Technologies Helping developers embrace distributed systems For the average developer, there is often a disconnect between writing code and running code, particularly when it comes to optimizing the distribution of that code. Most developers will have some level of experience with DevOps and agile these days, but few have experience building highly distributed systems at the Edge. With the advancement of edge-enabled platforms, one of our biggest challenges at Section has been to help developers understand why they should look to expand beyond simple content delivery. While basic CDN-type workload is still vital to application performance optimization, edge compute platforms, like Section, now make it possible to migrate more advanced logic out of centralized cloud infrastructure to leverage performance, security and scalability benefits along the edge continuum. “We’ve got a ways to go in many respects” says Daniel Bartholomew, Section Co-Founder & CTO. “Firstly, we need to get the physical component of the edge built, but we also need to bring the developers - I’m talking about the entire developer community - on a journey away from their two and three tier architectural services design into microservices, and from microservices into distributed computing.” “We need to bring the developers - I’m talking about the entire developer community - on a journey away from their two and three tier architectural services design into microservices, and from microservices into distributed computing.” - Daniel Bartholomew, Co-Founder & CTO of Section What are the advantages of moving workloads to the Edge? We’ve come a long way in educating developers and the larger technology community on the benefits of edge computing. But just to reiterate, some of the advantages of edge computing include: Low Latency By moving workloads as close as possible to the end user and removing unnecessary data exchange between the cloud and the end user, latency is reduced. Therefore, edge computing can be most effective for those microservices or applications that are latency sensitive. Reduced Data Backhaul Processing data at the Edge reduces the need to transmit high volumes of data back to centralized applications. This reduces the cost-of-service and the time-to-deliver results for end-users. As we head towards more IoT and IIoT, or applications delivering high volumes of telemetry and chat or high bandwidth imaging needs in-home, health and industrial use cases, processing at the Edge will be critical to delivering a better, more efficient Internet. Innovation Edge computing enables a myriad of never-before-possible use cases that require ultra low latency, from online multiplayer gaming to autonomous vehicles and beyond. Security With edge computing, sensitive data doesn’t have to be sent across the network, but can instead be processed on the end user’s device or closer to it, which can serve to reduce the potential application attack surface area. Furthermore, activities such as authentication and validation of end user identity are well suited for the Edge, and enforcement of API routing policies can help ensure that end user traffic gets to the right cloud environment. With edge computing offering the ability to process data in near real-time, control centers receive information as it happens, giving DevOps teams the insights they need to reduce the risk of errors and incidents before they have a wide-reaching impact. Empowering the developer In order for developers to be able to leverage the benefits of edge computing, they need much greater flexibility and control than traditional CDNs are able to offer. At Section, our ultimate goal has always been to improve the Internet by empowering innovators with simplicity, flexibility and control at the Edge to create better digital experiences. Instead of hard-coding Varnish Cache into the Section platform as a caching CDN or hard-coding ModSecurity in as a web application firewall (WAF), for instance, our co-founders set out from the start to allow developers to bring their own workloads into the full development lifecycle. By contrast, the incumbent CDNs, by virtue of operating in their own fixed and inflexible networks, are unable to bring the level of flexibility and control that developers need to realize the full benefits of edge computing. Leading edge compute platforms also provide the necessary tooling to extend the same DevOps principles that underpin core application development to deployment at the edge, enabling greater flexibility, scalability and improved performance optimization. Why Section is different Not all edge compute platforms are the same. Section is a global network of federated Kubernetes clusters, which extends far beyond basic content delivery. By leveraging compute capacity from leading hosting providers and installing an edge fabric on top, Section makes it easy for developers to ship code that is immediately distributed across its Composable Edge Cloud. Our goal is to give the developer an environment that feels familiar and integrates into their existing development pipelines. By abstracting away the reality of the edge expanding into thousands of nodes, developers are able to view the edge as a single deployment plane. Unlike many legacy CDNs, Section provides a full development lifecycle experience based on Git-backed workflows. This enables developers to clone environments down locally for testing before pushing to staging and production, reducing the risk of introducing errors into production. By providing developers with standard tooling and workflows, they are able to more easily interact with the Edge how and where they need, ultimately paving the way for the next generation of applications."
"198","2020-11-12","2023-03-24","https://www.section.io/blog/how-adaptive-edge-engine-works/","We recently highlighted the results of a series of tests proving the superior performance of Section’s Adaptive Edge Engine (AEE) over a major cloud provider. Across the series of tests, Section’s Edge proved to be consistently faster with less extreme risk than the Cloud deployment. The average latency for the slowest 1% of Section Edge users was equal to the average latency of all users in the Cloud case. In this post, we will be uncovering the inner workings of the AEE that are delivering these compelling results. What is the Adaptive Edge Engine? Without automated workload orchestration, edge compute will be prohibitively expensive for many workloads. The AEE (patent pending) is a dynamic, context-aware workload scheduling and traffic-routing technology. In setting out to build this technology, our goal was to facilitate the creation of systems that effectively manage lower latency and cost optimization for edge workloads. We also want to avoid the limitations of traditional content delivery solution architectures, including single datacenter hosting, single datacenter hosting with CDN, multi datacenter hosting and CDN with serverless, all of which limit end-user performance due to network latency issues and computational cost parameters. Developers can use Section’s Adaptive Edge Engine to optimize dynamic edge networks across the attributes of their choosing. For example, customers can craft unique strategies for obtaining performance and cost targets, meeting security/regulatory requirements, or even preferring low-carbon/energy-efficient infrastructure (helping bolster a more sustainable energy model). The AEE: Four Primary Goals At the outset, our four primary goals for the AEE were to: Minimize end-user latency by shortening network paths and determining the best possible path based on signals from the entire workload pool. Provide broad runtime system support, enabling easy deployment of complex application logic written in a variety of languages. Ensure that edge workloads efficiently reserve and use compute capacity so that applications are only deployed in locations while they add value and are withdrawn otherwise. Enable low friction workload adoption, consistent with our collective passion to deliver a better Edge for DevOps Engineers with minimal hassle. To achieve these goals and overcome the limitations of content delivery solutions on the market, Section’s AEE makes extensive use of the strategy pattern - in order to support the broadest set of use cases without altering the core technology. How Does the AEE Work? The AEE is a system built of several components. These include: Location Optimizer Determines where a workload should be deployed. The Location Optimizer is a general purpose system for implementing optimization “strategies”. The customer defines their strategy, a combination of objective(s) and constraints and the Location Optimizer will solve for the optimum. Optimality is conditional. No solution is optimal for all systems and under all circumstances. Each customer needs the flexibility to drive results toward their business needs at the time. Some businesses may need to maximize performance within a fixed budget; others may seek to minimize cost for an accepted level of performance; still others may have geographic or security concerns that out-weigh both cost and performance. The flexibility of the AEE Location Optimizer allows any such combinations to be optimized. Health Checker Once the workload has been deployed, the health checker ensures it is where it should be. The Health Checker enables health check strategies that will be specific to each different environment. A specific workload might have a unique test or health check that it requires. The Health Checker also accepts strategies. Both these components interface with Section customers. Customer and workload-specific requirements drive how the Location Optimizer and Health Checker work for any given environment. The other components in the system are working to make sure that Section is doing what it needs to, in terms of deploying the workload, routing traffic to it, etc. These components are: Traffic Director Responsible for routing the traffic to wherever the workload is deployed using DNS and BGP. Controls traffic flows to the optimal endpoints when application traffic is desired; Removes traffic flows from endpoints when application flows are not desired; Allows traffic to flow to endpoints based on the outcomes of health checks performed on an application per endpoint; Controlling the rate of traffic flow to an endpoint. Workload Controller Deploys customer environments to endpoints. Scales the environment horizontally and vertically as needed. Endpoint Controller Manages the endpoints. Manages the number of nodes running; Determines where the endpoints are located; Spins them up when we need them and down when we don’t; Controls the size of the application resource allocation within the endpoint. These three components are always running in the background, making sure that there is constant alignment between customer preferences and what the AEE discovers and implements, so that the requests get to where they need to as quickly and efficiently as possible. The Intelligence Backbone Any Location Optimizer strategy that optimizes relative to traffic volumes needs a traffic data feed. The basic choices are: past data or forecasts. We currently have a “forecastRPS” (requests per second) strategy which uses the user-traffic-forecast as a data feed. ML/AI is a core Section capability that allows better data feeds that will: Enable more sophisticated strategies and; Increase the effectiveness of the AEE components, e.g. making HPA (Horizontal Pod Autoscaler) and HCA (Horizontal Cluster Autoscaler) work better. The actions of the AEE are passively coordinated through a central data store recording the current states of all environments on Section crossed with all available endpoints. We call this the state store and the data it retains is amazingly simple. The complexity of the myriad use cases is retained in the strategies the customer helps define. The AEE is robust and powerful in direct relation to its simplicity. As the different components do their jobs: optimizing location strategies, creating and destroying endpoints, health-checking every workload, etc., they are reading from and writing to the state store. The components safely ignore each other and still develop coordinated, efficient outcomes. The AEE is not a set of activities or rules that are executed as an ordered series of jobs. The components are separate, each running and executing its own process with or without extra logic provided in customer strategies. By basing those processes on information in the state store, each component becomes connected, leading to a robust, continuous, coordinated system with functions spanning concerns such as “Where are the necessary endpoints?”, “How many copies of the workload are needed?”, and “How much does this customer favor latency over cost?” Conclusion Ultimately, Section’s Adaptive Edge Engine serves five key objectives for our end-users: Enable system builders to define their intent. Using the AEE, system builders can direct the AEE to find an optimal solution to latency management based on their specific performance needs and budgetary constraints. Provide support for system builder defined restrictions. System builders can tag their workload with properties that let them control which nodes their workload can execute on. Customize the edge network. The AEE can expand and contract the total available edge network by automating the purchasing of commodity server infrastructure informed by signals from the entire workload pool. Enable decision frequency. The AEE can evaluate and adjust the deployment and routing of each workload independently, adjust the network accordingly, and converge end-user traffic to that determination within a ten minute window. Create zero server management. The AEE allows system builders to place their focus on business imperatives as opposed to operational server and network management responsibilities. As we continue to develop the AEE over the coming months and years, we are excited to see how we can expand its use as a framework while delivering the greatest value. We will be looking at how we can help customers define their optimal strategy within it and hopefully arrive at patterns that allow people to build their own strategies for the AEE to easily run for them."
"199","2020-07-24","2023-03-24","https://www.section.io/blog/scaling-horizontally-vs-vertically/","Capacity planning is a challenge that every engineering team faces when it comes to ensuring the right resources are in place to handle expected (and unexpected) traffic demands. When demand for your application or website is increasing and you need to expand its accessibility, storage power, and availability levels, is it better to scale horizontally or vertically? That decision depends on a number of factors. Is request volume steadily growing and/or is the current growth experiencing spikes that lead to service degradation. These types of considerations, coupled with an application’s unique make-up, need to be evaluated when determining the optimal scaling approach. What is scalability? The scalability of an application can be measured by the number of requests it can effectively support simultaneously. The point at which an application can no longer handle additional requests effectively is the limit of its scalability. This limit is reached when a critical hardware resource runs out, requiring different or more machines. Scaling these resources can include any combination of adjustments to CPU and physical memory (different or more machines), hard disk (bigger hard drives, less “live” data, solid state drives), and/or the network bandwidth (multiple network interface controllers, bigger NICs, fiber, etc.). Scaling horizontally and scaling vertically are similar in that they both involve adding computing resources to your infrastructure. There are distinct differences between the two in terms of implementation and performance. What’s the main difference? Horizontal scaling means scaling by adding more machines to your pool of resources (also described as “scaling out”), whereas vertical scaling refers to scaling by adding more power (e.g. CPU, RAM) to an existing machine (also described as “scaling up”). One of the fundamental differences between the two is that horizontal scaling requires breaking a sequential piece of logic into smaller pieces so that they can be executed in parallel across multiple machines. In many respects, vertical scaling is easier because the logic really doesn’t need to change. Rather, you’re just running the same code on higher-spec machines. However, there are many other factors to consider when determining the appropriate approach. Horizontal Scaling (scaling out) Vertical Scaling (scaling up) Databases In a database world, horizontal scaling is usually based on the partitioning of data (each node only contains part of the data). In vertical scaling, the data lives on a single node and scaling is done through multi-core, e.g. spreading the load between the CPU and RAM resources of the machine. Downtime In theory, adding more machines to the existing pool means you are not limited to the capacity of a single unit, making it possible to scale with less downtime. Vertical scaling is limited to the capacity of one machine, scaling beyond that capacity can involve downtime and has an upper hard limit, i.e. the scale of the hardware on which you are currently running. Concurrency Also described as distributed programming, as it involves distributing jobs across machines over the network. Several patterns associated with this model: Master/Worker*, Tuple Spaces, Blackboard, MapReduce. Actor model: concurrent programming on multi-core machines is often performed via multi-threading and in-process message passing. Message passing In distributed computing, the lack of a shared address space makes data sharing more complex. It also makes the process of sharing, passing or updating data more costly since you have to pass copies of the data. In a multi-threaded scenario, you can assume the existence of a shared address space, so data sharing and message passing can be done by passing a reference. Examples Cassandra, MongoDB, Google Cloud Spanner MySQL, Amazon RDS *See the ongoing discussion around the need to change the Master/Slave terminology, leading to its removal in 2018 from the Python Programming Language. The decision to scale out or scale up In choosing between the two, there are various factors to consider. These include: Performance - Scaling out allows you to combine the power of multiple machines into a single virtual machine with the combined power of all of them. This means you’re not limited to the capacity of a single unit. First, however, it’s worth working out if you have enough resources within a single machine to meet your scalability needs. Flexibility - If your system is solely designed for scaling up, you are effectively locked into a minimum price set by the hardware you are using. If you want the flexibility to choose the optimal configuration setup at any time to optimize cost and performance, scaling out might be a better option. Regularity of Upgrades - Again, flexibility is important here. Building an application as a single large unit will make it more difficult to add or change pieces of code individually without bringing the entire system down. In order to deliver a more continuous upgrade process, it’s easier to decouple your application and horizontally scale. Redundancy - Horizontal scaling offers built-in redundancy in comparison to having only one system in vertical scaling, and thus a single point of failure. Geographical Distribution - When you need to spread out an application across geographical regions or data centers in order to reduce geo-latency, comply with regulatory requirements, or handle disaster recovery scenarios, you don’t have the option of putting your application in a single box. You have to distribute it. Cost - As more large multi-core machines enter the market at significantly lower price points, consider if there are instances in which your application (or portions of your application) can be usefully packaged in a single box and will meet your performance and scalability goals. This might lead to reduced costs. In conclusion: a seamless transition between the two models? It doesn’t always make sense to choose between horizontal and vertical scaling. Moving between the two models is often a better choice. For instance, in storage, we often want to switch between a single local disk to a distributed storage system. Building flexibility into the system, where some layers of the application run on vertically scaled machines and other layers on horizontally scaled infrastructure remains a matter of designing for parallelization. To achieve this, (i) design it from the outset as a decoupled set of services, making the code easier to move, meaning you can add more resources when needed without breaking the ties between your code sets; (ii) partition your application and data model so the parallel units don’t share anything. It’s likely that the industry will increasingly migrate towards a horizontally distributed approach to scaling architecture. This trend is driven by the demand for more reliability through a redundancy strategy, and the requirement for improved utilization through resource sharing as a result of migration to cloud/SaaS environments. However, combining this with a vertical scaling approach can allow us to benefit from both paradigms."
"200","2020-09-28","2023-03-24","https://www.section.io/blog/prepare-website-high-traffic-events/","It’s that time of the year when the busy holiday season is almost upon us. Upcoming eCommerce events to prepare for include Click Frenzy (November 10, Australia), Singles’ Day (November 11, China), Black Friday (November 27, US/worldwide) and Cyber Monday (US/increasingly worldwide), not to mention all the online sale events that will be taking place from now through the end of the year. The 2020 holiday season will of course be impacted by COVID-19, which has affected consumer behavior in a number of key ways. In a recent report from McKinsey Global Institute, it was reported that there has been a 15-40% growth in online shoppers in most categories and this is likely to continue post-COVID. Thanksgiving and Black Friday hit new records in 2019 and this year is anticipated to be even busier with so much eCommerce behavior having moved online. This is why it’s even more important than ever to prepare for high traffic ahead of time. Example of traffic spike where website traffic increased by a factor of 5875% versus normal daily peak over the prior 7 days. Overview: Preparing for High Traffic Any outage or slowdown (“the new downtime”) will impact brand reputation and lead to lost revenue at the precise time you are aiming to maximize revenue streams. There are many ways to ensure your website and web applications are ready for a surge of high traffic. We recommend you follow the following three key steps: Set up a Virtual Waiting Room Virtual waiting rooms (VWR), also known as online queuing systems, can be used to protect your website or web application during peak traffic events, so that some users can still conduct searches and transactions while others are placed in a ‘waiting room’. Essentially, a VWR is risk insurance. You may never need it, but once it’s in place, you know that you can throw an unlimited amount of traffic at your website and still deliver an acceptable user experience rather than being offline for all users. We recently re-engineered the logic of Section’s Virtual Waiting Room module to use a modified FIFO model. This allows Section admins to apply more logic to queueing decisions, for instance, we have set thresholds for visitor access allowances and for the size of the head of the queue waiting area. We have also implemented logic to check for a valid session ID before users are placed in the queue to prevent bot activity taking up valuable spots. Leverage a Smart Caching Strategy One of the most sensible things to do in advance of the holiday season is to leverage a smart caching strategy. Start by offloading anything you can to the edge, particularly dynamic content, to reduce the number of requests made to your web servers. The decreased load on your origin servers can reduce strain during high load periods, as well as deliver lower hosting costs. Also consider cache techniques such as hole-punching, which allow large portions of the page to be cached while customization elements like account information are left uncached, enabling eCommerce sites to cache their most important transactional pages, while leaving the website servers clear to effectively handle the critical checkout process. Properly Scale Infrastructure Ensure your infrastructure is right sized, pain points are understood and monitored, and you can scale with traffic volume. Modern edge computing platforms facilitate traditional CDN workloads, such as caching and image optimization, while also offering greater flexibility to move more advanced logic closer to end users. This allows web applications to benefit from lower latency and savings in bandwidth due to fewer requests having to travel back to centralized infrastructure. Run a Testing and Monitoring Program Before peak events, perform load testing to understand your system’s behavior under normal and peak conditions, uncover any unexpected faults, and correct them ahead of time. Set up a monitoring and/or observability program both before the peak event and during. Section provides this out-of-the-box. Monitoring gives you essential visibility into how your systems are behaving under normal conditions, providing a set of performance baselines, allowing you to prepare an appropriate strategy for scaling. Utilizing performance monitoring during peak events allows you to gather useful information for the next high load period, or worse case scenario, gives you immediate insight into any performance issues, allowing you to intervene as quickly as possible. Preparing for High Traffic with Section There are several ways that you can best engage with the Section team to prepare for high traffic events. These include: Let us know when your email campaigns are scheduled to go out. Since this is typically the first event that triggers increased traffic to your site, Section engineers can proactively be prepared to field inbound support requests from your team. As mentioned above, our Virtual Waiting Room solution can offer solid peace of mind during high traffic events and can be implemented in relatively short order. If you feel your traffic increases are bordering on unmanageable, please contact our team to get VWR added to your environments. Although never a desirable place to be in, if your site experiences any sort of outage or issue during spiky traffic events, leverage Section’s maintenance page functionality to ensure the right message is getting to your impacted customers. Sign up online or contact sales to get started with Section and get ahead of the curve before the holidays hit."
"201","2020-09-18","2023-03-24","https://www.section.io/blog/enterprise-nodejs-traps-to-avoid/","As part of our current series on Node.js and as a follow-on from Five Enterprise Apps Using Node.js Under the Hood, in this post, we look at three major traps when building enterprise Node.js apps and workarounds for how to avoid them. As we’ve seen, many enterprise-level organizations - from NASA to Netflix - are using Node.js to build their applications. While there are many advantages to using Node.js, there are some pitfalls that developers can fall foul of while developing their applications. Let’s take a look at three major ones and some of their workarounds. 1. Bugs introduced by dynamic typing In programming languages, type systems are rules used to manage data in variables, expressions, and functions. Dynamically typed languages check the type of the data when the program is running. Statically typed languages check data types when the program is being compiled. Dynamically typed languages have several advantages over statically typed languages, such as: They are more concise: shorter code is quicker to write, read and maintain; There are more possibilities for interactivity: dynamic typing is a good fit for interactive, REP-esque programming for rapid prototyping, real-time debugging of running program instances and live coding; Test cases can catch runtime errors if you have a good test suite; Dynamic languages encourage a prototype-based approach, for instance in JavaScript and Node.js. That said, dynamically typed languages can easily introduce bugs due to a lack of type checking. How to avoid this? TypeScript is an open source language that is a superset of JavaScript. It has various useful attributes: Its main innovation is that it adds static typing to JavaScript, while being transcompilable to JavaScript. It lets you describe the shape of an object, providing superior documentation and the opportunity to have TypeScript validate that your code will run correctly before you run it. TypeScript code is changed into JavaScript code via the TypeScript compiler or Babel. The resulting code is clean and simple and runs anywhere JavaScript runs: in a browser, your apps or on Node.js. As well as preventing bugs, TypeScript can improve the code editing process when used with a code editor with TypeScript integration. 2. Challenges with callbacks Callbacks are fundamental to JavaScript’s design. In web browsers, events are taken care of by passing references to functions that act like callbacks. In Node.js, callbacks, until recently, have been the only way that asynchronous elements of your code could communicate with one another. They continue to be heavily used – for instance, package developers design their APIs around callbacks. Some easy mistakes to make when using callbacks include: Invoking a callback more than once. How to avoid it: One way is to add a return keyword before every callback invocation. Deeply nesting callbacks Keeping a number of queued tasks in the background, each with its own callback, is known as “callback hell” and can lead to the code becoming difficult to understand and laborious to maintain. How to avoid it: Use a utility Node.js package, such as Async.js, that handles asynchronous JS patterns. Declare these tasks as small functions and link them together. Expecting callbacks to run synchronously In other programming languages, we are used to two statements executing one after the other, except when there is an instruction to jump between statements. In JavaScript and Node.js, with callbacks, a specific function tends not to run well until the task it is waiting on is complete. How to avoid it: Anything that needs to take place after a callback has fired should be invoked from within it. Most of the issues that spring from nested callback functions can be overcome with the use of promises in Node.js. A promise is an enhancement to callback functions in Node.js. A promise can help alleviate the problems associated with nesting multiple callback functions together. The essential concept of a promise is the return value. Promises can be nested within one another to improve the look of code and make it easier to maintain, particularly when one asynchronous function is called after another. Edge Hosting Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Learn more and get started on a free plan 3. Performance bottlenecks due to overuse of CPU-bound tasks The biggest challenge developers face with Node.js is its inability to quickly process CPU bound tasks. When Node.js receives a CPU bound task, it sets all the CPU available to process it first, then answer other queued requests. This leads to slow processing and a delay in the event loop. As Node.js processes tasks asynchronously, it executes JS code on its single thread on an event basis, which can lead to an event loop. How to avoid it? Updates to Node.js have improved this issue in the last few years. In 2018, multithreading was introduced as an experimental feature with the 10.5.0 update. A new module called Worker threads enables the use of threads that execute JS in parallel. They are intended to help with performance of CPU-intensive JS operations. However, note that Worker threads can only be performed on machines with multiple cores since Node.js allows you to use one core per thread. Heavy parallel processes can be run on a different thread; however, this feature is still experimental in Node.js v12 (now on v14). While Node.js is a relatively easy runtime for newcomers to get started on, there are some surprising gotchas. Fortunately, as we have seen, there are workarounds for the major traps and pitfalls."
"202","2020-08-05","2023-03-24","https://www.section.io/blog/enterprise-apps-using-nodejs/","Ten years ago, Node.js was considered to be on the bleeding edge of technology, the domain of the curious and the bold. Today, enterprise-level organizations like Netflix, PayPal, and NASA are implementing the server-side JavaScript runtime for two primary reasons: speed and scale. In this post, we take a look at five different enterprise apps using Node.js under the hood to understand why it has become the new standard for enterprise applications. Netflix As of April 2020, the world’s largest streaming service had over 193 million paid subscriptions worldwide, including 73M in the US. Its online streaming library includes movies and TV shows, originals and acquisitions, and it is available in more than 190 countries. Netflix started to use Node.js back in 2013. At the time, as a single Java monolithic application, it was struggling with 40-minute startup times, slow builds, huge developer machines and the need for developers to context switch and write the same lines of code twice, for server and client sides separately (they were using Java on the server side and JavaScript on the client side). According to Kim Trott, Director, Runtime Engineering at Netflix, this state was “affecting our ability to be productive and move quickly and be able to rapidly innovate and build out all the A/B testing that we do.” They found they were having to do many things twice: write code, debug, data access, render, etc. In an interview with Node.js, Yunong Xiao, Principal Engineer at Netflix, boiled down its choice to go with Node.js to “three big reasons”: 1. A common language “JavaScript is the lingua franca of the web today… so that’s a big help in its uptake because folks already know the language so it makes it easy for them to learn the run-time.” 2. Performance “For us with such a big shop at such a big scale, the more performant something is, the better we are at controlling cost and saving cost on the infrastructure.” By moving its website to a single-page application (SPA), Netflix saw load times go down to sub 60 seconds. 3. Modules Ecosystem “We don’t want to have to invest in reinventing the wheel every time so if there’s a great suite of modules that help us get our work done, that’s a huge boon.” The rich module ecosystem around Node.js, much of it open source and available on npm was attractive to the Netflix team. The fact it is focused on the UI needs and UI frameworks and libraries was also appealing (and different to the Java ecosystem). PayPal PayPal has over 325 million active accounts worldwide, allowing users to interact virtually with each other, make payments and issue money transfers without the need for debit or credit card disclosure. PayPal was the host of the inaugural NodeDay in 2014, being one of the first major enterprises to adopt the runtime. In a PayPal Engineering blog post on the decision to move to Node.js for their application platform, they admit they “slipped Node.js in the door as a prototyping platform.” Once “it proved extremely proficient”, they decided to give it a go on production and moved all their consumer-facing web applications from Java to JavaScript and Node.js. Historically, PayPal’s engineering teams were segmented into two groups – people who coded for the frontend (using HTML, CSS and JS) and people who coded for the backend (using Java). There was a lack of easy communication between the two: “the primary blocker at PayPal has always been the artificial boundary we established between the browser and server.” As at Netflix, the adoption of Node.js and the ability to use JS to write browser and server-side applications allowed PayPal to unify its engineering disciplines into one combined team of full-stack engineers. Their first adopter of Node.js in production “wasn’t a minor application” – it was the PayPal Account Overview page, one of the most heavily trafficked apps on the website. They decided to mitigate the risk by building an equivalent Java application in parallel. This set the playing field for an interesting comparison between the two, which came out strongly in Node.js’ favor: The Node.js application was built twice as quickly with fewer engineers working on it; There were 33% fewer lines of code; There were 40% fewer files; Requests per second in Node.js was double the Java application; They saw a 35% decrease time in the average response time for the same page; and Pages were served 200ms faster with Node.js. Edge Hosting Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Learn more and get started on a free plan Microsoft Over recent years, Microsoft has undergone a substantial change to its business model, embracing digital transformation, and encouraging its customers to do the same. It still supports and sells computer software, consumer electronics, personal computers, and related services, but its core business is now about cloud, SaaS and PaaS, as well as forays into AI and ML Microsoft was another early adopter of Node.js, implementing a native Windows version in 2011. In a talk at NodeDay 2018 on Microsoft’s digital transformation journey with Node.js, Chris Dias, Principal Program Manager at Microsoft, linked its continued success at the company to two “essential KPIs” for the success of the overall company: Productivity - Node.js “is a very productive stack”, says Dias. “The first time I sat down and wrote a small backend system in Node, I was shocked by how productive I was and how much more capability I could get done compared to any other technology stack at the time.” Revenue – “Being able to bring a solution quicker to market with more features will help grow your revenue”, says Dias. In a discussion with Open Source Blog, John Papa, then Microsoft technical evangelist, listed various additional factors behind Microsoft’s move to Node.js, including: The engaged and welcoming community around Node.js; The ability to add on what you want using middleware, which makes it attractive to JS frameworks such as Vue, React, Angular, which each have their own CLIs that run on Node.js, helping developers build, generate and test; The ecosystem value that npm provides – according to Papa, it’s “a huge influence over why Node.js has been so successful” as the number of packages available allow you to do “just about anything you need or want to do.” Using Node.js eliminates the need for context switching – constantly working in the same technology, he says, “keeps you in the same mindset.” Adding, “anybody who’s ever started working on something involved or complex, you know how important it is to not get pulled away from it and then back in. Keeping your context consistent is a huge reason why full-stack JavaScript has been successful.” LinkedIn LinkedIn has over 700 million global users in its business-focused social network in over 200 countries and territories worldwide. It too was an early large-scale user of Node.js. In fact, LinkedIn first adopted Node.js when it was barely a year old. The engineering team was drawn to Node.js to rebuild its core mobile services when they were unable to scale Ruby on Rails, and they had run into trouble working with microconnections in the mobile app. They decided to go with Node.js for the mobile app’s backend and the switch achieved impressive results (“thinner, lighter, faster”, according to Kiran Prasad, Senior Director of Mobile Engineering at LinkedIn). These included: Much better performance – app running up to 20x faster Using a fraction of the resources on the server-side – much smaller memory footprint The move to an asynchronous evented system “enabled us to move to a model where the client makes a single request for a page” as “the code was simplified and we moved to stateless servers” (Deepank Gupta, Senior Software Engineer) Improved unity between frontend and backend mobile teams – to the extent they were combined into a single unit The engineers felt comfortable coding in JavaScript since it is a language used so widely Another major improvement they saw was in terms of developer productivity. Development time in Node.js was very fast. “It was fast by all standards,” LinkedIn’s mobile development lead Kiran Prasad told VentureBeat. “I’ve worked at startups and big companies like Yahoo, and yeah, it was fast.” It only took two to three hours to write the Node.js prototype. He added that it was also fast because it used HTML5 in the web app and the engineering team were able to reuse much of the same code in the native applications for iOS and Android. This allowed the LinkedIn developers to focus more on application development as opposed to firefighting. Walmart As one of the world’s biggest retailers, Walmart had an operating income of around $20.6 billion in 2020. Its CEO, Doug MacMillan, has described it not just as a retailer with stores located within ten miles of 90% of the US population, but a technology and “an innovation company.” During 2018, the retailer spent $11.7 billion in technology investment, making it the third largest IT spender in the world behind Amazon and Alphabet. Like most of the other enterprises listed here, Walmart was one of the forefront users of Node.js. Early adoption efforts included: The hapi framework, initially developed to handle the scale of Black Friday; and Electrode, a platform for building universal React/Node.js applications with a standardized structure, best practices and modern tech baked in. Electrode accelerated the use of Node.js within Walmart Labs, and now powers most of the Walmart eCommerce site. Walmart Labs decided to make a number of adaptations to Node.js, while implementing Electrode, including: Developing a new approach to allow teams to deploy any version at any time Creating a zip artifact of the application with settings manifest built-in, pushed to Walmart’s internal Nexus repository As Walmart migrated its web applications to Node.js, engineers found its own open-source product, OneOps, to be “the natural choice to host our platform.” Walmart uses OneOps to implement hosting, deployment and continuous integration (CI) for its Node.js applications. Walmart also turned to Node.js for its mobile applications. Ben Galbraith, VP for mobile engineering, said the company did so because they wanted to create “a website that would be rich and dynamic… on devices that weren’t too powerful.” On its re-engineered Node-powered mobile app, all front-end code is executed on the back end. Don Almaer, VP for Mobile Architecture, says, “We rely on services all over the world. We do not control all of these services. Node allows us to front all these services… and scale up very nicely. It’s perfect for what we’re doing in mobile.” Why has Node.js become the standard for Enterprise Applications? Other enterprise companies that have adopted Node.js include GoDaddy, IBM, eBay, Twitter, Yahoo! and Groupon. These are the main reasons why Node.js has become so popular for enterprises: Highly performant (one of the most efficient server-side frameworks); Powered by Google’s V8 JS engine, which enforces runtime speed; Scales easily; Written in JavaScript: No more context switching between languages when writing across the stack; JS benefits from improvements in language and runtime design compared to traditional backend languages such as Python and PHP; Many popular languages compile/convert into JS, so you can use TypeScript, Scala, LiveScript, etc.; Software development is fast; Event-driven – a strong solution for heavy I/O operations, data flow, and multiple connection requests; Portability – available across all major operating systems, and well-supported by many web hosting providers; npm provides access to hundreds of thousands of reusable packages to extend Node.js and create feature-rich enterprise web applications; Open source – there are nearly 3,000 contributors to the Node.js runtime alone; and It has a robust, engaged and welcoming community."
"203","2020-08-21","2023-03-24","https://www.section.io/blog/five-use-cases-nodejs-edge/","In our last post about Node.js, we looked at five enterprise apps that use the server-side runtime. Many tech leaders, including Uber, Walmart and Netflix, to name a few, have adopted Node.js largely for two reasons: speed and scale. In this post, we will identify five use cases for Node.js at the Edge. With each use case, we will look at what it is, how Node.js is different in its implementation from other JavaScript frameworks, related packages, and real world examples. Five Best Use Cases Node.js offers many advantages, including a faster development process, because it allows developers to use JS across the stack, uniting the language and data structures (JSON). It was designed with low latency and streaming in mind, and is an optimal choice for certain types of web applications. These include: Micro APIs Node.js can be used to build small, targeted APIs for simple use cases such as search and logging of user-tracking data. Node.js enables simpler implementation for handling micro APIs compared to alternatives like Ruby on Rails, for several reasons, including the ability to: Simply expose your JSON objects with a REST API for the client to consume; Avoid having to do multiple data format conversions, by using a uniform data serialization format across the client, server, and database; Node.js allows you to push the database writes off to the side to deal with later; and The system maintains responsiveness under heavy load. There are many Node.js API frameworks available offering the ability to extend the capabilities of Node.js and abstract away some of the more complicated aspects of development. Most Node.js frameworks offer fully-featured content exploration, support for customization, flexibility, extensibility, etc. Express is the most popular Node.js framework, which describes itself as a “fast, un-opinionated, minimalist framework for Node.js.” Its focus is performance and providing exactly what you need. Other options with impressive API generators include Sails, Feathers, and Meteor. Single Page Application (SPA) Hosting In Single Page Applications (SPA), the entire application essentially fits onto a single page to provide a desktop app-like experience. SPAs have become a standard approach for making engaging user experiences. Node.js offers the ability to combine dynamic content and code features along with static file delivery of the SPA. Two examples of major enterprises using Node.js as a tool to build a single page web application are Netflix and Trello. Trello found Node.js to be a good fit since “we knew we wanted instant propagation of updates, which meant that we needed to be able to build a lot of open connections, so an event-driven, non-blocking server seemed like a good choice.” Node.js is able to efficiently handle the asynchronous calls and heavy I/O operations required for building SPAs. This type of app benefits from Node.js’ asynchronous data flow to enable seamless data updates due to the event load as the page receives new data without having to refresh. Node.js is also a particularly effective fit for SPAs with data-driven single page applications that are using the server to talk to a backend framework (such as GraphQL, Redux or Apollo), while the client-side does all the HTML rendering. Since the server-side and client-side use the same languages, developers don’t need to context switch and are able to use the same language structures and approaches for the full stack, saving time and resources. This leads to better maintainability, as well as faster development time. Server-side Rendering Server-side rendering allows an application to render the web page on the server instead of in the browser, sending a fully rendered page to the client whereby the client’s JS bundle takes over, allowing the SPA framework to operate. By leveraging server-side rendering at the network edge, you can: Deliver faster page load times, providing a better user experience; Improve SEO (low latency) and correctly index web pages; Reduce load on your origin servers; Experience assist with loading the page when the user has a slow Internet connection or an outdated device; and Leverage easier social sharing - SSR provides a featured image and elaborate snippet when your website’s content is shared over social media. Using Node.js for SSR delivers different advantages to the typical examples of Node.js applications, namely, its library support and browser characteristics as opposed to its concurrency model. One enterprise example of this type of use is Airbnb, who a couple of years ago, migrated from Ruby on Rails to “a new service that will deliver fully formed, server rendered web pages entirely in Node.js”. The goal was to render almost all the HTML used for the Airbnb product. AirBnB’s developer team found they were able to leverage off-the-shelf components with which they had existing operational experience to address any anomalous behavior thrown up by using SSR with Node.js. Edge Hosting Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Learn more and get started on a free plan Static Site Hosting Node.js can be very useful for serving static content and precompiled applications at the edge. Serving static files essentially involves serving your HTML, CSS and JavaScript pages as they are. They are called static because they are not changed by the server nor run, they’re simply sent back as files for your browser to parse through. Node.js is compatible with many static site generators (SSG), such as Gatsby.js, Next.js, Nuxt.js, and others. A static site generator allows you to use any framework, but the end result will still be simple, lightweight HTML and CSS. The advantages of static vs. dynamic include: Speed (no database queries to run, no templating and no processing on every request); Version control for content (in a static site, the content is usually stored in flat files and treated as any other component of the codebase as opposed to in a dynamic site where content usually sits in a database elsewhere, separate from the codebase and its version control system); Security (there are fewer opportunities for security vulnerabilities); and Handling unexpected traffic surges (since serving static HTML pages only consumes minimal server resources). Create Real-Time Applications When Node.js was built by Ryan Dahl, he was aiming to create real-time websites with push capability inspired by applications like Gmail. So, what was so revolutionary about that back in 2009 when Node.js first came out? After twenty years of stateless web based on the stateless request-response paradigm, Node.js provided the first web application with real-time, two-way connections where both the client and server are able to initiate communication, allowing them to freely exchange data. This was in contrast to the standard web response paradigm, where the client initiates communication. Node.js is also based on the open web stack (HTML, CSS and JS) and runs over the standard port 80. Examples of real-time applications where Node.js shines include online games, collaboration tools, video conferencing, document sharing and chat, among many others. These types of applications have numerous users performing heavy I/O operations, meaning the risk of overloading the server is high. Node.js allows the collaboration environment to update seamlessly with the help of Event API and WebSockets. In looking at a specific example, the chat application is a sweet-spot example for Node.js because it is: Lightweight High traffic Data-intensive (although has a low processing/computation rate) Runs across distributed devices Doing this on alternatives like Ruby on Rails or Django would create too much load on the server because each active client eats up one server process. With Node.js, the server doesn’t need to maintain separate threads for each open connection. It is also simple compared to traditional multi-threaded Java or ROR frameworks; therefore, you can create a browser-based chat application in Node.js that uses minimal resources to serve many clients. There are many more applications perfect for Node.js, including streaming apps, the Internet of Things (IoT), rewriting body content, and redirecting segments of users to different backends. With so many developers opting to build on Node.js, more attention is developing around the benefits of hosting Node.js applications at the Edge. Section’s Node.js edge hosting solution is helping developers realize these benefits across the various use cases discussed above, and beyond. If you’ve got a specific Node.js + Edge use case you’d like to explore, we’d love to chat."
"204","2020-05-22","2023-03-24","https://www.section.io/blog/sustainability-edge-computing/","The tech sector has been under mounting scrutiny over the last decade by environmental groups, businesses, and customers for its increasing energy consumption levels. Attention has recently been turning to the ways in which edge computing can help build more sustainable solutions. The Impact of Coronavirus Lockdowns on Internet Usage While worldwide energy consumption has significantly dropped as a result of global lockdown restrictions, Internet usage has seen a huge spike. NETSCOUT, a provider of network and application performance monitoring tools, saw a 25-35% increase in worldwide Internet traffic patterns in mid-March, the time when the shift to remote work and online learning happened for much of the world’s population. That number has stayed pretty consistent. There has been particularly increased use of bandwidth-intensive applications like video streaming, Zoom and online gaming. The spike in Internet use has implications for the sector’s sustainability targets and makes the urgency of a call for change even louder. Before we look specifically at sustainability in the edge computing field, let’s pull back to take a look at the wider sector. The Energy Challenge The information and communication technology (ICT) ecosystem accounts for over 2% of global emissions, putting it on a par with the aviation industry’s carbon footprint, at least according to a 2018 study. Did you know that data centers use an estimated 200 terawatt (Twh) hours each year? This represents around 1% of global electricity demand and data centers contribute around 0.3% to overall carbon emissions. Where these figures could lie in the future is harder to predict. Widely cited forecasts say the ICT sector and data centers will take a larger slice of overall electricity demand. Some experts predict this could rise as high as 8% by 2030, while the most aggressive models predict that electricity usage by ICT could surpass 20% of the global total by the time a child born today hits his or her teens, with data centers using over one-third of that. Keeping Future Energy Demand in Check However, improvements in energy efficiency in the data center field are already making a difference. While the amount of computing performed in data centers more than quintupled between 2010 and 2018, the amount of energy consumed by data centers worldwide only rose by 6% across the same period. The ICT sector has been hard at work to keep future energy demand in check in various ways. These include: Streamlining Computer Processes The shift away from legacy enterprise data centers operated by traditional enterprises such as banks, retailers, and insurance companies, to newer commercially operated cloud data centers has been making a big difference to overall energy consumption. In a recent company blog, Urs Hölzle, Google’s senior VP technical infrastructure, pointed to a study in Science showing the gains in energy efficiency. Hölzle wrote, “a Google data center is twice as energy efficient as a typical enterprise data center. And compared with five years ago, we now deliver around seven times as much computing power with the same amount of electrical power.” The study shows how an overall shift to hyperscale data centers has helped reduce the amount of traditional enterprise data center capacity and subsequently reduced overall energy consumption. One of the authors is Jonathan Koomey, a former Lawrence Berkeley National Laboratory scientist, who has been studying the subject of data center energy usage for over two decades. Koomey says that forecasts that focus on data growth projections alone ignore the energy efficiency gains the sector has been making. Facebook has also been exploring ways to maximise efficiency over the last decade. The social media giant started the Open Compute Project in 2011 to share hardware and software solutions aimed at making computing more energy-efficient. One recent novel idea is that data centers could act as energy suppliers and sell excess electricity to the grid instead of keeping it as an insurance policy. Kevin Hagen, Iron Mountain’s VP of environment, social and governance strategy, describes backup generators and UPS batteries as “useless capital.” Instead, he asks, “What would it look like if all that money was actually invested in energy systems that we got to use when we were trying to arbitrage energy during the day and night on a regular basis? What if we share control with the utility, so they can see there’s an asset and use it back and forth?” New Ways to Cool Data Centers According to Global Market Insights, cooling systems represent on average 40% of entire data center energy consumption. Data center design specialists have been looking into different ways to approach reducing energy needs specifically for cooling, which have been largely approached in the same way for the last three decades. These include locating data centers in cooler areas, using AI to regulate the data center’s cooling systems to match the weather, warm-water cooling, immersion cooling, and rear door cooling systems. It’s an important problem to be focused on since data centers worldwide contribute to industry’s consumption of 45% of all available clean water. A Focus on Sustainables Greenpeace has been putting data centers under the spotlight for over a decade, calling for them and other digital infrastructure to become 100% renewably powered. There has been great progress since 2010 when Greenpeace started its ClickClean campaign when IT companies were negligible contributors to renewable-power purchase agreements; today Google is the world’s largest corporate purchaser of renewable energy. Apple was at the top of Greenpeace’s list of the top greenest tech companies last year. It already boasts 100% renewable energy to power all its global data centers, and is currently focused on cleaning up its supply chain. Greenpeace has recently called out AWS, however, for a lack of renewables in Virginia’s “data center alley”, which it says is powered by “dirty energy.” Corporate data centers have traditionally been part of the “dirty energy” problem, but are beginning to participate in seeking out renewable energy. Some of the initiatives in this space include DigitalRealty’s announcement on using wind energy in its 13 Dallas, TX data centers and IronMountain’s Green Power Pass specifically for enterprise customers to be able to participate in renewable energy purchase for its growing number of data centers. The Role of Edge Computing in Working Towards More Efficient Solutions Edge computing is also increasingly being looked to as an area that can help work towards more efficient solutions. These include: Energy Resource Efficiencies A technical paper published on Arxiv.org earlier this month lifted the hood on Autoscale, Facebook’s energy-sensitive load balancer. Autoscale reduces the number of servers that need to be on during low-traffic hours and specifically focuses on AI, which can run on smartphones in abundance and lead to decreased battery life through energy drain and performance issues. The Autoscale technology leverages AI to enable energy-efficient inference on smartphones and other edge devices. The intention is for Autoscale to automate deployment decisions and decide whether AI should run on-device, in the cloud, or on a private cloud. Autoscale could result in both cost and efficiency savings. Other energy resource efficiency programs are also underway in the edge computing space. Reducing High Bandwidth Energy Consumption Another area edge can assist in is to reduce the amount of data traversing the network. This is especially important for high-bandwidth applications like YouTube and Netflix, which have skyrocketed in recent years, and recent months in particular. This is partly due to the fact each stream is composed of a large file, but also due to how video-on-demand content is distributed in a one-to-one model. High bandwidth consumption is linked to high energy usage and high carbon emissions since it uses the network more heavily and demands greater power. Edge computing could help optimize energy usage by reducing the amount of data traversing the network. By running applications at the user edge, data can be stored and processed close to the end user and their devices instead of relying on centralized data centers that are often hundreds of miles away. This will lead to lower latency for the end user and could lead to a significant reduction in energy consumption. Smart Grids and Monitoring Can Lead to Better Management of Energy Consumption Edge computing can also play a key role in being an enabler of solutions that help enterprises better monitor and manage their energy consumption. Edge compute already supports many smart grid applications, such as grid optimization and demand management. Allowing enterprises to track and monitor energy usage in real-time and visualize it through dashboards, enterprises can better manage their energy usage and put preventative measures in place to limit it where possible. This kind of real-time assessment of supply and demand can be particularly useful for managing limited renewable energy resources, such as wind and solar power. Examples in the Wild Schneider Electric Schneider Electric, a specialist in energy management and automation, is beginning to see clients seek more efficient infrastructure and edge computing solutions. This includes helping organizations to digitize their manufacturing processes by using data to produce real-time feedback and alerts to gain efficiencies across the supply chain, including the reduction of waste and carbon emissions. Natalya Makarochkina, Senior VP, International, Secure Power at Schneider Electric, says edge computing is allowing this to happen in a scalable and sustainable way. Makarochkina highlights two Schneider customers that have used edge computing to improve sustainability and make efficiency savings. The first is a specialty grocer that used edge computing to upgrade IT infrastructure and reduce its physical footprint and consumption requirements, which led to a 35% reduction in engineering cost and a 50% increase in deployment speed for the end user. The second is a co-working space provider that used Schneider’s edge infrastructure options to launch an on-site data center that offers IT infrastructure on demand, offering tenants greater operational efficiencies. “Edge computing and hybrid cloud play a pivotal role in sustainability because they are designed specifically to put applications and data closer to devices and their users. This helps organisations to better react to changes in consumer demands and improve processes to create more sustainable products.” - Natalya Makarochkina, Senior VP, International, Secure Power at Schneider Electric Section At Section, we’re working to provide more accessible options for developers to build edge strategies that deliver on sustainability targets. Our patent-pending Adaptive Edge Engine technology allows developers to piece together bespoke edge networks that are optimized across various attributes, with carbon-neutral underlying infrastructure included in the portfolio of options. Other attributes include cost, performance, and security/regulatory requirements. Similarly to Facebook’s Autoscale, Section’s Adaptive Edge Engine tunes the required compute resources at the edge in response to, not only the application’s required compute attributes (e.g. PCI compliance or carbon neutrality), but also continuously in response to traffic served and performance. Our ML forecasting engine sits behind the workload placement and decision engine which continuously places workload and scales the underlying infrastructure (up and down) so we use the least amount of compute while providing the maximum availability and performance. Summary Not many would argue that technology’s immersion in our day-to-day lives is only expected to expand. Technology providers must work together to create sustainable solutions to meet these growing demands. While not an end-all solution, edge computing has the potential to play a key role in helping to control the negative impacts that accompany rising energy demands."
"205","2020-05-07","2023-03-24","https://www.section.io/blog/jamstack-edge-computing/","In recent years, there has been a growing movement in the developer community to use static site generators to build websites, a trend that harkens back to the early days of the Internet when a website was simply a folder of HTML documents that consisted of a handful of tags. We gradually moved towards dynamic sites with graphical headers and more complex navigation, which required a great deal more technical skill to author. Content management systems (CMSs) then emerged, such as WordPress and Drupal, to help non-technical users manage their own content and control a site’s design and display. However, CMSs have developed a reputation for being cumbersome and slow to work with, and often struggle with issues around scale and security. In response, there has been a resurgence in static sites due to their simplicity, security benefits, and ability to quickly serve content. Today’s static site generators offer a better developer experience, based on the most modern technology and JavaScript frameworks such as Vue.js and React. The JAMstack: What is it? The Jamstack, a term coined by Mathias Biilman, the CEO and co-founder of Netlify, is gaining traction with frontend developers. The Jamstack isn’t a tool or a programming language, but a modern web development architecture based on client-side JavaScript, reusable APIs and a pre-built Markup. According to Jamstack, it represents “a new way of building websites and apps that delivers better performance, higher security, lower cost of scaling, and a better developer experience.” While Jamstack highly encourages as much pre-built markup as possible, good for speed and SEO, prebuilt markup isn’t a requirement. One of the keys to lower latency is the ability Jamstack offers to serve pre-built files over an edge compute platform. When deployment is a stack of files that can be served anywhere, scaling simply involves serving those files in more places. By serving Jamstack projects directly from the edge, significantly faster speeds and superior performance can be unleashed. JavaScript, APIs and Markup: The Jamstack Foundations Why the name? Jamstack stands for JavaScript, APIs and Markup, which can be thought of as the Jamstack foundations. JavaScript Dynamic programming during the request and response cycle is handled by JS, entirely on the client-side. You can use pure JavaScript or any other framework or library available, such as React or Vue. APIs APIs allow you to use backend functionality without a database or backend engine on your server. All server-side processes or database actions are abstracted into reusable APIs, accessed over HTTPS with JavaScript. You can use public or private APIs. They can be custom-built or leverage third-party services. Markup Templated markup should be prebuilt at deploy time, typically through the use of a static site generator for content sites, or a build tool for web apps. You can write your own HTML and CSS code You can adopt any framework, such as Hugo, Jekyll, or Gatsby, to build the presentation layer of your site. Note, however, the Jamstack doesn’t have to include all these. It might be any site built with a static site generator, such as Jekyll, Hugo or Gatsby. A list of example sites can be found here. What Jamstack sites have in common is not being dependent on a web server. The Benefits for Developers The Jamstack has been rapidly gaining traction with JavaScript developers. Let’s look at why. A More Straightforward Deployment Process Jamstack looks back to the days when most websites on the Internet were static and the deployment process was straightforward. There’s no need to send files via FTP with Jamstack. Instead, deployments are git pushes of a dist folder, allowing you to publish a new version of your website by executing a single command from the terminal. Time Savings One of the biggest benefits for developers is the time saved at each step of the development process. Code doesn’t need to be written from scratch. The deployment process is more straightforward as you don’t have to consider the database or backend. You simply deploy a static website on which most of the content is already present. Loose Coupling and Separation of Controls Developers can benefit from loose coupling and separation of controls, enabling more targeted development and debugging. You build with cloud functions when you need to do server-side code since you don’t have a traditional server-side language to reach for. Overall Benefits Faster performance Infinitely scalable Higher security (due to reduced attack surface area) Less expensive Rebuilt automatically every time content is updated Content is served directly to the edge A New Type of Frontend Development The Jamstack is part of a wider revolution in the frontend development ecosystem, which has been rapidly gaining momentum over the last few years (check out our recent article on micro frontends). Fundamentally, frontend development has massively changed in several key respects. Git has largely replaced FTP, not just for version control, but also as the main way that developers collaborate, communicate and publish. Browsers have become much more powerful, going from being merely document viewers to application run times. The API economy has exploded with microservices, which you can talk to directly from the browser. Modern JavaScript and a world of frontend build tools has exploded and changed what the daily workflow of any frontend developer looks like. Another developer trend that’s part of this is the movement towards web components. Components look the same across channels: web, mobile and desktop, and enable the trend toward design systems, which keep a consistent company style across all products and web properties. For frontend teams, web components allow them to move much more quickly since they are universal and re-usable. Component systems, micro frontends and the Jamstack are all part of the overall lowcode/nocode trend. Edge Computing and the Jamstack One of the main advantages to building sites with this approach is you can move away from the concept of having a single origin. This means you no longer need to rely on a single point of failure. It also means you can move away from having to do a round trip to your server in the geography where it is based. If the server is in New York but your customer is in Tokyo, with a single server, the Tokyo-based customer will have to wait longer before they see the initial HTML. Since Jamstack projects don’t rely on server-side code, instead of living on a single server, they can be distributed. In a Jamstack architecture, a page request doesn’t need to hit an origin server for HTML. Instead, it fetches HTML from a distributed edge network where the HTML file has been pre-built and is already primed to be downloaded. Serving as much of your app as possible directly from the edge unlocks extremely low latency and powerful performance. It also makes horizontal scaling significantly more straightforward since an edge platform can serve many more requests concurrently, and there is no need to worry about the load capacity of a server or database during traffic peak periods. It can also boast security benefits. For instance, access to an end server can be verified directly at the edge using JSON Web Signature (JWS). This means requests can be made secure without extra load time from a full round trip to authentication servers. The architectural framework of the Jamstack is a natural fit for edge computing. Using an edge compute platform like Section, frontend teams don’t need to worry about how to stand up infrastructure or orchestrate workload deployment to the most optimal locations. The edge platform abstracts these complexities, so frontend teams can concentrate on improving user experience and building the next great product."
"206","2020-03-31","2023-03-24","https://www.section.io/blog/micro-frontend-architecture-edge-computing/","On a recent episode of The InfoQ Podcast, Section VP of Technology Wesley Reisz chatted with Luca Mezzalira, VP of Architecture for DAZN, to discuss how their team has approached micro frontend development. This interview really brought into focus the parallels between the progression of micro frontend development and the evolution of edge compute. Building on the concept of microservices, the micro frontend movement has gained a lot of momentum in recent years. As organizations like Spotify, Upwork, and HelloFresh have harnessed this popular frontend design pattern, we’re seeing growth in adoption rates within the wider technology community, particularly among organizations with larger teams and code bases. By migrating away from monolithic structures, larger teams are able to iterate more quickly without stepping on each other, ultimately resulting in higher velocity. What is a micro frontend architecture? A micro frontend architecture is an approach to building an application modeled after some of the benefits found in microservices. Many organizations have adopted microservices to avoid the limitations of large, monolithic backends and to enable teams to scale the delivery of independently deployed and maintained services (each with potentially their own velocity, release pipeline, and technology). Today, microservices is a well known style of building server-side software; however, many companies continue to struggle with monolithic frontend codebases, which can offset the benefits that microservices offer. Furthermore, for web applications, the frontend is continually increasing in importance. Weld (a web app/creation tool), for instance, is 90% frontend code with an extremely light backend. Its CEO, Tom Söderlund, says having a monolithic approach to a large frontend app becomes unwieldy. Good frontend development is not easy. Scaling frontend development to allow multiple teams to work simultaneously on the same large and complex product is even more difficult. Micro frontends have recently emerged as a way to emulate the success of microservices, splitting frontend monoliths into a set of loosely coupled applications which can act independently. These applications can then be assembled together to create a single, user-facing application. What are the benefits of micro frontends? There are various benefits that can accompany micro frontends, including: Breaking frontend monoliths into multiple smaller pieces can reduce coupling of frontend components and improve overall management of the system; Smaller codebases are typically simpler and easier for developers to work with, avoiding the complexity that can develop from unintentional and inappropriate coupling between components that shouldn’t know about one another; A micro frontend architecture can enable decoupled teams to develop frontend code autonomously, increasing their efficiency and effectiveness; Independent deployability of micro frontends reduces the scope of a set deployment and accordingly, associated risk; Technology choices, codebases, teams, and release processes should be able to operate independently of one another and evolve under their own steam instead of requiring excessive coordination; Upgrades, updates, or even rewriting parts of the frontend, including architecture, dependencies, and user experience, can be achieved incrementally and performed when it makes sense instead of having to stop everything and upgrade all at once. A word of caution Micro frontends, much like the microservices on the backend that they’re modeled after, suffer from some of the same challenges. While the use of micro frontends come with the benefits described above, they can also increase the complexity and coordination requirements of your app. Just as you should be using microservices to solve a specific problem (such as enabling independent velocity of different backend teams), implementing micro frontends should also be leveraged to solve a specific problem they’re designed to address. The phrase “use the right tool for the job” is particularly important in microservices and micro frontends. A real world micro frontend example To break micro frontend architecture down further, it’s helpful to relate these concepts to a real world example. This article from Cam Jackson describes a practical micro frontend example – a website on which customers order takeout. There are multiple pages: a home page where customers can browse and search for restaurants; a page for each restaurant with a menu, discounts and special requests; and a profile page for customers that lets them see their order history, track delivery time, and select their payment options. Each page is complex enough to justify a separate team for each one, and each of those teams should be able to work on their page independently of the others. They want to be capable of developing, testing, deploying, and maintaining code without having to be concerned about conflicts or coordination with the other teams. The customer experience, however, should be of one single, seamless website. There is typically a micro frontend for each page in the application, and there is a single container application, which takes care of common page elements and cross-cutting concerns. The container application also brings the different micro frontends together onto the page and tells each one when and where to render itself. The container application and the micro frontends integrate together using JavaScript. You can see the end result deployed live at https://demo.microfrontends.com, and the full source code can be seen on Github. Micro frontends at the Edge When components are decoupled, it makes them easier to move around independently and you can, therefore, more easily act on optimization decisions. Edge computing promises better user experiences by processing workloads in closer proximity to end users. However, not every workload in an application is an ideal fit for the Edge. For example, complex processing that is not latency sensitive, such as long-term analytics and modeling, is typically more suitable to run in centralized infrastructure (i.e. cloud, data center). In micro frontend architectures, developers have more flexibility to make determinations around where workloads are most suitable to run. Case Study: Deploying a Nuxt app at the Edge Adore Beauty, Australia’s longest running online beauty store, started to work with the Section platform several years ago to optimize performance and reliability during the busy holiday sales season. Following impressive sales lift results using simple site acceleration techniques and queueing, the tech team at Adore Beauty began to work with Section on running their Nuxt application at the Edge, closer to their end users. With Magento at its core, the Adore Beauty application is broken into various microservices that allow them to decouple functionality between frontend display, product information, shopping cart functions, etc. As such, they have the flexibility to move components independently to optimize application performance, security, and scalability. As a result of leveraging Section’s completely containerized Node JS module, the Adore Beauty technical team could focus on writing code that optimizes the user experience instead of worrying about the complexities associated with hosting, deploying, and scaling. “It means our team can stay lean and focused,” says Gareth Williams, CTO, Adore Beauty. “We can add value to the customer by pulling together all of our microservices and relying on Section to do the scaling.” Getting the Nuxt app deployed as close to the end user as possible allowed the Adore Beauty team to benefit from increased speed with lower redundancy, and gain access to Section’s engineering expertise. Going forwards, Adore Beauty want to migrate as many of its microservices as possible over to Section for hosting and scaling. Edge compute platforms, like Section, abstract the backend complexities of managing deployment of these components along the edge continuum. The benefits are similar to those gained by a micro frontends approach: increased flexibility, scalability, and greater autonomy for your team."
"207","2020-04-06","2023-03-24","https://www.section.io/blog/saas-trends-covid-19-pandemic/","Last November, Gartner forecasted worldwide public cloud revenue would grow 17% across 2020. Software-as-a-Service (SaaS) is at the center of this growth. Gartner predicted the SaaS market would swell to $116 billion this year due to “the scalability of subscription-based software.” According to Synergy Research Group, overall SaaS spending hit the $100 billion annual run rate last summer. SaaS has been the most buoyant element of the cloud market for some time and cloud usage is certainly soaring right now with companies and consumers turning to the cloud and its enabling technologies to fuel the massive shift to remote work, entertainment and communication. Public cloud has so far done an amazing job of meeting unprecedented and unplanned-for demand in a way that simply would not have been possible a decade ago. Within the volatile environment of the COVID-19 pandemic, we were curious to pull back and look at some of the trends going on in SaaS right now and predict how they might unfold across the rest of the year and beyond. TREND 1 Huge surge in demand for SaaS collaboration tools due to remote being the “new normal” Just recently, Microsoft had to apologize for a blog post, which claimed it had seen a “775 percent increase of our cloud services in regions that have enforced social distancing or shelter in place orders.” In a clarification issued a couple of days later, the tech giant explained the almost 8x increase was only connected to monthly users of Microsoft Teams, its collaboration platform used for video, and only for a one-month period in Italy. Nonetheless, the company said there has been a rapid acceleration of demand for Microsoft Azure services worldwide with Teams adding 12M users in one week during mid-March. As is clear from the news, many SaaS collaboration tools are seeing a huge spike in demand. Zoom has likewise been experiencing unprecedented growth with its daily users worldwide more than quadrupling. The video conferencing app is being put to use not only by businesses implementing remote-work tools for the first time, but also schools offering online classes, and everyday consumers turning to it for group video chats with friends and family. Slack said it had also broken user records as demand hugely surged for its chat app over the course of March. While there are clear concerns about the impact of coronavirus on the economy, including in the tech sector, as a remote-first company, we can’t help but be curious about the spike of investment in digital transformation tools that will not only allow mission-critical business operations to continue in the short term, but could also help facilitate a transition to the digital workplace in the “new normal” following the pandemic. TREND 2 Low and no code platforms are “red hot” (Forrester) as enterprises look for new ways to build more software more quickly According to Forrester, low and no code tooling is catching on with developers in a big way. Half of the developers they surveyed said they have adopted or plan to adopt low-code tools. No or low-code tools are increasingly becoming available to non-technical staffers with the ultimate goal of bringing business and product closer together and enabling faster iteration. We’ve just seen this vividly in action with no-code software start-up Unqork being used by the city of New York to build and launch a coronavirus crisis-management software platform in a matter of days. The city was able to go live with the service in 72 hours, using only visual drag-and-drop tools to create it. The online portal can be customized for other cities, counties or states. While there’s a large number of small and mid-size players, such as Mendix that already hold a strong market position, the big three cloud vendors are in the midst of working out how to dominate the low code platform space and win the next “cloud battle”. TREND 3 Technical know-how will be increasingly required across virtually all aspects of the enterprise According to a survey from late last year by digitally-native technology services company Globant, while a majority of companies were working toward strengthening digital strategies and said collaboration was a top priority, fewer than 10% of employees were actually involved with digital transformation efforts. Furthermore, only around a third of companies said they saw themselves as “innovative” and rated their digital maturity as “cutting edge”. Many companies have been forced to hasten digital initiatives with the move to “all remote” and some will be questioning whether digital transformation will become a new ongoing priority rather than just a temporary one. One method to achieve ongoing digital transformation company-wide is “dev diaspora”, a term used by Forrester in its 2020 software development predictions, to envisage more technologists embedded within different company divisions. According to Forrester, developers will continue to move outside the IT side of the house and into line of business operations. Software is “too important to be left just up to the IT org.” TREND 4 Cloud integration is a growing priority for solution architects As many midsize and large companies adopt a multi-cloud or hybrid IT strategy (according to Gartner, over 75% of midsize and large organizations will have done so by 2021), they face the challenge of decentralization. Decentralization poses problems when you need to integrate applications, for example, to optimize business processes or gain real-time access to data across multiple systems and providers. In response, cloud integration has become a priority for solution architects. Different strategies for creating integration between cloud-hosted, SaaS and on-prem applications include: Integration platform software Integration platform as a service (iPaaS) SaaS vendor out-of-the-box integrations Custom code tailored to integration needs Function platform as a service (fPaaS) TREND 5 SaaS vendors increasingly use integrations to make their platforms holistic destinations Many companies use different software for different aspects of their business, which can make accessing, processing, and integrating data more challenging, and communication slower. Business efficiency can ultimately suffer from usage of too many SaaS applications. In response, SaaS vendors are increasingly offering integrations, either by third-party APIs or equipping their own software with integration capabilities. Slack is at the forefront of this. It’s currently in the midst of rolling out a major revamp and has gradually been announcing integrations with other vendors, most recently launching a new app to integrate the calling features within Microsoft Teams into the Slack platform. Alongside this integration, Slack is announcing VoIP phone integration with Zoom, Cisco Jabber, RingCentral, and Dialpad. This lets Slack users call on these VoIP calling providers to dial phone numbers directly from within the Slack interface. When Slack and Zoom announced their partnership last year, they hit the nail on the head when describing the reasons behind the integration: “It’s our shared belief that teams are better equipped to be nimble and responsive to change when they have a clear line of sight to their organization’s strategy and goals. But the only way that can happen is when teams have access to painless communication.”"
"208","2020-01-31","2023-03-24","https://www.section.io/blog/work-where-youre-awesome-remote-culture/","While many companies may hold similar philosophies when it comes to workplace flexibility, Section’s ‘work-where-you’re-awesome’ policy clearly sums up the intentions underpinning the remote work movement. Beyond attracting and retaining talent, companies that embrace workplace-neutral cultures are thriving. The Decision to Build a Location-Neutral Company Section explicitly set out to build a remote company. But along the way we have adjusted, recognizing that remote is not for everyone. Stewart McGrath and Daniel Bartholomew co-founded Section in Sydney, Australia back in 2012. Frustrated with the idiocy of the entire Sydney workforce hitting the roads from 7am-9am and 4pm through 6pm, Dan and Stewart set out to create a culture of trust with freedom. Trust to get the job done and freedom to get it done from the location which makes sense for the individual. In 2016, Boulder, Colorado became the company’s new HQ. At the time of the move, many of the Section team members remained in Australia. Our Australian team was working 100% remote, while our US team set up 100% in-office. This caused a number of communication issues as our in-office people discussed and agreed in different forums to our remote Australian team. We clearly had this wrong. On disbanding our US offices and moving back to 100% remote globally, we then found some of our team feeling disconnected and interested in coming into a shared office space. We learned that working from home is not necessarily the right solution for everyone. We needed to find a balance which was inclusive of a variety of personal needs. This gave birth to our work from where you are awesome policy. With some dedicated office space in Denver, Boulder and Sydney, our team now has the option to work from home, the road, or an office space. We just ask you to be awesome. In some respects, this also delivers a type of work-life blend that, today, Sectioneers have come to value. (We could write an entirely separate article on the ‘work-life blend topic’, but the essence is that we don’t think you should have to make a choice between your work and your life — it’s all just life, and we all deserve to thrive.) Over the past several years, the spread of physical locations among the team has further diversified, and our ‘work-where-you’re-awesome’ policy continues to feed our organizational health and growth. Why Remote Working Benefits DevOps Our interest was naturally piqued late last year when Senior Staff Engineer at GCPCloud Seth Vargo tweeted about the benefits of a remote-friendly culture: **Recruiter:** *We’re looking to attract top talent for our NY and SF offices!* **Me:** *No you’re not.* **Recruiter:** *?* **Me:** *Top talent doesn’t have a location. Invest in a remote-friendly culture or be honest that you’re only looking to attract “transient people who write good software”.* The Tweet set off a firestorm of discussion about the value of remote-friendly culture in the DevOps world. Seth Vargo, who works for Google’s cloud computing service from home in Pennsylvania, argued back to various detractors (and received the support of many others) about the benefits that the flexibility of remote work engenders, particularly in fostering a positive DevOps culture. The topic has been trending for several years. According to GitLab’s 2018 Global Developer Report, remote teams score higher in collaboration, visibility and DevOps satisfaction than in-office teams, leading GitLab to conclude that “a remote workplace culture is more conducive to DevOps adoption”. One of the reasons behind this, GitLab asserts, which we can attest to from our experience at Section, is that “without the convenience of physical proximity, working remotely requires a commitment to open discussion and an understanding that team members must be able to easily view projects and receive updates”. This leads to a culture that prioritizes visibility. According to the Global Developer Report, 67% of remote teams say they have visibility into the work of other team members compared to 57% of in-office teams. Any remote worker can speak to the need to communicate often and effectively to make sure that others are kept informed about their decisions and progress, and vice versa. As GitLab puts it, “without the convenience of physical proximity, working remotely requires a commitment to open discussion and an understanding that team members must be able to easily view projects and receive updates.” Furthermore, GitLab adds, because remote teams use tools that purposefully embrace concurrent work, the challenges of siloed workflows can be significantly reduced. This kind of open communication can help to build a culture based on trust and transparency, allowing DevOps teams to be more efficient, more collaborative, and “more easily adopt a model that fosters cross-functional communication and workflows”. Remote work structures have also been identified as a great way to build strong disciplines in the workplace. As Tomasz Tungaz notes, “Remote forces you to do the things you should be doing any way earlier and better“. We have invested time and effort in meeting discipline, communication structures and values. Quality of Life Benefits Our Section Engineer Interview Series has helped reinforce how much we all value the benefits that remote work can offer, both in terms of how it can foster a more innovative, collaborative approach to achieving results, as well as its impact on improving quality of life. Jason Stangroome, who’s lucky enough to live near the Australian shoreline, likes to start his day early with a walk along the beach, “ideally just as the sun is rising. Maybe get the shoes off and let the sand get in the toes and let the water come up. Maybe go get a coffee and just sit there and take it all in for a little while before I start my day.” Having the chance to sit and watch the waves with the sand beneath his feet offers a quiet time of reflection before getting stuck into the “bleeding edge tech we get to play with.” Glenn Slaven, meanwhile, lives out in the Australian bush with his wife, their four children, two dogs, two guinea pigs, two alpacas and a cat. “Working from home is awesome,” Glenn says. “I know it’s becoming more common in the industry, but there are still a lot of places that expect to see you face-to-face all the time; otherwise they don’t think you’re working. So this work from home thing is fantastic. It allows me to see more of my family. It allows me to live out here in the middle of nowhere, which I couldn’t do if I went and got a job in an office. I could, but I’d have to commute for two hours each way.” Along with working from home, Glenn’s favorite thing about working at Section is “the people… It’s a great company. We’ve managed to hire some really great people.” Pavel Nikolov based in the Sydney area enjoys the flexibility that remote working affords him to spend quality time with his son. He says his favorite time of the day is when he picks up his son from school and they cook dinner together, “every single evening. No exception.” Furthermore, Pavel is a night owl. “I’m more productive at night”, Pavel admits. “I don’t know why. I’ve always been like this.” Having a flexible workplace and hours allows Pavel to contribute during his most productive times of the day. Molly Wojcik, Section’s Director of Education & Awareness, recently relocated from the Denver area up to the Rocky Mountains in Steamboat Springs, Colorado. Molly says, “From a work perspective, I don’t really feel like much has changed. I’m just living a better life, which in turn, keeps me more energized and productive in my work.” As an outdoor enthusiast, Molly says, “Sometimes I’ll go do a sunrise ski with my dog before work, which is such a refreshing way to kick-start the day.” Maintaining a Strong Company Culture Even though it’s usually tomorrow in Australia when the US is online, we strive to maintain a strong company culture across our remote team. Regular virtual events, in addition to our day-to-day ops meetings include: How I Dev, Our Version of a Brown Bag Every two weeks, we host our (optional) How I Dev series, where one of our team members or a group of collaborators do an informal presentation on something that might have extended value for the group, or to share something cool that they’ve built. Topics vary from development best practices to deep technical dives (such as on Prometheus querying) to specific challenges and how they were overcome. It is often developer-focused, but not always. One popular session involved a show and tell of our personal workspaces, allowing a way for us remote workers to get a glimpse into each other’s office setups. How I Dev allows us to share best practices and learn from one another, as well as helps engender a strong sense of connection among the whole team. Virtual Happy Hour on Friday Afternoons Social events are important for strengthening team bonds, but with a globally distributed team, it can be challenging to gather for outside-of-work conversation. One ritual that we all look forward to every week is Friday Happy Hour — ours just happens to be virtual. Somebody typically starts a video session around 4pm on a Friday (in the local time zone) at which point many will grab their favorite beverage of choice (craft beer, tea, kombucha, Old Fashioned, …), and we’ll just shoot the s*!& for about 30-minutes. It gives us all a chance to wind down the week and connect on a different level. Shuffl Bot in Slack Something new that we’ve recently started trying out is the Shuffl bot in Slack. The app uses Slack usage data to randomly pair teammates who perhaps don’t interact very often, and encourage them to arrange a catch-up. While we’ve only gone through one cycle, early feedback has been very positive. Summary Beyond simply declaring a ‘work-where-you’re-awesome’ policy, fostering a positive, collaborative and productive environment requires mindfulness, both at the organizational level and the individual level. Yes, systems and tools help keep communication flowing and work progressing, but ultimately, it’s about being proactive with outreach, transparent and inclusive in communications, and trusting of each other’s commitment to growth."
"209","2020-09-24","2023-03-24","https://www.section.io/blog/section-engineer-interview-ivan-hamilton/","In this interview, Ivan talks about running 40 head of cattle at his family farm, firefighting with the New South Wales Rural Fire Service, racing his two Jackawawas Scuby & Sandy, and the challenges and rewards of working at the bleeding edge of technology. Ivan Hamilton, Software Engineer Can you describe what you do at Section? What I do at Section? Mmm… I do anything and everything. I’m one of the engineers and that means I either build, design or fix our core systems, the systems that are delivering the delivery. A tautology there! It’s essentially delivering the platform features. It’s a pretty varied role. I’ve had a broad career across networking, software development, systems engineering and business intelligence, so I often get called into things that cross boundaries… “This is crossing a few areas, get Ivan!” Of those areas, are there certain ones you especially gravitate towards? I gravitate towards the ones that other people can’t solve. It’s a blessing. I don’t really work very hard. I enjoy this stuff. It’s not a torment to come to work every day. It never has been. The neighbor’s kid, when he was 17 or 18, asked me what I did… For a lot of people that can be a difficult question, especially in the age of specialization that we are in now. I told him, imagine doing math exams for eight hours every day, that’s kind of what I do. I’m solving puzzles and problems, and trying to come up with answers to things. For me, that’s fun. For him, he looked at me as if he couldn’t imagine anything worse. What’s the most challenging aspect of your job? Currently, it’s that we use such a broad range of technologies, although we’re starting to move away from this a little. In the past, we used existing technologies and assembled them together. We’re now starting to build more of our own ground up componentry. That’s because we’re beginning to do things that are outside what anyone’s ever done. We push systems in directions that no one else seems to really do. No one else goes, yes, that’s a nice thing, I’ll have a million of them. Two years ago at KubeCon, I’d hear people talking about the size of the clusters they run and I’d think that’s tiny compared to the number of workloads we run at Section. They might run more machines, but we run more different workloads. We push things in directions that are unusual and because we’ve adopted existing technologies, we have so many different components from different areas in different languages, so you’re switching a lot. It’s probably similar to someone who’s multilingual. If they’re constantly changing languages, they might forget what they were just talking about. It’s like hey, I’m composing music, now I’m painting, now I’m developing dance. You eventually find yourself sitting at the piano with a paintbrush in a leotard. What keeps you motivated? I’m not sure there’s a single answer here. It’s probably a few main motivators. Curiosity - I’m like a small child constantly asking “Why?"". I love understanding something new. Satisfaction in growth of others - I get a real buzz out of helping others learn and develop. Be the change - It’s fruitless waiting for “someone” to change a situation. Become part of the new order you desire. Testing myself - Sometimes, I just like to see what I’m capable of. We’re all pretty amazing with some determination behind us. … Having said that, recognize limits - know when to walk away. What single piece of advice would you give fellow engineers? It’s funny because the IT thing sometimes attracts people who like dealing with machines more than people. Sometimes people think I’m just here to write the code and make the machine do it. On anything beyond a trivial effort, your code will get read and changed more times than you wrote it. You’re not actually working with the computer; you’re working with the people who will have to fix and understand your code going forward. So my advice is to work for the people who will have to work with your things next. Essentially, be a collaborator. At the end of the day, after those changes and fixes and people reading your code, the most value you give is how well you work with others, not how well the computer accepted what you wrote. You’ll often learn to code in an individual setting, only to find out later, you need to actually write the code for another human to read. There’s something in the joke - write code as if the person who has to maintain it is a violent psychopath who knows where you live. That’s how you should be writing it. Write it for the next person. Can you describe where you live/work. It’s fairly remote, right? Everyone has a different version of what remote is for them. Home for me is about 200 acres around 50 miles from Sydney. The land has been in the family about 200 years now. My mother was born in this house and I grew up not far away. We used to come down here as kids when it was my grandfather’s place to help him with whatever he was up to. When my grandfather passed away, my parents wanted to move back. They felt civilization was encroaching a little too much where they were. About ten years ago, my brother decided to build a house on the property too. Six years ago, I was living in Sydney but visiting out here, loving being back on property and doing physical things, working with the animals, fixing fences and machines… It was strangely around the time that I got a call from Dan (Section Co-Founder & CTO), saying hey, we need someone to work at Section and you can work anywhere. I thought Ah… That’s how it panned out and here I am, still here, still liking it. Occasionally, people can hear cows in the background or my mother on a tractor… They’ll ask, what’s that noise? That’s my 70-year-old mother plowing the field. You do a lot of the maintenance on the land? Yeah, you have to. My mother was one of three girls and my great uncle, her uncle, would say he’d never seen men work as hard as my grandfather worked my mother and her sisters. The girls were worked like men. My mother is more of a man than I’ll ever be! We run about 40 head of cattle, a few obligatory chickens, and my brother (who several years ago built another house on the property) has a few ostrich as well. If you took your own time into account, it’s by no means a lucrative venture - more of a lifestyle choice. Did you tinker with stuff as a kid? Yeah, I was always pulling apart things. Anyone with a bit of property and kids will end up with motorbikes, so I’d always be pulling those apart and fixing them with my father. Those were some of my favorite times. Like a lot of kids, I pulled a lot of things apart that never went back together. How did you find your way into programming? When I was growing up, the idea that “computer things” could be a career wasn’t a thing. They were just these strange machines and there weren’t that many of them around. My primary school had one. In high school, there were fifteen in the whole school. You touched them a couple of times a year at most. This is pre-mainstream Internet. You were starved of information. I had an interest in computers since I was a little kid and liked it. I thought I was OK with it but I had nothing to judge it against. At high school, there were a handful of kids who’d write code. Of those, none were as interested as I was, so when we started going through some national competitions and they decided to invite some of us to take part in the International Olympiad in Informatics, it was the first real feedback I had that yeah, you’re actually not bad at this. Where did that take you? Bonne, Germany. Hell of a thing as a 15-16 year old to end up on the other side of the world where the vending machines had beer, with 170 odd, like-minded, computer-loving kids. I remember having a conversation there with the chaperone for the team from Zimbabwe. They took us to a grand hall to have an opening event and he said, look at all these socially awkward kids who can’t talk to anyone. It was just all noise and natter. He said, these kids finally have someone to talk to who can understand what they want to talk about. It let me know, I am OK at this sort of stuff. Prior to that, did you have ideas about what you wanted to be when you grew up? It wasn’t until late teens, looking to exit school, I thought, it’s possible this could be work. There was a local computer club I’d been involved with and I’d been turning up down there. I was struggling with finishing high school. I couldn’t see relevance in a lot of it, so I decided to drop out. One of the guys there said they knew someone who needed someone to work. He passed on my details and so I ended up going for an interview with this little company. It was basically a few technical questions, do you know what IRQs and DMA channels are, and I said, yep, yep, yep. Alright then, we’ll see you tomorrow. This is the address, you’ll be meeting up with one of our people there, and away you go. How old were you? I would have just hit 18. That was my introduction to business and the real world. It was a small consultancy that supported software and hardware for businesses. I found myself in a mining company helping deploy a new computer network. At that point, the consideration was purely that this kid understands how to plug things into computers and put things together. They weren’t aware I could write code. It didn’t take long until they found that out. They also developed custom software for some of their customers. I got involved in that and since then, it’s just been an all industry experience. What’s your proudest accomplishment? That’s a difficult one. I’m not sure I really have one. You think about what’s recent. Two weekends ago, I had three trainee firefighters pass their assessments who are now able to go out. I know it’s not about me, it’s about them, but it was great seeing them finally getting to attend and passing their assessments. One of them has already been on the truck and out to a callout. How long have you been doing the firefighting thing? About five years now. When I was living in Sydney, I turned on the TV one night and saw a helicopter dropping water on my mother’s mailbox. I called and asked my mother if there were fires and she said, yes, but that’s OK, there’s a couple of people out here from the New South Wales Rural Fire Service taking care of things. It’s our state volunteer service. When I moved out here and now I’m working from home, I had the opportunity to avail myself and join up. Someone was here to take care of my mother and her mailbox, so I joined up to take care of other people’s mothers and their mailboxes. Being fully supported in doing that with Section is great. Just drop a note in that I’m not here and go. It doesn’t happen that often, but as we’re a bit of a distance from anything else, the nearest fire station aside from our volunteer one is 45 minutes away, so if someone gets their chimney fire out of control, it can make a big difference. What do you see as the most game changing technology on the horizon? I think in terms of changing things it’s got to be AI and ML technologies. That’ll start to change our everyday software development and systems management stuff. You’ve got so much open code and data these days that the systems could tell you, other humans are making this change and I think you should too. We’re already starting to apply some of that to predicting traffic patterns. It’s interesting because we teach them what to do, but we have no idea what they’ve really learned. You think, I’d taught it to distinguish between fruits but in fact, it’s just picking the color of a leaf. You can never be quite sure what it’s learned. It doesn’t get to reason with you and say, this is why I think this is an apple, it just says apple. As a human being, we can never comprehend what it is really looking at. On the horizon, that’s one of the biggest changes coming. We’ll be getting these recommendations and systems telling us to do stuff and we won’t necessarily be sure why, but when we look at it and evaluate it, we’ll say, that’s a good idea. Do you have any elements of fear when it comes to that stuff? Not really. One of the things that gets pulled out often is self-driving cars. I wouldn’t trust that, etc. At the end of the day, if the final statistic shows they’re safer, does it really matter why or how? Insurance companies won’t care. You’ll reach a point where for you to drive as a human, it’ll be less safe. They don’t know but statistically the robots are driving better than the humans… We’re flawed creatures, us human beings, and as long as it’s better, I’m not sure how much else matters. I don’t know if you’ve seen the DARPA challenges with bipedal robots having to do set tasks, like walking through a door, turning a valve, etc. Every year, the footage comes out and the robots are just falling over, tripping up… If you’re worried about the robot overlords, this is where they’re at. They’re a long way from mass control. I appreciate the fear because general AI: something that is capable of reasoning and cognition… Electronics in silicon are thousands of times faster than our little neurons fire… When we finally make something that has those abilities, we will be toys to it. Our cognition and ability will be so inferior. We’ll have to make sure we get our hands on the power cord. What’s something most people don’t know about you? Most people who know me are aware of my two little Jackawawas (Jack Russell x Chihuahua) dogs - Scuba & Sandy. Most don’t know: I used to race them in Flyball competitions. It’s a relay sport, and a small dog on the team gives the whole team an advantage (the jump height is set by the smallest dog on the team). (Scuba and Sandy not pictured) What’s your favorite… ? Morning beverage Reasonable coffee. Not particularly good/bad coffee - but a crossing point of least effort & maximal result. Beer Left Hand Brewing’s Nitro Milk Stout (homed in Colorado with Section) - It’s a truly horrible favorite to have since it’s not available in my home country (Australia). I’ve tried various local imitations, but nothing quite hits the spot. It’s a special treat, typically mixed with a visit to Section U.S. offices. Locally, a pint of imported Kilkenny Irish Ale is a staple. Part of your day Afternoon until Dusk. Finish work for the day. Get up. Get moving about. Get outside. Leave the house. Attack the never-ending list of TODOs. (Bluetooth-enabled hearing protection is awesome - working next to a loud machine/tool & listening to a podcast at the same time!) Vacation spot It’s not a spot exactly, but my favorite vacations are where I take a motorcycle and go somewhat off the beaten track. I’ve done some great trips with friends through Cambodia & Vietnam… I’m there playing tourist, but because of my size (6'5”), sometimes the locals find me interesting too."
"210","2020-09-02","2023-03-24","https://www.section.io/blog/section-engineering-education-community/","When we started the Engineering Education (EngEd) Program at Section last year, we never imagined how quickly it would evolve into a highly engaged global community. Our goal was to build and nurture a community focused around technical content created by the next generation of software engineers. Today, our thriving community includes student contributors from around the world. Almost exactly one year ago, we recognized a gap in the springboards available to students with aspirations of building careers in software engineering. Beyond focusing on their studies, students are challenged with identifying and securing competitive internship opportunities and building connections with the business community – all this, while many also hold part-time jobs, often in coffee shops, retail, or on-campus work-study programs, to fund tuition and living expenses. At the same time, at Section, we were working to develop strategies around building a scalable content engine that focuses on technical resources for software engineers. We thought, why not tap into the minds of the next generation of software engineers? Our pilot program involved a small group of 3-5 local (Colorado, US) university students. We now have around 150+ articles written, 90+ members in the Slack channel community and over 40 published student contributors from all over the world (including the US, Kenya, India, Nigeria and Russia). The level of collaboration and community among the students has likewise exceeded our expectations. We find ourselves in the unexpected yet fantastic place of having to quickly work out how we can keep scaling to meet demand. “Watching this diverse community grow so quickly in such early stages is truly remarkable. I really enjoy seeing all of the interaction among the EngEd community, not to mention the high quality content that they’re contributing. At Section we have an incredibly experienced and capable tech team and they have enjoyed learning from the content contributed. We are enjoying surfacing and sharing from the next generation of tech.” - Stewart McGrath, Co-Founder & CEO, Section Earlier this year, we hired our superstar Community Content Manager, Hector Kambow, to lead the growth of the program. Hector immediately set to work getting the word out, beginning with local university partners then reaching out to organizations that work with similar pools of students to help spread the word. According to Hector, “We’ve really gained the majority of our momentum via word-of-mouth from the student contributors themselves who continue to bring in new faces. The diversity of our EngEd contributors is something we are truly excited about.” “I was drawn to contributing to EngEd because I noticed a gap in the covered topics. Articles were primarily about computer science so I thought I could bring my web development experience. I had just started learning back-end development and wanted to write the guide I’d wished was available and store my learning for future reference. I’m constantly amazed at the diverse range of articles that EngEd contributors write. From machine learning to cloud storage to software development, the EngEd program aims to provide a whole range of guides, tutorials and articles for aspiring computing engineers written by computing students across the world.” - Louise Findlay, EngEd Contributor and Peer Reviewer After one in-person event at a student hackathon at the University of Colorado Boulder, all recruitment since has been virtual due to the pandemic. The program is open to anyone in a qualifying program of study who wants to enroll. There is an application process, beginning with the completion of a simple enrollment form. We are looking for students in the computer science field who have a good understanding of software and a desire to research and write technical content. Topics are typically suggested by the students themselves before being reviewed and approved by the team; and students are compensated for their published contributions. We use the first article to build a relationship with the contributor. Then each post goes through an internal editing process, in which Section’s engineers engage in a constructive review process with the student contributor over GitHub. You can sense the value of this discursive process for the students, and familiarizing them with workflows in GitHub provides an added professional skill set. Section engineers have enjoyed interacting with the students in this way and seeing the community grow and flourish, many commenting that they wish there had been something similar when they were at university. “Having shared a similar schooling experience with many of these students, it’s easy for me to put myself in their shoes to realize the value that students can reap from the EngEd program. I just wish that this type of program would have been around when I was in school!” CJ Brewer, Director of Solutions Engineering, Section Beyond the value that is delivered to the students, the benefits to Section are manifold: Creating a recruitment pipeline (we have already hired two engineering interns directly from the contributor pool who are now actively developing UI elements in our product and interfacing with our customers); Building a technical content engine; Generating awareness of Section among the next generation of software engineers; and Building a community: seeing the interaction between students in different programs around the world has been amazing. We are currently focused on the scalability of our processes and as part of this, creating a peer review program to help us with scaling up and to foster additional opportunities for dialogue between the students themselves. One of the biggest highlights for us has been seeing how our contributors engage with one another, commenting on each other’s articles in the Slack channel and sharing ideas for future ones. “The Peer Review Program has allowed me to not only deepen my knowledge of cutting-edge technologies or learn about new ones entirely, but the technical review process has also helped me become a better writer myself. By paying special attention to grammar and technical fact-checking, and through a collaborative process with student authors, articles are publication-ready and filled with accurate and up-to-date information on industry trends and tools.” Sophia Raja, EngEd Contributor and Peer Reviewer One of the primary benefits for the student contributors is the opportunity to become a published author. Each one has their own author bio page that includes all the articles they’ve written, which we hope can act as a useful resource for potential employment opportunities. Hector Kambow: “What drew me to managing the program initially was the potential for all the connections that could be formed. As a student, I would have been super excited to be part of something like this to learn more, build my skill set and get paid along the way. I can see how valuable it is for our contributors to get insight into how startups and collaborative development environments work and hopefully by connecting with each other, they might go on to build technologies of their own. The option is there for them to say, “You can work on the frontend, I can work on the backend, let’s work on a project with each other.” We are excited to see what’s next. Our current goals are getting the program to the stage where it’s a full-on content engine rolling on itself that can easily scale as needed. We are also continuing to focus on improving the quality and length of the articles, which began as short-form foundational content and now encompass a wide breadth of topics and in-depth tutorials. Above all, elevating the level of engagement within the community itself is what excites us most: we can’t wait to see where the students take this next."
"211","2020-08-12","2023-03-24","https://www.section.io/blog/section-drupal-association-partnership/","The Drupal Association selected Section to power the community tier of the Drupal Steward program, which extends Drupal security solutions to the global Drupal community. Millions of developers, from entrepreneurs to large enterprises, have chosen to build their applications on Drupal, making it one of the largest open source communities in the world. As a leading content management solution, Drupal’s core technology offers robust feature sets, but its true power is delivered in the flexibility that it offers developers. Built on the same underlying principles of modularity and configurability, Section and Drupal are a natural fit. For years, Section has been helping Drupal developers accelerate, secure, and scale their applications with leading Edge technologies that include advanced caching, next-gen web application firewalls, dynamic traffic routing, and more. The Drupal Association recently approached Section to power the Drupal Steward security program for its community. The Drupal Steward program provides critical application protection against vectors targeting application-layer attack. Harnessing the flexibility and control of the Section platform, Drupal was able to build and offer a powerful Community tier package that provides affordable protection against the exploitation of highly critical vulnerabilities within an application. Users of the Drupal Steward community tier will be able to leverage Drupal-specific protection at the edge, protecting them from any highly-critical vulnerabilities that may emerge, even before they are able to install the latest security updates. In addition, users will be able to access the benefits of Section’s global delivery platform. In a press release announcing the program, Heather Rocker, Executive Director of the Drupal Association said, “I am proud that we can advance Drupal’s commitment to enterprise-grade security. The Drupal Steward program and its security protections should give the world the confidence to build the next generation of digital experiences on open source technology.” Section is proud to be a long-time supporter of open source projects, recognizing that innovation is best achieved through collaboration. As Daniel Bartholomew, Co-Founder and CTO of Section, stated in The Section Manifesto, “We built Section from the same values that led to the creation of Linux and Git. Transparency, collaboration, and community.” We’re proud to continue to support the Drupal community and empower Drupal developers to build better, faster, and more secure applications. Explore Section’s offerings as a Drupal Association Partner."
"212","2020-07-30","2023-03-24","https://www.section.io/blog/jetrails-podcast-interview-stewart-mcgrath/","Section CEO Stewart McGrath recently sat down with Robert Rand on the JetRails podcast to chat about Helping Your Website Live On The Edge. The discussion covers everything from how Section got its name, to how we think about Edge and its application for today’s developers. This post highlights select snippets from the interview. You can view/listen to the entire episode at The JetRails Podcast. What is Section? We’re essentially an organization who help engineers get control of and have access to those workloads in distributed formats beyond the centralized infrastructure. How did you get the name Section.io? There’s a little bit of a story behind it. We’ve dropped the .io these days, trying to make things more simple, so we’re just Section. The origins of the name were as usual in most organizations, standing around a whiteboard with a bunch of people who were interested in thinking about what this application meant for the future of the Internet. Our mission is really about changing how applications work on the Internet, and we were thinking about Section when we were thinking about the volume of traffic that was going to traverse the Internet in the future. We were thinking about that as an oncoming wave, so some of us at Section enjoy surfing and the part of the surfing wave that is often talked about in surfing parlance is the “section” of the wave. You either make the section or you don’t make it. The bits between the broken parts of the wave are probably the most interesting to surf on. It also has a little bit of a connotation with respect to the container based structure of our platform in that applications are essentially deployed in sections or components on the Section platform at the edge. What can you tell me from your vantage point as a leader in edge networking? What goes into an edge network? Why have these become so important in the market? Any mystery there that you typically run into as you explain what your company has been able to achieve and why you’re in demand? It’s possibly a minefield of a question. Before I answer that question, we should probably tackle the big one, which is, “Where is the edge?"" We live on a spherical planet and the Internet is a series of interconnected devices in a mesh; therefore the reality is that there probably isn’t an edge to the Internet. However, when we think about edge, people have come at the problem in the provider and the service provider space and thought about the edge as essentially where their infrastructure is. We like to talk about where the edge is as where application controllers or owners start to cede control of components of their application to other parts of the Internet chain. Again, some of these legacy service providers, whether it’s cloud or CDN or even telcos, will all talk about the edge essentially being where their infrastructure is, as that’s where they have or will seek control of components of an application. When we think about it from an application owner’s perspective, as an application is delivered from centralized infrastructure right through to an end user, which may be an Internet browser or an application user, there’s actually steps all along the way there where control can be obtained, maintained or ceded subject to the application case. We think about the edge not just as one layer in that delivery chain out to the end user, but more of an edge continuum in that all of those locations are edges to the Internet where certain components of the application can be run, delivered and parts of the control space can be ceded along the way. Back to your question, how do we think about an edge network or what the Section edge network is? We think about that edge network as being able to run on any parts of that application delivery chain, whether that be from the origin infrastructure through to the infrastructure layer, cloud, through to running in telco and on telco compute technology, right through to on-premise or even down to the application device itself. I know you mentioned CDNs, so there’s the content delivery network side of things where instead of pulling information out of one web hosting server, you’ve seeded it across the edge network, so let’s say there are copies of images or other cached files and information, stored across the globe, so there will be a local version that will be delivered very fast in many cases. You have other optimization levels to deliver at the right sizes for the right browsers. What else is happening out there? I know because your team has this containerized solution, there are a lot of different things that you can drop into the solution to go above and beyond just the CDN implications and the optimization that can go with it. That’s true. We’re reaching a point now where more and more parts of applications are being delivered from distributed infrastructure. You mentioned a couple of those parts of an application, which is delivering images or changing the images on the fly even, which you refer to as image optimization, for better performance in the browser. We’ve also got application elements such as security layers being delivered from the edge, such as web application firewalls (WAFs), an example where customers are running logic at the edge to prevent spurious attacks on their website. Perhaps running bot mitigation software at the edge to prevent scraping of proprietary information or prevent probing attacks on their websites. We’re seeing more marketing style application elements being delivered from the edge as opposed to just straight performance and security elements of the application being delivered from the edge. We now have things like server-side multivariate testing being injected into the delivery chain from the edge. Like a more advanced A/B testing where you can have multivariate so A/B/C all mixed together to find the winning permutations and such of different changes to content or site elements? Precisely. You can drop that in from server-side as opposed to in the browser, providing some great benefits from a performance and useability perspective. Really all these components are parts of application logic, which are more easily injected at the edge through this container structure we’ve delivered. Also, it provides a scalability paradigm that engineers really enjoy because they don’t have to think about these components of their application running on their servers and centralized infrastructure. They can focus on running the important parts of the application that need to run on the centralized infrastructure while letting the edge run these other components or other logic elements of the application. Your team is one of the leaders in Varnish at the edge. That’s been interesting in terms of opportunities there to enhance performance for those users that are so reliant on those caching layers to be able to operate an advanced website in a more performant way because otherwise – without caching layers – Magento is a bit of a brick. I think that’s fair. We all know Magento loves a bit of caching, and not just image caching but HTML document caching, which becomes a really important thing in terms of scalability and performance. I think that’s the case for most modern applications to be honest. If you think about even static site delivery, WordPress, Drupal, you name it in the CMS landscape… I think everyone is looking for performance at this point. If we’re not, we’re trying to educate them because if they’re not, it’s impacting their marketing campaigns and their conversion rates and so many metrics that they do care about, they have to be doing things to their website itself to be efficient. There are absolutely things happening in the various hosting layers that are going to be crucial. It’s interesting. It has been interesting watching this conversation develop over the last ten years in terms of web performance where ten years ago in the web performance conversation, it was more about, does web performance improve conversion? Does it improve optimization in terms of user experience? I think that conversation has changed now to not how does it, but how do we most cost effectively deliver that performance for our application at any scale? We’ve moved on from the, should we to how do we? We know that a lot of the site owners and higher-level folks involved in these organizations and businesses that need this technology, sometimes it’s a bunch of technobabble and it’s too much. Our clients often trust us to bring the right options to them and that’s important in general. Our goal is to have the right advanced stack so it meets the needs and advances that expectations of our clients. There are things that your stack in particular is interesting, especially with its alignment in terms of how you look at things. At JetRails, we don’t like a black box. We like businesses to be able to choose the right tech. We want to be able to simplify the management of that and have great components – WAFs, ,tools to block out bots and DDoS attacks, but if no one is monitoring or managing the tools and seeing what’s going on and tamping things down, it doesn’t always go as smoothly as we’d like. There’s the human element to security and these other facets. Our goal is to be able to say we have a 24/7 NOC, operations center where we can monitor and manage and maintain these things, and handle it for you, but you need to have the right tech. Your team gives us different options of AI-powered WAFs, best-in-class tech that can be stacked together. We don’t have to choose because the CDN is really strong there but these other components that are part of it are maybe not as much. Basically, you’ve containerized it; that is so interesting. I don’t know if you have a lot of competitors that are doing something similar. For us, by and large, it’s been a really good offering for clients that want to be able to choose the best from column A, B, C and so on. Is that where you differentiate in the market? Is that one of the biggest value propositions of what you’ve created, and the processes and procedures that you’ve put into place? You’ve definitely hit on something that’s key to how we think about the two major components of Section really: flexibility and control. As you mentioned, because we offer a containerized edge infrastructure, that means we can offer engineers flexibility in what they run at the edge. Yes, we do offer multiple different WAFs on our infrastructure, different image optimization tech, different bot tech, different acceleration tech, virtual waiting rooms, and more. And the opportunity for engineers to bring customized workloads to Section, whether that’s containerized or serverless web load, which we’ll run at the edge for them. What that means… When I used to buy CDN as a CIO, when I was thinking about going to those content delivery networks, I had to make choices whether I would go with CDN 1, which was stronger in the performance element but weaker in security or CDN 2, which was stronger in security but weaker in performance. I couldn’t have the best of both worlds. We built Section specifically to provide that sort of flexibility for engineers so they could choose the right software for their application at the right location; for example, some customers choose a higher end web application firewall, but a lower-end image optimization solution, or maybe they don’t even need an image optimization solution, so they choose not to deploy it. There’s great flexibility there. The other component that is really important for engineers is familiarity and control. Being able to approach those elements with a proper application development lifecycle against the edge. So, we have fully Git-backed workflows sitting behind the Section platform, as well as an API-first mentality. We are really thinking about this from the application standpoint as opposed to the network standpoint. If you think about a CDN world, it really did spawn from solving a network problem, a networking issue, and what we’re talking about these days are more application style issues. Asking things like how do I get my application to work more effectively for more users, but also in a cost efficient manner so that we can provide the right software, in the right application, at the right time. How about data and analytics? I know some of your competitors try to provide an amount of information and help users understand how the network is being provided and the tools that are in place are affecting users. Have you found that to be something that’s in demand, that your team has had to invest a lot into addressing or consistently pay attention to? I think I mentioned earlier one of the elements we need to give engineers and not just developers of applications, but also operations, organizations, maybe DevOps teams depending on how you frame your internal team, need to have is to feel the control of the edge environment. In order to provide control, we need feedback loops so metrics is a really big part of how we think about running the edge. We log all our requests that pass through our platform and replay that in a searchable logs environment, so ElasticSearch, Kibana, Logstash, are key interfaces in the Section console. We also crunch that up into some metrics so we can some time series information there. That’s available within less than 30 seconds of a request passing through our platform. It’s not technically real-time, but it’s pretty close that engineers and operations teams can find out what’s going on at the edge at all times. We’re passionate users of diagnostics and information. We encourage surfacing that in our customers’ environments as well. One of the challenges as we think about distributed systems moving forwards and edge compute is being able to visualize what’s happening in a global application environment. We’ve spent quite a bit of time working with a company called Metalab, a company who designed Slack from the ground up as the UX, working on a new interface that will be released in the Section Console shortly, which is a 3D visualization of global traffic that is useful not just on a day-to-day operations perspective for engineers, but also useful to be able to describe to the CFO, CMO or the CEO in an organization what’s happening in the system at any particular point of time. I had a great talk with one of our customers who’ve been a customer for a long time – a large car manufacturer – and had been running a security layer and a caching layer on the Section platform. When I was talking through this visualization, an early prototype of it, with the engineering team, they said, ‘Ah, that’s where that layer is running globally’, so it was a real lightbulb moment for these folks to be able to see how and where the distributed system is running in some sort of visualization. The answer to your question is yes, we’re strong on metrics and very passionate about them, and passionate about engineering teams being able to use those metrics to describe the story of what they’re doing and what wins they’re having with an edge compute platform. Anything new coming down the pipe or final thoughts you want to share before we close? There’s plenty new and exciting coming. I mentioned a new DX, developer experience, that is coming down the pipe that is going to be game changing I think in terms of how engineering teams can visualize and communicate around their distributed architecture, i.e. their edge compute. That’s really exciting. I can’t wait to get that into the market and get people testing it. We’re also spending a fair bit of time thinking about and have built a machine learning structure behind our traffic forecasting to help the accuracy of moving traffic around to the right places at the right time and bringing up infrastructure in the right place at the right time, so we’re seeing some really good results from the neural net we’ve built there. I’m pretty excited to see that in the wild as well."
"213","2020-07-07","2023-03-24","https://www.section.io/blog/section-engineer-interview-gary-ritzer/","We chatted with Section Engineer Gary Ritzer to learn more about the career path that led him to Section, some of the languages that he likes (and does not like) working with, how laughing helps him stay focused and having fun, and his adventures in homebrewing. Gary Ritzer, Software Engineer What were you into as a kid? I was just kind of a nerdy kid. I liked to take things apart and see how they worked, how they were put together. Usually I was able to put things back together that I’d taken apart, but not always. Electronics, technical things, everything… a TV set, radios. I remember one time I took apart a TV set while it was plugged in. I didn’t get that far with it. I knew I shouldn’t be doing it. I wasn’t one of those kids that stuck a fork into an electric outlet, but I didn’t really know what it meant. I learned very quickly though which parts of the TV you don’t want to touch. Do you still tinker with stuff like that? I find that as an adult and a homeowner, there’s enough things that break, which I have to take apart and put back together. It’s a useful thing now, but I don’t do it just for fun anymore. It’s easy to see how that led you into software engineering. My first computer I bought and then took it apart. I upgraded all kinds of stuff and I built a bunch of PCs, like a lot of people did back then. I needed to understand how the computer worked and basically, whenever I approach a new thing, I just keep doing that. I always end up going down that route. Tell me about your career path in software engineering. It’s varied quite a bit over the years. My first software job was working on income tax software. I worked for a bank in New Jersey and they had their income tax software, which was written in C and it was archaic, but it was one of those things. That was a pretty self-contained app. My next job involved doing a lot of database things. Basically, we pulled a huge data feed from mainframe systems (they were still big back then) and we would build usable interfaces and reports off that, which you could run on your computer. It was still pretty boring, but the thing that was nice about it was learning how to use SQL databases and things like that. I always like to learn new things. Nowadays, those databases tend to run on big hardware so you can write really bad queries and they are still pretty efficient, at least they won’t take too long. Back then, you had to write them right as computers were nowhere near as powerful as they are today. Tell me a little about what you do at Section. I do lots of things at Section. I start on one thing, work on it for a little while, then have to put it down and work on something else. There’s lots of context switching. I’m used to it to a certain degree but not to the extent that we do it at Section. It’s also my first DevOps job. Before, I was mostly just Dev and other people dealt with the Ops part. Plus, we have the support element rolled in here. Lots of context switching, which can sometimes take away a little from productivity, but at the same time, it keeps it interesting… Startup life. It’s my third startup. You think I’d be used to it. What’s one of the things that keeps you motivated? New things, new technologies that come out. When I read about new programming languages, I tend to want to dig into them and get familiar with them at least, if not learn them and write actual programs. The latest one is Rust. I haven’t done much with it yet, but I have all kinds of things I want to do with it. It’s a language up my alley. Most of my career, I’ve spent doing C++. Rust is similar to that. I’d like to get more fluent in it, but I haven’t really had time yet. What have some of the steeper learning curves been at Section in terms of languages and the leading edge technologies we work on here? Well, I just don’t like JavaScript. I have a mental block with it. Some of the worst code I’ve written is in JavaScript just because it doesn’t make sense to me yet. I like static, strongly typed languages, and it’s really not that. It lets you write bad stuff. It seems to work, but except it doesn’t always work. I’m pretty vocal about that with members of my team, letting them know I don’t really like JavaScript. Everyone knows and laughs about it. What languages do you like? Go is a good language. I like C++, but I haven’t used it in a while. Python is OK, but it doesn’t have the strong typing that I would like. Rust seems to have all the goodies of everything, which is one of the reasons I want to learn more about it. What is your proudest accomplishment, not necessarily career related? I really don’t know. This is one thing I struggle with the most. I have lots of accomplishments I’m proud of, but if I was going to rank them, I don’t know how to begin to do that… I guess one that comes to mind – The last dog that I had, Duke (he’s dead now), he always looked up to me as one of the greatest things in his world. I always felt that was one of my greatest accomplishments: that I was able to be the human that he thought I was. I have a dog now, Squidy, but I don’t know if she looks at me in the same way. She more or less does her own thing, and I feed her. That’s it. There have been lots of opportunities in my career where I ended up on projects that seemed to be impossible, at least other people couldn’t solve them, and I was able to. Here at Section, it’s funny I was talking about JavaScript… eventually I’ll get confident with it where I’m not beating myself up about it. What do you like most about working at Section? I think it’s the people. Across the board, we have some really awesome people that work here. I enjoy showing up for work every day, granted it’s over a Zoom connection. Still, I like that. I also really like remote working. Also, the cool stuff we get to work on. The platform is pretty cool and we are doing a lot of cool things with it. It’s nice to be on the cutting edge, no pun intended. What single piece of advice would you give fellow engineers? This is one of the things I tell people all the time: you’ve got to find a way to have fun doing what you’re doing. Make things enjoyable, find ways to laugh. I’m laughing all the time. It keeps me sane and makes things more enjoyable for me and hopefully the people I work with. It’s important to laugh once in a while and not have to be so serious. Sometimes in this job, you get really bogged down in hard-to-figure-out problems and one of the ways I’ve been able to get unstuck when I have those things is to just get up, walk away and find something to laugh about. I’ll do something that’s funny or enjoyable to take a little of the cloud off it, then I can come back and attack the problem again and be successful. If you’re not laughing and having fun doing what you’re doing, even when things aren’t great, you may be in the wrong line of work. What do you see as the most game changing technologies today or on the horizon? You’ve got to like the edge stuff. I can’t even wrap my mind around all the possibilities we could do with it, and we’re just scratching the surface right now. We’re in the right place to do it at Section with a platform that can enable this stuff. I think we’re all in the right spot. This is a great place to be. What are you reading or listening to or tinkering with on the side? Any or all of the above? Besides trying to understand Rust, I have a house that I’m trying to renovate, which I’m doing myself, so I spend a lot of time doing that. Mainly fixing things instead of improving things. There are a lot of improvements I need to do. I enjoy that. I like doing things with my hands, but I just wish it didn’t hurt so much afterwards. You get a weekend and you can work one day, but the next, you’re sore and have to sit on the couch. What is something most people don’t know about you? Everyone here at Section knows, but most people I know in general don’t know that I make beer. I just made two batches. I don’t think either of them are great as they’re old kits, but they’re OK. I don’t know if I’d give them out. I have them in my kegerator and I’ll work my way through them. These kits were on sale a couple of years ago and I thought I’d try them. Two years went by and I didn’t make them. I did put fresh yeast in them, but they’re not as good as I’d like. I do experiment as well as use kits. I have made things just from recipes - go to the store and get the ingredients. That’s where some of my best brews have come from. You can’t reproduce it. I don’t write it down. What are the favorite styles of beer to make? Definitely an IPA. I love IPAs. I’ve been thinking about changing it up. Stouts are good. The typical batch I do is 5 gallons. What I’d like to do is make a bunch of smaller ones, maybe half that and make more of them at the same time. Try variations on the same recipe and find out what I like. But, I’m happy I made two batches of beer during the pandemic. That feels pretty big for me. What’s your go-to beer that you buy from a store? Odell’s IPA hands down. It’s always good. It’s what I drink most of the time. If you could choose any figure in history dead or alive to interview, who would it be, and what would be the single most important question you would ask? I really like music, so the person I’d pick for this is John Lennon. His life was cut short and there’s a lot of iffy history about his early times and I’d really like to know why he got into music. I know he grew up in a troubled household, but I don’t know why he got into music. He could have been any number of things, why would he pick music? How would he know he was going to be so great at it? Favorite movie or binge-worthy TV show? I like things that are funny so I can laugh. Silicon Valley is one of my favorite shows of all time. It’s over now. It’s hilarious. The first couple of seasons were the best, but every season was real good. I was disappointed, I watched Space Force last week, a new Netflix thing; it wasn’t really that good. Favorite vacation spot? I’ve never been much of a vacation taker. I’ve been to more cool places for work than on vacation. For me, for a vacation, it’s just nice to give my brain a rest and go somewhere where it’s quiet and not think about work stuff. I need to let that go every once in a while. I’ve done a lot of staycations. In years past, I used to drive out to the East coast to see my family - my brothers and my mother. I haven’t done that in a couple of years. I need to do it this year. Just things that are not work. I’m not someone who likes to go and sit on a beach because I don’t tan, I just burn. Favorite part of the day? The morning. I’m usually up between 5-6am every day, a little later on weekends. I get up and am excited to get going on the day. I drink coffee or tea. When I eventually sit down to work, it’s my most productive time. Especially with Section. There’s only a few people in the US I work with on a daily basis. In the afternoon, the Aussies come on, and we generally have more meetings, there’s more discussions and less work. However, I usually get done as much in the first 3-4 hours of the day at Section as I would normally do all day at other places that I’ve worked."
"214","2020-03-19","2023-03-24","https://www.section.io/blog/tips-for-first-time-remote-workers/","COVID-19 has forced global adoption of work-from-home policies and many are going about setting up a home office environment for the first time. As with any sudden change, there are a lot of questions and moving parts as people learn to embrace working remotely for an indefinite period of time. From delineating clear boundaries to ensuring you have the right tools to stay on task and remain connected with your team, the transition to becoming a remote worker can take some adjustment. As a remote-first company, we’ve been at this for quite some time. Recently, many of us have been fielding questions from friends and family who have been thrown into remote work for the first time, so we figured there must be a wider audience out there who may also be feeling a little lost. Here are some tips for those who are new to remote working. While we understand that all may not be practical or possible for every circumstance, hopefully there are some helpful takeaways for everyone. Reduce distractions Try to carve out a dedicated space for each person in your home that needs to work. Simply working in the lounge or at the dining room table won’t work for long if you’re not the only person in the house. Having a dedicated workspace helps maintain distinctions between work and non-work time. Set expectations and boundaries among everyone in your space. The above statement goes double for kids. It’s important that your family understand that just because mom or dad are in the next room, it doesn’t mean she or he is available to chat or play all day. (Obviously, this is age-dependent.) Set a schedule Be clear with family about “work hours” and “non-work hours” and set expectations for both. Have a morning routine. Tempting as it may be to sleep in and just roll out of bed to the desk, you’ll likely be more productive in the long run if you set an alarm, get dressed and ready just like you would if you were going to the office. Look for your own personal transition point that can replace a commute. Perhaps heading to the local park for a walk or even just popping downstairs and back in can help you make the mental shift to start focusing on work. Develop a daily cadence that involves taking regular breaks. Get up and walk around. Schedule coffee, snack and lunch breaks with the family or have a virtual lunch date or happy hour with a friend or co-worker. Make sure you have non-work hours. On that note, don’t use the time you save by not commuting to simply work longer hours. That will burn you out and may cause resentment with your family. Make sure you have the right tools Recommendations include, but are not limited to: Computer with required software to fulfill your responsibilities External monitor to boost productivity Stable Internet connection Phone Comfortable chair (unless you prefer a standing desk) Good headphones, preferably of the noise cancelling variety Notebook (physical or digital) to keep track of to-do’s and meeting notes Stay connected with your team Perhaps the most difficult part of an abrupt transition to remote work is the feeling that part of your social life has been taken away. We’ve come up with some fun ways that have helped us maintain a strong company culture and connect with each other on a more personal level, including: weekly all-hands meeting via Zoom, Friday (virtual) happy hour, How I Dev, and Shuffl bot in Slack. In order to facilitate most of this, we rely on lots of tooling to keep communications flowing seamlessly. Messaging software (Slack, MS Teams, etc.) For those using Slack, realizing the value of channels can be useful. As an example: #section-humans, #stand-up, #prs, #sales-ops, #random. We’ve found that using special interest channels is a helpful way of not cluttering other channels: #golang, #eng-cli-snippets, #hackathon-idea, as examples. Keep related conversations contained in comment threads to prevent clutter. Use reactions more than reply comments – it makes things clearer and more efficient. If you need to archive a channel, it’s easy to do and can be used for projects, incidents, events, etc. If you find that you’re in a back-and-forth chat that’s not resolving quickly, jump into a video call to talk it out. (Pro tip for Slack users: Check out the Slack/Zoom integration.) There are lots of resources dedicated solely to messaging etiquette and efficiency tips, such as this one: Slack Etiquette, Part 1: Organize & Be Mindful Video conferencing software for virtual meetings (Zoom, Google Hangouts, Skype, etc.) While video calls are not the same as face-to-face conversations, many of the same communications can be facilitated remotely. Lean into the opportunity to meet virtually and give the same attention as you would in a traditional, in-person meeting… i.e. dismiss the temptation to check Twitter or email at the same time! Create agendas and distribute them ahead of time so attendees come prepared, and always end with a summary of what was accomplished and next steps to focus on. (This goes for all meetings, virtual and in-person.) Having good meeting rituals in place helps build trust and collaboration. Meetings could start with a check in for each team member in which they share how they’re feeling that day, and what else may be going on that might impact productivity or mood. Be sure to turn your video on! This will let you and your fellow attendees pick up on visual communication clues that might otherwise be lost, and give a chance to more directly connect. Make sure you have good headphones. Project management software (Trello, JIRA, TargetProcess, etc.) Keeping everyone on the same page around assigned responsibilities, progress, and next steps is essential, particularly in remote workplace environments. If you’re part of a software development project, having a tool to specifically keep track of your issues, milestones, and artifacts, can be invaluable. Bonus: Use project boards as supporting visuals in meetings and check-ins to keep tasks on track. Knowledge management tools (Guru, Confluence, etc.) Internal wikis can help connect teams that work remotely by keeping informational resources up-to-date and ensuring any changes are easily shared. This is a great opportunity to improve documentation and organization of policies, processes, how-to’s, and links to other helpful resources. Email Don’t forget regular old email, which is often still the primary method for external and longer-form communications. All this points out how async a lot of remote work can become. Writing is a must-have instead of a nice-to-have. Clear, frequent communication is essential to ensure others around you know about your progress and decisions. For remote first-timers, this is different - try not to get frustrated. It will take time to get used to the new setup, both for you, those you share your home with, and your co-workers who are likewise getting accustomed to a new normal. Tap into your determination to persevere. Difficult days, especially during the transition period and these uncertain times, are part of the journey. On a broader note, it will be interesting to see how we progress from here in the future - will this large-scale experiment in working from home lead to more companies adopting more flexible workplace policies? In a recent report on the Digital Workplace, Gartner encouraged I&O leaders seeking out ways to provide a foundation for the digital workplace to “lead through disruption”, pushing on with digital initiatives “to ensure the business is ready for the future.” This type of widespread disruption might indeed do just that."
"215","2019-04-17","2023-03-24","https://www.section.io/blog/how-i-dev-series/","At Section, we strive to keep our global team connected, and one of the ways in which we do this is through a weekly How I Dev series, our version of a brown bag. First, a little history on our remote culture. Section was founded back in 2012 in Sydney, Australia. In 2016, we moved our HQ to Boulder, Colorado, but half of our team is still based in Australia. As the company has grown and life has happened, the spread of physical locations among us has expanded yet further. How It Works Every Monday/Tuesday (i.e. the end of the US’ Monday/beginning of Australia’s Tuesday), one of our team members (or a group of collaborators) does an informal presentation on something that benefits the entire group. Despite being called How I Dev, it doesn’t have to be 100% developer-focused but often is because most of us are engineers. We actually spent one of the sessions doing a show and tell of our personal workspaces, with the notion of sharing why we like certain setups - everything from standing desks and monitor size to guitars and zen gardens. The subject matter is entirely up to whoever is presenting that week. This means we cover a wide range of topics, including general knowledge sharing, development best practices, deep technical dives, and specific challenges faced and overcome (or soon-to-be overcome through inputs from the larger team). Examples of past session topics include: Approach and process around client performance reviews Elm overview Examining local bastion SSH scripts Workspace show and tell (mentioned above) Deep dive into cgroups Breaking down PromQL At the end of each session, the presenter nominates a new presenter for the next week, and that person decides on the topic they want to cover. Often, this is a groupthink decision based on the challenges faced among teams in any given week. Why It Works How I Dev is a popular weekly event among our team and allows us to both share knowledge and to build strong relationships of trust and transparency locally and from afar. There are a few key factors that keep this series running: Participation/attendance is optional You might think that optional attendance would translate to zero attendance. In this instance, it has the opposite effect. Aside from having a genuine interest in the subject matter, the support for fellow presenters builds investment and camaraderie among the team. Furthermore, when there’s no pressure to show up every week, it conveys a completely different feeling than a typical meeting might. No strict guidelines We don’t place strict guidelines around content, format or style and therefore there is little stress around the series. We all know it is intended to be a casual knowledge share, thus it is treated as such. Knowledge archives Each session is recorded, archived and added to Section’s wiki for easy access at any time, serving as a great resource for onboarding new staff as well as providing an ongoing refresher for the rest of us. Different perspectives Every challenge, system or language can, of course, be approached from various angles. How I Dev offers the opportunity for valuable input exchange among our team that is specific to a particular topic. Rather than learning only the presenter’s way of approaching something, the discussion often produces many takeaways as a result of, ‘Did you think of trying this?’… or ‘Why did you choose to do this?’ type questions in the Q&A throughout each session. We hope in sharing this practice that other teams might benefit from our learnings. We can surely attest to its value in developing and furthering best practices while creating a strong connection among our remote team."
"216","2019-11-21","2023-03-24","https://www.section.io/blog/section-lumen-cdn-edge-compute-partnership/","This blog has been updated to reflect the rebrand of CenturyLink to Lumen Technologies. We are proud to be working alongside Lumen Technologies to power edge compute technologies that empower modern enterprises to build and scale applications. The Lumen CDN Edge Compute solution leverages the Section platform to give developers flexibility and control to customize their edge environment. “Businesses across all industries are looking to quickly create and deploy more rich web content which requires an agile IT infrastructure that fosters rapid innovation,” said Bill Wohnoutka, vice president, global internet and content delivery services, Lumen, in the official press release. “With CDN Edge Compute, businesses can use a global edge delivery footprint that allows for new web content and capabilities to be pushed to users quickly and easily, and that can scale to support mass audiences and high traffic events.” Lumen was recently recognized by IDC as a leader in the global CDN landscape, particularly being highlighted for their forward-thinking approach to product transformation. “The company also stands out as a Leader in the CDN services market as a result of its new edge-based service offering and strong partner ecosystem,” the report states. “By combining Lumen’s network infrastructure and expertise with Section’s Edge Compute Platform, we’re helping enterprises build and scale applications to meet the growing demands of the dynamic compute landscape,” said Stewart McGrath, Co-Founder & CEO of Section. “This partnership is centered around the shared vision that developer empowerment is the key to innovation. By providing tooling that gives application engineers full control over configuration and deployment of workloads along the edge compute continuum, together, we’re working to build a better Internet.” For more information around the Lumen-Section partnership, please contact us."
"217","2019-11-11","2023-03-24","https://www.section.io/blog/checkpoint-at-pivotal-moment/","For anyone who listens to the NPR podcast How I Built This, you’re familiar with the standard question that host Guy Raz asks every founder he interviews, “How much of your success do you attribute to your hard work and determination, and how much to luck?” While the question typically elicits humble reflections describing a combination of the two, it’s interesting to contemplate that question, not in retrospect, but rather as a company on the precipice of extreme growth potential. Potential carries heaps of responsibility - responsibility to make the right decisions at the right time and execute, execute, execute. All companies, startups and Fortune 500 alike, face pivotal moments in their growth cycles where key decisions can lead to hockey stick growth or umm, otherwise. These decisions span across the entire organization - product vision and development, sales and marketing strategy, talent acquisition and retention, financial strategy and management, and everything in between. Ultimately, it’s the combination of these cross-functional decisions that determine our destiny. So we ask ourselves, are we in the right place at the right time doing the right things? Edge Computing: The Opportunity and Challenges Ahead As with the early stages of any category, hype is inevitably accompanied by uncertainty, and this is certainly true of the edge computing market. Is the Opportunity Massive? Gartner has predicted that the “Edge will eat the Cloud”. Given that Gartner estimates Cloud is currently worth more than $200B per annum and growing at more than 17% CAGR, this is no small prediction. The Linux Foundation expects Edge will overtake Cloud in just 5 years. So, the opportunity at the Edge is expected to be huge. In fact, Chetan Sharma believes that the Edge industry could be worth $4 Trillion by 2030. What is driving this opportunity? Moving workloads to the Edge allows engineers to build more scalable, more secure and faster applications, as they can take advantage of: Reduced Latency - Executing and delivering closer to the end-user; Reduced Data Backhaul - Handling massive volumes of data at the distributed Edge rather than shunting back over expensive networking to bloated central infrastructure; A Security Perimeter - Ability to run in-depth defense with specific mitigation software and networking defenses; and Volumes of Cost-Effective Compute Power - Computers distributed in execution locations are ready to be leveraged. The transition to Cloud happened relatively quickly. Engineers are now used to the abstraction Cloud affords from physical machines and the benefits of instant spin up (and down) of new machines. There are very few “box hugging” ops folks left who really care about the specific server on which their workload is executed. Edge is the next evolution of the Cloud abstraction. Not only should engineers no longer care on which server their workload runs, but they also should not care in which location it runs. They should really only care that their workload runs in the most performant location for their specific cost and compliance parameters. Can We Address the Challenges? The key challenges of Edge are both industry-wide and user-specific. Developing a truly successful Edge fabric to meet the needs of the 2030 Internet will be dependent on certain levels of cooperation and standardization. Closed networks of proprietary software (e.g. the legacy CDNs Fastly, Akamai, Cloudflare, etc) which served the Internet from 2000 until now will not be able to solve for the hundreds of thousands of Edge locations and truly custom workloads that engineers will need to run in those locations. Work underway by collaborative open standards bodies such as the Open Networking Foundation (ONF) and Multi-access Edge Computing (MEC) are meeting this challenge head-on to solve for a truly successful Edge fabric. The Edge should be both widely distributed and highly fluid, yet we need to present that flexible distributed Edge compute fabric as a “simple to approach” compute infrastructure for both dev and ops engineers. We need to provide them with the same level of familiarity and control that the Cloud ultimately had to provide to entice ops engineers away from their racked servers. Our Vision for a Developer-Empowered Edge From day one at Section, we believed we would improve the Internet by providing engineers with access to and control of workloads at the Edge. It was a few years ago that we dreamed up this vision, and ‘the Edge’ wasn’t really a thing back then. So, we didn’t use those exact words, but pretty damn close. We have also stuck by three guiding principles in this process from day one: Open, Control, Easy. The framework we provide (and the Edge) should be Open. Engineers should have real Control over that Edge, and we should make it Easy for them to use. Simple, huh! So our vision of the Edge is one where Engineers can run: Any Workload: Container, serverless, best-of-breed solutions… Bring your own custom workload, or Deploy best-of-breed software within minutes by leveraging our Edge Module Marketplace. Anywhere: Access the most optimal Edge locations for your application Vendor-neutral Composable Edge Cloud: Infrastructure Edge Telco Edge (Exchange, Headend, 5G, etc.) On-Premise Edge Highly Performant Intelligent Scalability: Spin up and tear down edge compute based on demand Security: Segregation, separation, and defense in depth DevOps-Centric Contol Normal application development lifecycles Observability: Real-time, comprehensive diagnostics framework The Honest Assessment We are currently investing on a daily basis to deliver our vision of the Edge. Is this the right time and the right direction and will all of this work lead to materially improving the Internet? The truth is, we don’t know yet. Yes, the analysts and industry boffins are now strongly articulating our day-one vision, and while this does feel great, it’s not proof. On the other hand, we have really been enjoying working with customers and partners on a day-to-day basis to help them deliver superior experiences for their end-users. Engineers are seeking better ways to deliver faster, more secure and more scalable applications. Overall, we now believe more strongly than ever, that if we give engineers access to and control of workloads at the Edge, make decisions that continue to be rooted in innovative developer experiences, and we execute execute execute, we just might be sitting across from Guy Raz answering that question, “How much of your success can be attributed to hard work and determination, and how much to luck?”"
"218","2019-11-01","2023-03-24","https://www.section.io/blog/section-digitalocean-partnership-edge-computing-developers/","Section has become an inaugural member of DigitalOcean’s Solutions Partner Program, a worldwide partner initiative for services and solutions providers who want to use the DigitalOcean cloud to help deploy and modernize their clients’ infrastructure. Section and DigitalOcean express shared philosophies around developer empowerment. DigitalOcean’s community of over 3.5 million developers and their worldwide network of data centers are rooted in open source principles and simplicity so developers and their teams can spend their time innovating instead of managing complex infrastructure. Similarly, the Section mission has always been carried out through a developer-first approach. Section’s DevOps-friendly Edge Compute Platform further removes the complexities associated with infrastructure management by optimizing workload distribution to meet performance, security, and scalability requirements for any application. “Distributed computing is not easy, even for the largest, most sophisticated engineering teams. Just like DigitalOcean, we’re focused on reducing complexities for web engineers.” - Daniel Bartholomew, Co-Founder & CTO, Section In speaking about Section joining the program, Section CTO Daniel Bartholomew explained, “Distributed computing is not easy, even for the largest, most sophisticated engineering teams. Just like DigitalOcean, we’re focused on reducing complexities for web engineers.” He added, “Most engineers don’t care exactly where their workloads are running, as long as they’re meeting the performance, security, and scalability requirements that they’ve defined. The Section platform optimizes workload distribution based on these specifications, so developers can focus on their core application functionality.” Section’s Docker-powered, Kubernetes-orchestrated Edge Compute Platform does exactly that, sitting on top of DigitalOcean (and other providers) to offer the most flexible edge network for developers while giving them complete code-level control over workload distribution. Developers can run any workload along the edge compute continuum to meet the specific demands of their application. Users have the option to either bring their own custom workload, or to leverage best-of-breed software available in Section’s Edge Module Marketplace. Solutions include web application firewalls (WAFs), bot mitigation, image optimization, A/B & Multivariate testing, Virtual Waiting Room, and many more. The partnership program is designed for companies building technology solutions for SMBs on top of DigitalOcean’s Developer Cloud. Partners benefit from stable and efficient pricing, marketing benefits to help better position DigitalOcean for their clients, access to world-class 24/7 service and support, and the opportunity to become part of a large robust community of technology experts and thought leaders. Section recently joined the DigitalOcean team at their Shark Week event, an annual company-wide gathering, where Daniel participated in a panel discussion alongside other key founding members of the Solutions Partner Program."
"219","2019-07-31","2023-03-24","https://www.section.io/blog/cdn-landscape-report-2019/","For the last four years, industry insider and CDN market analyst at Bizety Technologies, Ernie Regalado, has been publishing an annual CDN Landscape Report. Mr. Regalado has worked in the CDN industry since 2007 and has been writing for six years, spotting many of the companies that have gone on to dominate the market while still in their early stages and highlighting the major shifts happening inside the industry, often before they’re widely noticed. The 2019 report, which is focused on content delivery network infrastructure, cloud security, and edge computing, highlights Section’s rapid growth. “Has anyone noticed that Section is growing up fast? The startup is doing a great job in developing its feature set, perhaps the next Fastly?” -Bizety newsletter (July) The 2019 CDN Landscape Report The annual Bizety CDN Landscape Report is an event in the CDN world, anticipated and widely read in the U.S. and beyond. This year, Regalado has co-written it with Jim Davis, principal analyst for Edge Research Group. Their report offers a comprehensive set of insights into the state of the current CDN industry, including the authors’ views on “where the market is headed in 2020 and beyond”. In it, they take a look at the top 50 CDN vendors in the market, in addition to several more that are currently in stealth mode. The overall focus this year is on “the accelerating shift to security and edge compute services”. Along with a future forecast, the report offers market revenue figures for 2018 and a comparative analysis of vendors in different segments, including media delivery, e-commerce, security services, and edge computing. While its main focus is on the major CDN players, it also looks at disruptive startups, particularly those that are looking beyond “simply trying to become a CDN” to create a new space in the market. Edge Computing is Driving Growth In defining the edge, Bizety/Edge Research Group ask what marks the difference between earlier usage of the word “edge” and its widespread use today by CDN vendors across the gamut. The answer in one word: “control”. “In the past, CDNs maintained absolute control of their PoPs: clients did not have control over resources in those PoPs because running software at the edge could disturb existing software services. That is no longer the case. CDNs are beginning to embrace edge compute, providing their clients with the ability to programmatically invoke functions that the vendor has developed.” The Report predicts that edge computing will be a primary factor in new feature development across the CDN industry across the next two to five years. It suggests that edge compute alongside network access will be “the foundation for another high wave of growth” over the next 18 months. Four Edge Leaders: Fastly, Section, Cloudflare, Stackpath While Bizety/Edge Research Group note that CDNs across the industry are increasingly looking to add edge services and use edge branding, in the section discussing leading vendors in the edge computing/CDN space, the authors hone in on just four companies: Cloudflare, Fastly, StackPath and Section. “This group of CDNs are leaders in edge computing product innovation. They have developed edge functions and edge processing capabilities into their technology stack.” -2019 CDN Landscape Report (Bizety/Edge Research Group) What sets Section apart is the edge focus that the platform has adopted from the beginning. Co-founders Stewart McGrath and Daniel Bartholomew created Section to make edge computing more accessible for developers and DevOps engineers. Section’s Edge Compute Platform enables the next generation of flexible applications that live closer to the end-user while still providing the computational power necessary for complex networked experiences. The Section platform was built to give DevOps teams choice and control in selecting where and when to run edge workloads. Developers can select from a wide array of open source tools and integrations. Bizety and Edge Research Group highlight the rich feature set Section offers within what it describes as a “pure-play edge platform as a service” that happens to support CDN services (the report notes that Fastly began its life in a similar way). While Section is the smallest in terms of current employee size and funding mentioned among the leading edge players, it is given the same high valuation as the three larger companies and is the only startup highlighted in the Report’s top five insights. For additional insights, you can also read Bizety’s CDN Industry Buzz for Q3 – 2019."
"220","2019-04-10","2023-03-24","https://www.section.io/blog/section-joins-lfedge/","We are delighted to announce that Section has joined LF Edge, an umbrella organization within The Linux Foundation dedicated to fostering collaboration around edge computing. We join other edge industry leaders in helping to create a common framework for hardware and software standards and be at the vanguard of the development of best practices that will ensure robust innovation in the edge compute ecosystem. Section’s Edge Compute Platform was built on the principles of developer flexibility and control, and being part of this esteemed community that is developing a shared framework around edge computing will allow us to play a crucial role in furthering our mission to improve the Internet. “As a software provider in the edge compute ecosystem, one of our biggest challenges is educating software engineers on how to build for increasingly distributed architectures. We see LF Edge playing a fundamental role in defining hardware and software standards that will accelerate adoption and innovation far beyond what we can imagine today.” - Daniel Bartholomew, CTO, Section What is LF Edge? LF edge is an umbrella organization with the goal of establishing an interoperable and open framework for future edge computing projects independent of hardware, silicon, cloud, or operating systems. LF Edge offers a set of open source development tools for a variety of use cases spanning the industrial, enterprise, smart home and consumer fields, along with multiple edges and domains. By ensuring that a company’s products are compatible with LF Edge, interoperability can be guaranteed, as can the avoidance of vendor lock-in issues in the future. Interoperability is important for companies adopting IoT and edge devices as they need to be compatible across sensors, edge devices, and the cloud backend. By providing an open framework that addresses market needs for edge and IoT devices, The Linux Foundation intends to “address market needs for edge and IoT by combining new and existing stacks and consolidating them into one singular, customizable framework”. The new organization will help guarantee improved harmonization across the rapidly growing number of edge devices (expected to outgrow 20 billion by 2020). “Edge computing continues to be a hot topic this year and we are excited that LF Edge is becoming the foundation that enables this innovation. We welcome Section to our growing ecosystem and look forward to their expertise in helping us create reliable and secure solutions.” - Arpit Joshipura, General Manager, Networking, Edge & IoT, The Linux Foundation Five Anchor Projects There are five anchor projects within LF Edge (all open source), including three existing projects: Akraino Edge Stack: For businesses running edge services with a cloud back-end that want to automate provisioning and achieve greater flexibility and scalability; AT&T contributed the original code. EdgeX Foundry: A platform-agnostic software framework designed to run on an industrial edge gateway and enable plug-and-play integration of microservices; originated at Dell/EMC. Open Glossary of Edge Computing: An evolving reference guide to technical edge terminology; run by a collaborative group including ARM, Ericsson, Packet, Rafay Systems, Vapor.io, and Section. In addition, there are two new projects: Home Edge Project: Samsung contributed the seed code for this run-anywhere services layer for home-based IoT devices. Project EVE: IoT software maker ZEDEDA donated the initial architecture to this edge-virtualization engine, which helps decrease latency and make workload distribution more efficient. Standardizing and Accelerating the Edge To date, there has been no standardization within edge computing. Through the creation of LF Edge, vendors, developers, OEMs and infrastructure providers are coming together to develop a common set of standards and achieve wider interoperability. The long-term goal is to build an open-source software stack, which brings together the leading telecom, cloud and enterprise edge technologies. At Section, we are excited to be a part of this community-forged movement, moving edge computing forward. “It has been spectacular to witness the overwhelming success that the Cloud Native Computing Foundation (CNCF) has had in advancing the microservices ecosystem. Similarly, we believe that LF Edge will be a key driver in accelerating the advancement and adoption of edge computing.” - Daniel Bartholomew, CTO, Section"
"221","2020-01-20","2023-03-24","https://www.section.io/blog/section-engineer-interview-jason-stangroome/","In this interview, we catch up with Section Engineer Jason Stangroome to learn more about what keeps him motivated, the never-ending challenge of prioritization, and a few of his favorite things - sunrise walks, cartoons and Belgian beer, to name a few. ### Jason Stangroome, Software Engineer Why did you decide to become a software engineer? I don’t think it was actually a decision. I remember from a very young age, probably about 4, we had a Commodore 64 in the house and I spent a lot of time on it. I reckon it was probably about the time I was 7 years old that I started to learn to read properly in school and all that kind of stuff. I started picking up programming books for the Commodore 64 and started working through the exercises (e.g. make this code, make this app, make this game, etc.). From there, I was just doing my own thing. So it just kind of became life… Whenever I wasn’t on the Commodore, I was playing with Lego or thinking about what the next program would be. What keeps you showing up for work? There’s always something new. Something new to learn. Some new tool or technology. A better way to do something that you’ve been doing. It doesn’t get boring. What is your proudest accomplishment? I don’t think that I could really put it down to just one thing, but I have started to notice recently, especially when I talk to people that haven’t been doing this as long… I’ve been doing IT as a job for 22 years now. I look back over those years and all the different jobs I’ve worked on and think wow, that’s a lot of stuff. Also, obviously, I’ve come a long way from where I’ve started. I look back at early jobs and think how differently I’d do stuff now. So overall, it’s just how much I’ve grown since I started this, which stands out to me, more than any one thing. Do you favor experience across a diverse set of skills or diving deep into a specialty? Can I have both? It sounds like you’ve had a lot of diverse experiences. In some respects, they’re diverse. In other respects, not. The first fifteen years it was almost all experience in the Microsoft space, but it was software development, it was IT admin, it was networking… At some point I was even responsible for running the phone systems and setting up how all that worked. Then you look at other people who’ve had completely different careers in unrelated fields, or some sort of middle ground where they might have gone off and worked in the Linux space much earlier or done mainframes, etc… So I definitely don’t think my experience is that varied. It’s definitely been a lot of Microsoft and commerce, but I think it can be as important to have a broad experience of being in different ecosystems, different types of business, government, different personality types… That’s probably as important as the work you’re doing. What soft skill or personality trait do you attribute most to your success? Relentless curiosity. Constantly wanting to know how and why things work, beyond what you need to be able to do the task. It annoys my wife because I don’t just do that for work, it’s all things. For example, I tend to theorize a lot. One that pops to mind… why does the kettle take so long to boil? How could we have a more efficient kettle? What technology exists that would make kettles faster? What’s the most challenging aspect of your job? At the moment, it’s juggling the priorities, the risks and the fun stuff. There’s so much stuff that we want to build and improve upon, but we’ve got limited people and time. There’s always the interesting problem that’s the most engaging one, but it’s not necessarily the most important one. So, you have to remind yourself to put that one down and focus on what’s actually important. Just all the trade-offs that come with it is definitely the hardest part. What do you like most about working at Section? First, I thought it might have been the flexibility of being a remote worker, and then I think about the bleeding edge tech we get to play with. But I think ultimately it’s the fact that what we are building is going to be used by millions of users across thousands of websites. They might not realize it, but their experience will have been enabled by what we’ve done, and there’s not many other jobs out there that would give that kind of reach. Maybe Google, Facebook, and a few others like that… But just the scale and the reach we have is probably the best thing. What single piece of advice would you give fellow engineers? I would give and have given: You’ve got to step away, you’ve got to take personal time. Burnout is a real thing and it will sneak up on you. What’s your favorite part of your day? I have a habit of getting up early in the morning and going for a walk. At the moment, that takes me along the beach. Ideally just as the sun is rising. Maybe get the shoes off and let the sand get in the toes and let the water come up. Maybe go get a coffee and just sit there and take it all in for a little while before I start my day. What are you currently reading? Are you a reader? I was a reader. I suck at reading these days. Most of my reading seems to be Slack and emails. Maybe a little Reddit. The last thing I really committed to reading and thoroughly enjoyed was the various works of Isaac Azimov or Philip K. Dick. Asides from reading, listening… most of the time I take my morning walk, I throw on Spotify and go to the Discovery playlist and just put that on. What type of music does that spit out? It’s mostly electronic. A fun one it spat out for me recently was King Luan, which it turns out is an Australian indie group and their song No Vampires Remain in Romania. It’s very weird. It reminded me – I don’t know if you’ve ever watched Forgetting Sarah Marshall, but there were strong similarities between that song and certain parts of that movie, which made me laugh. Just totally random. Are you tinkering with anything on the side at the moment? The closest thing I’ve got to tinkering at the moment is a NUC, a self-contained PC that I’ve got set up as a Factorio game server. I’m going to try and set it up so that it’s also an access point for multiplayer gaming. Name an interesting fact that most people don’t know about you. I didn’t get into music – in terms of listening to and appreciating it – until really late in high school. Prior to that, I probably couldn’t have told you the names of any artist or any songs. Then I realized, when among a group of friends at school, I should probably start to pay attention to music. Standing desk or sitting desk? This is a sore point for me. Both. I cheaped out when I bought it and got the one that’s hand-wound to change from standing to sitting, and it is a lot of effort to get it to wind up to standing. Ideally, I’d probably do my mornings standing and my afternoons sitting, but I found out the hard way. It’s actually easier to get the jack out the trunk of my car and lift the car up so I can change a tire than it is for me to lift my desk up. It’s ridiculous. The winder has so much effort involved. When we move, I’ll take the opportunity to get a different one . Early bird or night owl? Definitely an early bird. Ideally up before the sun is to actually watch the sunrise. I really feel just how much my productivity suffers as the evening wears on. I can be sitting there and just feel it leaking out of me. What do you do for fun asides from coding? I like to watch a lot of cartoons, go to the cinema, find cool new places to eat and play Factorio. What cartoons? Futurama is always on loop. Archer and Rick and Morty are also big faves. On my recent trip to Colorado, I had my hotel TV on The Cartoon Network. Just low volume, all night. Love it, everything. If you were to apply a theme song to your life, what would it be? Have you ever seen the Disney film, The Emperor’s New Groove? There’s a character in there called Kronk, and there’s a scene where he’s running through the town trying to be secretive and sneaky, but the whole time he’s mumbling his own theme song. Favorite morning beverage? Coffee. Espresso, sometimes a latte. When I’m at home, probably just an Espresso. I make it just with a little Nespresso machine, stick the pot in, put the cup in there and push the button. Favorite movie? I like a lot of movies so this one’s really hard, but I went with one that’s good all round, which is The Life Aquatic with Steve Zissou, one of the Wes Anderson films. Bill Murray. Kind of a Jacques Costeau parody. If you could have a superpower? Based on recent experiences, I’m going to go with fast healing like Wolverine. Every time I travel, I pick up something, whether it’s a cold, flu or stomach bug, something… I’m just wiped out for a week, so I’d be super grateful not to have to deal with that. Favorite vacation spot? Hawaii Favorite snack? Those little packs of two-minute noodles, but I add egg and something like chorizo. Childhood hero? I really don’t think I have one and I kind of operate today under the idea that you don’t want to meet your heroes, the people you look up to… It ruins the nice shiny exterior. In the end, they’re just another flawed human being, like all of us. Favorite hour beverage? I’ll go with a beer, and basically anything that’s done in a Belgian style. In our last interview with Glenn, he said to ask how many Disney parks you have visited? I’ve been to three of them. I’ve been to DisneyWorld in Florida twice. Disneyland in California twice and Disneyland in Paris, once. Disneyland California is the favorite as they have the Pixar Pier with the Cars ride, which is super cool. I think I’m going to try and do Disney Tokyo next."
"222","2019-10-23","2023-03-24","https://www.section.io/blog/section-engineer-interview-glenn-slaven/","In this interview, Section Engineer Glenn Slaven chats about building cool stuff, taking time to enjoy sunsets, and life out in the country with his wife, four kids, two dogs, a cat, two guinea pigs, and two alpacas. ### Glenn Slaven, Software Engineer Why did you decide to become a software engineer? It was really the one thing I was good at in school. There wasn’t any question from the beginning. Computer science was the one subject I excelled in, so it was really clear that this was what I was going to do. What keeps you motivated? I mean, really it’s a multi-faceted thing. What keeps me motivated to get up, go to work and do the job is a combination of wanting to do something interesting, wanting to be able to feed my family, wanting to work with interesting people and do interesting things and keep building stuff that’s unique. There’s a part of it that’s doing a job and getting paid, having income, all that sort of stuff. There’s a part that drives me to do any job, but then there’s the part that drives me to do this job and do it well, and that’s more about the people and the cool stuff we’re building. You hold the title as Section’s first employee. What keeps you at this job? Yes, I think it’s been seven years. So, this is the longest job I’ve ever had. In software, we move around a lot. It’s always the way I got advancements and pay raises, was to go find another job. That was just the way it worked. What keeps me here is a combination of the people and the convenience. Working from home is awesome. I know it’s becoming more common in the industry, but there are still a lot of places that expect to see you face to face all the time otherwise they don’t think you’re working. So this work from home first thing is fantastic. It allows me to see more of my family. It allows me to live out here in the middle of nowhere, which I couldn’t do if I went and got a job in an office. I could, but I’d have to commute for two hours each way. The flexibility, the people and add on top of that, the fact that we work with technology that’s so cutting-edge, there’s blood on the floor. It’s pretty cool. What is your proudest accomplishment? My kids. What’s it like having four kids? Once you’re outnumbered, you kind of lose track after a while. It’s pretty full-on, although it’s funny we used to live out of suburban Sydney, which I imagine is like the suburbs anywhere. We were the large family with four kids and it was very weird and you get all the jokes. Then we move out here to the country and suddenly we’re the small family. There are people with eleven kids, eight kids. I don’t know what the right answer to that question is. How do you cope with four kids? Maybe the right answer is you don’t and you just go crazy. What do you guys do for fun as a family? Take my kids to their activities! At the moment, we’re getting together and introducing the kids to Firefly - it’s a sci-fi TV series. There’s only one series and it’s pretty awesome. It’s been great fun introducing my kids to old stuff that we enjoyed. We all sat down and watched Stranger Things while my wife and I geeked out on the nostalgia of the 80s. We introduce them to all the old music and everything. What soft skill or personality trait do you attribute most to your success? That’s an interesting one. I don’t know. I think flexibility probably. You know the old axiom that change is the only constant. Never truer than here. So being able to just go, “You know what, that didn’t work, we’ll try something else.” It’s hard because you get invested in the things you do, but being able to just go OK, that didn’t work, let’s move on. I try to do that. Adaptability is important in this industry. I can’t imagine that electrical engineering changes as rapidly as software engineering does. I don’t know. Maybe it does. I’m not an electrical engineer. What’s the most challenging aspect of your job? Constantly having to learn new things. I joke, but that’s probably it. I walked into this job because Dan [Daniel Bartholomew, Section Co-Founder & CTO] thought I could do it, and I’ve spent the last few years trying to prove him right. I was a Microsoft shop guy before this, and I’d only worked in .net. I dabbled a little with PHP (I shouldn’t have said that with the recording on as I try to pretend I don’t know PHP…), but prior to this job, I’d worked for years in .net. I was very much part of the Microsoft mothership and ecosystem. I come over here and it’s Apache and Varnish and command line. What’s command line? Having to learn all this technology is challenging. And it hasn’t stopped. Every couple of months, we turn around and pick up something brand spanking new that I haven’t heard of before and suddenly we have to not just be able to know it, but run and deploy it a thousand times and have it run properly. I think the most challenging part is definitely picking up brand new things that are often challenging, like M3DB, that are often not yet supported with mature documentation, and make them work. We haven’t always won, but by and large we’ve succeeded here, which I think is a testament to the fantastic people we’ve hired. What do you like most about working at Section? While it is the biggest challenge, being able to work with the new stuff is very cool. Also, I know Mani said the same thing, but it’s true, the people here are great. It’s a great company. We’ve managed to hire some really great people. I mean everywhere I’ve worked, there’s always been people who are difficult to work with. I just don’t see those people here. I don’t know how we’ve managed to achieve that. I don’t know whether it’s just because of the type of company it is. I don’t know whether it’s because Dan and Stew [Stewart McGrath, Section Co-Founder & CEO] are just really good at screening people, but we’ve managed to avoid hiring people that I don’t want to work with. Over on the engineering side, I hired the first 4-5 people, so that probably helps too. What single piece of advice would you give fellow engineers? Go to bed. Seriously. It’s the same thing I tell every person who starts here. Don’t push it. Don’t work overtime. If there’s a crisis, sure, deal with it. On a day-to-day basis, stop work, have a life. Getting that right. Part of the problem with us as software engineers is that we’re mostly nerds and we love it. We love what we do, even if from time to time what you’re working on is frustrating, we’re doing this because we enjoy it, so it’s often hard to stop. I have family. They want to see me. You have friends. They want to see you. Go have a life outside of the keyboard. I know I sound like an old man when I say that, but it’s important. I’m better now, but I used to be terrible at going to bed. I’ve always been a night owl. It doesn’t make for a happy person the next morning when you’ve stayed up until 1 or 2am. What’s your favorite part of your day? There’s a really cool part of the day around 4pm in the afternoon. The window on the other side of the room looks out over mountains, which is just awesome. The sun is going down and the sky lights up, and that’s a cool part of the day. I enjoy that view quite a lot. It’s one of the benefits of being able to live out here. This is a pretty cool part of the world. We’ve been here now for five years at the end of this year, and it’s just magic. Living out in the country, do you have animals? Yes. In fact, my tagline used to be, “He has more animals than children, and he has a lot of children.” Let me go through the list. I’ve got two dogs, a cat, two guinea pigs and two alpacas. We did have more animals, but we’ve lost a couple. The alpacas get shorn about once a year. There’s a guy who comes out and does it. They’re the weirdest looking things when they’re shorn. I think I posted a picture the last time it happened. They’re very cool animals. They have a lot more personality - I thought they would be like long-legged sheep, but they’ll actually come to you. My wife trained them to come when you click, and it’s pretty cool. We don’t do anything with the wool. Not yet. That was the idea, but time is the one thing we don’t have a lot of. I’ve got bags and bags of it sitting here. The plan was to do something with it but we just haven’t yet. We didn’t have precise ideas, just make something. It’s more my wife’s field. She’s very talented with all the handy, crafty stuff. What are you currently reading? Hahaha. I’m a voracious reader. I was banned from buying any more books a few years ago as we ran out of space, so I had to switch to Kindle and audio books. I resisted for the longest time because I have a great affection for paper books, but we ran out of bookshelves. My other great failing, however, is that I am a perennial multi-book hogger. I’ve actually been reading the same books for quite a while. I’m still in the middle of The Lies of Locke Lamora. I’m also in the middle of Rivers of London, which is more of a modern fantasy comedy by Ben Aaronovitch. And I’m also reading Made Man. So I’m in the middle of three books at the moment. One of these days, I’ll finish them. I’ve done it all my life. I have two different ways of reading. I either sit down and read a book in one sitting. I did that with The Magicians. That was a mistake, as that was a big book. I sat down to read it one afternoon and I finished it at 4am the next morning, which was a very bad idea, but I couldn’t put it down. I read a lot of fiction. I love novels. What are you listening to? Most of my music listening happens in the car. I tend not to listen to music while I’m working. It actually distracts me. I know some people find it helps with focus, but I find it distracting, so most of my listening happens in the car. One of the things about living out here is that you do a lot of driving because everything is a long way away. So you know, the kids activities, you drive into Cooma - half an hour away. Go to church on Sunday - half an hour away. Take the kids to drama in Canberra- two hours away. It’s what you do. The upshot of that is that you listen to a lot of music, and Spotify has been fantastic because I’ve been able to introduce my children to the joys of the music I grew up with. What’s more, I don’t know why, but they actually like it. Our playlists are these strange mishmashes of Jack Stauber and FEiN, and these weird artists that I hadn’t heard of, but my kids are introducing me to things that they’ve discovered through YouTube and stuff. And David Bowie, Queen, They Might be Giants, Crowded House, and all the stuff I grew up with. We get this mishmash. My kids are now finding the old 70s and 80s playlists on Spotify, listening to them and bringing me songs, asking me, do you know this one? I’m like, yeah, I know that one! My son started playing Yes! songs a while ago. I was like, how did you even find this? Apparently, it was on an anime. Name an interesting fact that most people don’t know about you. When I first got married, I practically didn’t know which side of the hammer to hold. It’s not something I did growing up, and I was very much a typical nerd who sat at the computer. I horrified my father-in-law, as he’s a very talented builder who can do anything. So he forced me to help with a deck he was building. Since then, I have actually taken on a number of projects and built all sorts of interesting things. Some of which I haven’t finished. (At some point, I’ll finish the number of sheds being built in my backyard.) It’s become something I enjoy doing more, and not something I ever thought I would enjoy. It is actually quite satisfying to think we- the whole family - built a pergola outside our back door. It’s satisfying to look at it and think you built the frame, screwed it all together - that’s pretty cool. Do you do a ton of planning before those projects? Yeah. I like to have an idea of where I’m going first. It’s something I started doing more recently. It still terrifies me. There’s no undo button. You cut the wood wrong. You have to go buy new wood. Where’s the ctrl-z button? That feeling of satisfaction crosses over into software engineering. It’s cool to be able to look back and think yeah, I built that. It’s the same energy. Standing or sitting desk? Sitting. Early bird or night owl? Night owl. Favorite morning beverage? Black coffee. Favorite happy hour beverage? Kosciuszko Pale Ale Favorite movie? The Princess Bride If you could have any superpower, what would it be? Come on, flying would be pretty cool. Favorite vacation spot? We’ve kind of moved to it (Berridale). I love the bush. When we lived in Penrith, we used to go up to the Blue Mountains. I don’t know if you know the geography of Sydney at all, but Penrith is the outer western fringe of Sydney. It’s in this sort of basin and it’s surrounded by this line of mountains called the Blue Mountains. It’s part of this great climbing range that Mount Kosciuszko is part of that runs across the whole eastern coast of Australia. The Blue Mountains are beautiful. We often went up there for bush walks. When we were looking for somewhere to get out of the city, we picked the bush. We now live in a place where we can go for walks just down the street. There’s a creek I can walk around. There’s bushland. We can go up the mountains to go skiing. Yeah, we moved to a vacation spot. It’s pretty cool."
"223","2019-08-20","2023-03-24","https://www.section.io/blog/section-engineer-interview-mani-batra/","In this interview, Section Engineer Mani Batra shares what drove him to a career in software engineering, the most challenging aspect of his job, what side projects he is currently working on, the single piece of advice he would give fellow engineers, and lots of other fun tidbits along the way. ### Mani Batra, Software Engineer Why did you decide to become a software engineer? I was always good with computers, but until I was 21 or 22, I had no idea what I was going to do. I liked computers, but that was it. I was working in the industry but I always thought I’d switch to something else, like an MBA. Maybe even modeling - that came up too! Then I read Steve Jobs’ biography and that made me realize how much power technology has to change the world. Since then, I’ve felt like I can’t do anything else. That was a watershed moment. I thought, this is what I’m going to do forever. What keeps you motivated? I have to think about this. I don’t think I believe that much in motivation because I’ve tried to get hacky with various things to get motivated. For me, right now, it’s more about discipline and just getting started. I know for a fact I love programming. I know that if I start, within fifteen minutes I’ll fall back into the flow and start loving it. But yeah, I’ve given up on motivation. Day-to-day, I don’t think in terms of motivation anymore. What is it about programming that you love? I don’t know. I think it’s maybe a combination of a few things. There’s always something new to learn. I think maybe it’s uncovering what is believed to be this magic that’s happening - the Internet is just working, but getting to the root of how it’s actually happening to get right to the bits of it… that’s awesome, that’s what I love. What is the most challenging aspect of your job? I think it’s focusing and not trying to do everything. So much cool stuff is happening and everyone is trying to do cool things, and yeah, you’re tempted to try and answer to every question or try and help out everywhere. I struggle with that. I’m trying to get better at it and realize you can’t do everything… because otherwise you’ll be average at everything. That’s 100% the most challenging aspect of the job. Making sure that what you’re prioritizing is actually adding value to the customer. You can be working on building some cool tech, but if it has zero value, which customers don’t care about… [it’s a waste of time]. What do you like most about working at Section? Hands down, the people. It’s the most amazing bunch of people I have ever come in contact with. Not just work wise, but generally, so helpful and just awesome. Even on the days when you’re frustrated for some reason or another, you know that they’re there for you. It’s just awesome. What is the worst technology you’ve ever worked with? The worst memories I have are working with Java in my first job (for a banking company). It wasn’t like Java was bad per se, more how it was implemented and the infrastructure and everything was a mess. There was no culture of improving things. That was a nightmare. I think that’s why I started to hate Java, but it wasn’t the technology, more how it was being used. It’s such a legacy industry in which people are afraid to make changes, maybe for valid reasons such as not wanting to introduce new risks, but [nevertheless resulted in a pile of…] there were piles of bad things being done over a period of time, which led to a very bad architecture. Are you Dev or Ops? I think both. I like making things, building things. I remember saying in my Section interview that for me, the meaning of life is building things. So surely dev. And I like figuring out problems and automating stuff, so I think both. Everyone in our company has that same attitude where you build up things then run them, and if there are problems, you fix them. So yeah, I would say both, surely. What single piece of advice would you give fellow engineers? I think that would be from the book I’m reading right now: you can be good at anything, but you can’t be good at everything. That’s what I struggle with. I really struggle with it to the point where I’ve felt guilty for not doing enough things during my day. I’m reaching a point now where I’m realizing there are only 24 hours in a day and you have to sleep… so yeah, I think if people understood that, there would be less frustration and stress generally. What are you reading right now? I’m reading this book, Essentialism: The Disciplined Pursuit of Less by Greg McKeown. It has a long tagline, but it’s mostly about trying not to do too many things. It has really put my mind at ease. Aside from that, I like history. I like reading lots of autobiographies. A lot of US presidents for some reason. I don’t know why. What are you listening to right now? I’m going through my Beatles phase right now. It’s Beatles all day, every day, on repeat. What’s a side project you’re working on, if any? Recently, one thing that was cool was spinning up our platform on the Google Cloud Platform as a curiosity side project. I did it during my out of hours (fueled by beer). That was fun. I thought, shit, yes, I did that! I have out of hours tonight, so I’ll probably pick a new one. I’m thinking of playing around with Anycast, but that’ll be decided tonight. We are working on such cool stuff that there is often crossover in what I decide to tinker with. Favorite off-the-wall comment in the #random channel? We were talking about pushing code to prod straight away. That’s when this was posted. What do you do for fun, aside from coding? Right now, I play guitar, which is quite fun. I want to play Beatles songs, but I’m not there yet. I’m currently learning how to switch between chords, which is so damn hard. Stewart (Section CEO) recommended Fender Play, which I started with. After a couple of levels though, the learning curve became too steep. There’s a guy called Justin Guitar who has a free course on his website. I’ve bought his book, which has chords and tabs for various songs. That’s what I’m using as a resource. Apart from that, reading and binge watching stuff. That’s what I do. Last weekend, I binge-watched The Boys on Amazon Prime. It’s such a fresh take. Basically, superheroes have gone bad and there’s this group of people trying to rein them in. Favorite morning beverage? Lots of coffee. Just plain black coffee. Favorite song of all time? Right now, it would have to be Help by the Beatles Favorite vacation spot? There’s a place called Terrigal, a train ride away from here (Sydney, Australia). Mountains and beaches, and there’s hardly anyone there. I’ve gone by myself a few times just to get away from it all. That’s been an awesome experience. Favorite snack? Is coffee a snack? Who was your childhood hero? I don’t think I’ve ever had any heros. When I was little, someone - maybe one of my teachers instilled in me, you should never have heros as you will put them on a pedestal. If it’s physically possible, you can do it. If you want to. So I don’t think I’ve had heros, but there have been a lot of people who have been inspiring. Favorite happy hour beverage? Ever since I started at Section, I really like beer. But if I had to pick just one drink of choice, it would be Lagavulin Scotch."
"224","2019-08-30","2023-03-24","https://www.section.io/blog/section-engineer-interview-pavel-nikolov/","In this interview, Section Engineer Pavel Nikolov chats about trying to keep up with the rapid pace of change in technology, his favorite part of the day, and why everyone should treat themselves as a stock option. ### Pavel Nikolov, Software Engineer Why did you decide to become a software engineer? Well, initially I believe the decision was influenced by my mother who is a math teacher. When I was in fourth grade, she sent me to take some math courses. In my country [Bulgaria], children go to school until seventh or eighth grade and it is the same kind of school everywhere (unless it is private with a special kind of program). Instead, my mother sent me to an experimental program where you start learning something specialized after fifth grade. In my case, it was math and programming. By the time I was at the right age to go to high school, it was too low level. We were studying math at the university level at my school; the same with programming. By the time I went to university, I had some work experience as I had already started work as a freelancer building web pages. In addition, I was playing computer games. Those things combined got me into software development. At that age, however, I had no idea what software development meant. I guess I got onto that path all because my mother sent me to that school. What keeps you motivated? What keeps me motivated is usually solving challenging programs, especially at scale, and especially in production – live systems. Something that involves very hard problems, where you think about them even when you’re not working and keeps your brain busy. And after that, if you manage to solve one of these problems, it makes you feel satisfied. What is your proudest accomplishment? In terms of work, one of the projects I’m most proud of is one I just completed. Before I started at Section, I worked on a very large, innovative project in a media publishing company. I’m proud of this project as it turned out to be a huge success and I was there from the beginning, so my presence had a significant impact on the project. I can’t take full credit as I was just a small part of a large team, but I had the opportunity to make some technical decisions there, which turned out to be a success, so I’m proud of that. Do you typically like to take ideas and create an execution plan, or do you like to be part of the ideation process? It depends. I like to be part of the thinking process and developing an architecture, as well as implementation. Ideally, I like both to be part of the team that comes up with the ideas and then building the implementation, not just handing off ideas. Ideas without implementation mean nothing. So, I like to be on both sides or somewhere in the middle. What personality trait do you attribute most to your success? That’s a tricky question to answer, but I’m eager to learn new things all the time. So with every person I communicate with, I try to find the opportunity to learn something from them. I think we can learn from everyone around us in one way or another. Same goes with software development and work, I try to surround myself with people who can teach me, and people from whom I can learn a lot of things. Success is not defined by whether I can do this one thing or not; it’s more like a never-ending journey. In my opinion, it’s a never-ending learning experience and then applying what you’ve learned to your next project, then the next, and so on. What’s the most challenging aspect of your job? Definitely the fact that technology is moving so fast that it is impossible to read all the information that gets published on a daily basis. Every hour, people publish information about technology on the Internet; you couldn’t read it all in a lifetime. So, keeping up with the technology and finding some way to stay in front and up to date, but also filtering out noise, that’s getting more and more difficult. Then, even if you learn something, a few moments later, it’s obsolete. Every day, we have to work on problems that didn’t exist yesterday. Software development is a job that requires constant learning. Basically, the moment you stop learning, your skills become obsolete. Of course, some things like fundamental programming will always be relevant, but you won’t be able to apply this into new software systems and stay on the cutting-edge if you don’t keep learning. That’s the challenge. Every project, every task is different. It’s a never-ending learning process. How do you keep up to date? Do the projects you’re working on lead you to discover new resources, or do you have regular resources that you check in with? Well, I do have regular resources that I check in with. I think it’s the opposite actually. I skip through a lot of the things that get published, and then try to find jobs that allow me to work with the resources I’m interested in. I don’t first find the job. I first learn something that’s interesting to me, and then find a project where I can apply it. What do you like most about working at Section? I like being able to work with technologies that I like. I also like that we are a small, distributed team. What I like the most probably is that I have the opportunity to learn new things. I like that it lets me work with edge computing and develop my skills in an area that is interesting to me. So you developed an interest around edge computing, or specific challenges within our ecosystem, then you found Section? I developed an interest around working with CNCF projects and solving problems at scale. Initially, I started as a web developer and I was between the front-end and back-end, then I moved a little more to the back-end, then to scalability, then to some automation, Kubernetes and microservices. Now at Section, I can apply many of the things that I already know, but I also have the opportunity to learn new things that are super interesting to me. I got excited during my interview with Dan [Section Co-Founder & CTO] when he said he wanted to change the way the Internet works; that seemed super interesting to me. What’s the worst technology you’ve ever worked with? To be honest, I have not worked with technology that’s bad. There is no bad technology. There are bad developers; people who have no idea what they’re doing and are writing code, but I’ve never seen bad technology. Some programs, systems or even programming languages have flaws, but if it’s not the best tool for your job, you just don’t use it. I’ve seen people who have no idea what they’re doing and are just copy and pasting something without understanding it. That’s not bad technology; it’s poor implementation of someone else’s idea. Would you consider yourself Dev or Ops? What’s the difference? There is no difference. Anyone who thinks that there is a difference, I think is wrong. You can’t do ops nowadays without being a developer, writing code and doing some kind of automation. Maybe twenty years ago, people who used to be in ops would unpack the boxes with new servers, put them in racks, install software on them and request more RAM and CPU when things broke down. Today, everything happens in the cloud. Everything is an API code. You can’t possibly do effective ops in any team size. It doesn’t matter if it’s a team size of two or 20,000, you always need to automate things as much as possible. This is where the dev came into ops. I believe that some people, especially the new generation of developers, will not get familiar with owning their own servers, ensuring there is enough ventilation and air conditioning and so on, to ensure your servers don’t melt. This whole thing doesn’t exist anymore. Obviously, it exists in the cloud, but we borrow them from cloud providers or hosting providers, which means it is now impossible to do operations without being a developer. So when you ask me if I am one or another, I don’t think there is a difference. Any developer can do both as long as they are familiar with the API of a given cloud provider. What’s your favorite part of your day? When I pick up my son from school, then we cook dinner together; that’s my favorite part of the day. He’s 5 years old. We cook all kinds of things. One of my hobbies is cooking. We cook every single evening. No exception. What single piece of advice would you give fellow engineers? My advice to everyone… every single person (they don’t need to be an engineer) is to treat themselves as a stock option. What I mean by this is invest in yourself. That mostly includes knowledge, but it also includes practice and testing things. Try to learn new things, things that are going to be used. First, obviously learn the basics. Things you’ll learn at university. Be able to write a program and write it well. After that, focus on your CV. Obviously, we only have limited time and we can’t learn every single thing. There are different paths that an engineer can choose to focus on. Maybe one engineer is interested in building APIs, another in game development. Once you have an area to focus on, then find the technologies for that path and keep investing your time in acquiring knowledge and skills that are beneficial to you and which increase your value over time. These skills are first useful for the companies where you work, but also if something happens with that company or with your current position, your skills will still be in demand in the future. My advice to everyone is to keep learning. Never think you know everything, that’s impossible. Just keep learning. What are you reading currently? m3db code, lots of that! Otherwise, the last thing I read was some Spanish grammar, which I was using to understand something. What are you listening to right now? What’s on your playlist? I’m listening to random music on Spotify, as well as Spanish and French music, as I’m trying to improve these two languages. What are you tinkering with on the side, if anything? I try to distract myself from thinking about work. When I’m deep in a challenge at work, my work doesn’t end when I’m away from the keyboard. I keep thinking about it. I need to distract myself. In order to do that, I try to learn languages. I was focused for some time on Spanish, and now my focus is on French. What’s something that most people don’t know about you? I prefer to go camping instead of staying in a hotel. I prefer to be closer to nature instead of being surrounded by buildings. Standing or sitting desk? I don’t like standing desks. Early bird or night owl? I’m most productive during the night. I usually wake up early to prepare my son for school, and I also stay up late. Recently, I’m trying to sleep more, but I’m more productive at night. I don’t know why. I’ve always been like this. What do you do for fun, aside from coding? I train in Brazilian jiu-jitsu. That’s probably one of my favorite things. If you were to apply a theme song to your life, what would it be? Three Little Birds by Bob Marley. Favorite morning beverage? Maybe coffee. Or water. I don’t know. Coffee. Favorite movie? Lock Stock and Two Smoking Barrels by Guy Ritchie. If you could have any superpower, what would it be? Be able to stop time. More time. So I could enjoy more. That’s the one thing I want. Time. Favorite vacation spot? I have a lot. All of them are camping spots. Some of them I don’t know how to describe and they don’t have names. I have favorite places in the mountains and on the beach. Ideally close to natural reserves, lakes, rivers. There is one lake in my country [Bulgaria] that I really like called Beglika. Here in Australia, one of the places I really like is called Greenpatch. It’s really nice camping with a secluded beach where I’ve been several times. Definitely one of my favorite spots. Favorite snack? I don’t do snacks. No snacks. Maybe eggs. That’s my favorite food. Any type of eggs. Mostly, I eat them raw. Who was your childhood hero? My grandfather. Favorite happy hour beverage? Maybe wine. Red or white, depending on what food I’m combining it with."
"225","2019-03-18","2023-03-24","https://www.section.io/blog/reisz-joins-section/","I am excited to officially join Section. Today marks my first full day as part of the Section team. My role will be to work across all areas of the business as Section continues to evolve the definition of edge compute. Prior to joining Section as the VP of Technology, I was the full-time product owner for an International Software Conference called QCon. As part of that role, I chaired four early adopter/early majority stage software conferences held annually in London, San Francisco, and New York. Serving over 4k senior developers and architects annually, QCon (like Section) is focused on developer experience and built specifically with software engineers in mind. Moving forward, I’ve stepped down from the product ownership of the conferences; however, I will continue to remain involved with QCon through chairing QCon San Francisco each November. In addition to the software conference that I’ll continue to chair, I’m also the host of a popular podcast called The InfoQ Podcast. The podcast has each over 1.2 million downloads in the two years since it’s inception. Before Section I had several roles with large enterprises and small startups. The longest period of which I was an architect with HP Enterprises at Fort Knox, Kentucky. There I was a member of the US Army Human Resources Command/Recruiting Command architecture team, principal architect for US Army Cadet Command, and did solutioning for HPE US Public Sector. In addition for about 13 years, I was an adjunct professor at the University of Louisville. I can’t tell you how excited I am to be joining a team that is facilitating the conversation with edge computing. I’m particularly excited to be able to contibute to what it means to run Kubernetes at the edge. I look forward to working with each of you!"
"226","2020-05-08","2023-03-24","https://www.section.io/blog/tutorial-deploy-containerized-workload-to-the-edge/","Section operates an edge compute platform. At first glance, you might think that’s just a content delivery network (or CDN). The distinction between what we call an edge compute platform and the traditional CDN you might be familiar with really comes down to choice. The edge compute platform allows users to run any containerized workload that meets the platform module contract. In the simplest terms, this means you can run the tools you need in front of your origin (or at the edge of your application boundary). While the tools/products you might typically find in a CDN can be run in the edge compute platform (things like Varnish Cache 4.x through 6.x, image optimizers like Cloudinary or Kraken, or security products like PerimeterX Bot Defender or Signal Sciences Web Application Firewall), the features you will find in the edge compute platform cannot necessarily be run in a traditional CDN – at least not universally. In effect, the edge compute platform flips the hardware-centric model of a CDN to a software-centric model of a platform. The result is a choice when it comes to what you run at your edge. One example of a feature the edge compute platform offers that really separates it from traditional CDNs is the ability to deploy custom code at the Edge or “Bring Your Own Workload”. Bring Your Own Workload Bring Your Own Workload refers to the capability for a customer to leverage the edge compute platform to write and maintain their own logic and deploy it at the edge of their application boundary. The edge compute platform then takes responsibility of orchestrating its global distribution, availability, elasticity, and lifecycle. You build it, the edge compute platform runs it. Before diving into what it might look like to write your own workload and deploy it on the edge compute platform, a discussion of the platform and how it operates might be useful. Edge Compute Platform Section’s Edge Compute Platform is based on the orchestration technology Kubernetes. The platform allows containerized workloads to be run in a “daisy-chain” configuration in front of an origin (or your site/application). The goal is that each module (think pod in Kubernetes terms) in that chain can provide a host of edge services. Some of the Edge Services powered by these modules include features like content caching/enrichment while others can provide security/blocking capabilities found with bot blockers and WAFs. The platform is responsible for ensuring these workloads are globally orchestrated across the desired Points of Presence (PoPs), scaled based on resource demands, and wired together. The platform, as well as the modules that are deployed for each environment, is controlled using a GitOps-based workflow. In addition to the modules that are inserted into the pipeline, the platform handles activities such as SSL termination/Geo enrichment and routing between different origins and alternate origins. Developing Your Own Workload The first thing we really need to understand is the proxy contract for any workload on the platform. As discussed, the platform takes on the responsibility of orchestrating your workloads’ availability, elasticity, and lifecycle. To do that, the system needs to be able to have insight into how it’s operating. The proxy contract helps the system understand how to do that and how to connect to the next module in the chain. The proxy contract requires: Workload should handle unencrypted web traffic (typically, this will be on port 80) next-hop:80 resolves to the next proxy upstream in the chain All access logs should be written to stdout and error logs should be written to stderr in JSON format A module validation script should be available at /opt/section/validate.sh Configuration files for the module should be in /opt/proxy_config/* Good candidates for edge workloads include things that direct traffic, enrich/decorate the request, or can live entirely at the edge. Things that are less desirable at the edge include applications with global persistence requirements or those that operate Linux kernel space code. How to Build Your Own Workload on the Section Edge Compute Platform Configuring DevPoP To begin local workload development, the first thing you need to do is build Developer PoP. DevPoP is a local instance of one of the Points of Presence you’ll find in the edge compute platform. You can build the entire PoP and tear it down in minutes. At its core, DevPop uses Minikube to build a single node cluster that operates a Section PoP and enables you to fully test your edge before deploying it to your live environment. First, let’s check Minikube is up and running. At a bash prompt, type: bash-4.4$ minikube status
 The output should be similar to this: host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
 NOTE: If any of your services are not running, revisit configuring DevPoP Next, let’s check that DevPoP has been deployed. bash-4.4$ kubectl get pods -A
 The output should be similar to this: kube-system            coredns-6955765f44-65zkk                     1/1     Running   7          34d
kube-system            coredns-6955765f44-xt77g                     1/1     Running   7          34d
kube-system            etcd-minikube                                1/1     Running   7          34d
kube-system            kube-apiserver-minikube                      1/1     Running   7          34d
kube-system            kube-controller-manager-minikube             1/1     Running   7          34d
kube-system            kube-proxy-bgrdj                             1/1     Running   7          34d
kube-system            kube-scheduler-minikube                      1/1     Running   8          34d
kube-system            storage-provisioner                          1/1     Running   11         34d
kubernetes-dashboard   dashboard-metrics-scraper-7b64584c5c-6n5f4   1/1     Running   4          26d
kubernetes-dashboard   kubernetes-dashboard-79d9cd965-xjz9n         1/1     Running   7          26d
section-bootstrap      bootstrap-c8d8f95fd-wwxpm                    1/1     Running   1          3m19s
section-delivery       default-http-backend-5dfbff8bc6-c5hkj        1/1     Running   0          3m3s
section-delivery       nginx-ingress-controller-shared-mtzmd        1/1     Running   0          3m3s
section-shared         api-proxy-678f9df47f-js4fg                   1/1     Running   0          3m15s
section-shared         event-handler-869cd99465-fl4c9               1/1     Running   0          3m14s
section-shared         git-daemon-669c9fc6bb-5tcrc                  1/1     Running   0          3m14s
section-shared         message-bus-7d48b56994-sqrsm                 1/1     Running   0          3m14s
section-shared         package-sync-vfgpp                           1/1     Running   0          3m15s
section-shared         time-sync-c4t8d                              1/1     Running   0          3m3s
section-shared         webux-f7ffcc568-npkcl                        2/2     Running   0          3m15s
 NOTE: As the tutorial mentions, each PoP is based on Kubernetes, so you can leverage existing Kubernetes knowledge to inspect and see the components of the platform. kubectl is the common way an administrator would interact with Kubernetes. You’re looking for pods using the section-shared namespace. If you don’t see these, you’ll need to config DevPoP on your Minikube instance by running: bash-4.4$ minikube ssh ""docker run --rm --net=host -v /var/lib/minikube:/var/lib/minikube:ro sectionio/section-init""
 You should see several pods start up with the section-shared namespace. Give them a few minutes to stabilize, and you’re ready to move on to deploying your environment. Deploying an Environment Next, deploy an existing environment (or create a new one in Section’s Console) to your DevPoP. Once you have the environment, clone it down to your local machine. git clone https://aperture.section.io/account/9999/application/9999/sample-application.git
 Once cloned, if you open section.config.json and, if it contains any modules at this point, you can remove the modules in the proxychain. We’ll be deploying initially without any modules. At this point, the proxy chain in your section.config.json should look something like this:  1{
 2   ""proxychain"": [
 3    ],
 4   ""environments"": {
 5       ""Production"": {
 6           ""origin"": {
 7               ""address"": ""my-s3-bucket.s3-website.us-east-2.amazonaws.com"",
 8               ""host_header"": ""my-s3-bucket.s3-website.us-east-2.amazonaws.com"",
 9               ""enable_sni"": false,
10               ""verify_certificate"": false
11           }
12       },
13       ""Development"": {
14           ""origin"": {
15               ""address"": ""my-s3-bucket-dev.s3-website.us-east-2.amazonaws.com""
16           }
17       }
18   }
19}
 Along with things like environment origin and alternate origins, the section.config.json file lists each of the modules you have in your stack and the order in which they’re loaded. At this point, we have built DevPoP, created an application in Section Console, and pulled it down locally. To deploy this environment to DevPoP, follow the normal workflow found in the Section docs. It will basically be something to the effect of: bash-4.4$ git add .
bash-4.4$ git commit -m ""Deploying to DevPop""
bash-4.4$ git push developer-pop
 You should be able to test and validate that site is working. Now it’s time to start building our own workload. Building Your Workload For this walkthrough, we’ll be using the example-simple reference. This example includes a simple nginx reverse proxy that participates in the request chain. (This example contains two examples: example-full is a more complete implementation that leverages a go-handler to better manage the application lifecycle and example-simple. The simple nginx reverse proxy.) For this tutorial, we’ll be using example-simple. The main files we’ll be using and discussing in example-simple are: create-module-assets.sh Dockerfile example-simple/.section-proxy.yaml example-simple/prepare.sh example-simple/validate.sh example-simple/proxy_config/ example-simple/proxy/nginx.conf create-module-assets.sh : A simple bash script that takes a single optional argument. It will call either example-full and example-simple (for this tutorial, we’ll be using example-simple). The script will package your workload source code and then generate the configuration files required to deploy onto the platform. Those configuration files are injected into the pod running the git-daemon. We’ll see how that works in just a bit. Dockerfile : This is a standard Dockerfile. You can use it to configure the container running your workload. example-simple/.section-proxy.yaml : This file contains the configuration information for your workload. The parts you need to pay particular attention too are the image, names, and container definitions. When Section operates your module, we will tune and recommend additional settings. example-simple/prepare.sh and example-simple/validate.sh : These are scripts that are automatically called before the script is deployed during a scaling event (prepare.sh) and before it’s available to serve content (validate.sh). These scripts should contain whatever is required for you to say your workload is prepped and ready to run. If you have no logic to run at these lifecycle stages, they must still be present and return an exit_val of 0. Any other exit_val indicates an invalid condition and will stop the deployment of the module. example-simple/proxy_config/ : A folder that contains (by convention) any files that you’d like to see deployed for your workload. These files are unique to an environment. So if you have an API key or other configuration that may be different for a module configuration, you can deploy it into the module via this folder. example-simple/proxy/nginx.conf : Things in this folder are available on all modules. nginx.conf is an example of one of those files required when setting up nginx. This file is common to all deployments of your module. To build this example-simple module, clone the repo and change directory to its root. You should be in the same directory as create-module-assets.sh. Run: bash-4.4$ create-module-assets.sh example-simple
 When the command prompt comes back, you should see: Built: gcr.io/section-io/example-simple:1.0.0-k1
 With this successful, you can use Docker and see the images available. In order to do this, you’ll have to make sure Docker is pointed at Minikube (the script does this for you in the context of its execution; however, you can easily do this from the command line). Run: bash-4.4$ eval $(minikube docker-env)
 Once you run configure things to point to your Minikube environment, type: bash-4.4$ docker image ls
 You should see the images you saw previously from the platform (these are images like nginx-ingress-controller, kube-proxy, and time-sync). In addition, you should see your new image: REPOSITORY                              TAG               IMAGE ID           CREATED           SIZE
...
gcr.io/section-io/example-simple        1.0.0-k1          11835f231f19       15 minutes ago    153MB
 One of the functions of the create-module-assets.sh script is also to add the configuration files required to deploy this module to the git-daemon. You can kubectl exec into that pod if you’d like to see the files have been correctly deployed. You’ll find them in a symlinked folder, located here: /opt/proxy-packages/active. bash-4.4$ ls -lrta | grep example-simple
drwxr-xr-x    4 600      nobody        4096 May  5 21:17 example-simple
drwxr-xr-x    2 600      nobody        4096 May  5 21:17 example-simple@1.0.0
 With the images in place, you can add this module to your environment and deploy it. Before we do that, let’s dive a little deeper into what we’re deploying with example-simple. example-simple is a nginx reverse proxy. If you look in the proxy directory (of example-simple), you’ll see the nginx.conf that defines how the proxy works. Part of the module contract says that all modules should call next-hop on port 80. next-hop is always defined on the platform to be the next upstream module of the current module (pod in kubernetes language). next-hop is injected into the networking stack by the platform and should always resolve to a VIP (virtual IP) that load balances to its members. Here is the portion of the nginx.conf that defines next-hop: 35upstream next_hop_upstream {
36    server next-hop:80;
37    keepalive 1;
38}
39
40server {
41    listen       80;
42    server_name  localhost;
43
44    location / {
45        proxy_http_version 1.1;
46        proxy_set_header X-Forwarded-For $http_x_forwarded_for;
47        proxy_set_header X-Forwarded-Proto $http_x_forwarded_proto;
48        proxy_set_header Host $host;
49        proxy_pass ""http://next_hop_upstream"";
50    }
51}
 This block of code shows the nginx configuration correctly calling next-hop. Custom lua or nginx C modules can be called on the request by inserting code here. While we’re in this file, it’s also worth mentioning logging. You can see the log_format that we’re creating and shipping into our logging pipeline here. The access and error logs are writing to locations in: /var/log/nginx/ If you open the Dockerfile (the one inside the example-simple folder), you’ll see that the actual locations are symlinked to stderr and stdout: 15RUN ln -sf /proc/$$/fd/1   /var/log/nginx/access.log
16RUN ln -sf /proc/$$/fd/2   /var/log/nginx/error.log
 The platform by default picks up and ships logs into the logging pipeline written to these files. Deploying Your Workload Open section.config.json and locate the proxychain block. At this point, it should still be an empty json array. Update it with your example-simple module. If you made no changes to the configuration, it should look like this: 1""proxychain"": [
2       {
3           ""name"": ""example"",
4           ""image"": ""example-simple:1.0.0""
5       }
6   ],
 Name can be anything you like, provided that it’s alphanumeric. Image is defined in the .section-proxy.yaml file. You can see it defined on line 17:  1metadata:
 2  configurationMountPath: /opt/proxy_config
 3  httpContainer: example
 4  image: gcr.io/section-io/example-simple:1.0.0-k1
 5  logs:
 6    additional:
 7      - container: example
 8        name: error.log
 9        stream: stderr
10    handler: example
11    http:
12      container: example
13      stream: stdout
14  metrics:
15    path: /metrics
16    port: 9000
17  name: example-simple:1.0.0
18spec:
19  containers:
20    - name: example
21      resources:
22        limits:
 NOTE: Line 4 has the image gcr.io/section-io/example-simple:1.0.0-k1 defined. While the two are similar, it’s only by convention. This is an actual version of the build and is different from the name. The name that you use in your section.config.json file will use this file to match the image name that is stored in the local repository (what you saw when you typed docker image ls). One last configuration before we deploy. What you used in your name field (in this case “example”), should match a folder under the root of the environment you cloned from the Section Console. All configuration files for the individual module will be placed in this folder. Your push to DevPoP will fail if this is missing. Here’s what your cloned environment should look like for example-simple: bash-4.4$ ls -a
.               example
..              local.config.json.sample
.git            outage_pages
.gitignore      section.config.json
custom_errors
 Once you’ve made these changes, type git add . to add the files to your local repository. Commit it with an appropriate message (ex: git commit -m “adding module to stack”) and then deploy it to DevPoP (git push developer-pop) bash-4.4$ git push developer-pop

Enumerating objects: 5, done.
Counting objects: 100% (5/5), done.
Delta compression using up to 16 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 344 bytes | 344.00 KiB/s, done.
Total 3 (delta 2), reused 0 (delta 0)
remote: Validating configuration for proxy example...
To http://192.168.99.125:30090/www.site.com.git
   2d813bb..8157bd7  build-own-module -> build-own-module
 Once complete, type: bash-4.4$ watch kubectl get pods -A

Every 2.0s: kubectl get pods -A                                                                                                                                                                                         

NAMESPACE                                 NAME                                         READY   STATUS    RESTARTS   AGE
kube-system                               coredns-6955765f44-dlxvz                     1/1     Running   2          6d21h
kube-system                               coredns-6955765f44-txjtl                     1/1     Running   2          6d21h
kube-system                               etcd-minikube                                1/1     Running   2          6d21h
kube-system                               kube-apiserver-minikube                      1/1     Running   4          6d21h
kube-system                               kube-controller-manager-minikube             1/1     Running   2          6d21h
kube-system                               kube-proxy-dpdg6                             1/1     Running   2          6d21h
kube-system                               kube-scheduler-minikube                      1/1     Running   2          6d21h
kube-system                               storage-provisioner                          1/1     Running   4          6d21h
kubernetes-dashboard                      dashboard-metrics-scraper-7b64584c5c-fgbwv   1/1     Running   2          5d18h
kubernetes-dashboard                      kubernetes-dashboard-79d9cd965-pxpgw         1/1     Running   4          5d18h
section-bootstrap                         bootstrap-c8d8f95fd-g5755                    1/1     Running   0          20h
section-delivery                          default-http-backend-5dfbff8bc6-jwwn6        1/1     Running   0          20h
section-delivery                          nginx-ingress-controller-shared-tqnm4        1/1     Running   0          20h
section-shared                            api-proxy-678f9df47f-sw7nc                   1/1     Running   0          20h
section-shared                            event-handler-869cd99465-g6rsh               1/1     Running   0          20h
section-shared                            git-daemon-669c9fc6bb-s487q                  1/1     Running   0          20h
section-shared                            message-bus-7d48b56994-9x7fb                 1/1     Running   0          20h
section-shared                            package-sync-sm8c2                           1/1     Running   0          19h
section-shared                            time-sync-k69kr                              1/1     Running   0          20h
section-shared                            webux-f7ffcc568-vqdls                        2/2     Running   0          20h
section-wwwsitecom-master-95b50d99c9097   egress-68cc85b4fb-9hmzm                      2/2     Running   1          20h
section-wwwsitecom-master-95b50d99c9097   environment-provisioner-8658b65cdb-lsks9     1/1     Running   0          20h
section-wwwsitecom-master-95b50d99c9097   example-655f95fcf6-cr58h                     2/2     Running   0          70s
section-wwwsitecom-master-95b50d99c9097   private-ingress-5c857f6677-6hdfv             3/3     Running   0          20h
section-wwwsitecom-master-95b50d99c9097   static-server-85bbd5fff8-dvcd4               2/2     Running   0          20h
 You will see the Kubernetes pods running, the section platform modes, and the pods associated with your environment. One of the pods will be the example we just deployed. In this case, it’s example-655f95fcf6-cr58h To get your Minikube ip address, type: bash-4.4$ minikube ip

192.168.99.125
 Add this ip address to your host file. Make sure that you’re pointing to the domain that you used when setup the site in the Section Console. Here’s what my host file looks like: ##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##
127.0.0.1        localhost www.webserver.com
255.255.255.255  broadcasthosto

192.168.99.125   mydemo.sectiondemo.com
 Now you can curl or go to a browser to run the proxychain and your new module in DevPoP. bash-4.4$ curl mydemo.sectiondemo.com
 One thing that is useful is to tail the logs your module generates when you make the request. You can use to see the requests as they go through your module bash-4.4$ kubectl logs -f -n section-wwwsitecom-master-95b50d99c9097   example-655f95fcf6-cr58h  -c example
 Note: To get the namespace and pod name, remember you can use kubectl get pods -A and to get the container name kubectl describe pod -n .... Hint: you defined it in your .section-proxy.yaml file. Wrap-up The distinction between the edge compute platform and a legacy CDN comes down to choice of the products and tools you run at the edge of your system. Section’s Edge Compute Platform allows users to run any of the prebuilt modules on the platform as well as your own workloads. In this tutorial, we walked through an example of building your own reverse proxy and inserting it into the pipeline. Using this pattern, you can insert your own logic to direct traffic, block requests, enrich data, or even serve requests directly at the edge. Wesley Reisz is VP of Technology at Section, Chair of the LF Edge Landscape Working Group, Chair of QCon San Francisco, & Co-Host of The InfoQ Podcast. Wes enjoys driving innovation in the edge ecosystem through awareness, community, and technology."
"227","2020-03-23","2023-03-24","https://www.section.io/blog/implementing-service-workers-on-server/","TL;DR: Service workers are a web standard most likely to be found in the browser. With some implementation code, service workers can be adapted to run server side. Server-side service workers can be used to proxy and manipulate the request. The edge can be divided into two spaces. The device edge and the infrastructure edge. Bounded by the telco concept of the last mile (or the final leg of the telecommunication networks that deliver services to end users), each side identifies an aspect of edge computing. On one side of the edge (the side closest to the users) is what is often referred to as the device edge (these are things like phones, cars, cameras). On the other side (the space between the last mile and cloud provider) is the infrastructure edge. The infrastructure edge isn’t new. It’s long been the space of traditional Content Delivery Networks (CDNs). These are generally caching/security solutions globally distributed to improve the performance, availability, and security posture of web applications. Recently, there have been interesting moves in the infrastructure edge, one of which is the notion of applying service workers to the server to create a serverless solution. Cloudflare Workers® is one such example. A fantastic bit of infrastructure edge tech, Cloudflare workers allows developers to write JavaScript functions that run on Cloudflare’s global network of points of presence (PoPs). By leveraging the V8 engine (specifically, V8 isolates), Cloudflare Workers allow you to implement service workers and run them on their infrastructure. The Evolution While not a new idea, Cloudflare was the first to apply the idea to the server and create a serverless approach to compute that leverages the Service Worker standard. Serverless, made popular by AWS Lambda, allows the developer of a function to focus on business logic and leave the operation/orchestration of the function to the provider (in this case, the Cloudflare network). To create Cloudflare Workers, Cloudflare used a control plane developed against the Service Worker API designed for the browser. The implementation essentially responds to events as if they were JavaScript Workers. Popular use cases for Cloudflare Workers include modifying requests, implementing routers, A/B testing, rewriting links in HTML and more. Experimenting with the Service Worker API So the question becomes, if Cloudflare Workers is based on a standard (the Service Worker API), can you run the same service workers on another network and provide your own server-side implementation? It turns out (at least after an initial MVP), the answer to that question is yes. This post is a tracer bullet. It’s a simple MVP of the Service Worker API implemented using Node/Express as the control plane. NOTE: This implementation doesn’t provide the full feature set or options needed for implementing service workers on the server. Instead, it asks the question: Is it possible? Further work is needed to move something like this to production. In this implementation, we take a Node.js/Express application and dispatch to the Service Worker code via a simple JavaScript wrapper. The wrapper dispatches to the Service Worker using event listeners. For the orchestration of these examples, I use the Node.js module available on Section’s Edge Compute Platform. The Section platform allows containerized workloads to be run at the infrastructure edge. The Node.js container is one of dozens of containers that are available and, in particular, allows developers to build JavaScript applications and deploy them to the edge. Service Workers Before we dive in, let’s back up a bit and level set on service workers. A service worker is JavaScript that operates as a type of event-driven web worker (or JavaScript that runs in a background browser thread). Service workers essentially act as proxy servers that sit between web applications, the browser, and the network (when available). They are intended, among other things, to enable the creation of effective offline experiences, intercept network requests, and take appropriate action based on network availability. Service Workers can be used for push notifications, background sync APIs, and handle network partitions. I first ran into service workers in a presentation by IIlya Grigorik of Google at Velocity in 2015 (you can watch a video of the presentation here). It should be no surprise that service workers are used extensively in applications ranging from Gmail to Chrome itself. The goal of service workers is really to run a bit of JavaScript that responds to browser initiated events (like an http GET) rather than user interface initiated ones generated in HTML. By sitting between the page and the server, the service worker can more seamlessly handle things like a network partition (or network outage) and give an application an offline experience. Once a service worker is made available, three functional events are supported: fetch, sync, and push. For this MVP, we take the Cloudflare hello world application and create our own control plane for running it. We will only be implementing “fetch.” I use Node.js/Express because it’s easy to shape the requests and responses and implement the invocation using event listeners. Hello World The canonical hello world example in Node.js/Express maps a route to a function that simply returns a “hello world” string. The actual implementation of Hello World in node.js looks something like this: 1const express = require('express');
2const port = process.env.PORT || 80;
3const app = express();
4
5app.get('/', (req, res) => res.send('Hello World!'))
6
7app.listen(port);
8console.log('Ready')
 We’ll do the same thing for our first Service Worker example, but instead of defining a method that returns a string, we declare and implement the FetchEvent found in the Service Worker standard. In JavaScript, we just pass along the request object generated in Express and then define an async function required by the spec called respondWith(). The respondWith() method provides an async response to the fetch. It looks like this:  1app.get('/', async (req,res) => {
 2  let ev = {
 3    request: req,
 4    respondWith: async function(responsePromise) {
 5      responsePromise.then(responseObject => {
 6        // only this one response header supported for PoC
 7        res.set('content-type', responseObject.init.headers['content-type']);
 8        res.send(responsObject.body);
 9      });
10    }
11  };
12
13  await fetchHandler(ev);
14});
 You can see we set a header from the service worker and then send along the body. Unlike a browser (where the default global scope is the window object), Node.js has a global scope scoped to the module itself. So to attach the newly created event to the global scope (“this”), we define a global addEventListener method with its implementation in the service worker we’ll pass in later. The await fetchHandler() method in the app.get() function assigns the event we defined with the request and the respondWith() method into that global scope. In the actual Service Worker code itself, the eventListener is created, and an async method to respond to the request is defined. We use the shape of the Response object defined by the Web APIs for the actual response itself.  1addEventListener('fetch', event => {
 2  event.respondWith(handleRequest(event.request))
 3  })
 4  /**
 5    * Respond with hello worker text
 6    * @param {Request} request
 7    */
 8  async function handleRequest(request) {
 9    return new Response('Hello worker!', {
10      headers: { 'content-type': 'text/plain' },
11    })
12  }
 NOTE: This is the “hello world” example used when you get started with Cloudflare Workers. The result is a Node wrapper that is called, executes the service worker, and returns “Hello worker!"". Proxy and Alter Request Okay, so we can get a basic hello world response using the pattern defined by the Service Worker, but can we do more? In the next example, we’ll define a service worker that uses the Fetch API (as implemented by node-fetch) to retrieve a web resource at: http://mydemo.sectiondemo.com/html_only/index.html The implementation is pretty straightforward at this point. We provide an implementation of node-fetch, npm install it, and then require it on the page. We use async/await to get the content from the backend page using the Fetch API. The Service Worker implementation looks like this:  1const fetch = require('node-fetch');
 2
 3addEventListener('fetch', event => {
 4  event.respondWith(handleRequest(event.request))
 5  })
 6
 7  async function handleRequest(request) {
 8    const uri = 'http://mydemo.sectiondemo.com/html_only/index.html';
 9    console.log(""About to call: "" + uri);
10
11    return new Response(await getContent(uri),{
12      headers: { 'content-type': 'text/html;charset=UTF-8' },
13      status: ""OK"",
14      statusText: 200
15    });
16  }
17
18  async function getContent(uri) {
19    let response = await fetch(uri);
20    let data = await response.text()
21    console.log(""data:"" + data);
22    return data;
23  }
 The result is the contents of the page are returned as if it was requested directly. Here’s the page we’re fetching: Because this Service Worker code sits in the request chain, we can also do things like manipulate the request, add headers, or rewrite portions of the response. Here is an example using the same code to do a simple replace on the body tag and inject a new CSS class.  1async function handleRequest(request) {
 2  const uri = 'http://mydemo.sectiondemo.com/html_only/index.html';
 3  console.log(""About to call: "" + uri);
 4  let body = await getContent(uri);
 5
 6  const modified_body = body.replace(
 7    ""<body"",
 8    ""<body class=\""dark\"""");
 9
10  return new Response(modified_body,{
11    headers: { 'content-type': 'text/html;charset=UTF-8' },
12    status: ""OK"",
13    statusText: 200
14  });
15}
 Conclusion Service Workers is a web standard that’s been around for a while. It’s been a really effective tool in the web developer’s toolkit to be able to create an intermediary between the webpage, the site, and the network. Cloudflare has been innovative in implementing a server side implementation to run these workers within their platform using V8. It’s given them the ability to provide a serverless solution on top of the Cloudflare platform to do things like manipulate the request and run code at the infrastructure layer. While there are other ways, using Node directly, Lua with OpenResty, or even Nginx with the OpenResty Module to do the same thing, it’s an innovative way to bring more control to developers at the infrastructure edge. As the Service Worker is a web standard, software engineers can also create harnesses to be able to run these service workers as well. This MVP demonstrates how Node/Express can be used to model the control plane required to build one. While there’s work to be done before an MVP like this can be called production ready (if such a thing was desired), it is possible to implement. This MVP shows a rough idea of how a harness might be implemented using Node. To learn more about Section, the Node.js module that I used to create the harness for the Service Worker, or any of the other BOT/WAF/custom containers that can be deployed on the Section Edge Network, please contact us and speak to one of our engineers about your use case. Wesley Reisz is VP of Technology at Section, Chair of the LF Edge Landscape Working Group, Chair of QCon San Francisco, & Co-Host of The InfoQ Podcast. Wes enjoys driving innovation in the edge ecosystem through awareness, community, and technology."
"228","2020-02-06","2023-03-24","https://www.section.io/blog/automated-testing-edge-configuration-verification/","In a recent article, we discussed how to manage changes to your edge configuration through a CD pipeline, and we specifically called out the importance of testing through each stage of the pipeline. Here, we’ll expand upon how to employ automated testing practices for verifying the behavior of your edge configuration. “If you’re not testing and verifying that each step in your continuous deployment (CD) pipeline is producing the expected outcomes, you’re essentially just shipping failure faster to production.” - Lindsay Holmwood, VP of Product at Section (from his talk at DevOpsDays Melbourne) Decide what guarantees you’re providing As you’re building out your CD pipeline and integrating testing into each stage, it’s important to understand and define what a successful system looks like so you can build tests that support and verify those requirements. Some of these decisions will stem from your users’ expectations and the guarantees (like service level objectives – SLOs, and service level agreements – SLAs) that you’ve promised. For example, an SLO might look like: 95e response time for HTTP requests in a one hour window is < 250 milliseconds 99.95% of requests should be served successfully The goal is to implement tests that reduce the risk of introducing errors into production and ensure SLOs continue to be met as changes are pushed through your CD pipeline. Change one, test one Chunking changes is one of the most critical testing practices when managing your edge configuration through CD pipelines. By breaking changes into smaller, verifiable units and testing as you go, you avoid being faced with failures at the end and not knowing what specific change introduced the failure. Order matters As you build tests into each stage of your CD pipeline, where you insert and run them most certainly matters! When you start adding tests to an existing CD pipeline, it’s tempting to batch all the tests at the end of all your changes: This makes it easier to see all the tests in one place, but it increases the delay between making a change and knowing if it worked. Worse still, it makes it harder for you to know what change caused the tests to fail – is the edge test failing because of the app change, the database change, or the edge change? Instead, we can interweave the tests between each configuration change: This closes the feedback loop faster between making a change and knowing if it works. It also improves debug-ability by reducing the surface area of changes. Speed matters It’s extremely important that tests finish quickly. A good baseline is to keep tests executing in under 10 seconds, and more preferably under 5 seconds. Because you can’t test everything, be sure to focus on tests that deliver the best and fastest verification of health and requirements. Examples of things to test for might include: Can I make a HTTP request and get a good response? Are there any obviously bad log messages? Is there a significant statistical deviation in metrics? Building your tests Goodness of fit tests, or statistical tests for steady-state systems, are helpful in identifying how changes affect a normal distribution. Here are some good starting points: Kolmogorov-Smirnov Kuiper’s Anderson-Darling Twitter’s BreakoutDetection R package Make feedback visual Raw data is useful, but visual representations help tell a more informed story around that data. You can even use visualization tools directly in the command line to further streamline your operations. (The below example uses gnuplot’s dumb | terminal | output)   1480 ++---------------+----------------+----------------+---------------**
       +                +                +                + ************** +
  1460 ++                                            *******              ##
       |                                      *******                 #### |
  1440 ++                    *****************                 #######    ++
       |                  ***                                ##            |
  1420 *******************                                  #             ++
       |                                                   #               |
  1400 ++                                                ##               ++
       |                                             ####                  |
       |                                          ###                      |
  1380 ++                                      ###                        ++
       |                                     ##                            |
  1360 ++                               #####                             ++
       |                            ####                                   |
  1340 ++                    #######                                      ++
       |                  ###                                              |
  1320 ++          #######                                                ++
       ############     +                +                +                +
  1300 ++---------------+----------------+----------------+---------------++
       0                5                10               15               20

CRITICAL: Deviation (116.55) is greater than maximum allowed (100.00)
 Level up tests by running them constantly Building a pipeline that integrates testing that is running all the time is the key to making this all work. One simple way to get up and running quickly is to use Consul from HashiCorp. If you have Consul deployed to all the different nodes in a cluster, you can drop in per node definitions for different services and then add monitoring checks specific to that type of service. The advantage to running your system in this way is that you only need a simple check to query the monitoring that is running constantly in the background, rather than having to do its own test. Summary When you automate testing through your CD pipeline, you’re able to increase your pace of iteration by identifying and addressing fault points faster without the operational overhead of testing on a change-by-change basis."
"229","2020-11-24","2023-03-24","https://www.section.io/blog/beware-prometheus-counters-that-do-not-begin-at-zero/","After using Prometheus daily for a couple of years now, I thought I understood it pretty well. But recently I discovered that metrics I expected were not appearing in charts and not triggering alerts, so an investigation was required. There were two related scenarios where expected metrics appeared to be missing, both concerned a web service hosted in a Kubernetes cluster. The first scenario was HTTP response count metrics divided by HTTP status code not appearing for uncommon status codes that had not previously been served by a particular Pod, e.g. 405. The second scenario was HTTP response count for error status codes, e.g. 500, that only happened during Pod start-up. The expected metrics were spikey, i.e. normally quiet but with occasionaly large values over a very short time. I knew, first from third-party reports, and ultimately by issuing my own HTTP requests to the service, that these responses were being served, but when I checked my charts the metrics did not show the same number I expected. My first suspicion was that my PromQL queries were incorrect, here’s one: sum(rate(http_response_total{}[2m])) by (status)
 There isn’t much to it: Query the last 2 minutes of the http_response_total counter. The scrape interval is 30 seconds so there should be enough data points in that window. Calculate the change over time because http_response_total is a Counter metric, always increasing. Sum all the matching series together, because there is a series with a different pod label for each replica of the web service, and group by status label. Rather than trying to debug each element of the query, I went directly to the raw counter values: http_response_total{status=~""405|500""}
 The raw counter values reveal the data that I thought was missing actually exists and also show a common pattern across the series that aren’t reflected in my summarised charts: they are all new series, i.e. each series has a data point with a non-zero value and there are no earlier data points for the same series inside my query time window. Why is this happening? It seems the rate PromQL function always returns zero for the first recorded sample of a series even when the sample value is non-zero. This is because the goal of the rate function is to compare multiple samples and interpolate the values in between. This interpolation behaviour is normally why counter metrics are ideal: they allow us to infer system behavior in the time window between scrape intervals, a capability not offered by gauge metrics. The problem with the first sample of a new metric series is that rate is attempting to compare against a non-existent previous value and Prometheus does not have enough data with which to interpolate. For counters, one might suggest that Prometheus should assume the missing previous value is zero, but the rate function also needs to know the timestamp of the non-existent sample to calculate the change over time, and the timestamp also isn’t available. It might be reasonable in some scenarios to assume the timestamp of the missing sample is one scrape interval earlier than the first known sample, but the scrape configuration is not part of the time-series database, and ultimately this is the current implementation in Promtheus that we need to work with. If most of your metric series (a series being a particular combination of metric name, label names, and label values) are long-lived you’ll rarely experience this issue because you rarely query a counter’s first data point. However new metric series are introduced quite often in some environments, like the environment I use: frequent Kubernetes deployments and Horizontal Pod Autoscaling create Pods with new names that in turn produce metric series with new pod label values and HTTP-related metrics with a status label that are not exported until a response with a given status code is first served. The problem impacts counter metrics predominantly because the value of a counter is essentially never consumed directly, instead a PromQL query function like rate or one of rate’s various friends is used to derive the value to render in a chart or compare against an alert threshold. In my experience, most interesting Prometheus metrics are counters, even histograms and summaries are implemented as counters under the hood. How could we fix it? It may be tempting to simply change the PromQL query to sum first and then rate which would almost eliminate cases where there are too few data points to interpolate. However this suffers from two errors: The rate function requires a range-vector as input but sum returns an instant-vector, although this could be solved with Recording Rules and more complicated queries. Any counter resets, e.g. due to container restarts, would corrupt the calculated value arguably worse than the current missing data issue, which is why we never sum then rate. I shared my experience with the Prometheus IRC community and received some helpful tips from Ben Kochie on approaches to mitigate this problem, at least in a Kubernetes environment or similar. For the HTTP 405 scenario, where the Pod has been running for some time but never served that status code before, we can modify the application to initialize all possible metric label combinations to zero during start-up. Naturally this can surface any issues with label value sets with a large cartesian product but there are operational benefits to addressing this high cardinality upfront by choosing to use fewer labels or bucketing values together instead of discovering an explosion of metrics at run-time. For the HTTP 500 scenario, where the Pod serves a number of error responses before it is first scraped, we can modify the Pod Readiness Probe so that requests are not delivered to the Pod until after Prometheus has scraped it at least once. This implies that the above change to initialize all metrics to zero has also been done. This would also require the Prometheus service discovery to be configured to scrape Pods that have not yet been marked as Ready."
"230","2017-03-08","2023-03-24","https://www.section.io/blog/devops-metrics-for-content-delivery/","Section has always been committed to providing the best, most detailed metrics and logs to our users. As the only Content Delivery solution that integrates fully with agile development and DevOps practices, we recognize the importance of having accurate metrics to diagnose, resolve and continuously improve your website performance and security. That’s why we’re happy to announce that we have recently given our metrics an update by bringing in Grafana, which is a highly flexible graphing tool that charts the metrics we provide from Graphite. Content Delivery Metrics in Grafana Grafana allows users to easily see vital metrics for Varnish Cache, ModSecurity, and other reverse proxies in their Section stack. We’ve created five Grafana dashboards in the Section portal to get you started, and users can also create their own custom metrics dashboards to get a specific view of their application. Here are the default dashboards every Section user will see in the Section portal: Traffic Summary: The Traffic Summary provides an overview of your website’s traffic, including HTTP requests and bytes served per minute. You can also view the HTTP requests by status code so you can see if there are high server or client side error codes. Our flexible alerting tool also allows you to set up alerts if error codes go up. Metrics for your Installed Proxies Varnish Cache Metrics Varnish Metrics Overview: This dashboard goes into detail on the hit, miss, pass ratios of Varnish Cache to help you see how Varnish Cache is handling each of your requests. These metrics give you a good understanding of how much content you are serving from the cache. You want to aim for a high percentage of cache hit rates - for example, a 98% cache hit rate would mean 98% of requests are being served from the cache, therefore they are being served more quickly and are not reaching your origin server. Varnish Request Performance Metrics: The “Varnish Request Performance” dashboard gives you data on time to serve and bandwidth by hit type, content type, and response type. This enables you to see the impact a cache hit has on the speed content is delivered and shows you what type of content takes the longest to serve. This dashboard is particularly useful for deciding what type of content you should focus on caching. Hit Rates by Content Type: In the “Varnish Cache Hit Rates” dashboard you can see cache hits by content type: HTML, Images, CSS, JavaScript. This is important because it will allow you to see if you are caching various content types successfully: if you are able to cache HTMl documents, for example, you will see a very fast Time to First Byte and reduced load on your server. ModSecurity Metrics If you have ModSecurity installed, the ModSecurity statistics dashboard shows the total amount of traffic that went through ModSecurity, your most active rules and most active countries. See immediately which countries are tripping which which rules on your WAF and how frequently. PageSpeed Metrics For users of the PageSpeed Front End Optimization module, Section will display PageSpeed metrics seen on their website, including information on what resources were not able to be rewritten, JavaScript minification failures, and other failures which allow users to ensure their files can be properly optimized. Learn more about logs and metrics In addition to detailed metrics, Section also provides a full set of logs from Kibana that allow you to troubleshoot issues in real time. To learn more about Section’s DevOps friendly Edge Compute Platform and see a demo, contact us or sign up for your own account."
"231","2017-06-15","2023-03-24","https://www.section.io/blog/content-delivery-continuous-integration-developer-pop/","Section is the only content delivery platform specifically designed to accelerate, scale and secure web sites with the development lifecycle in mind. But before we get to why this is so important, you must understand how CDNs came about, what problems they were created to solve, and why they traditionally have stood far apart from the development cycle. In the below post I go through a brief history of CDNs, why CDNs don’t integrate with continuous integration/continuous delivery cycles, and how Section’s Developer PoP solves these problems for devs, ops, SysOps and SecOps teams. Skip to the bottom if you just want to hear about how the DevPoP changes content delivery. History of CDNs When content delivery platforms were first conceived they set out to solve a couple of network layer problems: Datacenters in offices had tiny inbound bandwidth. As soon as you got a few users on your site, your link would be overwhelmed and your site would go offline. Users had low bandwidth and high latency connections. Dial up connections were the norm, and users were willing to wait for pages to load. Web engineers quickly realized that the bulk of the data was images in web pages. Using HTTP caching directives like Expires and Cache-Control would allow the web browser to cache content in the local user’s browser, so some data could be removed from that link. This meant that as a user browsed the web site they would not download the same content over and over again - but, this only worked for each specific user. Some smarter network engineers realised that they could introduce a HTTP reverse proxy between the user and the web server that could cache some of the web page’s content. If these HTTP reverse proxy servers were positioned strategically, they could help solve both of the aforementioned problems - not enough bandwidth at the server hosting, and high latency and low bandwidth on the user side. Thus, the CDN was born. By putting many of these servers between the user and the web server, the aggregated bandwidth was much greater than a single hosting facility could provide. When one of the HTTP reverse proxies was able to able to serve an image from cache it removed some workload from the web server hosting, freeing it up to serve more users. Simultaneously, because these proxies were located on many global servers rather than in one spot, they moved the content closer to the end user. This reduced the effect of latency on the end user’s side, further improving the website’s performance to the user. Internet Advancements and CDNs Keeping Up While CDNs did solve these two core problems of early websites, as the Internet advanced the problems were solved in other, more direct ways. Bandwidth at the web server eventually became cheaper and link sizes were increased. Then, cloud computing moved web server hosting out of the on-premise world into mega datacenters which had enough bandwidth to serve the entire site (without a CDN). At the same time, end user bandwidth and latency improved. Dial up was replaced with DSL and cable, meaning that the connection times both at the web servers and on the end user side got much faster. What then? Despite these improvements in connections, CDNs were still around and more vendors entered the market with little to set them apart except for price. This competition between vendors is still going on between CDN companies that have not been able to differentiate themselves with higher value services. Some vendors then started to reach for higher value services. At the time, the HTTP reverse proxies that supported CDNs didn’t have a great deal of control over what they would cache. HTTP caching directives were primarily used. This meant that engineers needed to meticulously examine their server configurations to make sure that the CDN would cache properly. CDN vendors added features to override situations where the server was or could not be configured correctly. CDNs were also not capable of caching personalized content - that is content that is unique to each user. This collided with web application framework defaults, where frameworks like PHP and ASP.Net would unnecessarily drop cookies on users. These cookie headers made content uncacheable. This spawned the birth of solutions such as the “dynamic site accelerator” and “edge side includes” which aimed to solve these problems. Dynamic site accelerators attempt to improve the speed of delivery (but not the scalability so much) of web pages that are not cacheable by removing overhead in the network layer. Instead of using default Internet routes to reach the web servers, the HTTP reverse proxies inside the CDN would measure connection, latency and throughput to the web servers. Where they found that hopping though one of their other proxies improved performance, they would route the traffic through that server instead of reaching back to the web servers directly. They also improved performance by reducing the number of TCP connects and disconnects between the CDN reverse proxies and the web servers. Reducing the number of TCP connects improved the performance in many places, including to any load balancer running in front of the web servers and the web servers themselves. SSL was also computationally expensive at the time, so reducing the number of SSL negotiations made an impact. However, the DSA didn’t deal with the underlying problem - developers found it difficult to build applications that were cacheable and the CDN did very little to help them. CDNs Enter the Website Security Market CDN vendors also reached for higher value services on the security front. As web sites transitioned from static content to dynamic applications security holes were introduced. Most dynamic applications were built on frameworks that had security vulnerabilities, and the applications themselves had vulnerabilities. Basically, anywhere you could enter data into a web site was an attack vector. One common attack was the SQL Injection Attack. Hackers would find the login screen to a web site, then type something like “;select * from users” into the username field. Sometimes the application would fail to login - but it would also send back a list of all the users in the system into the web page! As a HTTP reverse proxy is looking at all these HTTP requests, it seemed that it was a logical place to inspect the traffic. So a Web Application Firewall is simply a HTTP reverse proxy that looks at properties of the request looking for suspicious things like “SELECT”, which should not appear in the request. If it finds them, it blocks them. CDNs and Agile Development Workflows As we saw, CDN started to fix problems with pipes, but as that problem was solved by better datacenters and better end user connectivity that solution either evaporated or became commoditized. CDN vendors needed to add more value to maintain profitability so they started to deal with problems in the application stack, like uncacheable content and security holes. This worked well for a while, especially in a waterfall project management world. However as application complexity increased the waterfall method became very expensive and error prone. Software development projects continually ran late, with a lot of bugs. This was often caused when integrating the work of smaller teams failed. For example, at the beginning of a project the system architecture was designed. Two teams would build one component each, and when finished, they would try and connect their components together. This action of connecting we call integration, where the components are integrated together. If the integration failed or had conflicts it caused huge headaches and loss of time for teams. The project management system needed a top down rebuild. Some early adopters tried methods that they eventually called “agile”. Instead of working in isolation with a huge project plan and strict contracts between teams, the teams did the opposite. They made small incremental changes, and integrated them very often. In doing so they reduced the overall cost of software development. As a colleague said to me, “when you are not good at doing something, do it more often”. He was referencing the need to continually perform the integration phase. This was the birth of continuous integration, or CI. Agile methods and engineering practices like CI took hold on the software ecosystem. Teams started to adopt these principles with success, applying them to the application code and also the database structures. Innovation in software started to increase. So did consumer demand for Internet products. Traffic to web sites started to increase, so teams reached for CDNs to help solve performance and availability problems. However, because the nature of original CDN design was to be only available as a service on the Internet, CDNs broke the fundamentals of these new agile practices. We saw earlier that the CDN was operating in the application space, however the CDN could not operate within the CI systems that the application developers were using. This means that teams were again faced with a late-integration problem, just like when they were doing waterfall projects. This late integration of CDN still manifests itself today. Examples of common problems that are caused by this late integration include: Failing to cache things that are cacheable Accidentally caching things and sharing them with the wrong users Accidentally blocking legitimate traffic with WAF These problems happen because the CDN only lives on the Internet, not on an engineer’s computer, where they do their work and are trying to continuously integrate that work. As a programmer changes an application, they test their changes on their computer. When they are happy with the changes they submit them for review (typically in a source control system like git). These reviews might be peer reviews, or they might be automatically tested by an automated test suite. If the review is successful, the changes are released to the live site. Because the programmer cannot see the entire system including the CDN until the changes hit the live site, they make mistakes that are not caught in the review and can impact the production site, end user, and site reputation. The Developer PoP: Brining Continuous Integration to Content Delivery When we founded Section, we thought what if this was not the case? What if the CDN was treated like a proper layer in the stack, like the application and the database are today? That’s exactly what Section’s Developer PoP does. Not only does our Edge PaaS behave as well as the traditional CDNs you’re used to, but it also empowers the entire team to drive their content delivery harder, getting better performance and security results. Our Developer PoP is a virtual machine that runs on your local computer, in your CI/CD processes. It is a mirror of what you can expect on our global delivery grid because it pulls the configuration of our global PoPs down into the developer machine. How Does a Developer Use the Developer PoP? If you’re an application developer, you can launch the Developer PoP on your local computer, so that you can browse your site through all the layers. In the old system, you would have been browsing directly to your application, skipping the Developer PoP. You couldn’t see what was going on with the CDN as you worked, and you hoped that your changes wouldn’t cause problems for your business and embarrassment for you. Now, you can see exactly what Section and any reverse proxies deployed within Section will do as you change your application. This can give you confidence that your changes are going to actually improve the system, and not break it. Think of it as reducing the number of problems you get in production, letting you focus on building a better application, rather than fighting fires. How do SysOps use the Developer PoP? If you’re in an organization that has a dedicated sysops team that is responsible for the CDN configuration, I bet you are sick of being alerted when the developers release code that conflicts with the CDN and doesn’t integrate properly. This can be solved. Don’t blame the developer for doing this. Developers have the best intentions in mind, however they don’t have the right tools given to them by their CDN. As a sysop responsible for CDN configuration, the Developer PoP doesn’t mean you relinquish any control. You can still control the CDN to ensure that the configuration is tight. However, your developers can run your configuration on their computer as they write code. This means that they are able to see when their changes fail to integrate properly. Think of it as empowering the developers so that they don’t interrupt you with failed integrations - better results for the application (customers) and fewer alerts for you. How do SecOps use the Developer PoP? In secops, your mind is on protecting users and the business by making the application safe. One problem that secops teams find with CDNs is that when the application changes, the security rules need to be reconfigured. Unfortunately, because the entire CDN industry hasn’t realized that they live in the application stack, code deployments often mean turning off WAF rules to resolve production incidents. When using the Developer PoP developers actually see the problems that their application changes make because the WAF is running on their computer. This doesn’t mean that they can turn on and off the rules - they can just verify that their changes are not introducing new problems. Additionally, as you decide to improve the security of the application, you too can use the Developer PoP to test new rules before promoting them to production. A Better CDN Development Experience is Possible The Developer PoP is for everyone - dev, ops, DevOps, and SecOps. Improving the tools that your team uses for building applications is something every engineer loves doing. No one has tried to tackle this in the CDN industry. Until now. Contact us if you want to learn more about the DevPoP or to see a full demo of how it works. Get Started Today"
"232","2021-12-20","2023-03-24","https://www.section.io/blog/how-to-overcome-security-concerns-at-edge/","Security is a top concern for every company today, so it should come as no surprise that it’s also one of the top five must-haves in an Edge as a Service (EaaS) solution. Edge computing topologies introduce both opportunities and threats when it comes to protecting applications from potential security breaches. On the one hand, processing data closer to end devices allows for earlier threat detection and mitigation, before attack agents are able to penetrate mission-critical operations. However, distributing workloads across a heterogeneous network of providers and infrastructure can also significantly expand the attack surface. EaaS solutions must be designed with protection in mind, providing excellent defense against a diverse set of threats across the infrastructure, network, transport, and application layers. Let’s take a closer look at some of the top security concerns related to each layer with some insights from our recent white paper on Solving the Edge Puzzle. DNS The domain name system (DNS) is often referred to as the phonebook of the Internet since it translates domain names to IP addresses, allowing browsers to load Internet resources. DNS provides the hierarchical naming model, which lets clients “resolve” or “lookup” resource records linked to names. DNS therefore represents one of the most critical components of networking infrastructure. Other widely used Internet protocols have started to incorporate end-to-end encryption and authentication. However, many widely deployed DNS services remain unauthenticated and unencrypted, leaving DNS requests and responses vulnerable to threats from on-path network attackers. Hence, when building out DNS services, it’s critical to maintain a security-first approach. TLS Transport Layer Security (TLS) is an encryption protocol that protects communications on the Internet. You can feel reassured that your browser is connected via TLS if your URL starts with HTTPS and there is an indicator with a padlock letting you know that the connection is secure. TLS is also used in other applications, such as email and Usenet. It’s important to regularly upgrade to the latest protocol for TLS and its predecessor, the SSL protocol. When working with TLS and/or SSL, you have to either work with your own certificates using a managed service such as DigiCert or use an open source version with a service like Let’s Encrypt or Certbot. The managed certificate authorities such as DigiCert will provide automated tooling to provision certificates. With the open source versions, you will find that you must build the services to manage auto-renewal components and the provisioning of new certificates. An added complexity is the question of how to deploy these protocols? In relation to distributed systems, you will have certificates that need to be running in multiple places. They might be running across multiple providers; for example, you might be using one specific ingress controller in one location and a different ingress controller in another. How do you ensure that your certificates are being deployed where needed to handle the TLS handshakes? And as the number of domains that you manage increases, so too do the complexities. This ties directly back to DNS since you need to ensure that you’re routing traffic to the correct endpoints containing the workloads where your TLS certificates are deployed. Further, you will have to consider the state of your systems at any point in time and how you route traffic, since you never want to service users incorrectly. Ultimately, servicing your user correctly is the end goal, meaning that when implementing TLS at the edge yourself, you must take into account all of these different components. DDoS: Protecting Layers 3, 4, and 7 When protecting against Distributed Denial of Service (DDoS) attacks across distributed systems, the first question to ask should be, where are my systems most vulnerable to attack? The primary layers of focus on protecting against DDoS attacks include Layers 3, 4, and 7 in the OSI model. Web application firewall (WAF) providers will provide DDoS protection for you at the application layer (i.e., Layer 7). However, in an edge computing paradigm that’s made up of heterogeneous networks of providers and infrastructure, there are more questions that need asking. The most important being: How do all the different providers I’m using handle network and transport-layer DDoS attacks (i.e., Layers 3 and 4)? All major cloud providers typically have built-in DDoS protection, but when you begin to expand across a multi-cloud environment, and further out to the edge, you need to ensure that your applications are protected across the entire network. This includes knowing how each underlying provider handles DDoS protection, along with implementing safeguards for any areas in your networks that may be underprotected. This takes us back to DNS and the question of how to handle traffic routing when one (or more) of your endpoints becomes compromised. Web Application Firewalls (WAFs) & Bot Management Tooling DevOps teams are increasingly choosing to deploy WAFs and bot mitigation tools across a distributed architecture, with the goal of detecting and mitigating threats faster. These solutions typically act as reverse proxies, monitoring, filtering, and blocking HTTP traffic to and from a web service based on a set of defined rules or logic. While many best-in-class WAF and bot management technologies have emerged – providers such as Wallarm, Snapt (WAF), ThreatX, Signal Sciences, Radware Bot Manager, and PerimeterX – the most common selection and deployment method for these solutions is via a CDN’s proprietary service, which are often limited in functionality and flexibility. In fact, we often speak with developers who are frustrated with the “black box,” built-in solutions of legacy CDNs, and demand more choice and flexibility. Edge as a Service: Solving for Security As covered in our whitepaper, an Edge as a Service (EaaS) provider can help overcome security concerns along with many of the other complexities involved in application deployment and management at the edge. Section’s EaaS platform is built on top-tier hosting providers, extending all of the network layer protection and capacity provided by industry heavyweights like Amazon, Google, Microsoft, Lumen, etc. In addition to the core security features built into the platform, Section partners with industry-leading security solutions that can be easily deployed alongside your distributed applications, including advanced WAF and bot management technologies."
"233","2021-10-21","2023-03-24","https://www.section.io/blog/5-must-haves-edge-as-a-service-solution/","The “as a service” model is relied on heavily by companies today to meet a wide range of needs. Software as a Service (SaaS) may be the most recognizable of these services, but Platform as a Service (PaaS) and Infrastructure as a Service (IaaS) are also widely used. A newer addition to the “as a service” catalog is Edge as a Service (EaaS). Edge computing—the use of cloud computing to put latency-sensitive and latency-critical workloads as close as possible to end users—provides tremendous performance improvements and data backhaul reductions. EaaS empowers innovators like software providers and infrastructure providers to build, integrate, and operate edge services efficiently and effectively. Not surprisingly, moving services to the edge can be complicated. Fortunately, EaaS helps make the transition much easier and speeds up time to value. Companies looking to better serve their customers and offload complexities for their teams are increasingly turning to EaaS. With EaaS, technical product teams get the operational enhancements they need without the time, effort, and capital it takes to train and educate internal IT teams that may not have expertise in building and operating distributed systems. What to look for in an Edge as a Service solution There are many factors to consider when evaluating EaaS solutions, and feature sets generally fall in one of two buckets, must-haves or nice-to-haves. The following five characteristics are absolute table stakes when it comes to selecting an EaaS solution. 1. Granular control The right EaaS solution enables you to focus on determining the needs of your team and your customers first, and then leverage the power of EaaS technologies to design and implement solutions that are tailored to meet those unique requirements. In order to achieve this, EaaS solutions must provide granular, code-level control to seamlessly integrate edge solutions into existing stacks and workflows. How the Section platform enables granular control The Section Control Plane is built on top of a GitOps paradigm, enabling DevOps teams to configure and manage their distributed environment from a centralized, integrated, secure control plane using familiar tooling. 2. Exceptional flexibility Many organizations just beginning their Edge journeys start by exploring the capabilities of familiar Cloud and Content Delivery Network (CDN) service providers. Cloud providers (e.g. AWS, Google, Azure, DigitalOcean, etc.) are well-versed at supporting diversity of edge workloads, but often fall short in enabling technical product teams to easily deploy and manage those workloads across a distributed footprint. CDN providers, on the other hand, have deep experience in distributing workloads across global edge networks, but the workloads supported are limited to static asset delivery and other basic, fixed functions, without support for truly custom workloads. EaaS solutions are specifically designed to solve for the shortcomings of existing Cloud and CDN capabilities, and therefore must be flexible enough to not only support custom workload requirements, but must also have the network reach and adaptability to distribute workloads in the most suitable locations to meet desired outcomes in a cost-effective manner. In choosing an EaaS solution, be sure to evaluate not only its ability to provide the solution you need today, but also consider how the solution can evolve as the company’s needs increase or evolve. How the Section platform solves for flexibility Flexibility is at the core of the Section platform. The Section Edge AppSpace enables organizations to easily distribute custom workloads and applications (e.g. containers, serverless, Node.js), while also offering out-of-the-box, add-on edge security and performance modules from leading solution providers. In addition, organizations benefit from Section’s Federated Edge and Multi-Cloud Network (AWS, Azure, Google, DigitalOcean, Equinix, Lumen, Rackcorp) powered by Section’s patent-pending Adaptive Edge Engine, along with the ability to bring your own infrastructure into a project. 3. Advanced security and compliance Security and compliance are top concerns for every company today. Edge computing topologies can help application architects meet compliance requirements, while also mitigating the impact of potential security breaches. Processing data closer to end devices allows for earlier threat detection and mitigation, before attack agents are able to penetrate mission-critical operations. EaaS solutions must be designed with protection in mind, providing excellent defense against a diverse set of threats across the infrastructure, network, transport, and application layers. Web application firewall (WAF) providers will provide DDoS protection for you at the application layer (i.e. Layer 7). However, in an edge computing paradigm that’s made up of heterogeneous networks of providers and infrastructure, there are more questions that need asking. The most important being: how do all the different providers I’m using handle network and transport-layer DDoS attacks (i.e. Layers 3 and 4) and more? How the Section platform solves for security and compliance The Section platform is built on top-tier hosting providers, extending all of the network layer protection and capacity provided by industry heavyweights like Amazon, Google, Microsoft, Lumen, etc. In addition to the core security features built into the platform, Section partners with industry-leading security solutions that can be easily deployed alongside your distributed applications, including advanced WAF and bot management technologies. (Visit the Section Edge AppSpace to learn more.) Section also helps keep your organization compliant. The patent-pending Adaptive Edge Engine intelligently deploys workloads and routes traffic based on your specified compliance and regulatory requirements, including PCI, GDPR, and more. 4. Simple and intuitive user experience The emerging edge compute paradigm requires a shift in thinking when it comes to application architectures and the systems and technologies that support those. The sole purpose of EaaS solutions is to help accelerate that shift for product teams by making it easier to distribute and operate edge workloads and applications. EaaS solutions shouldn’t require you to re-architect your entire application, but rather should easily adapt to your application’s architecture. Look for features such as flexible APIs, local testing enablement, CI/CD integration, robust observability tooling, and other capabilities that ease integration and ongoing management for your teams. How the Section platform approaches UX We like to think of Section as “The Easy Button for Edge”. A few of the key underlying platform capabilities that deliver an accelerated time to value for our customers include: Adaptive Edge Engine Section’s patent-pending Adaptive Edge Engine (AEE) intelligently and continuously tunes and reconfigures your edge delivery network to ensure your edge workloads are running the optimal compute at the optimal location for your application. By offloading the burdens of infrastructure provisioning, workload orchestration, scaling, monitoring, and traffic routing, the AEE helps reduce your operational overhead while ensuring that the platform is running in the most cost-effective way to meet your desired outcomes. GitOps As mentioned earlier in this article, Section is built on GitOps principles, making it easy for DevOps teams to interact with and integrate the Section platform into their existing systems and workflows. Observability Suite Section’s suite of observability tooling gives organizations the insights they need to understand how their edge stack is performing at any given time. Section’s innovative Traffic Monitor goes beyond standard dashboards and graphs to give users an immersive and holistic Edge observability experience. 5. Outstanding business value and scalability Your EaaS solution should inherently deliver the touted benefits of edge computing, including improved performance, security, reliability, scalability, and ultimately ROI. Plus, with EaaS handling all your edge computing requirements, your development teams will have more capacity to focus on core product innovation to drive company growth. As your business grows, your EaaS should just expand with you so you can continually provide responsive and reliable applications. How the Section platform delivers business value and scalability Built on the backbone of Kubernetes, the Section platform is designed to scale with your business. As more organizations and operators continue to adopt and support Kubernetes-based cloud-edge patterns, the ecosystem will continue to mature. However, not every organization will have the resources and/or expertise to build and operate these systems themselves. This is where edge platforms (like Section) bridge those gaps, offering DevOps teams familiar tooling to take advantage of the benefits that Kubernetes has to offer without the complexities that come along with it. EaaS: Edge Computing Simplified Getting to the Edge with your applications doesn’t have to be difficult. And taking the first step is simple — contact Section today to learn more about how EaaS is improving the Internet and how Section’s innovative edge technologies can help you achieve your business goals."
"234","2021-09-27","2023-03-24","https://www.section.io/blog/how-to-solve-the-edge-puzzle/","Finding the right technology solutions these days can feel like trying to hit a moving target. With web applications both increasing in complexity and evolving in a landscape where consumers are demanding higher quality experiences, companies are scrambling to find solutions to support these expanding consumer expectations. Two of the top new demands from consumers across the globe are faster interactions and a lower tolerance for security breaches around personal data. While cloud computing and the emergence of the large cloud hosts has caused a centralization of compute resource, the next wave of compute innovation is happening at “the edge”. As organizations look to capitalize on the benefits of edge computing, many are quickly realizing the complexities associated with building and operating distributed systems, leading them to seek out solutions to help to solve this edge puzzle. The Edge & The Cloud Edge computing is the practice of capturing, storing, processing, and analyzing data near the client, where the data is generated, instead of in a centralized data-processing warehouse. Most developers are familiar with cloud deployments, but running workloads across a distributed edge introduces a new set of complexities and considerations. Moving from managing a single deployment endpoint to hundreds or more requires different understanding and skill sets, especially if different microservices also need to be served from different edge locations. More than that, developers are increasingly being asked to program in a new computing paradigm. Now, they may be asked to work on edge computing as well as hybrid cloud infrastructure. To muddy the waters even further, the parts of the application being moved closer to the end user are also changing. In recent years, application owners have started to migrate more advanced logic, security, and persistent data stores out to the edge. However, the adoption curve has been slow, mainly due to the extreme complexities involved in designing, building, and operating a distributed deployment model. White Paper: Solving the Edge Puzzle In this white paper, we break down the complex and always shifting Edge puzzle into component pieces to help developers and DevOps teams understand how to reap the rewards of edge computing. Read the white paper Overcoming Complexities with Edge as a Service (EaaS) The reality of deploying and managing workloads at the edge is not simple. The complexities that come with building and operating cloud-edge networks and infrastructure are real. Every organization, team, and application has unique requirements when it comes to designing distributed systems. Because of this, many teams start down the path of building their own bespoke systems. This can quickly become overwhelming, considering all of the complexities related to system design and implementation. The real power of the edge arises when we provide application developers the opportunity to seamlessly run the software of their choice at the edge and application operations teams the simplicity of a single delivery plane so they have a reduced operational footprint (even with a larger geographic delivery footprint). In addition to simply understanding edge computing, it’s important to also know how the technology can actually work for you. Solutions like Edge as a Service (EaaS) can help companies looking for ways to use edge computing to power next-gen digital experiences for SaaS, PaaS, and application providers. Edge as a Service, or EaaS, is a solution that enables software providers to accelerate their path to Edge. With EaaS, software providers gain instant access to an edge deployment model for their solutions without having to build and manage their own edge network. This saves these providers time, effort, and resources by keeping their focus on their products and services instead. As an EaaS provider, we are quite close to the challenges that our customers encounter around edge technologies. We’re often pulled into projects during the early stages of research and discovery, where we’re able to offload the build and management of many, if not most, of the critical components, ultimately accelerating the path to edge for organizations across a diverse range of use cases. Section’s EaaS simplifies the steps involved in deploying your application to the edge. We take care of the massive complexities and resources necessary to support distributed provisioning, orchestration, scaling, monitoring and routing, allowing you to focus on innovation. To learn more about solving the edge puzzle, check out this white paper."
"235","2021-12-15","2023-03-24","https://www.section.io/blog/3-edge-computing-challenges-for-developers/","Organizations are seeking to migrate more application logic to the edge for performance, security and cost-efficiency improvements. In our recent blog post about Considerations for Containers at the Edge, we touched on challenges this edge migration poses for developers compared to cloud deployments: Typical cloud deployments involve a simple calculation of determining which single cloud location will deliver the best performance to the maximum number of users, then connecting your code base/repository and automating build and deployment through CI/CD. But what happens when you add hundreds of edge endpoints to the mix, with different microservices being served from different edge locations at different times? How do you decide which edge endpoints your code should be running on at any given time? More importantly, how do you manage the constant orchestration across these nodes among a heterogeneous makeup of infrastructure from a host of different providers? It’s worth diving deeper into this topic from a developer perspective with some insights taken from our recent white paper on Solving the Edge Puzzle. The question is relatively straightforward: is it possible to continue developing and deploying an application in the same (or similar) fashion as with the cloud or centralized hosting, yet still have users enjoy all the benefits of the edge? The development challenge can be distilled down into three areas: code portability, application lifecycle management and familiar tooling as it relates to the edge. Code Portability Why do we need code portability? As discussed above, an ideal state allows for similar development/deployment across various ecosystems. Workloads at the edge can vary across organizations and applications. Examples include: Micro APIs – hosting small, targeted APIs at the edge, such as search or full-featured content exploration with GraphQL, to enable faster query response while lowering costs. Headless applications – decoupling the presentation layer from the back end to create custom experiences, increase performance or improve operational efficiency. Full application hosting – rather than beaconing back to a centralized origin, databases are hosted alongside applications at the edge, and then synced across distributed endpoints. This can almost be viewed as a hierarchy or progression in edge computing, as the inevitable trend for application workloads is moving more of the actual computing as close to the user as possible. But as developers adopt edge computing for modern applications, edge platforms and infrastructure will need to support and facilitate portability of different runtime environments. It’s important to recognize that while private vs public cloud vs edge may seem like architectural decisions, these are not mutually exclusive. Centralized computing could be reserved for storage or compute-intensive workloads, for example, while edge is used to exploit data or promote performance at the source. Seen through a developer lens, this means application runtimes must be portable across the edge-cloud continuum. How do we get there? Containerization is the key to enabling portability, but it still requires careful planning and decision making to achieve. After all, portability and compatibility are not the same thing; portability is a business problem, while compatibility is a technical problem. Consider widely used runtime environments, such as: Node.js, used by many businesses, large and small, to create applications that execute JavaScript code outside a web browser; Java Runtime Environment, a prerequisite for running Java programs; .NET Framework which is required for Windows .NET applications; and Cygwin, a runtime environment for Linux applications that allows them to run on Windows, macOS, and other operating systems. Developers need to be able to run applications in dedicated runtime environments with their programming language of choice; they can’t be expected to refactor the code base to fit into a rigid, pre-defined framework dictated by an infrastructure provider. Moreover, the issues of portability, compatibility and interoperability don’t just apply to private vs public cloud vs edge, they are also important considerations across the edge continuum as developers adopt global, federated networks featuring multiple vendors to improve application availability, operational efficiency and avoid vendor lock-in. Simply stated, multi-cloud and edge platforms must support containerized code portability, while offering the flexibility required to adapt to different architectures, frameworks and programming languages. Application Lifecycle Management In addition to code portability, another edge challenge for developers is easily managing their application lifecycle systems and processes. In the DevOps lifecycle, developers are typically focused on the plan/code/build/test portion of the process (the areas in blue in the image below). With a single developer or small team overseeing a small, centrally managed code base, this is fairly straightforward. However, when an application is broken up into hundreds of microservices that are managed across teams, the complexity grows. Add in a diverse makeup of deployment models within the application architecture, and lifecycle management becomes exponentially more complex, impacting the speed of development cycles. Image source: edureka In fact, the additional complexities of pushing code to a distributed edge – and maintaining code application cohesion across that distributed application delivery plane at all times – are often the primary factor holding teams back from accelerated edge adoption. Many teams and organizations are turning to management solutions such as GitOps and CI/CD workflows for their containerized environments, and when it comes to distributed edge deployments these approaches are usually a requirement to avoid increased team overhead. Familiar Tooling Which brings us to the third challenge for edge deployment: tooling. If developers plan for code portability and application lifecycle management, but are forced to adopt entirely different tools and processes for edge deployment, it creates a significant barrier. As the theme of this post makes clear, efficient edge deployment requires that the overall process – including tooling – is the same as or similar to cloud or centralized on-prem deployments. GitOps, a way of implementing Continuous Deployment for cloud native applications, helps us get there. It focuses on a developer-centric experience when operating infrastructure, using tools developers are already familiar with including Git and Continuous Deployment tools. These GitOps/CI/CD toolsets offer critical support as developers move more services to the edge, improving application management, integration and deployment consistency. Beyond more general cloud native tooling, as Kubernetes adoption continues to grow, Kubernetes-native tooling is becoming a stronger requirement for application developers. Kubernetes native technologies generally work with Kubernetes’s CLI (‘kubectl’), can be installed on the cluster with the Kubernetes’s popular package manager Helm, and they can be seamlessly integrated with Kubernetes features such as RBAC, Service accounts, Audit logs, etc. Edge as a Service: Consistency is Key As covered in our whitepaper, the key to accelerating edge computing adoption is making the experience of programming at the edge as familiar as possible to developers, explicitly drawing on concepts from cloud deployment to do so. But the added complexities that a distributed edge deployment brings introduces new challenges to achieving consistency across these experiences. That’s why we offer an Edge as a Service platform, allowing developers to leverage code portability and use simple, familiar lifecycle management processes and tools at the distributed edge. Section’s EaaS platform offers GitOps-based workflows, Kubernetes-native tooling, CI/CD pipeline integration, RESTful API, automated SSL/TLS integration and a complete edge observability suite. This, combined with the other benefits of EaaS for application deployment, gives developers the cost and performance benefits they’re looking for in an edge platform, without the need to master distributed network management."
"236","2021-11-22","2023-03-24","https://www.section.io/blog/top-considerations-for-containers-at-edge/","Some things are just made to go together, like containers and edge computing. Containers package an application such that the software and its dependencies (libraries, binaries, config files, etc.) are isolated from other processes, allowing them to migrate as a unit and avoiding the underlying hardware or OS differences that can cause incompatibilities and errors. In short, containers are lightweight and portable, which makes for faster and smoother deployment to a server… or a network of servers. Edge computing leverages a distributed compute model to physically move compute, storage, data and applications closer to the user to minimize latency, reduce backhaul and improve availability. Effective edge computing requires efficient deployment (both in terms of time and cost) of an application across many locations and – often – many underlying compute platforms. I think you see where this is going… Containers offer two key benefits when it comes to edge computing: Portability makes containers ideal for edge applications as they can be deployed in a distributed fashion without needing to fundamentally rearchitect the underlying application. Abstraction makes containers ideal for deployment to non-homogenous federated compute platforms, which are often found in distributed edge networks. Kubernetes at the Edge The above presupposes a suitable edge orchestration framework is present to coordinate the distributed compute platform, and that’s where Kubernetes comes in. It provides the common layer of abstraction required to manage diverse workloads and compute resources. Moreover, it provides the orchestration and scheduling needed to coordinate and scale resources at each discreet location. However, Kubernetes, itself, does not manage workload orchestration across disparate edge systems. Edge as a Service (EaaS) platforms have emerged to help fill this gap, which we’ll get to in just a bit. Top Considerations When Putting Containers at the Edge All that said, significant considerations remain when deploying containers at the edge. These can be distilled into three distinct categories: distributed edge orchestration, edge development lifecycle and deployment framework, and edge traffic management and routing. Distributed Edge Orchestration Moving to a truly distributed compute network adds complexity over typical cloud deployments for one simple reason: how do you maximize efficiency to meet real-time traffic demands without running all workloads in all locations across all networks all the time? Consider a truly global edge deployment. Ideally, compute resources and workloads are spun up and down in an automated fashion to, at a minimum follow the sun (roughly), yet remain responsive to local demand in real time. Now add in a heterogenous edge deployment, where demand and resources are monitored, managed and allocated in an automated fashion to ensure availability across disparate, federated networks. All of this involves workload orchestration, load shedding, fault tolerance, compute provisioning and scaling, messaging frameworks and more. None of this is simple, and the term “automated” is doing a lot of heavy lifting in the above scenarios. But doing it correctly can accrue significant benefits in terms of lowering costs at the same time as increasing performance. Similarly, effective use of federated networks increases availability and fault tolerance, while decreasing vendor lock-in. Finally, it can improve regulatory compliance with initiatives such as GDPR, which requires data storage in specific locations. Edge development lifecycle and deployment framework Typical cloud deployments involve a simple calculation of determining which single cloud location will deliver the best performance to the maximum number of users, then connecting your code base/repository and automating build and deployment through CI/CD. But what happens when you add hundreds of edge endpoints to the mix, with different microservices being served from different edge locations at different times? How do you decide which edge endpoints your code should be running on at any given time? More importantly, how do you manage the constant orchestration across these nodes among a heterogeneous makeup of infrastructure from a host of different providers? Effectively managing edge development and deployment requires the same granular, code-level configuration control, automation, and integration that is typical in cloud deployments, but on a massively distributed scale. Some of the more critical components of an edge platform include comprehensive observability so developers have a holistic understanding of the state of an application, and cohesive application lifecycle systems and processes across a distributed edge. Edge traffic management and routing Deploying containers across a distributed edge fundamentally requires managing a distributed network. This includes DNS routing and failover, TLS provisioning, DDoS protection at layers 3/4/7, BGP/IP address management and more. Moreover, you’ll now need a robust virtualized network monitoring stack that provides the visibility/observability necessary to understand how traffic is (or isn’t) flowing across an edge architecture. And to truly manage distributed network, infrastructure and operations at the Edge, you will likely require an edge operations model with an experienced team comprised of network engineers, platform engineers and DevOps engineers with an emphasis on site reliability engineering (SRE). This is problematic, to say the least, for SaaS vendors and others who don’t have full network operations teams and systems on site. Edge as a Service All of this explains the existence of EaaS solutions for containerized edge application deployment. EaaS leverages the portability of containers for efficient deployment across distributed systems, but abstracts the complexities of the actual network, workload and compute management. This allows organizations to deploy applications to the edge through simple, familiar processes – much in the same way they would if deploying to a single cloud instance – and provides the necessary tools and infrastructure to simplify and automate configuration control. Ultimately, EaaS gives organizations all the cost and performance benefits of edge computing, while allowing them to concentrate on their core business rather than distributed network management."
"237","2021-05-20","2023-03-24","https://www.section.io/blog/containers-as-a-service-edge-computing/","Container technology derives its name from the shipping industry. As opposed to transporting goods as individual units with various packed sizes, goods are placed into steel containers, which are a standardized size, allowing for easier storage and more seamless transportation. The container can be moved as a unit, which saves time and money. In the tech world, containerization is a method used to package an application allowing the software, and its dependencies (including libraries, binaries and configuration files) to run together, isolated from other processes. By packaging them into a container, they can migrate as a unit, avoiding the differences between machines such as underlying hardware or OS differences, which can cause incompatibilities and critical errors. Containers also help enable smoother deployment of software to a server or network of servers. Container technology gathered momentum in the 2000s with the introduction of Free BSD Jails. However, it wasn’t until the introduction of the container management systems Docker Swarm and Apache Mesos that containerization really began to take hold within the wider industry. When Kubernetes was released in 2017, it quickly became the de-facto standard for container management systems because it made container orchestration significantly easier to manage and more efficient. Containers as a Service (CaaS) In 2020, a survey conducted by the Cloud Native Computing Foundation (CNCF) found that 84% of respondents were running containers in production. With containers now widely considered a standard unit of deployment, many organizations have adopted Containers as a Service (CaaS) solutions to streamline their container orchestration (i.e. Kubernetes) operations. What is CaaS? It is a service model that enables users to manage and scale containers, applications and clusters. It does this through a container-based virtualization, API, or a web portal interface. While there are different types of implementations available, all CaaS offerings essentially do the same thing. That is, they help organizations manage their containerized applications in a safe, scalable manner, whether on the cloud, on-prem or (as we’ll go into more) at the Edge. Benefits of CaaS include: Faster time to market - due to a more streamlined development process (it literally takes seconds to create, start, replicate or destroy a container). Standardization and portability - allowing you to run your containers across different clouds, avoiding vendor lock-in. Quicker deployments - CaaS abstracts the details of the underlying infrastructure. Cost-effectiveness - Costs are typically lower since users choose and pay for only the CaaS resources they need. Security - As a result of the isolation that containers have from each other, security is naturally baked in. CaaS also makes it easier to manage your host system and launch updates and security patches in a timely way. Containers at the Edge Amongst other benefits, edge computing offers lower latency, reduced data backhaul, and higher availability for Internet-independent applications. Containerization has quickly become an important part of edge computing. Since containers are a lightweight, portable virtualization solution, they have various logistical benefits within a distributed compute model. Edge computing requires time- and cost-efficient deployment and management of an application to many locations and possibly many underlying compute types. Provided a suitable edge orchestration framework is present, containers have two key factors which make them suitable for Edge: The portability of containers make them suitable for edge applications as the applications can be deployed in a distributed fashion without requiring significant rearchitecting of the underlying application. The abstraction of containers make them suitable for deployment to non-homogenous federated compute which is often found at the Edge. Many solution providers have already containerized (i.e. Dockerized) all or portions of their applications, making it easier to migrate them to the Edge. The Role of Kubernetes in Edge Computing Since one of the key strengths of Kubernetes is its ability to offer a common layer of abstraction atop physical resources (compute, storage, networking), Kubernetes is a useful tool for developers and operations teams to leverage for deployment of applications and services in a standardized fashion on disparate compute, including at the Edge. This is important in the cloud, but it is critical at the Edge because of the much greater diversity and volume of hardware and software resources. To effectively manage edge nodes, enterprises need a management layer that enables dynamic orchestration and scalability, which Kubernetes provides. At Section, we migrated to Kubernetes from our custom-built orchestration framework a few years ago. With Kubernetes as our backbone, a few of the benefits we have experienced first-hand include higher availability of services, fewer interruptions during upgrades and patches, and flexible tooling. Additionally, it gives us the ability to remain infrastructure agnostic, which translates to greater flexibility and reach with our Composable Edge Cloud options. The endpoint orchestration which Kubernetes facilitates is an important part of edge computing. However, it must be augmented with three fundamental layers: Overall distributed edge orchestration Edge application development lifecycle and deployment framework Edge traffic management and routing Why Edge as a Service? Deploying and operating applications at the Edge requires a dynamic, cohesive system which manages distributed traffic routing and endpoint orchestration while simultaneously providing a seamless and simple developer and operations team experience and delivering a secure, reliable target deployment network. Expertise, planning, and continuous monitoring are non-negotiables when it comes to having containers spread across different regions. Edge as a Service (Eaas) handles the complexities of having to manage multi-cloud/edge deployments, including CaaS. Familiar development workflows make it as easy to deploy to the edge as developers have become accustomed to with cloud. One of the primary benefits is the orchestration of workloads to meet real-time traffic demands, maximizing efficiencies so you’re not running all workloads in all locations at all times. Use Case for CaaS at the Edge: SaaS Cloud-Native Deployment Model Wallarm, a leading WAF solution provider, came to Section with the goal of being able to extend a cloud-native deployment model for their customers. Their traditional deployment model required customers to install the solution in their centralized infrastructure, which introduced operational burden and slowed their time to value. With a containerized version of their application, Wallarm has been able to leverage Section’s Edge as a Service to build out the Wallarm Cloud WAF solution. By building on top of Section, Wallarm doesn’t have to think about managing the underlying infrastructure layer and is able to take advantage of Section’s Composable Edge Cloud to deliver more value for their customers. On top of that, Wallarm customers can go-live with a distributed WAF in minutes via a simple DNS change. To break down the benefits of the distributed deployment model even further, let’s consider a simple example. Imagine a SaaS solution (like Wallarm) which may be traditionally deployed in a customer’s AWS-East instance, but their application serves a significant portion of traffic in the APAC region. The distance that traffic has to traverse over the network introduces latency issues for end users accessing that application. When you distribute the workload across a global footprint (via EaaS), you’re instantly able to deploy closer to end users, wherever they may be, resulting in significant performance gains. For SaaS providers to directly build this same edge footprint, it would require a large investment in initial infrastructure, not to mention the ongoing management of operations, which requires a huge amount of specialist expertise. Edge as a Service provides a turnkey solution allowing organizations to focus on their core products."
"238","2021-04-23","2023-03-24","https://www.section.io/blog/building-operating-edge-networks-infrastructure/","This is the final post in a three-part series digging into the complexities that developers and operations engineers face when building, managing, and scaling edge architectures. In the first post, we looked at the complexities of replicating the cloud developer experience at the edge. In the second part, we discussed how to approach application, selection, deployment, and management for the edge. In this post, we’ll focus on the complexities of managing the network, infrastructure, and operations in a distributed compute environment. We are in the midst of a foundational technological shift in communications infrastructure. IDC predicts that by 2023, more than 50% of new enterprise IT infrastructure will be deployed at the edge and the edge access market is predicted to drive $50 billion in revenues by 2027. To interconnect this hyper-distributed environment, which spans on-premise data centers, multi-clouds, and the edge, the network is in the process of evolving and becoming more agile, elastic, and cognitive. Let’s dive into a high-level overview of some of the critical components you need to consider when building and operating distributed networks. Areas that we’ll cover include: DNS TLS DDoS mitigation (Layers 3, 4, 7) BGP/IP address management Edge location selection and availability Workload orchestration Load shedding and fault tolerance Compute provisioning and scaling Messaging framework Edge operations model and team Observability NOC integration As you’re evaluating all of these considerations, bear in mind that working with Edge as a Service can solve many of these complexities for you. DNS: A Critical Component in Networking Infrastructure The domain name system (DNS) is often referred to as the phonebook of the Internet since it translates domain names to IP addresses, allowing browsers to load Internet resources. DNS provides the hierarchical naming model, which lets clients “resolve” or “lookup” resource records linked to names. DNS therefore represents one of the most critical components of networking infrastructure. DNS Services are Often Vulnerable to Threats Other widely used Internet protocols have started to incorporate end-to-end encryption and authentication. However, many widely deployed DNS services remain unauthenticated and unencrypted, leaving DNS requests and responses vulnerable to threats from on-path network attackers. Hence, when building out DNS services, it’s critical to maintain a security-first approach. DNS in a Distributed Computing Environment DNS is lightweight, robust and distributed by design. However, new approaches to computing architecture, including multi-cloud and edge, introduce new considerations when implementing application traffic routing at the DNS level. In a distributed compute environment, DNS routing entails ensuring that users are routed to the correct location based on a set of given objectives (e.g. performance, security/compliance, etc.). When routing, you need to take into account service discovery. The majority of the time, people use DNS for public-facing service discovery, but this can be challenging to pull off. Routing is complicated both in terms of ensuring that users are routed to the right location and that failover is handled - in other words, what will you do to help update routes when systems fail? Section’s Approach to DNS, Failover, and Service Discovery As an Edge as a Service provider, Section uses a combination of managed DNS services for routing: Amazon Route 53 and NS1, which, when used in combination with Section’s Adaptive Edge Engine workload scheduler, allows us to determine which endpoints users are routed to. NS1 provides a broader and more granular set of filters, which enable state-based latency routing. (Its “up” filter is a particularly useful feature for multi-CDN or multi-cloud architecture.) “NS1 brings automation, velocity, and security to modern application development and delivery. Enterprise infrastructure is evolving faster than ever before. Emerging technologies make it possible to spin up microservices and cloud instances in minutes. DevOps teams are churning out code 40 times faster than legacy production environments. New edge and serverless architectures are taking computing out of the data center and closer to devices enabling global real-time applications. Those organizations not born in the cloud-native era face the additional challenge of connecting legacy applications with new technology in the never-ending race to meet user demands for performance while driving efficiency and security.” Kris Beevers, CEO, NS1 By working with managed DNS services that are at the forefront of new developments, Section keeps applications available, performant, secure, and scalable with integrated Anycast DNS hosting, while also removing the burdens of configuration, security, and ongoing management. TLS: Provisioning, Management, and Deployment Across Distributed Systems Transport Layer Security (TLS) is an encryption protocol, which protects communications on the Internet. You can feel reassured that your browser is connected via TLS if your URL starts with HTTPS and there’s an indicator with a padlock assuring you that the connection is secure. TLS is also used in other applications, such as email and usenet. It’s important to regularly upgrade to the latest protocol for TLS and its predecessor, the SSL protocol. When working with TLS and/or SSL in distributed environments, you have to either work with your own certificates using a managed service such as DigiCert or use an open source version with a service like Let’s Encrypt or Certbot. The managed certificate authorities such as DigiCert will provide automated tooling to provision certificates. With the open source versions, you will find that you have to build the services to manage auto-renewal components and the provisioning of new certificates. An added complexity is the question of how you deploy these protocols? In relation to distributed systems, you will have certificates that need to be running in multiple places. They might be running across multiple providers, for example, you might be using one specific ingress controller in one location and a different ingress controller in another. How do you ensure that your certificates are being deployed where needed in order to actually handle the TLS handshakes? And as the number of domains that you manage increases, so too do the complexities. This ties directly back to DNS since you need to ensure that you’re routing traffic to the correct endpoints containing the workloads where your TLS certificates are deployed. Further, you will have to take into account the state of your systems at any point in time and how you route traffic, since you never want to be servicing users incorrectly. Ultimately, servicing your user correctly is the end goal, meaning that when implementing TLS at the edge yourself, you have to take into account all these different components. Section’s Edge as a Service includes advanced, managed SSL services to procure, install, renew, and test SSL/TLS certificates for your web applications. DDoS: Protecting Layers 3, 4, and 7 When protecting against Distributed Denial of Service (DDoS) attacks across distributed systems, the first question to ask should be, where are my systems most vulnerable to attack? The primary layers of focus for protecting against DDoS attacks include Layers 3, 4, and 7 in the OSI model. Firms like Wallarm, Signal Sciences, ThreatX, and Snapt will provide DDoS protection for you at the application layer (i.e. Layer 7), and when you deploy these through Edge as a Service providers like Section, you’re able to leverage a distributed deployment model out-of-the-box. However, in an edge computing paradigm that’s made up of heterogeneous networks of providers and infrastructure, there are more questions that need asking. The most important: how do all the different providers I’m using handle network and transport-layer DDoS attacks (i.e. Layers 3 and 4)? All major cloud providers typically have built-in DDoS protection, but when you begin to expand across a multi-cloud environment, and even further out to the edge, you need to ensure that your applications are protected across the entire network. This includes knowing how each underlying provider handles DDos protection, along with implementing safeguards for any areas in your networks that may be underprotected. This also takes us back to DNS and the question of how to handle traffic routing when one (or more) of your endpoints becomes compromised. Section’s Adaptive Edge Engine works in conjunction with best-in-class WAF offerings, along with advanced DDoS mitigation to ensure your applications are always protected across your entire compute environment. BGP/IP Address Management The Border Gateway Protocol (BGP) is responsible for examining all the available paths that data can travel across the Internet and picking the best possible route, which usually involves hopping between autonomous systems. Essentially, BGP enables data routing on the Internet with more flexibility to determine the most efficient route for a given scenario. BGP is also widely considered the most challenging routing protocol to design, configure, and maintain. Underlying the complexities are many attributes, route selection rules, configuration options, and filtering mechanisms that vary among different providers. In an edge computing environment, rather than announcing IP addresses from a single location, BGP announcements must be made out of multiple locations, and determining the most efficient route at any given point becomes much more involved. Another important consideration when it comes to routing is load balancing at the transport layer (Layer 4). Building a Layer 4 load balancer is complicated for the following reasons: It must support BGP announcements. You need to own the IP space, which can be very costly. You need to understand BGP, which may require a team of network engineers to truly manage the system. You need to be able to announce in locations all around the world (which are also the most peered locations). Finally, you need to take into account where you’re load balancing traffic to. The distributed system that you’re running applications on must be able to support packets that are being load balanced from the load balancer in front. The complexities of routing across multi-layer edge-cloud topologies are perhaps the most daunting when it comes to building distributed systems. This is why organizations are increasingly turning to Edge as a Service solutions that take care of all of this for you. Edge Location Selection and Availability An effective presence at the edge is based on having a robust location strategy. By moving workloads as close as possible to the end user, latency is reduced. Selecting the appropriate geographies for your specific application within a distributed compute footprint involves careful planning. At Section, we operate an OpEx model and benefit from a very different kind of network to cloud providers and traditional content delivery networks. Our flexible strategies and workflows allow us to tailor the correct edge network for each of our customers, delivering on performance gains and reducing costs for them. The Section Composable Edge Cloud is built on the foundations of AWS, GCP, Azure, Digital Ocean, Lumen, Equinix Metal, RackCorp, and others. We regularly add more hosting providers and have the capacity to deploy endpoints based on the specific needs of our customers who want to define their own edge. Compliance also plays a role in the selection of edge locations. Increasingly, regulations and compliance initiatives, such as GDPR in Europe, are requiring companies to store data in specific locations. Edge as a Service providers with flexible edge networks enable DevOps teams to be precise about where they want their data to be processed and stored, without the burdens associated with ongoing management. Workload Orchestration Managing workload orchestration across hundreds, or even thousands, of edge endpoints is no simple feat. This can involve multiple components. You need to start with where you want the workload to be defined (e.g. full application hosting, micro APIs, etc.) Next, ask where will it be stored? Finally, take into account how the workload is actually deployed. How do you determine which edge endpoints your code should be running on at any specific time? What type of automation tooling and DevOps experience do you need to ensure that when you make changes, your code will run correctly? Managing constant orchestration over a range of edge endpoints among a diverse mix of infrastructure from a network of different providers is highly complex. To migrate more advanced workloads to the edge at a faster rate, developers are increasingly turning to flexible Edge as a Service solutions, which support distribution of code across multiple programming languages and frameworks. Load Shedding and Fault Tolerance A load shedding system provides improved fault tolerance and resilience in message communications. Fault tolerance allows a system to continue to operate, potentially at a reduced level, in the event of a failure within one or more of its components. In regards to load shedding and fault tolerance at the edge, the primary area of concern is ensuring that the systems handling your workloads and servicing requests aren’t overloaded. Essentially, how do you make sure that one location isn’t set up to infinitely scale and how do you ensure that load is distributed appropriately? Compute Provisioning and Scaling Load shedding and fault tolerance brings us on to auto scaling and configuring auto scaling systems. One of Kubernetes’ biggest strengths is its ability to perform effective autoscaling of resources. Kubernetes doesn’t support just one autoscaler or autoscaling approach, but three: Pod replica count - This involves adding or removing pod replicas in direct response to changes in demand for applications using the Horizontal Pod Autoscaler (HPA). Cluster autoscaler - As opposed to scaling the number of running pods in a cluster, the cluster autoscaler is used to change the number of nodes in a cluster. This helps manage the costs of running Kubernetes clusters across cloud and edge infrastructure. Vertical pod autoscaling - the Vertical Pod Autoscaler (VPA) works by increasing or decreasing the CPU and memory resource requests of pod containers. The goal is to match cluster resource allotment to actual usage. If you’re not using a container orchestration system like Kubernetes, compute provisioning and scaling can get very challenging very quickly. Messaging Framework The messaging system provides the means by which you can distribute your configuration changes, cache ban requests, and trace requests to all your running proxy instances in the edge network or CDN, and report back results. This involves two primary components: Workload orchestration Receiving updates - if your system needs to receive a message or update, how do they get it, and is it getting there reliably? At Section, we rely on MCollective for this, a framework that allows us to build server orchestration or parallel job execution systems, meaning we can programmatically execute administrative tasks on clusters of servers. Edge Operations Model and Team To truly manage distributed network, infrastructure and operations at the Edge, you will likely need an edge operations model with an experienced team comprised of: Network engineers Platform engineers DevOps engineers, with an emphasis on site reliability engineering (SRE) If you don’t have some or all of these specialists, or don’t have the expertise or resources in edge computing to manage the network, infrastructure, and operations, you can work with an Edge as a Service provider whose solutions abstract away many of the complexities associated with edge computing. Observability It’s imperative to treat observability as a first-class citizen when designing distributed systems, or any system for that matter. Reliable and real-time information is critical for engineers and operations teams who need to understand what is happening with their applications at all times. Observability is also a key element in disaster recovery planning and implementation. Your infrastructure needs to be observable and flexible so that you can understand what has broken and what needs to be fixed. If a critical error occurs, you need visibility into your system to keep the incident as brief as possible versus experiencing a protracted disaster. Observability is a cornerstone of Edge as a Service offerings. As part of a comprehensive edge observability suite, the Section Console includes the ELK stack (Elasticsearch, Logstash and Kibana) for storing, searching and visualizing logs for your application and each of your edge modules, along with a next-gen Traffic Monitor, which gives DevOps engineers, application owners and business execs a straightforward way to visualize how traffic is flowing across their edge architecture. NOC Integration Enterprises are taking steps to unify their network operations centers (NOCs) and security operations centers (SOCs). Why? By creating alignment between these two often siloed teams, organizations can reduce costs, optimize resources and improve the speed and efficiency of incident response and related security functions. You need to take into account the expertise of your team when planning a NOC and/or SOC integration. Not everyone will have the range of crossover experience necessary to pull off a successful integration. Conclusion While we’ve covered a lot in this overview, this is just a subset of the critical components and complexities that come with building and operating cloud-edge networks and infrastructure; and we really only scratched the surface on each of the considerations included above. Every organization, team, and application has unique requirements when it comes to designing distributed systems. Because of this, many teams start down the path of building their own bespoke systems, and typically become overwhelmed rather quickly with all of the complexities that play into design decisions and implementation. As an Edge as a Service provider, Section is often pulled into projects during the early stages of research and discovery, where we’re able to offload the build and management of many, if not most, of the critical components, ultimately accelerating the path to edge for organizations across a diverse range of use cases."
"239","2021-03-10","2023-03-24","https://www.section.io/blog/complexity-developer-experience-cloud-edge/","Over the last fifteen years or so, developers have become very familiar with cloud deployment. Whether using AWS, Azure, GCP, Digital Ocean, or a more niche provider, the dev experience is fairly similar, no matter which cloud you’re on. For a developer, cloud workloads typically include: Identifying where your highest concentration of users are and selecting a single cloud location that will deliver the best performance to the maximum number of users. Connecting your code base, hosted in code repository tooling (e.g. Github, GitLab, Bitbucket) Automating build and deployment through CI/CD tooling (e.g. Jenkins, CircleCI, Travis CI) These processes are fairly straightforward when all code and microservices are feeding into a single deployment endpoint. However, what happens when you add hundreds of edge endpoints to the mix, with different microservices being served from different edge locations at different times? In this kind of environment, how do you decide which edge endpoints your code should be running on at any given time? More importantly, how do you manage the constant orchestration across these nodes among a heterogeneous makeup of infrastructure from a host of different providers? Image source: Gartner In this blog, we’ll look at some of the complexities involved in streamlining the developer experience around edge deployment and ongoing management. The Edge Developer Experience: Managing Complexity As edge computing gains traction both in its own right and as part of a hybrid cloud infrastructure, increasing numbers of developers are being asked to program in a new computing paradigm. Building for the edge is a complex and always shifting puzzle to be solved and then managed. Some of the complexities surrounding the edge developer experience include: Code and configuration management Edge runtimes Distributed diagnostics and telemetry Application lifecycle management Code and Configuration Management Every application is unique. Developers need granular, code-level control over edge configuration to fit the unique needs of their application. At the same time, they require simple, streamlined workflows to continue to push the pace of innovation and maintain secure, dependable application delivery. With various industry players competing for a share of the edge computing market, from hyperscalers to CDNs, there are many considerations for evolving the developer experience to adapt to edge nuances. For example, many traditional CDNs have hard-coded proprietary software into their solutions (e.g. web application firewall technology), offering very limited configuration options. Thus, developers can find themselves backed into a corner with legacy CDNs offering edge services, forcing them to bolt on additional solutions that inevitably erase some of the benefits they were seeking to solve with the CDN solution in the first place. Furthermore, developers are increasingly looking to migrate more application logic to the edge for performance, security, and cost-efficiency gains. A few examples of workloads moving to the edge include: Micro APIs - Hosting small, targeted APIs at the edge for use cases such as search or fully-featured content exploration with GraphQL (which enables faster responses on user queries while lowering costs). Headless Commerce - Decoupling the presentation layer from back-end eCommerce functions allows you to push more services to the edge to create custom user experiences, achieve performance gains, and improve operational efficiencies. Full application hosting at the edge - More and more developers are exploring the idea of hosting their entire application at the edge. Rather than beaconing back to a centralized origin, hosting databases alongside apps at the edge and then syncing across distributed endpoints is quickly becoming a reality that has the potential to become the new normal as edge computing matures. The types of workloads being considered for edge deployment are many and diverse. In order for developers to progress towards migrating more advanced workloads to the edge, they require flexible solutions that support distribution of code across programming languages and frameworks. Edge Runtimes Runtime describes the final phase of the program lifecycle, which involves the machine executing the program’s code. As more developers continue to adopt edge computing for modern applications, edge platforms and infrastructure will need to support different runtime environments. Complexity factors into runtimes at the edge in relation to interoperability, i.e. the complexity of managing runtimes across distributed systems. Developers need to be able to run applications in their dedicated runtime environment with their programming language. Systems that support diverse developer needs, therefore, must be able to support this to be useful to all. One of the most widely used runtime environments for JavaScript is Node.js, used by many businesses, large and small, to create applications that execute JavaScript code outside a web browser. Some other well-known examples of runtime environments include the Java Runtime Environment, a prerequisite for running Java programs, .NET Framework which is required for Windows .NET applications, and Cygwin, a runtime environment for Linux applications that allows them to run on Windows, macOS, and other operating systems. With developers building across many different runtime environments, Edge as a Service offerings need to be able to support code portability. We can’t expect developers to refactor their code base to fit into a rigid, pre-defined framework. Instead, multi-cloud and edge platforms and services must be flexible enough to adapt to different architectures, frameworks and programming languages. Distributed Diagnostics and Telemetry It is critical for developers to be able to have a holistic understanding of the state of their application at any given time. Observability becomes increasingly complex when you consider distributed delivery nodes across a diverse set of infrastructure from different providers. As reported in a Dynatrace survey of 700 CIOs, “The dynamic nature of today’s hybrid, multicloud ecosystems amplifies complexity. 61% of CIOs say their IT environment changes every minute or less, while 32% say their environment changes at least once every second.” To add to the complexities of trying to keep up with dynamic systems, that same report revealed that: “On average, organizations are using 10 monitoring solutions across their technology stacks. However, digital teams only have full observability into 11% of their application and infrastructure environments.” An effective solution for multi-cloud/edge observability should be able to provide a single pane of glass to draw together data from many locations and infrastructure providers. This kind of visibility is essential for developers to gain insight into the entire application development and delivery lifecycle. The right centralized telemetry solution will allow engineers to evaluate performance, diagnose problems, observe traffic patterns, and share value and insights with key stakeholders. At Section, we conducted quite a bit of user research leading up to the build of our Traffic Monitor which gives engineers, application owners and business leaders a straightforward way to quickly see how traffic is flowing through their edge architecture. What we found across interviews with development teams is that they crave tooling that enables them to understand what’s going on with their application traffic and edge/multi-cloud workloads at a glance, while also giving them the ability to easily drill down deeper into individual logs when needed. For example, the Traffic Monitor provides an easy way for DevOps teams to quickly observe errors across their edge stacks, then click to drill into the root cause of those errors without having to form complex queries to get the information they need. Centralized Application Lifecycle Along with configuration flexibility/control and comprehensive observability tooling, developers need to be able to easily manage their application lifecycle systems and processes. With a single developer or small team overseeing a small, centrally managed code base, this is fairly straightforward. However, when an application is broken up into hundreds of microservices that are managed across teams, coupled with a diverse makeup of deployment models within the application architecture, this can become exponentially complex and impact the speed of development cycles. When we add the additional complexities of pushing code to a distributed Edge and maintaining code application cohesion across that distributed application delivery plane at all times (even during an application deployment cycle), application lifecycle management for edge becomes vastly more complex than the centralized approaches used to date with cloud. GitOps To streamline operations, many teams take advantage of GitOps workflows. GitOps is a way of implementing Continuous Deployment for cloud native applications. It focuses on a developer-centric experience when operating infrastructure, by using tools developers are already familiar with, including Git and Continuous Deployment tools. While responsibilities and oversight of different parts of an application’s code may be siloed within an organization, all of the code needs to feed into a unified code base. In order for developers to be able to move more services to the edge, they need tooling that is underpinned by GitOps principles for a fully integrated edge-cloud application lifecycle. CI/CD As an integral part of GitOps workflows, developers need flexibility and control when it comes to integrating edge deployment processes into existing continuous integration/continuous delivery (CI/CD) pipelines. There are three main principles worth following when managing changes to your edge configuration through a CI/CD pipeline: Optimize for fast feedback. Identify steps within the pipeline that need optimizing by tracking execution time on individual stages. From the time you push a change to version control to making the change live should take no longer than five minutes. Fast feedback is important for quickly ensuring your changes meet business needs and cutting out technical debt and unnecessary costs. Chunk your changes, test immediately. Instead of changing multiple things in batches and then testing for the effect, interleave the changes and tests, and stop execution immediately if the tests fail. By turning changes into small, verifiable units, you lessen the risk factor. Push all changes through the pipeline. You lose the benefits you’re striving for if you accommodate changes outside the process. Delivery and maintenance of code to distributed edge infrastructure is more difficult to execute with the speed and consistency required to achieve the same application experience for end users at all times as if they were all connecting to one centralized application in the cloud. Consistency is Key The key to accelerating edge computing adoption is making the experience of programming at the edge as familiar as possible to developers, explicitly drawing on concepts from cloud deployment to do so. The added complexities that a distributed edge deployment brings introduces new challenges to achieving consistency across these experiences. “When we look at the challenges of scale and operational consistency, the edge cannot be seen as a point solution that then needs to be managed separately or differently across hundreds of sites – this would be incredibly complex. In order to be successful, you need to manage your edge sites in the same way you would the rest of your places in the network – from the core to the edge. This helps minimize complexity and deliver on the operational excellence that organizations are striving for.” Rosa Guntrip, Senior Principal Marketing Manager, Cloud Platforms, Red Hat The right Edge as a Service platform will help minimize complexity, enabling developers to focus on innovation and executing mission-critical tasks instead of juggling all the pieces involved in managing edge/multi-cloud workloads. Be sure to check out the next piece in this series How to Approach Application Selection, Deployment, and Management for the Edge"
"240","2021-02-23","2023-03-24","https://www.section.io/blog/balance-control-simplicity-edge-as-a-service/","Technology providers are increasingly demanding a more granular level of control across the full stack, including the desire to tailor their edge compute solution to their specific application needs. This same appetite for greater control, however, can concurrently lead to a challenging rise in complexity in terms of network selection, workload orchestration and infrastructure provisioning. Edge as a Service (EaaS) can help organizations overcome these complexities by offering a simple, managed approach to deploying and maintaining applications at the Edge. The Type of Granular Control Possible at the Edge Not all edge computing solutions are alike, but a modern Edge as a Service provider should enable greater granular control over your tech stack and edge computing requirements than a traditional cloud or CDN provider can offer. With the right Edge as a Service provider, you’ll also have the option to integrate edge into your multi-cloud strategy. A true cloud/edge deployment model can be incredibly valuable in optimizing performance and lowering latency, but the challenge of managing workload orchestration across hundreds or thousands of edge endpoints is real. Edge as a Service can remove the strain and resource gap problem by managing this for you. Granular control at the edge becomes particularly complex in four key areas: Location Strategy - Getting the right workload to the right place at the right time Security & Compliance - Protecting assets across a distributed network Application Development Lifecycle - Efficient code management and deployment Observability - Consolidation of edge metrics Location Strategy: Working with Today’s Internet Infrastructure An effective edge presence revolves around the right location strategy. After all, the basic idea behind edge computing is placing workloads as close as possible to end users. Ensuring the appropriate geographic distribution of endpoints that make the most sense for your specific application is possible with the right EaaS provider. At Section, we have built a very different type of network to traditional CDNs, allowing our users to access the full range of benefits from existing and emerging Internet infrastructure. We operate an OpEx model, allowing us to use flexible strategies and workflows to best meet the needs of each individual customer and maximize performance and cost savings for them. The Section Composable Edge Cloud is built on the foundations of providers such as AWS, GCP, Azure, Digital Ocean, Lumen, Equinix, and RackCorp, to name a few; we regularly add new hosting providers and can deploy points of presence (PoPs) on-demand to help customers define their own edge. Geographic location is one of the factors behind our choice of PoP infrastructure. Others include connectivity, provider range, and the specific needs of our customers and their markets. The way that we deploy architecture enables flexibility for our users to select the networks that work for them and to host different file types on different hosting providers, including their own private networks, enabling performance optimization and cost efficiencies. As a managed Edge as a Service provider, Section aims to simplify distributed computing, including location decisions, workload orchestration, and traffic routing. With the Adaptive Edge Engine (AEE) automatically maximizing performance and cost optimization, it’s rare that our customers need (or desire) the extra layer of direct control over locations. We built the AEE specifically to automate decision-making and orchestration, so that DevOps engineers don’t need to worry about this, and can trust that their workloads are running in the optimal locations to meet real-time traffic demands, allowing them to focus on outcomes, not configurations. Simplifying Regulatory Compliance at the Edge Another layer of complexity many DevOps teams and business executives are concerned about relates to regulation and compliance. We are living through a moment of significant change in regards to this. Increased regulatory requirements and compliance, such as PCI compliance and GDPR, are already underway, and greater regulation of the tech industry is imminent in the US, Europe and China. This is already leading to changed business models, and could dramatically alter the tech landscape in the years to come. According to Louis Lehot, founder of L2 Counsel, “legal functions will need to keep pace to ensure compliance with existing and new regulations….We will want to make sure our companies are using providers that know how to protect the storage of intellectual property and avoid potential infringement.” The Edge allows for DevOps teams to be more precise about where their data is processed and stored. With Section, for instance, you can spin up your own private edge network for your application, and leverage enterprise-grade security solutions for your data and applications. This lets you protect your applications against leaks or fraud while ensuring regulatory compliance. Application Development Lifecycle Every application is unique, and there is no one size fits all when it comes to application development lifecycle management across teams. Systems shouldn’t define workflows; rather, they should be flexible enough to adapt to different development environments and workflows. In order for developers to harness the power of edge computing, edge configuration and deployment needs to have the same level of familiarity and standardization that has become commonplace with cloud deployments. This includes integration across existing code repositories, CI/CD pipelines, and edge deployment tooling. Git-backed workflows allow developers to easily manage and integrate application code to speed up deployments. CLI tooling is another way to accelerate time to value with edge deployments. We recently released our own sectionctl CLI utility that helps bridge the gap between cloud and edge workflows with familiar tooling. Observability: Consolidation of Edge Metrics Software is becoming exponentially complex, and hence the challenges associated with maintaining observability across systems are becoming increasingly difficult. When you consider this in the context of edge platforms that are orchestrating workloads and traffic into and out of dynamic edge networks, you can imagine the amount of “unknown unknowns”. With any system, developers need to be able to get to the information they need quickly and easily, and drill into granular detail to help them evaluate performance, diagnose issues, observe patterns, and share value with key stakeholders. We built the Section Traffic Monitor with these critical needs in mind. Making Edge Computing Simpler for Technology Leaders Edge as a Service providers, such as Section, are appealing to the need for flexibility in edge deployments, simplifying how developers build and run applications across an expansive and diverse edge network. Underlying that simplicity, however, are many levers that work together to deliver faster, more secure, and more cost-effective applications. We work hard to allow you to define the edge as you want it. The Section platform abstracts many of the complexities of edge computing away, so that organizations can focus on implementing the edge strategies that are most suitable for any given application."
"241","2021-11-29","2023-03-24","https://www.section.io/blog/section-powers-victorian-government-sdp-platform/","Section has been selected by the Victorian Government’s Department of Premier and Cabinet (DPC) as part of a competitive tender process to provide Content Delivery Network (CDN) and Edge Computing services for its Single Digital Presence (SDP) platform. Section will provide the Victorian Government a range of solutions, including application performance optimization, security/edge hosting and routing, streamlined by DevOps workflows, and meshed with DPC’s Continuous Integration / Continuous Delivery (CI/CD) capabilities. The solution is designed to support the government’s efforts to improve efficiency, accessibility and user experience by consolidating valuable public information and applications through its SDP initiative. Section makes it easy for government and commercial organizations to deliver faster, more personalized and secure digital experiences by removing the burdens associated with provisioning, monitoring and scaling these distributed workloads. Unlike inflexible cloud and legacy CDN platforms, Section gives DPC freedom to better control, protect and grow future edge services and other capabilities for its SDP platform. DPC will look to leverage Section’s Edge Services Marketplace, which includes readily available open-source and third-party options for web application firewalls (WAFs), bot blocking, caching, server-side A/B testing, image optimization and more. “The Victorian Government valued our flexible, modular, vendor-agnostic platform, providing a range of options to meet its needs today and into the future.” - Matthew Johnson, General Manager, APAC at Section “Consolidating and distributing application workloads helps governments efficiently connect with citizens in exciting new ways. DPC recognized that a robust content delivery and edge computing platform is critical to the optimal user experience it seeks to create with its SDP platform,” said Matthew Johnson. The SDP platform is designed to make it easier for people to find, understand and use Victorian Government information. Initiated in 2016, SDP began with a business case to consolidate all DPC websites onto a common platform. Since then, over 100 websites with content from all 9 Departments plus Victoria Police have been consolidated on SDP to realize the full benefits of a whole government approach to digital delivery. Section is partnering with Salsa Digital, an open source focused company of digital engineers that works with governments across Australia in providing end-to-end digital experiences, as part of the Victorian Government’s SDP project. Salsa delivers expert services for governments around development, infrastructure, governance and edge capabilities. Looking ahead, the DPC has identified opportunities to push more edge-enable solutions, such as QuantCDN, a static site generation and hosting platform."
"242","2021-04-01","2023-03-24","https://www.section.io/blog/application-deployment-management-complexities-edge-computing/","As discussed in the first part of this series The Complexities of Replicating the Cloud Developer Experience at the Edge, over the past couple of decades, cloud computing has become the preferred deployment model for developers. There are many reasons for this, which can mainly be summed up by cloud simplifying the management of delivery of services to end users, with functions that span compute, storage and delivery. At the same time, many application creators have relied on complementary CDN technology to boost performance, security, and scalability. Placing parts of an application at the edge have provided obvious performance benefits but have also added certain complexity to the aforementioned simplicity of cloud by adding an additional and discrete delivery layer. With the demand for faster user experiences being driven by emerging and evolving use cases (e.g. IoT, gaming, etc.), application creators are increasingly looking to offload more services to the edge. At the same time, application operations teams are looking to simplify their delivery stack. Bringing more of the application delivery cycle into a single cohesive edge delivery solution can achieve both of these goals concurrently. While cloud providers have the flexibility to support a diverse range of workloads, developers working in the cloud are limited to a single provider’s network, or alternatively are responsible for managing workload orchestration across multiple providers. CDNs, meanwhile, may have expansive global networks of infrastructure, but they are typically unable to support general purpose workloads beyond basic content delivery. Why CDNs are not the Best Option for Edge Computing CDNs are often thought of as the first evolution of edge computing. However, content delivery encompasses only a small subset of all edge workloads. As the diversity of edge workloads has expanded beyond content delivery, existing solutions fall short in terms of what they’re able to support. Many CDNs were built around open source technologies, such as Varnish Cache and ModSecurity. Typically, they have customized the code base so much over the years, that developers using them are locked into “black box”, proprietary solutions that don’t offer the flexibility and control necessary to fit the unique requirements of each application. Furthermore, growth in adoption of container technology and serverless functions has completely changed the game, leaving many legacy CDNs unequipped to support modern applications. With Kubernetes becoming the preferred container orchestration platform, edge solutions built on Kubernetes are significantly better positioned to support the needs of modern developers. The Complexities in Moving Diverse Workloads to the Edge Now, let’s take a deeper dive into some of the complexities involved in moving more diverse workloads to the edge, including selection, deployment and ongoing management. Web Application Firewalls (WAFs) & Bot Management DevOps teams are increasingly choosing to deploy WAFs and bot mitigation tools across a distributed architecture, with the goal of detecting and mitigating threats faster. Managing a WAF or bot mitigation deployment across a multi-cloud/edge network is no simple feat, however, and developers are turning to Edge as a Service platforms to help manage this. Additionally, while many best-in-class WAF and bot management technologies have emerged - providers such as Wallarm, Snapt (WAF), ThreatX, Signal Sciences, Radware Bot Manager, and PerimeterX - most legacy CDNs still don’t give developers the option of deploying third-party solutions. Fastly, for example, recently acquired Signal Sciences, recognizing the need for more advanced WAF technology beyond their own proprietary solution. At Section, we often speak with developers who are frustrated with the “black box”, built-in solutions of legacy CDNs, and demand more choice and flexibility. Edge as a Service providers need to support developers’ software choices, particularly when it comes to as critical a service as security. Image Optimization Beyond simple caching of images, developers, especially in the e-Commerce sector, are increasingly seeking out image optimization solutions (e.g. Optidash) that optimize and transform images on-the-fly. Benefits include: Faster page load times for end users Improvements to operational efficiency Removing the load on centralized infrastructure Just as with security solutions, most legacy CDNs don’t support third-party software that specializes in point solutions. What’s more, if you’re operating a multi-cloud/edge environment, you will have to install and manage these types of image optimization tooling across the entire network. Edge as a Service solves this by acting as the orchestration layer to ensure that workloads are running in the right edge location at the right time. Testing & Experience Optimization Marketers, product managers, developers and others need the ability to effectively test and optimize applications across the client-side, server-side, single page application (SPA), mobile, redirects, and so on. Conventional A/B testing solutions use JavaScript tags to manipulate content on applications, which reduces site performance with flicker and increased latency. Modern tools like SiteSpect, however, rethink this model by sitting in the flow of HTTP traffic. This allows them to support multiple user experience optimization techniques, including client-side, server-side, redirects, and SPA optimization. Legacy CDNs can’t support this new architectural model and therefore require extra hops in the HTTP delivery chain, ironically negating many of the performance benefits they are aiming to solve. By supporting distributed deployment of more advanced workloads, Edge as a Service providers, like Section, make it easy to integrate solutions like SiteSpect into your edge stack. “This goes to show how flexible both SiteSpect’s and Section’s platforms are, and how great their DevOps and technical support teams are in order to accommodate our needs. Since migrating to this new deployment, the traffic routing and customer experience have been seamless, and the performance and stability have improved tremendously.” - Mike Henriques, CIO, Temple & Webster | read full case study Load Balancing While most hyperscalers and edge providers offer load balancing, these solutions are often restricted to their own environments. Therefore, if you migrate your application to a different cloud or data center, the hyperscaler or edge provider’s proprietary load balancer won’t be able to follow. In the instance of a traditional load balancer that is being deployed to the cloud, you need to use a virtual appliance. If you then decide to use a load balancer in a second cloud, that virtual appliance will need to be re-configured again… and so on for every cloud or data center it operates in. There is no communication between these two appliances. In this instance, you are operating two (or more) separate clouds that your teams will need to manage separately. Organizations that use multi-cloud/edge networks are then faced with having to separately configure, monitor and manage delivery and security for each distinct environment. Similarly, for any application that changes hosting location, adjustments must be made on an individual basis. This not only increases complexity, but takes up valuable resources and limits much of the flexibility that is supposedly a key benefit of a multi-cloud/edge model. Edge as a Service handles load balancing across multi-cloud/edge networks. For example, Section’s Adaptive Edge Engine has built-in health checks to monitor and automatically migrate traffic and workloads based on real-time traffic demands. Beyond this, a Layer 7 load balancer can sit on top of heterogeneous networks and route HTTP requests based on customized rules. Containers In a small environment with only a handful of systems, managing and automating orchestration is fairly straightforward, but when an enterprise has thousands of individual systems that interact with each other on some level, orchestration automation is both powerful and essential. Containers are lightweight by definition with a low footprint, making them perfect candidates for running on edge devices. The main reason machine learning models leverage containers is because legacy devices can still interact with cloud services like AI/ML to achieve fast computation in-place. Containers can be deployed to the device of your choosing and can be built using the architecture of your choice so long as it can run the container runtime. Updating containers in-place is simple, particularly when orchestration solutions like Kubernetes are used. Consider SaaS providers who traditionally offered on-premise or single point of presence installations. As customers increasingly demand distributed deployment models, SaaS providers are faced with the build vs. buy dilemma. The management of these complex clusters of devices, services, and networks can get highly complicated very quickly. With Edge as a Service, SaaS providers can simply containerize their applications and accelerate their path to the edge, rather than building out and managing their own edge networks. Serverless Serverless computing, also called function as a service (FaaS), enables the execution of event-driven logic without the burden of managing the underlying infrastructure. The name ‘serverless’ is characterized by the freedom that it gives developers to focus on building their applications without having to think about provisioning, managing, and scaling servers. The concept of serverless was originally designed for cloud environments, eliminating the ‘always-on’ model to save on resource consumption, among other benefits. In recent years, advances in edge computing technology have led more developers to migrate serverless workloads to the edge. The benefits of serverless at the edge, when compared to alternatives like containers and VMs, include lighter resource consumption, improved cost efficiencies, code portability, and speed of deployment. However, not all workloads are suitable for serverless models and it’s important to understand the requirements of a given workload when determining the most appropriate deployment model. Considerations such as code dependencies, cold starts and their effect on performance, security, and resource requirements are critical when designing edge architectures. Edge as a Service providers can help streamline serverless deployments by offering flexible language support that allows developers to simply ship code and offload the responsibilities of deployment, management, and scaling of the underlying infrastructure to the edge compute platform. Overcoming the Complexities with Section As we’ve just started to look at in this series, the reality of deploying and managing workloads at the edge is far from simple. The real power of edge arises when we provide application developers the opportunity to seamlessly run the software of their choice at the edge and application operations teams the simplicity of a single delivery plane so they have a reduced operational footprint (even with a larger geographic delivery footprint). Section’s Edge as a Service simplifies all of the steps involved in deploying your application to the edge. You also gain the round the clock support of our dedicated team of expert engineers. We take care of the massive complexities and resources necessary to support distributed provisioning, orchestration, scaling, monitoring and routing, allowing you to focus on innovation."
"243","2019-01-22","2023-03-24","https://www.section.io/blog/edge-compute-use-cases/","Edge computing sets up a new paradigm for running software applications outside the cloud. A report from Gartner in October 2017 revealed that around 10% of enterprise-generated data already came from outside traditional data centers or cloud, and predicted this figure to rise to 75% by 2022. Current Use Cases Some of the industries benefiting from edge compute and driving demand for the future include: **Internet of Things (IoT)** The Internet of Things (IoT) is one of the most significant drivers of edge computing. As the number of connected devices explodes, a vast amount of data is being collected. By placing the majority of the processing within the devices themselves or at the edge in an IoT gateway, as opposed to in the cloud, the reliability of service improves in terms of speed, consistency, and pricing. **Industrial Internet of Things (IIoT)** The Industrial IoT takes this idea to an industrial level. Much of the telemetry that existing industrial devices pick up is ignored or lost over time (according to GE Digital, as much as 97%). By gathering and analyzing this data at the edge, industrial IoT gateways can maximize the efficiency and efficacy of industrial automation. Logistics and Fleet Management Logistics providers are already deploying edge compute (often in combination with the IIoT) for various essential tasks, such as tracking the movement of goods across warehouses and in their yards. Fleet management services in industries such as truck provisioning can already leverage the benefits of edge computing, including lower costs, higher safety, and optimized performance. When a company needs to ingest, aggregate and send data from several different operational data points for processing, an edge compute fleet management solution can help a fleet manager proactively service its vehicles to get the most uptime at the lowest costs via localized computing. Hospitals and Health Infrastructure Hospitals are already using edge computing to collect information and leverage data gathered from wards and operating rooms in near real-time, giving staff more control and flexibility over the data. Similarly to the other use cases outlined, a decentralized approach increasingly makes sense as the volume and velocity of data goes up. Sending all data to and from the cloud or data center for processing doesn’t make sense for those health organizations and hospitals that have embarked on a digital business strategy. By creating a network of smaller datacenters closer to where the data is produced - with specific purposes and features tailored for each use case, clinicians and healthcare staff can benefit from data being processed much faster. Furthermore, as each edge data center handles less information, security is increased because the volume of potentially vulnerable data is reduced, making it more difficult for hackers to infect an entire network. The proximity of edge data also makes it more available offline, which is particularly helpful in a healthcare setting when vital decisions need to be made in real-time. Network Functions Network functions (which comprise routers, switches, and firewalls) typically beget images of large metal boxes. Network Function Virtualization (NFV), however, has turned these closed systems into software that is able to run in a Virtual Machine (VM) or Docker container. Network functions therefore must run at the edge. The necessity for real-time performance with packet forwarding and security means that they can’t be outsourced to the cloud. The combination of edge computing with NFV makes it easier to manage the configuration and lifecycle of the newly virtualized network functions and enables the emergence of flexible edge computing platforms, such as Section’s Edge Compute Platform. eCommerce and Retail eCommerce sites demand low latency and scalability to ensure maximum sales. Seasonality in business is more typical in eCommerce than other industries, such as during a special offer period or holidays, so reliability is an important issue for maintaining service across such periods. By making it easy to scale up or down, edge computing can support eCommerce retailers through peaks and troughs in demand. Meanwhile, retailers can benefit by gathering point of sales data at individual stores and transmitting the data back to central sales and accounting systems. Instead of having to send data to the cloud, it can be sent to more localized edge servers for collection and analytics, enabling faster responsiveness and improving security by minimizing the application attack surface. Gaming Low latency is imperative for online and mobile gaming as in-game experience depends on how quickly data can be collected, processed and acted upon by servers. This is especially true for multiplayer gaming, which due to its bandwidth needs and latency requirements is an ideal fit for edge computing. Multiplayer ping latency can hit single-digit milliseconds by matching games via location and placing game servers close by. The lower the latency between the server and the gaming device (whether a console, PC or smartphone), the lower the lag time and the more satisfying the gaming experience. It’s likely that the gaming community will support a better experience and increased user satisfaction, particularly given the recent rise in competitive gaming. Furthermore, managed edge platforms are enabling the growth of game-streaming services by delivering the low latency requirements that in-game interaction requires (i.e. quick response to keystrokes). Storage Gateways If a storage gateway is placed at the edge, this gateway can serve as a read/write cache, enabling the offering up of files that are cached locally in case the cloud storage service is unavailable. It can also offer the appearance of a quick local storage array even on a very slow uplink. Edge computing can help fix problems related to access to data stored in the cloud (whether speed or availability related). Video Conferencing Poor video quality, delays, and frozen screen shares are not uncommon in video conferencing today. These problems are often due to a slow link within the cloud in which multiple voice and video streams have to be multiplexed together. These kinds of quality of service issues can be resolved by placing the server side of voice and video conference software closer to callers. A set of voice and video servers placed in a more distributed environment worldwide can enable a more reliable, consistent experience for conference participants. Virtual Assistants Voice recognition and virtual assistants such as Amazon’s Alexa are now officially in the mainstream. As with other applications, sending data back to the cloud for voice and/or video content is expensive. Users don’t expect a hit to their data plans yet still demand optimal responsiveness in terms of speed. Edge computing provides a solution to this by enabling the execution of machine learning inference models, like those used in video and voice analysis, to run on the devices themselves or in smaller data centers closer to the end-users. If speech is converted to text at the edge, a megabyte voice recording can easily be converted to only a few bytes of text, significantly reducing the processing cost. Autonomous Vehicles According to Intel, autonomous vehicles, as a result of their hundreds of on-vehicle sensors, will generate 40TB of data for every 8 hours of driving. It is impractical, unsafe and cost-prohibitive to send all that data to the cloud; analysis and decision-making compute must be done in real-time with ultra-low latency to guarantee safety for passengers and the wider public. It is also unnecessary to send the entire set of data to the cloud as it has a very short-lived value; the most important factor is the speed of actuation in response to the data, hence why edge computing is a fit for autonomous vehicles and an important enabler of their innovation and adoption. Content Delivery Content delivery is the original reason behind the rise of CDNs such as Akamai and Limelight. By caching content, end users enjoy improved experiences online. Today, when that content is cached at the edge instead of the cloud, whether it is video, music or a traditional web page, end users benefit from optimized performance. Meanwhile, for developers, edge computing can free up the need to rely on a single CDN provider for all content delivery related services (e.g. caching, WAF, image optimization, etc.) and offer more flexibility to choose services and customize needs for each application or site. Future Use Cases As a result of the advances in edge computing, developers may soon have the ability to build a whole new class of consumer and industrial applications. At the Gartner Symposium/ITxpo in Orlando, Florida in 2018, David Cearley, Vice President and Fellow, told the crowded audience, “The future will be characterized by smart devices delivering increasingly insightful digital services everywhere”. Cearley added, “We call this the intelligent digital mesh”. He uses “intelligent” to signify the increasing amount of AI used in computing, “digital” to describe the growing combination of the digital and physical worlds, and “mesh” to signify the expanding connection points between people, enterprise, devices, content, and services. Smart Cities One of the prime use cases for the intelligent digital mesh is smart cities. The main goal of a smart city is to optimize functions, help propel the city’s economic growth and improve quality of life for its residents through the use of smart technology and data analysis. The world’s population is growing more quickly than ever before. The United Nations recently predicted that 68% of the world’s population will live in urban areas by 2050. According to a 2017 report from the National League of Cities, 66% of cities in the US said they were investing in smart city infrastructure. Using smart city initiatives to help manage a city’s growth could be ever more useful; for example, smart traffic management to monitor and analyze traffic flows can be used to optimize streetlights use or prevent roads from over-congestion based on rush hour schedules. Smart transit companies can better coordinate their services and fulfill the needs of riders in real-time. Ride-sharing can decrease the amount of regular traffic and provide a useful public service in a smart city. Energy conservation and efficiency can be improved via smart devices, such as energy sensors and smart grid technology. Environmental concerns such as air pollution and climate change can also be monitored and addressed. The potential of smart cities the world over and their need for continuous connectivity can be enabled through edge compute and localizing digital infrastructure. In order for smart cities to become a reality, however, major investment in effective edge computing will be a necessity. Applications Not Yet Imagined There are many possibilities for edge applications that remain unimagined. As edge computing becomes more of an adopted reality, there are seemingly boundless opportunities for everything it can and will enable."
"244","2019-01-14","2023-03-24","https://www.section.io/blog/edge-computing-gaming-benefits/","A couple months back, we published an article talking about game developers looking to the edge and the transformative value it can offer, particularly in relation to faster and more reliable gathering and processing of in-game data that the edge supports. This article focuses more on the benefits of edge compute for gamers themselves and draws out a host of examples that gamers (and game creators) can relate to. Despite the mobile games industry being valued at over $50 billion in 2018, downloads and in-app purchases have plateaued in recent years. This is in part due to a lack of innovation and the expectation of seamless and sophisticated gaming scenarios that demand lots of storage (locally and in the cloud) and maximum processing power, which gaming companies have not yet been able to deliver. Current network, storage, and processing limitations have made delivery of this kind of sophistication on a mobile or IoT device for online gaming, virtual reality (VR) and augmented reality (AR) difficult. Edge computing, however, promises better gaming experiences by lowering latency and improving accessibility at a more affordable cost to gamers. When workloads run at the edge of the network (instead of being sent to a few centralized locations for processing), data need only travel the minimum necessary distance, reducing associated lag time and enabling more interactive and immersive in-game experiences. Furthermore, edge computing is paving the way for more subscription-based models that could ultimately put some money back in gamers’ pockets by reducing the need for game and hardware investments. A Better In-Game Experience Improved Multiplayer Experience Edge computing boosts the opportunity to serve multiplayer gaming, which is both latency sensitive and bandwidth intensive. By matching a gamer by its location then placing game servers closer to them, multiplayer latency can reach single-digit milliseconds, which dramatically decreases any lagginess. Hatch, a spin-off from Rovio - the mobile cloud gaming company behind Angry Birds, is a Packet customer (like Section) that benefits from its micro data centers deployed in cities, close to users and its unique business model in which manufacturers and developers can implement specialized hardware at Packet’s edge data centers. This allows Hatch to quickly update and refresh the 90+ games on its monthly subscription platform as the need arises, ensuring its users get superfast access to the latest developments in their mobile games. On Packet’s services, Hatch runs low latency multi-player-gaming streaming services to users with low-end Android devices. According to Zachary Smith, CEO of Packet, “[Hatch] needs fairly specialized ARM servers in all these markets around the world. They have customized configurations of our server offering, and we put it in eight global markets across Europe, and soon it will be 20 or 25 markets. It feels like Amazon to them, but they get to run customized hardware in every market in Europe.” Hatch could do the same thing in the public cloud in theory, however, the costs would make that an inefficient business model. Smith says, “The difference is between putting 100 users per CPU versus putting 10,000 users per CPU”. Smith believes the new model will be of interest to the latest developer generation that will be driving the next set of innovations in software. Enabling Better VR/AR A key advantage to edge compute for VR and AR experiences is the ability to reduce dizziness associated with low latency and slow frame refresh rates. This can lead to a laggy experience that is frustrating, potentially nausea inducing and ultimately disorienting. AR services need an application to analyze the output from a device’s camera and/or a specific location so that a user’s experience when visiting a point of interest can be supplemented. The application needs awareness of a user’s position and the direction they are looking in, provided via the camera view, positioning techniques, or both. Following analysis, the application is then able to offer additional information in real-time to the user. As soon as the user moves, that information needs to be refreshed. Hosting the Augmented Reality service on a Mobile Edge Computing (MEC) platform instead of in the cloud is beneficial because supplementary information relevant to a point of interest is highly localized and frequently irrelevant beyond the particular point of interest. The processing of information from the camera view or user location can also be performed on a MEC server instead of a cloud server to benefit from the lower latency and higher rate of data processing possible at the edge. The huge success of the AR game, Pokemón Go, was largely due to the way it enabled rich user interactions with the real world. Through geotagging and a connection to users’ Google data, the app could collect large amounts of data per user, including location, player movement and Internet connectivity. The game’s worldwide success disarmed Niantic (Pokemón Go’s creators) however, who only had a minimal global presence. Server crashes, hacks that invaded user privacy and various other disruptions were experienced, leading to angry venting by users on the web and a slew of bad publicity. It’s not clear, but likely that the game’s servers were hosted on the Google Cloud Platform, which couldn’t handle the unexpectedly high volume of users. Edge computing, however, is an ideal scenario for these types of games. By moving processing to the edge, closer to the end user, similar apps could offer a superior user experience by reducing latency and service disruptions. Improved Security/Privacy Privacy challenges were another significant issue with the first iteration of Pokemón Go. Reports of hacking grew in number due to the game being able to access critical pieces of user data, including camera, contacts, location and Google account. Edge computing can better overcome this problem as well by keeping processing localized in neighborhood data centers, or on the device itself, rather than sending sensitive data over the network, back to the cloud. Accessibility The Evolution of Cloud Gaming / Subscription Services Cloud gaming looked set to catch on and become the future of video gaming back in 2009 when OnLive, the first cloud game streaming service launched. At the time IGN wrote, “this next generation cloud technology could change videogames forever” leading to time in which “you may never need a high-end PC to play the latest games, or perhaps even ever buy a console again”. The service, which at one time received a valuation of $1.8 billion, closed down for good only six years later (in April 2015), however, unloading its patents to Sony along the way. OnLive was intended to be the simplest iteration of “pick-up-and-play” on the market, with games running on the company’s servers and the video and audio streams compressed for transmission across the Internet to be played in the homes of gamers. The service ran into its first set of challenges in 2012 when it closed after running up $40 million in debt and losing many of its employees. It reopened in 2014, launching as a monthly subscription service, initially for $14.99, a sum which was later reduced to $7.95. The company eventually closed its doors for good the following year as the business was simply unsustainable. However, although the business failed partly because of doubts over its ability to deliver a lag-free experience, latency-free cloud gaming sold via subscription was still a revolutionary idea. The success of other streaming subscription models that work in this vein such as Netflix, Hulu and Spotify demonstrate the potential for such an idea in gaming. Indeed, new subscription services such as Sony Playstation Now and Nvidia are beginning to gather steam in a way that OnLive never did. Sony PlayStation Now offers “an instant, ever-changing collection of hundreds of PlayStation games - ready to download on PS4 or stream on PS4 or PC”. Last year, Nvidia unveiled a beta version in Windows of its new game streaming service, Geforce Now, which similarly to OnLive, offers users access to a library of video games in the cloud in exchange for a monthly subscription fee. A high-end PC is not needed to run the gaming client. Game-streaming services like Sony PlayStation Now and Nvidia are placing a lot of faith in edge computing enabling their success. Latency can quickly destroy a user experience; a video game needs to be able to respond to keystrokes. Any commands issued must travel over the network in each direction to be processed fast enough by the data center for the gamer to feel like the game is responding to each keyboard and mouse stroke in real time. The sole way to ensure that kind of latency is to place the computer and processing power of the gaming data centers as close as possible to the end user. In a recent demonstration of the service at AT&T’s Spark conference in San Francisco, Nvidia showed that the demo game, which had a resolution of 1920 by 1080, had only 16 milliseconds of delay between the laptop and AT&T’s data center in Santa Clara using its edge network. Affordability Reduced Hardware Requirements One of the great benefits of gaming subscription services for gamers is the way in which they reduce the need for regular investments in new systems (e.g. new PC, Sony Playstation, Xbox, etc) and the corresponding need to frequently update those systems and purchase the games and the components required to run them, such as graphics cards, processing, etc. At the AT&T Spark demonstration, Paul Bommarito, vice president of Americas Enterprise Sales for Nvidia, said, “So in the past to get this level of experience, you would need a workstation with a graphics processor costing a few thousand dollars,” said Bommarito. “With GeForce Now and the graphics acceleration taking place in the cloud, you can get that level of beautiful experience on a $200 laptop. I think the best thing is 5G. If you think about that mobility capability of this high-bandwidth, low-latency network, the ability to have this gaming experience anytime, anyplace, anywhere, with GeForce Now on any device, our customers are going to love it.” The Future of Gaming at the Edge Edge compute makes online gaming more commercially viable than cloud compute was able to. As latency is so essential to the success of immersive mobile cloud gaming, as well as to VR and AR, compute frameworks have not been able to match its promise until now. By placing the gathering and processing of large amounts of information at the edge of the network as close to the user as possible, these challenges can start to offer the kind of low latency required to make online gaming an ongoing success. Improved network performance in areas such as delay and packet jitter directly translates to improvements in application performance, including in areas critical to the success of online gaming, such as motion-to-photon latency and frame loss. As Matt Caulfield, self-identified “edge computing and distributed systems enthusiast”, recently wrote in a post on Medium, “The lower the latency between a game console or PC gaming rig and the backend server, the lower the lag. The rise of competitive gaming suggests that the massive gaming community is willing to pay a premium for a better experience.” As a result of a subscription-based edge streaming model, gamers will no longer need to regularly purchase updated hardware or software, and instead, subscribe to an edge-hosted gaming platform they can access from existing devices. Users will be able to connect to the continually evolving library of games, connecting to it remotely while the edge hardware is kept up to date elsewhere. Perhaps edge computing, with its promise of dramatically low latencies, will reignite the streaming model in games once more. At a panel discussion at AT&T’s Spark conference, Microsoft Azure’s Royeka Jones described edge compute as “the enabler that will allow infinite possibilities around what we can do with technologies.”"
"245","2018-11-07","2023-03-24","https://www.section.io/blog/game-development-at-the-edge/","As we look ahead to the future of edge computing, there are many use cases that are driving demand. However, one area that we are especially interested in is online gaming, where we believe that our platform has the opportunity to provide transformative value for game developers. Due to the increasing computational requirements of esports and other online games, video game developers are launching larger and larger cloud infrastructures purely devoted to collecting in-game player data. Game performance depends on how quickly this data can be fetched, processed, and acted upon by their servers. As in-game data gets more complex, this process gets slower and latency increases. Recently, the game industry has started to explore edge computing as a solution to this problem. Where traditional architectures send enormous amounts of data to a few centralized locations for processing, distributed architectures are able to process more data at the edge and only send aggregated or anomalous data back to those few centralized locations. When workloads run at the edge, data travels the minimum possible distance and players have a latency-free experience. By running data workloads at the edge, gamers get a better in-game experience, and the company managing the game is able to significantly reduce costs associated with running the centralized infrastructure. Mobile Gaming Is Driving the Need for Change Video games are increasingly going mobile. According to SuperData Research, over $59 billion was spent on mobile games last year in comparison to $8.3 billion on traditional gaming consoles. Demand for mobile gaming that is immersive and interactive is starting to push the physical limitations of today’s slim designs and the networks and infrastructure of traditional cloud computing. Edge computing is helping to solve this problem, fueled in telecom by the move towards 5G. Edge computing could make it possible to hold a high-end console in your hand, reducing the battery strain on mobile devices while still offering a high-end user experience. Not to mention, access to a 5G connection through geographic proximity will lead to lower latency for all games and an overall improved mobile gaming experience. Earlier this year, AT&T ran a demo at the E3 gaming convention in LA, showing what 5G and edge computing could mean for gamers. Using three fixed base stations within the conference venue, along with up to 3 user equipment (UE) units placed in close proximity to the base station receivers, the 28 GHz demo showed how 5G and edge computing are enabling immersive experiences outside the home. Low Latency Is Critical for the Gamer Experience Online games involve continual interactivity and near-immediate feedback, which requires low latency. The game has to respond to the gamer’s commands, which must travel back-and-forth over the network to be processed by datacenters fast enough so the gamer feels the game is responding to each keystroke and mouse movement in real time. This sense of real-time immersion is what makes a game engaging. Thus the big challenge of streaming a game from the cloud is latency. Even a few milliseconds delay in how quickly a game responds to your mouse click results in frustration for the gamer. If the data center is located too far away or network connections are weak, the user experience will suffer and consequently use may drop off. This is why edge computing is being touted as the future for gaming. The easiest way to ensure latency is at its lowest is to place the data processing as close as possible to the gamer. Example: Pokémon Go As games move towards augmented reality and virtual reality (AR/VR) mobile gaming, the need for edge computing also increases. Pokémon Go was the first viral AR hit. The game, developed by Niantic for iOS and Android, lets players capture, battle and train virtual Pokémon that appear in the real world. Pokemon Go is mobile exclusive because it relies on each user’s real-time GPS and clock data to bring the user into contact with virtual Pokémon. It’s likely that as Pokémon Go develops, it will need to increase in complexity in order to keep users engaged. Features such as real-time interactions between players will require even lower latency. Simultaneously, it is highly probable given Pokémon’s popularity, other developers will follow suit, bringing more mobile, location-based AR games to market. As these games become more popular and offer greater interactivity and sophistication, latency and edge compute will be crucial."
"246","2020-09-16","2023-03-24","https://www.section.io/blog/edge-computing-build-vs-buy/","At Section, we are increasingly being approached by enterprises whose bespoke edge needs have them asking themselves whether to build vs. buy when it comes to edge compute capabilities. These enterprises are weighing up: If they can achieve the customization they need to suit their unique requirements; The elapsed time and product and engineering effort it will take to build out Edge due to its complex and diversified nature; The diverse infrastructure relationships they will need to maintain; The skills they will need to hire and maintain to build and operate the networking, compute and orchestration technologies; and Whether it makes sense to pursue a build strategy in an area which is most often outside the core business. Types of Edge Use Cases In a recent report, MarketsandMarkets projected the entire edge computing market will grow from $3.6B in 2020 to $15.7B by 2025 at a CAGR of 34.1%. Drivers of this include the growing adoption of IoT across industries, increasing demand for low-latency processing and real-time automated decision-making solutions, and an urgent requirement for surmounting hugely increasing data volumes and network traffic. As edge computing is increasingly prioritized as a key strategic initiative, the types of enterprises we are seeing approach Section regarding their custom edge needs include: Software providers who are losing business because they don’t have a cloud offering and need to SaaS-ify their product offering (e.g. WAF providers with only on-prem solutions). Hosting providers looking to expand their footprint and capabilities with edge app hosting. PaaS providers looking to build or augment their solutions with edge services. Enterprise engineering teams who don’t have, or don’t want to invest in, the resources required to build and operate advanced edge provisioning, orchestration, scaling, routing, and monitoring. Edge Computing “Build” Pioneers Some of the pioneers of edge computing, such as eBay, Netflix and Chick-fil-A have made serious investments in edge engineering teams and technology to meet their data, latency and availability needs. Let’s look briefly at the investments in edge technologies that each of these edge pioneers have made. eBay Back in 2018, eBay announced a three year plan to replatform and modernize its backend infrastructure, including decentralizing its cluster of data centers and moving to an edge computing architecture. eBay’s primary servers were originally located in Phoenix, the Salt Lake City environs, and across Nevada. The company is now in the midst of moving these online services and data closer to users distributed across the US. The goal: “to create a faster, more consistent user experience, saving 600-800 milliseconds of load time.” eBay simultaneously announced a build of its own custom-designed servers. Overhauling the physical eBay infrastructure also necessitated a redesign and rebuild of the entire logic and software stack that runs on the infrastructure. No small task. This is a long-term move that has required significant investment from the company. Netflix Netflix has spent years building infrastructure for content delivery on a global scale, including a significant core deployment on AWS and a CDN using thousands of Open Connect caching servers distributed across data centers and ISPs in over 200 sites around the world. The streaming giant is still in the midst of its edge computing transformation. Recently, its Vice President of Networks, Dave Temkin, said the streaming giant was looking at edge computing for its production side of the house to alter how content is shared during series production. In TV and film production, edge computing can perform “data thinning”, i.e. transcoding, to compress large datasets down to smaller files, which can be sent across the network for review by creatives on the other side of the world. Chick-fil-A Back in 2018, Chick-fil-A announced it was in the middle of an extensive edge computing strategy, setting out to run a Docker-based edge compute infrastructure on 6,000 devices in all 2,000 of its restaurants around the US. The problem they were facing was the fact that many of their restaurants were doing over three times the volume of foot traffic than they were originally designed for. One of the ways Chick-fil-A set out to solve this was by “invest[ing] in a smarter restaurant.” The goal: to improve customer service and operational efficiency through a data collection and analytics strategy rooted in “making smarter kitchen equipment” and developing an IoT platform for Chick-fil-A’s restaurants. As these examples highlight, some of the arguments for building your own edge infrastructure might include: Greater predictability; More control and flexibility; and Potential development and operational efficiencies. Build vs. Buy However, some would make the argument, in the case of a business like Chick-fil-A, why would a fast food restaurant choose to become a technology company? The time and investment involved in building and running your own edge infrastructure, including orchestrating resources, managing scaling, routing, and monitoring, is immense. Replatforming is not for the faint-hearted and many SMBs and large enterprises have still not migrated to the cloud, let alone been able to access the latency and availability benefits of edge computing. At the time of the Chick-fil-A edge deployment, there weren’t many (if any) options to shop the required solutions out, so they were forced to build. There is also often the misconception that building automatically gives you the best solution, and buying means you’re not innovative. Now, however, edge platforms like Section make it possible to build on top of proven edge solutions so companies that want to innovate can do so while focusing on their core business, thus accelerating their path to the Edge. Section’s Adaptive Edge Engine (patent-pending) uses machine learning to intelligently optimize edge workload placement based on real-time traffic demands. Our Adaptive Edge Engine abstracts the underpinning decision-making, execution and monitoring to offer developers a reliable, turnkey solution, which optimizes application performance. This will lead to more efficiencies, which will have an impact not just on budgets, but also promote greater sustainability. Working with Section to Build Your Edge Application, SaaS or PaaS Internet users are demanding more secure and faster digital experiences. However, SaaS, PaaS and application providers are frustrated with the amount of time and technical capability required to move their applications closer to their end users, which can deliver those better digital experiences. Section is an Edge Compute platform which enables any provider to quickly build their own Edge Solution without needing to worry about the massive complexities (and resources involved) of distributed provisioning, orchestration, scaling, monitoring and routing. With Section you can deploy an Edge application in minutes or an Edge PaaS or SaaS in hours. You will also gain the 24x7x365 support of our dedicated team of engineers. Sign up online or talk to a solutions engineer to get started with Section."
"247","2021-08-02","2023-03-24","https://www.section.io/blog/deborah-diaz-joins-section-board-of-directors/","Section is delighted to announce the appointment of prominent technology executive Deborah Diaz to its Board of Directors. Deborah’s 30+ years of large-scale business transformation in senior leadership roles across prestigious public and private sector organizations will enable her to provide valuable insights to help drive Section’s next phase of growth. “We are honored to welcome Deborah to Section’s Board of Directors” said Stewart McGrath, Co-Founder & CEO of Section. “The emerging Edge ecosystem is still early in its maturity, and Deborah’s commercialization experience in steering technology innovation and business growth as both an IT leader and board director makes her a tremendous value-add for our organization, customers, and industry as a whole.” Deborah is currently CEO of Catalyst ADV, a technology and strategic growth advisory firm, and has extensive board director and advisory board experience with Primis (Nasdaq: FRST), Intel, Equinix, Forcepoint, Dell, BRMI, and other private companies. Previously, she served as NASA’s Chief Technology Officer and Deputy Chief Information Officer where she managed global system infrastructure, risk management, financial and regulatory stewardship, data protection, and technology innovation. She also served as CIO for Science and Technology at the U.S. Department of Homeland Security and was the Deputy CIO for the USPTO. “I am excited to be joining Section’s Board of Directors working directly with their leadership team to advance their competitive Edge technology and business growth” said Diaz. “ Section is poised to take advantage of new opportunities to extend and change hybrid cloud environments for enhanced scalability and security. I look forward to enhancing their innovative vision and accomplishments for the future.” -Deborah Diaz, CEO of Catalyst ADV This appointment follows the recent announcement of Paul Savill joining Section’s Board of Directors. As Section’s fifth member, Deborah builds on the diversity of experience, perspective, and leadership represented among Section’s Board and will serve as a key advisor as we continue to build innovative edge technologies which fulfill our mission to improve the Internet."
"248","2022-08-23","2023-03-24","https://www.section.io/blog/choosing-hosting-locations/","You’re ready to deploy your application to the cloud, and now you need to make a big decision: where will you host it? By where, we don’t mean “which provider” – we literally mean where will it be deployed geographically? Have you thought through the hosting region and its ramifications? Because this is a BIG decision that will strongly impact your application and user experience. The cloud is not – despite many misnomers – everywhere, all at once. If you’re using one of the hyperscalers (AWS, Azure, GCP, etc.), you’ll need to choose one or more hosting regions. We’ve written before about the challenges and impact of selecting that region, and the enormous complexity if you try to choose more than one region (in fact, this complexity is the reason Google recommends choosing single locations). To make the decision you’ll need to think through things like latency, pricing, machine type availability, resource quotas and other critical considerations. Now let’s discuss why you’re about to get that decision wrong. Incomplete Data First, you’re not clairvoyant and this is a decision that’s inevitably made with incomplete data. Let’s take latency as an example. Do you know where your users are located? We assume you have market projections, but do you actually know the location and volume distribution? Almost certainly, the answer is “no,” so organizations do the next best thing: they make an educated guess and pick a particular cloud instance for deployment. This decision means that users within that region (such as AWS EC2 US-East) enjoy a premium experience, while the experience for everyone outside that region degrades – and the farther they are geographically from the selected region, the worse the experience. Do you know how bad that experience is, or are you waiting for complaints? Has it cost you any customers? How many? Shifting User Patterns Moreover, consider that access patterns shift over time, even on a daily basis. If your primary audience is business users, for example, it’s likely they’ll begin to log off as the workday ends. Going back to our earlier East Coast example, come 5 p.m. Eastern time you’re now hosting an application in a region where you statistically have the fewest users. These shifts also happen over longer periods as a user base evolves as grows. For an application that’s gaining increasing popularity in Asia, the U.S. East Coast no longer looks like the smart deployment decision. How do you know when to change your hosting location? Even worse, how do you know that your current decision isn’t hampering growth in other regions? You’re back at square one: making decisions with incomplete data. Environmental Changes Another consideration: the world will also be changing around you, both moment-to-moment and in jarring leaps. For instance, it’s inevitable that there will be another major cloud outage… will it take down the region you’ve selected? Or consider compliance – how quickly can you shift your hosting strategy to accommodate changes driven by GDPR or CCPA? In other words, there are numerous reasons that any hosting decision you make will be the wrong one. Changing that location is going to cost a lot of effort and money to fix, and that next location will also be wrong – for the exact same reasons. The harsh reality is that in a global and dynamic computing environment, it’s literally impossible for a single geographical instance to be optimal always and everywhere. Your best hope is that it’s good enough. Of course, that begs the question: why settle for anything less than optimal? At Section, our belief is that modern application hosting should allow workloads to traverse the world’s available compute to run securely at the right place and time. We’ve designed our cloud-native hosting system to eliminate the need for developers to choose and manage hosting locations manually and arbitrarily, and rather specify hosting requirements based on application intent (reliability, performance, compliance, cost, etc.). This intent is communicated through policy-based rules used by Section’s automated orchestration system – the Adaptive Edge Engine – to dynamically and optimally determine workload distribution. Best of all, this global distribution can actually be more cost effective than using a hyperscaler. It’s time to make the right hosting decision. Arrange a demo of Section today."
"249","2022-08-15","2023-03-24","https://www.section.io/blog/running-containers-at-the-edge-devops-paradox/","What is the edge? It’s a complex question to answer, and the answer often depends on who you ask. As the podcast hosts note, the term “edge” is ambiguous and hard to grasp. If you talk to someone at a hyperscale data center, for example, they may define the edge as the network perimeter of the region you’re operating in. Put another way, the edge is the limitation of their network boundary. But for many workloads, choosing a single hyperscale facility is a tradeoff between performance and ease of management. Imagine this all too common scenario: a company has developed a low-latency application and deployed it to a single hyperscale region, only to find that they were never able to get the performance they expected or needed. So, the logical next step is to rearchitect that application so it can run in multiple data centers. Unfortunately, even after rearchitecting the app and running it in multiple hyperscale locations, this company still has not solved its performance challenges. Our co-founder and CTO Dan Bartholomew recently joined the DevOps Paradox podcast to talk about this frustrating scenario and how Section is helping companies run containers at the edge. In this episode, Dan discusses how Section thinks about the edge as a compute continuum. He explains how Section views the edge as a large network of location that can run your application, where a hosting system is responsible for the placement of the application. In an ideal world, this means that under certain conditions it might be best to run your application in a single hyperscale facility. In other circumstances, however, it might be most appropriate to have your application run in multiple hyperscale regions. Or, there may even be other times when your application should be running in tens or even hundreds of locations simultaneously. But accomplishing this degree of flexible, distributed application hosting is extremely difficult. As Dan explains, Section considers all potential compute locations – from hyperscalers all the way down to the ISP – to determine where your applications should run. This is how we answer the question, “what is the edge?” It’s our view that distributed compute offers the potential for broad benefits in terms of not only improved performance and ease of management but also increased availability and resilience, better scalability, decreased cost, and more. Listen to the entirety of Dan’s conversation with DevOps Paradox hosts Darin Pope and Viktor Farcic."
"250","2022-07-05","2023-03-24","https://www.section.io/blog/building-distributed-k8s-orchestration-with-hyperscaler/","At Section, we’ve been hard at work providing a Cloud-Native Hosting system that continuously optimizes the orchestration of secure and reliable global infrastructure for application delivery. If you’re a customer, you know Section’s sophisticated, distributed clusterless platform intelligently and adaptively manages workloads around performance, reliability, compliance, or other developer intent to ensure applications run at the right place and time. In short, we make it easy to optimally run modern apps. What would it take to roll your own version of the Section platform using off-the-shelf hyperscaler products from AWS, Azure, or GCP? And how close could you get to what Section offers? Let’s run a little thought experiment using AWS as an example. To replicate the adaptive global Section platform, you’d first use AWS CloudFormation’s multi-region capabilities to simultaneously deploy identical Kubernetes clusters to all the AWS regions you select. Then, using AWS Global Accelerator it would be possible to dynamically route users via anycast to the Kubernetes cluster in the lowest-latency region for each user. The standard Kubernetes Horizontal Cluster Autoscaler and Horizontal Pod Autoscaler would allow management of your Kubernetes clusters so each region is only running the minimum infrastructure to service the active workload in the region. How is this like Section? This scenario would make it possible to deploy and manage apps using standard Kubernetes tools like kubectl, replicating what Section provides with our KEI Kubernetes Edge Interface. It would also allow deployment of applications in regions that are appropriate to your particular application (mirroring a key aspect of Section’s value proposition) – but only as long as those locations correspond to AWS regions. And it would ensure you only pay for the increased infrastructure in regions with an active workload (again, like Section). Okay, so the big question: what are you missing taking this approach versus using the Section platform? First and foremost, this approach requires you to be responsible for monitoring and operating all global Kubernetes Nodes. In contrast, Section’s Adaptive Edge Engine intelligently and continuously tunes and reconfigures your distributed delivery network to ensure your workloads are running the optimal compute for your application. In other words, we do this for you. Second, this is a single-cloud deployment, limiting reach and resilience. Your region selection is limited to those provided by AWS, and when there is an inevitable outage your application will go down. Section, on the other hand, uses a federated multi-cloud on-demand network – we call it our Composable Edge Cloud – that distributes Kubernetes clusters across a vendor-agnostic selection of leading infrastructure providers (including AWS, Azure and GCP). This both extends the geographic locations your application can run, and means that even when there’s an outage with one provider your application workloads will dynamically adjust to run on another vendor’s network. You will also pay for the minimum AWS infrastructure in each region, even when there is no traffic. With Spot Instance pricing this could be small enough to be negligible, and it’s possible using KEDA that you may even be able to scale-to-zero. However, this spend management and optimization is all on you, and be prepared to pay regardless of traffic if you don’t invest the necessary administration time. Section charges only for active workloads, and can dynamically spin workloads up and down based on policy parameters (for example: run containers where there are at least 20 HTTP requests per second) so you won’t pay at all when there’s no traffic. Finally, you will be responsible for duplicating and maintaining clusters in each AWS region, and there won’t be any centralized management console to provide the necessary visibility into status and performance across all clusters and regions. At Section, the distributed network effectively becomes a cluster of Kubernetes clusters (i.e, clusterless) as our AEE automation and Composable Edge Cloud handles the global orchestration. Meanwhile, the Section Traffic Monitor and Section Console provide a single view of status, usage, traffic flow, and more. So where does this leave us? Overall, it’s possible to get many but not all of the same benefits you get with Section using a hyperscaler. However, with this sort of self-managed approach, it’s incumbent upon you to correctly configure and then continue to orchestrate the distributed network. Or, as we like to say, you can just use Section. If you aren’t already a Section customer, get started with a platform demo today."
"251","2022-02-04","2023-03-24","https://www.section.io/blog/edge-developers-lead-kubernetes-adoption/","A survey released by SlashData™ for the Cloud Native Computing Foundation (CNCF) recently looked at “The State of Cloud Native Development” based on feedback from more than 19,000 developers. The report found that there are now nearly 7 million cloud native developers around the world, with larger enterprises and more experienced developers in North America and Western Europe driving the rapid adoption of these technologies. The CNCF’s first project, Kubernetes, is now the most widely used container orchestration platform, and the study reports that nearly one-third of all backend developers (31%) are using Kubernetes. In a recent Toolbox article, Section’s Vice President of Product, Walt Noffsinger, noted that Kubernetes is playing an increasingly critical role in edge computing. In looking ahead to 2022, Walt anticipates the following: Hosting and edge platforms built to support Kubernetes will have a competitive advantage in being able to flexibly support modern DevOps teams’ requirements. Edge platform providers who can ease integration with Kubernetes-aware environments will attract attention from the growing cloud-native community. - Walt Noffsinger, VP of Product, Section We expect 2022 will be another big year for the edge, and the findings of this latest survey indicate the trend is accelerating. According to the data, adoption of Kubernetes among edge developers has increased by 11% in the past 12 months, which is nearly three times the growth rate among backend developers overall. The report’s findings highlight several other interesting edge computing developments from those participating in the most recent survey. For instance: Edge application developers have the highest usage and awareness of containers (76%) and Kubernetes (63%) when compared with developers in other areas of focus, such as quantum computing, 5G, blockchain and cryptocurrencies, computer vision, robotics, etc. While container usage among edge developers is basically unchanged over the past year, Kubernetes usage has increased by 10 percentage points to 63%. While nearly half of all edge developers (48%) use serverless architecture compared with just one-third of all backend developers, it’s seen a recent drop in usage since peaking at 56%. Edge developers show a strong preference for private clouds (64%), public clouds (63%) and on-premise servers (59%), compared with multi-cloud (44%) and hybrid cloud (42%) environments. Complex Cloud Infrastructure Still a Hurdle As a whole, the report points to the fact that edge application developers “gravitate towards a more secure and less complex infrastructure.” This is not surprising, especially when it comes to the challenges associated with running a multi-cloud or hybrid cloud IT infrastructure. While a multi-cloud or hybrid-cloud approach offers a tremendous amount of flexibility, it’s extremely difficult to orchestrate workloads across hundreds or even thousands of endpoints. We discussed several of these challenges in a blog post on “The Move Towards a Multi-Cloud and Hybrid IT Infrastructure” last year. As Walt noted in his predictions for edge computing in 2022, “as [Kubernetes and containerized applications] usage increases, so too will organizational expectations. Companies will demand more from edge platform providers in terms of support to help ease deployment and ongoing operations.” These edge platform offerings must make the experience of programming at the edge as familiar as possible to developers, while also being flexible enough to adapt to different architectures, frameworks and programming languages. Section’s EaaS: An Easy Button to Solve for These Challenges Section’s Edge as a Service (EaaS) can solve for these challenges among teams that don’t have the resources or expertise to build and manage such complex systems. What’s more, Section’s EaaS platform helps organizations overcome security concerns by extending the network layer protections offered by industry-leading public cloud providers while also offering a wide array of additional security solutions. Get in touch if you’d like to learn more about how Section can give your customers all the benefits of a distributed deployment model without the headache of building and managing multi-cloud and hybrid systems."
"252","2022-03-08","2023-03-24","https://www.section.io/blog/aspects-of-edge-compute-to-reduce-edge-complexity/","As organizations look to capitalize on the benefits of edge computing, many are quickly realizing the complexities associated with building and operating distributed systems – including sourcing distributed compute, resource management/placement/sizing/scaling, distributed traffic routing, managing many locations and non-homogenous providers – leading them to seek out solutions to help solve the edge puzzle. It’s important to understand these complexities, which is why we’ve discussed them in great detail in our previous blog posts. But as with any endeavor, when faced with adversity or confronted by hurdles, keeping an eye on the prize can ultimately help you forge ahead. Every organization, team and application has varying and distinct requirements when it comes to designing distributed systems. Given that, it’s not uncommon for teams that start down the path of building their own bespoke systems to quickly find themselves overwhelmed by all the complexities that play into design decisions and implementation. As an Edge as a Service (EaaS) provider, we’re often pulled into projects during the early stages of research and discovery, where we’re able to offload the build and management of many, if not most, of the critical components. As such, we know first-hand that accelerating the path to edge for organizations – across a diverse range of use cases – begins with reducing complexity from the outset. Moreover, providing application developers with the opportunity to seamlessly distribute and run the software of their choice is where the real power of the edge comes into play. To that end, let’s shine a spotlight on a few critical areas where complexity can creep into your edge deployment scenario, and how using an “as a service” strategy can combat that to reduce edge complexity. Deployment Flexibility (How and Where) Organizations moving to the edge should expect greater flexibility in both how and where they deploy to edge resources compared to using legacy CDNs or centralized cloud environments for edge. The two areas to consider here are network provisioning and deployment pipelines – in other words, how are your getting your application workload to the edge and where is the edge? Both represent areas where unintended complexity can stymie edge outcomes. The Where If edge compute is appropriately provisioned and distributed across a heterogenous network of providers, not only do you get increased flexibility, it also ensures increased resiliency. The Section Composable Edge Cloud, for instance, is built on the foundations of providers such as AWS, Azure, Digital Ocean, Equinix, GCP, Lumen, and RackCorp; we are adding new edge location providers and can deploy points of presence (PoPs) on-demand to help customers define their own edge. Yet managing such a federated multi-cloud and edge network can increase complexity exponentially. This is where our Adaptive Edge Engine again saves the day by using a sophisticated decision and execution engine to automate distributed infrastructure provisioning, workload orchestration, scaling, monitoring, and traffic routing. The How Deployment pipelines also need to be flexible to keep pace with technology changes and application workload demands. Engineers must be able to deploy fast, easily, and in a safe, reliable and repeatable way. DevOps and continuous delivery (CD) help to support a more responsive and flexible organization that can better respond to changing requirements, and ensure quicker time-to-market across the software delivery cycle. That’s why Section leverages your existing Kubernetes managed application structure – no need to rewrite your application to get it to the edge. What’s more, you’re not locked into specific application providers – feel free to use any containerized application from any registry (be it open-source or private). Management Control Once your application is at the edge, a modern EaaS provider should enable greater granular control over your tech stack and edge computing requirements. That control allows for better tailoring of application workloads, allowing you to focus on customer or team needs and leverage EaaS for deployment, rather than rearchitecting or retooling to fit the requirements of a cloud or CDN platform. EaaS solutions like Section’s Edge Platform provide the necessary granular, code-level control needed to seamlessly integrate edge solutions into existing stacks and workflows. Code configuration and management in particular are areas where complexity can impact the developer experience. Beyond the granular, code-level control over edge configuration mentioned above, there are also considerations in terms of location strategy, security and compliance, application development lifecycle, and observability. At Section, our approach is to combine that fine-grained control when and where you want it, with powerful automation and underlying AI to hide and abstract that complexity where you don’t. For example: The Kubernetes Edge Interface (KEI) provides a K8s consistent interface to deploy and manage workload on the Section Global Network. Working with KEI is analogous to working with K8s to deploy to a single K8s cluster but will actually deploy to Section’s “Global Cluster of Clusters” on the Global Network pursuant to deploy Policy you prescribe via the KEI. Adaptive Edge Engine delivers superior performance/cost efficiencies, including optimal cost to latency – so you don’t end up overpaying for underutilized (“always on”) resources or under serving customers. It also enables direct control of max cost – to liberate the fear of paying more than budget constraints in any given month by keeping costs in check. Performance-wise, Adaptive Edge Engine also allows you to specify the network shape that suits your users – eliminating constraints related to compliance or specific providers. Abstracting Edge Complexity Section was founded with a mission to ensure edge simplicity and has thus built a very different type of network to legacy CDNs and cloud providers. That starts with offering an OpEx model for edge compute, allowing us to use flexible strategies and workflows to best meet the needs of each individual customer, maximizing performance and cost savings. Then we simplify network provisioning and deployment, so you can get to the edge fast, without having to change your development cycles, process or tooling. And without having to worry about managing and adapting to a range of different operators. Then we give you the granular control – not to mention support – needed to optimize your application workloads at the edge, and automation that ensures you don’t have to dive deep into the edge to improve overall customer experience. The Section Edge Platform offers organizations the ability to benefit from the expertise and resources of an EaaS provider who can provide turnkey solutions for customizing and managing these complex systems. By abstracting the complexities of edge computing, organizations can focus on delivering better applications, not operating distributed networks. Let us help you realize these edge outcomes quickly and easily so you can enjoy all the benefits of a dynamic, customized Edge – at the same cost as cloud – without sacrificing DevOps simplicity, control, or flexibility. It’s that simple. :)"
"253","2022-02-16","2023-03-24","https://www.section.io/blog/simplifying-managed-multicloud-edge-kubernetes-with-helm/","One of the central observations we made in our recent blog post about Edge Computing Challenges for Developers was the need for familiar tooling, noting that if developers are forced to adopt entirely different tools and processes for distributed edge deployment, it creates a significant barrier. Efficient edge deployment requires that the overall process – including tooling – is the same as, or similar to, cloud or centralized on-premises deployments. The exponential growth in Kubernetes adoption means it’s arguable that Kubernetes skills are the most in-demand skill set for IT professionals today. However, soaring adoption of Kubernetes has also led to the emergence of more and more fully-managed Kubernetes solutions from hyperscalers, edge computing platforms, and managed service providers. The maturation of these managed solutions has abstracted much of the low-level operations work, such that the skills required to effectively run Kubernetes clusters in a sense equate to expertise with specific tooling such as Helm, Prometheus Operators, CI/CD tools, etc. In fact, as Kubernetes adoption continues to grow, Kubernetes-native tooling is becoming a fundamental requirement for application developers. Kubernetes native technologies generally work with Kubernetes’s CLI, can be installed on the cluster with the popular Kubernetes package manager, Helm, and can be seamlessly integrated with Kubernetes features such as RBAC, Service accounts, Audit logs, etc. Image source: CloudARK In that sense, while a solid understanding of Kubernetes is still very important, a focus on containerization itself (i.e., the Dev side of DevOps) and the tooling necessary to achieve this is perhaps the more critical skillset. Being able to build and integrate microservices into DevOps life cycles, and incorporate critical functionality like automatic feature rollouts with zero downtime and container health checks, is a skill that has significant versatility across organizations who are using Kubernetes in production today. Another one of our recent posts touches on the challenge that the use of this familiar tooling, like CI/CD solutions (e.g., Jenkins, CircleCI), API-backed CLIs (e.g., kubectl), package managers (e.g., Helm, Kustomize), etc., poses when it comes to the edge. Simply put, what’s achievable with a single cloud instance becomes significantly more complex at the edge, where you can be running hundreds of clusters, with different microservices being served from different edge locations at different times. How do you decide on which edge endpoints your code should be running? How do you manage across heterogeneous provider networks? Helm Chart Enablement Let’s take the example of Helm Charts, which are an excellent tool for automating Kubernetes deployments. Yet even if you’re using Helm, the level of complexity quickly escalates when deploying applications across multiple clusters, regions, providers and environments. At Section, our overriding goal is to eliminate edge complexity. We believe all you should have to do for edge deployment is simply hand over your application manifest, make some strategic settings choices, and then leave it to Section to run your applications performantly, reliably, securely, and efficiently. Section’s new Kubernetes Edge Interface (KEI) ingests your existing Helm Charts and intelligently distributes and scales your containerized applications across a Composable Edge Cloud, providing: Multi-cluster support across a flexible Composable Edge Cloud Intelligent workload placement and traffic routing Kubernetes-native tooling Ultimately, familiar workflows help you move faster – especially when they integrate with your current processes. To learn more about our out-of-the-box Helm Chart support, and see just how easily it allows you to configure and manage your edge deployments with Kubernetes-native tooling, request a demo."
"254","2022-10-20","2023-03-24","https://www.section.io/blog/kubernetes-award/","We’re thrilled to share that Section has earned Frost and Sullivan’s 2022 New Product Innovation Award for the North American Kubernetes industry. It’s an honor to be recognized for excelling in Frost & Sullivan’s rigorous evaluation criteria used to determine its Best Practices award recipients. Section developed the Kubernetes Edge Interface (KEI) as a software-as-a-service (SaaS) solution that, coupled with our Adaptive Edge Engine (AEE) and Composable Edge Cloud(CEC), enables customers to easily deploy workloads to a dynamically optimized and massively distributed Kubernetes based hosting platform. Due to the development of KEI by Section, organizations can deploy application workloads across a distributed edge as though it were a single cluster. Section lets development teams already building Kubernetes applications continue using familiar tools and workflows, such as kubectl or Helm, yet deploy their application to a superior multi-cloud, multi-region and multi-provider network. The reduced burden on development teams to manage Kubernetes and try to choose the optimal deployment footprint frees them up to focus on improving their applications instead. In a special Best Practices recognition report, Frost & Sullivan analysts detail the research behind Section’s 2022 New Product Innovation Award and why we were selected. Here’s an excerpt from their findings: Frost & Sullivan has tracked the prevalence of containers, which have become ubiquitous in data networks, and the growth of Kubernetes to better manage those containers. Most customers are wary of running Kubernetes - but still want the benefits that a well-orchestrated management system provides. This is leading enterprises to seek ways to deploy Kubernetes solutions that they can customize, repeatedly if desired, to improve container management. This is the hallmark of the Section solution, which automates the management of containers using easily adjusted parameters to meet the customer’s specific needs and constraints. This, along with the high level of customization and automation of manual tasks provided by the Section KEI solution, earns Section the 2022 Frost & Sullivan New Product Innovation Award in the North American Kubernetes industry. Frost & Sullivan’s Best Practices awards recognize companies in a variety of regional and global markets for demonstrating outstanding achievement and superior performance in leadership, technological innovation, customer service, and strategic product development. Industry analysts compare market participants and measure performance through in-depth interviews, analysis, and extensive secondary research to identify best practices in the industry. Check out Frost & Sullivan’s complete write-up on Section’s recognition ."
"255","2022-12-14","2023-03-24","https://www.section.io/blog/lock-in/","Over the past decade and a half, consumers have increasingly encountered providers who have created a walled garden when it comes to their product. A number of items you use on a daily basis fit this description, be it your phone and the hardware needed to accomplish a simple task like charging, or perhaps you’re a cyclist who just purchased a new mountain bike. If you wish to upgrade parts, good luck finding something that will work if it’s made by a different brand. Cloud and Edge compute companies are no different. More often than not, a provider for web hosting services, container orchestration, or even those who proclaim to be multi-cloud, will require that you reconfigure or rewrite your app so that it will function properly with their platform. Once you do this and deploy, it becomes increasingly difficult, expensive, and sometimes impossible to move away regardless of performance. In the immortal words of one of the great Star Wars characters, General Ackbar, “It’s a trap.” You probably know it as “lock-in.” I’ve been fortunate to work with a number of entrepreneurs over the course of my career. At least some of them might admire the “trap” to which General Ackbar refers when viewed through a capitalist lens. If you create a walled garden and capture those who enter, you can spend less on customer happiness while investing more in raising the walls. But something about that doesn’t feel right. While I am an entrepreneur at heart, and a capitalist, I’ve always believed there is a better way to build a company around what serves customers’ needs, rather than just the bottom line. Imagine my enthusiasm when I first began working with Section and learned of our founders’ vision. CEO, Stewart McGrath and CTO, Daniel Bartholomew, would not be well-fitted for The Empire. Their vision of Edge-Compute and Multi-Cloud is centered around a garden without walls, where customers can leverage any number of different cloud and infrastructure providers without the hassle and shackles associated with “lock-in.” Simply put, they’re part of a different cinematic universe. They’re much more like Tron: “I fight for the users.” Dan and Stew built Section without trying to “trap” anyone. Since the beginning, their focus has been to create a platform that serves devs and engineers in a way that allows them to build whatever they choose. Does this make things more difficult for Section? Not necessarily. It simply means we’re different. We hold ourselves to the highest standard when it comes to our customers’ experience. Be it the platform itself, customer support, or our focus on ensuring that users achieve whatever they envision, we’ve simply invested more. Why? Because we are devs and engineers. We know what this community needs and deserves. Welcoming everyone into a garden without walls may not be the easiest route, but if it means tech creators have the ability to build what they’ve envisioned without constraint, the time and effort is beyond worth it. Give Section a try for free, and see what you can create."
"256","2022-12-12","2023-03-24","https://www.section.io/blog/2023-predictions-kubernetes/","With the end of the year upon us, Section co-founder and CEO Stewart McGrath recently offered his perspective on four trends to watch in 2023 when it comes to Kubernetes, container orchestration and the edge computing landscape. Recently published by Spiceworks, here is a summary of Stew’s predictions for the coming year: The rise of Kubernetes as a Service. As workload management continues to expand to serverless and virtual machines, and the operations ecosystem (e.g., security and observability) matures and hardens, we will see Kubernetes more abstracted from users… Cloud computing gave us compute as a service. Kubernetes is one layer above that compute, and a natural fit for an “as a service” offering; in 2023, we’ll see that take off. The rise of telcos – again. We will continue to see investment in Edge infrastructure from ISPs, telcos, CDNs, hosting companies and hyperscalers. And we will see the emergence of a need from these infra providers for application-level technologies to enable developers to place their workload on that infrastructure. Data distribution to go mainstream. Facilitating the distribution of data or Edge applications brings challenges for consistency. Fortunately, there has been significant investment in solving these problems by organizations… Caching, distribution and replication are all techniques these organizations are employing to let us have our data available in distributed footprints but still with ACID (or close to ACID) properties. The Edge will remain a nebulous and disputed concept. The debate will continue to rage about where the Edge is and whether some distributed systems are more or less “Edge-y” than others. What will not be disputed is that distribution of applications to wider hosting footprints has advantages with respect to elements such as latency, reliability, redundancy and data backhaul cost. So maybe a new phase will emerge focusing on application distribution rather than Edge. You can read the full list of trends to watch next year, a review of how Section’s 2022 predictions scored, and a bonus take on the anticipated evolution of cloud computing in the next five years by checking out the full article at Spiceworks."
"257","2022-12-20","2023-03-24","https://www.section.io/blog/mastodon/","The recent controversy over Twitter’s ownership and evolving policies have driven massive interest and growth for the open-source Mastodon software for running self-hosted social networks. Mastodon is unique in that it is decentralized and distributed; no single entity owns or controls it. Anyone can stand up a Mastodon server and create an isolated community – or they can become part of the Fediverse for a more global and connected Mastodon experience. In fact, as of this writing, there are now almost 6 million accounts and over 9,500 Fediverse-connected instances. Using data from this recent ZDNet article as a benchmark, that’s a 125% growth in connected instances in the last 3 weeks. The intent with Mastodon – or any social network, for that matter – is to connect users and communities. If you’re successful, whether connected to the Fediverse or not, that may mean two things: 1. The potential for explosive growth and 2. An appeal to a geographically dispersed base of users. Together, these factors mean you need to plan ahead for scale. Mastodon consists of a Ruby on Rails backend, a JavaScript frontend, Sidekiq jobs management and a PostgreSQL relational database. You can learn more about the underlying systems from the Mastodon documentation, or check out our tutorial on how to deploy Mastodon. While providers like Section are making it easier than ever to deploy a Mastodon server with just a few clicks, developers might also wonder about the process and benefits of deploying Mastodon at scale. The simple answer is that without the right provider, you’re likely stuck. Let’s take a look at why. User Experience A primary consideration with any online community is user experience, specifically, how responsive are Mastodon instances to user queries and load. A key factor impacting responsiveness is latency, which is a reflection of proximity between users and server instances. The farther away those Mastodon instances are from users, the slower the response. Which begs the question: do you know where your users are located, and are they all fairly close together? Because a typical cloud is not – despite many misnomers – everywhere, all at once. With most providers (including the hyperscalers like AWS, Azure, GCP, etc.), you’ll need to choose one or more hosting regions. If you choose one (say USA East), then every user outside that region gets a progressively poorer experience based on distance. If you choose more than one region, congratulations, you’re now responsible for managing a distributed network. If you’re trying to reach a global audience with Mastodon, this is a problem. The answer is to distribute your instances globally. That puts Mastodon as close as possible to users for a better experience. Section makes this simple. Database Load As mentioned above, another issue impacting responsiveness is the load on backend systems, specifically the PostgreSQL database. As a Mastodon instance experiences user growth, that increased scale can cause a slow-down in how quickly the PostgreSQL can process and respond to all those new database calls. This can actually be exacerbated by global distribution, as you’ve now got distributed servers “phoning home” across a distributed network to a centralized database (this can also impact cost; see below). The answer is to distribute the data out to where the users and servers are located. Section has teamed with PolyScale, whose intelligent serverless caching sits between the Mastodon application front-end and the back-end PostgreSQL database. The beauty of this architecture is that it still uses a single database backend, creating one source of truth for Mastodon. But the distributed PolyScale data caches sit physically close to the distributed Mastodon instances on Section – and closer to your users – dramatically improving latency, responsiveness and load-handling ability. Routing between these data caches and Mastodon instances is dynamic, optimizing performance by ensuring that database calls go to the nearest cache. This architecture also allows developers to keep PostgreSQL databases small and efficient, lowering overall hosting costs. “Global distribution is a critical consideration for modern applications, but it can create a significant challenge for DevOps teams when it comes to efficiently managing the underlying compute hosting and backend systems,” notes Ben Hagan, CEO at PolyScale. “Section and PolyScale solve these twin problems with a few clicks, creating an ideal platform for deploying Mastodon at scale.” Reliability, Scalability and Resilience Do you remember the Twitter Fail Whale? It’s a notorious graphic that indicated that Twitter was experiencing technical difficulties. Unless you’re considering reliability, scalability and resilience for your Mastodon instance(s), you might want to think about what your comparable Mastodon graphic might be. There are two important considerations for maintaining availability of Mastodon – or any cloud workload, for that matter. The first is the provider network. If your provider goes down, so does Mastodon. The way to avoid this single-provider point of failure is to use a global, redundant, federated network of providers, as we do here at Section. If any one provider goes down, we’ll route traffic to other providers to keep your Mastodon instances running. The second consideration is single versus multi-cluster deployment. Simply put, a single cluster not only represents another single point of failure for Mastodon, it also impacts the ability to scale Mastodon in response to load. The solution is a multi-cluster deployment, but running a multi-cluster containerized environment is hard – unless you’re using Section. We use a clusterless deployment model (basically, a cluster of clusters) that make it as easy to run on Section as it is to deploy a single cluster, yet you enjoy all the benefits of a multi-cluster environment. Optimal Cost Deploying workloads such as Mastodon at global scale can quickly get expensive and create major management and operations headaches. Considerations include number of instances, when, where and how frequently those instances are running, how “chatty” those instances are with backend systems, etc. Managing and optimizing all of these factors manually is difficult, if not impossible. Section’s distributed global network allows organizations to control workload placement while our dynamic orchestration engine optimizes resource utilization to always deliver the best performance for cost outcome. Section adapts to developer policies, such as “run containers only in Europe and where there are at least 20 HTTP requests per second” – allowing the distributed compute platform to limit the target deploy field and continuously adjust within that field accordingly. And our partnership with PolyScale ensures that scalability without overloading backend systems. Start Now If you’re ready to take your Mastodon deployment to the next level, check out our tutorial to see how easy it can be. And if you’re just getting started with Mastodon, we encourage you to start off right with Section. We make it simple to deploy Mastodon globally with just a few clicks."
"258","2022-12-15","2023-03-24","https://www.section.io/blog/multi-cloud-kubernetes/","Cloud computing industry expert and author David Linthicum recently wrote in InfoWorld how he believes 2023 may finally be the year of multi-cloud Kubernetes. David rightly notes that today we’re living in “a reality of complex cloud deployments that leverage more than a single cloud provider most of the time.” He continues by stating: “Everyone is supporting some type of container-based development and container orchestration on their respective cloud platforms. However, what’s missing is turnkey technology that focuses on development and deployment of multicloud solutions. Or distributed and heterogeneous container orchestration and container development that can deploy containerized applications across public cloud providers.” Of course, there are many reasons a multi-cloud strategy may make sense for your business. Maybe you don’t want to stay tied to just one cloud provider in case prices escalate. Or maybe you want to ensure reliability by eliminating a single point of failure. Or perhaps you want to use multiple providers to make sure you have capacity when or where you need it most. In short, there are a lot of reasons a multi-cloud strategy makes sense. At the same time, modern applications are making multi-cloud deployments more feasible through a modular, containerized architecture; these containers can readily span more than one cloud provider. This is why Kubernetes clusters are often used to simplify multi-cloud management. So, let’s take a closer look at global multi-cloud deployment… what does it take to manage an application that spans multiple providers worldwide, moving workloads closer to users to maximize performance, using different providers to ensure availability, etc? The short answer is: it’s complicated. Let’s assume you use a giant cluster that has a wide range of nodes available to handle workloads around the globe. It would be cost prohibitive to have all those nodes running across cloud providers all the time; you’re going to somehow need a hands-on approach to schedule nodes to run when and where needed. Ideally, you would spin them up/down and move them around in response to user needs and demand. To do that, you’ll use Kubernetes components like Cluster Autoscaler, Horizontal Pod Autoscaler and Vertical Pod Autoscaler to manage that giant cluster. Cluster Autoscaler can increase the number of nodes; Horizontal Pod Autoscaler increases the number of replicas of your application; Vertical Pod Autoscaler increases the resources that are used by a pod. But if you want to run a single, large cluster, how do you know where to add nodes? There are no Kubernetes extensions that address that challenge. This single, giant cluster – spanning multiple regions and cloud providers – also becomes extremely difficult to maintain. Your ops team would spend much of their time focused on constantly fixing problems that arise (just imagine the single point of failure that would occur when you might need to replace your DNS). A better approach would be to have many smaller clusters strategically placed – say, for example, in Sydney, Hong Kong, Paris, Amsterdam, New York, California, etc. – and each of these points of presence could exist on a different cloud provider. You could now run your workload everywhere, but you can probably guess the impact this approach would have on your public cloud costs. Moreover, you’re still facing an orchestration problem. What if your usage only spikes during the workday? Wouldn’t it be better to have the workloads follow the sun? You could spin up resources at 9 a.m. in Paris and spin them down elsewhere, then do the same at 9 a.m. in New York, and so on. Unfortunately, here again, this would be incredibly challenging for even the largest ops team to manage manually. And while automation tools exist, what happens if traffic begins to spike in Sydney while it’s daytime in Europe? Prioritizing those workloads to scale and follow user demand becomes incredibly cumbersome to manage. A better solution is to use Section, and spend your time building great apps. Section’s Adaptive Edge Engine (AEE) seamlessly orchestrates workloads around the globe on different cloud providers in different regions in response to developer intent. Want to “run containers only in Europe and where there are at least 20 HTTP requests per second”? But wait, what if you also want at least two replicas for reliability, but no more than 15 servers because that’s your budget limit? Section will handle all of that transparently in the background. Section’s AEE is a collection of components that manage all different aspects of global deployments, including LocationOptimizer for assessing where a workload will run, HealthChecker for monitoring workload health, TrafficDirector for routing traffic to healthy workloads wherever they are deployed, and more. AEE fills the gap for teams that don’t have the resources or expertise to build and manage multi-cloud deployments across hundreds – or even thousands – of endpoints, at a fraction of the cost you’d typically pay for active / active or even active / passive deployment access to multiple cloud providers across regions. AEE continuously and intelligently tunes and reconfigures delivery networks to ensure workloads are running the optimal compute for the specific application based on real-time traffic demands. Looking to find ways to help your organization save time and money in the new year? Check out Section to see how we help you get a grip on multi-cloud Kubernetes. You should focus on your applications; leave the infrastructure provisioning, workload orchestration, scaling, monitoring and traffic routing to us."
"259","2022-11-21","2023-03-24","https://www.section.io/blog/intellyx-digital-innovator-award/","Industry analyst firm Intellyx, which focuses on enterprise digital transformation and the leading-edge vendors driving it, announced it is recognizing Section with its Fall 2022 Digital Innovator Award. Now in its second year, Intellyx’s Digital Innovator Awards are only given to companies who make it through the firm’s rigorous briefing selection process. Intellyx notes that each year it hears from thousands of vendors, but less than 1% of those companies that contact the firm end up being selected for the award. In recognizing Section, Intellyx’s president Jason Bloomberg says: “At Intellyx, we get dozens of PR pitches each day from a wide range of vendors. We will only set up briefings with the most disruptive and innovative firms in their space. That’s why it made sense for us to call out the companies that made the cut.” This award affirms what so many other industry researchers and analysts have come to realize: that application hosting as we know it is broken. Of the thousands of global data centers on the planet today, choosing the right one to host an application is largely a manual and arbitrary process. What’s more, distributing application workloads across multi-cloud and multi-cluster is incredibly complex. That is precisely why Section is drawing high praise for its cloud-native hosting system that continuously optimizes the orchestration of secure and reliable global infrastructure for application delivery. Section’s sophisticated, distributed and “clusterless” platform intelligently and adaptively manages workloads around performance, reliability, compliance, cost or other developer intent to ensure applications run at the right time in the right place. This Digital Innovator Award from Intellyx comes just a month after Section was recognized by Frost & Sullivan with a New Product Innovation Award in the North American Kubernetes Industry. In selecting Section for its New Product Innovation Award, Frost & Sullivan praised our Kubernetes Edge Interface solution that, coupled with our Adaptive Edge Engine and Composable Edge Cloud, enables customers to easily deploy workloads to a dynamically optimized and massively distributed Kubernetes-based hosting platform. Create your project and deploy your containers in minutes by trying the Section platform for free today."
"260","2022-09-06","2023-03-24","https://www.section.io/blog/static-v-dynamic-distribution/","We all know there’s a big difference between static and dynamic systems. In a static system everything is in statis, systems are fixed and stationary. A dynamic system, in contrast, is characterized by constant change. Most would characterize the cloud as a dynamic footprint. Usage patterns can shift, traffic and bandwidth constantly ebb and flow, applications can scale and adjust as needed (if set up appropriately). Yet macro cloud hosting decisions are made as though the landscape is static, especially when it comes to location. Developers choose a hosting region, deploy their app and that’s it. Usage may change, but the hosting location stays the same. Why? In simple terms, because it’s too difficult. Distributing application workloads and coordinating compute resources is highly complex. To accomplish it, companies basically need to take on management of their own distributed network, which is costly and resource intensive. As a result, most organizations and hosting companies simply can’t contemplate the level of work needed to adjust to shifting cloud demand outside of basic datacenter resource scaling. Companies will load balance within a datacenter, but not across multiple datacenters and geographies. To scale geographically, most hosting providers – including all the hyperscalers – require companies to provide the tools and skillset to orchestrate and manage a distributed network. Dynamic Application Distribution with Section Then there’s Section. We recognize that application demand is dynamic, not static, and we’ve designed our hosting to match – allowing your application to intelligently traverse the world’s compute resources to run at the right place and time. Whether that’s across town to isolate workloads and improve resilience, or around the globe to accommodate fluctuations in user demand or nuances of regulatory compliance, the cloud-native Section platform constantly adapts to your application’s unique requirements. Teams deploy to Section just as though it were a single cluster, using familiar Kubernetes tools, processes and workflows. Section gives you the opportunity to specify application intent (around security, performance, resilience, etc.) using simple policy-based rules like run containers where there are at least 20 HTTP requests per second, and our patented Adaptive Edge Engine will automatically and dynamically adjust application hosting to meet changing demands. If you’re ready for a hosting solution that matches the needs of your dynamic application, get started for free with Section today."
"261","2022-08-22","2023-03-24","https://www.section.io/blog/andy-piggott/","Section has announced the appointment of Andy Piggott as its Chief Product Officer (CPO), overseeing development of the industry’s only dynamic, distributed global compute platform for cloud-native applications. With more than two decades of experience building internet-scale businesses, Piggott is leading Section’s product strategy and development as it continues to automate and modernize application hosting. “The deployment of application workloads has been converging in hyperscale datacenters for years. Developers today are realizing the needs and benefits of deploying workloads within milliseconds of end users and Section is enabling them to quickly and easily deploy their workloads globally using standard developer tools,” said Stewart McGrath, CEO and co-founder of Section. “We’re thrilled to have Andy join the team to help drive product innovation that optimizes and simplifies distributed application hosting for developers.” An expert in Product Led Growth for Anything as a Service (XaaS) companies, Piggott is skilled in working with customers to create exceptional product experiences. As Section moves into the next phase of its development and growth, he will be instrumental in driving customer-led innovation for the Section platform as it revolutionizes how applications are deployed. “Organizations with modern, containerized application workloads shouldn’t have to roll the dice on hosting locations or self-manage multi-cluster or multi-cloud deployments,” notes Piggott. “Section is delivering on the true promise of cloud-native computing with its dynamic and intelligently optimized global platform for application delivery, and I look forward to continuing to improve that experience for the DevOps community.” Piggott most recently served as Senior Vice President of Customer Experience at Minim and previously held leadership roles across Marketing and User Experience at Dyn (acquired by Oracle). After earning a bachelor’s degree in computer science and software engineering, Piggott began his career as a software developer and since then has successfully led organizations of varying sizes and scale across a variety of emerging industries."
"262","2022-01-10","2023-03-24","https://www.section.io/blog/section-welcomes-shaun-andrews-to-board-of-directors/","Section is proud to welcome Shaun Andrews, Executive Vice President and Chief Marketing Officer at Lumen Technologies, to the company’s Board of Directors. “Shaun is a tremendous addition to our Board,” said Stewart McGrath, Co-Founder & CEO of Section. “His diverse experience across complex product portfolios lends valuable perspective to help support Section during this exciting phase of growth and beyond.” Shaun’s career journey has been driven by a passion for shaping amazing customer experiences through technology innovation. With the next wave of innovation happening at the Edge, he brings great insight to help support Section’s positioning as a leader in the Edge as a Service category. “Edge technologies are increasingly becoming a core component of modern applications, and Section’s DevOps-friendly platform helps technology teams accelerate their edge strategies. I look forward to helping the company build on an already strong foundation to enable better customer experiences at the Edge.” - Shaun Andrews, EVP & CMO at Lumen Technologies Shaun previously served as Lumen’s Executive Vice President, Product Management. Prior to this role, he was Senior Vice President of IP and Real-Time Communications for Level 3 Communications and held several senior roles at IntelePeer, WilTel and SBC Communications. Following Section’s Series B funding round in April 2021 led by Lumen Technologies, Shaun joins Section’s existing Board of Directors, which includes Foundry Group Partner Ryan McIntyre, Catalyst ADV CEO Deborah Diaz, Section Co-Founder & CEO Stewart McGrath, and Section Co-Founder & CTO Daniel Bartholomew."
"263","2021-06-18","2023-03-24","https://www.section.io/blog/section-joins-google-cloud-edge-partner-ecosystem/","Google Cloud continues to advance their edge initiatives, most recently announcing the expansion of their edge and ISV ecosystem, with Section Edge as a Service included among 15+ new partner solution offerings. As part of this partner community, Section makes it easier for developers to adopt edge strategies by offloading many of the underlying complexities associated with infrastructure provisioning, workload orchestration, scaling, monitoring, and traffic routing to Google Cloud and Google’s Edge Infrastructure. “Section’s DevOps-centric approach to simplicity, flexibility, and control at the Edge aligns with Google’s vision to make edge computing more accessible for innovators. As we continue to expand our global edge footprint, Section will make it easy for enterprises to distribute applications across our edge infrastructure backbone; thus delivering all the benefits of edge coupled with the familiar simplicity and flexibility of cloud.” Tanuj Raja, Global Head, Strategic Partnerships at Google Underpinning the capabilities that the Section platform brings to Google Cloud developers is the patent-pending Adaptive Edge Engine (AEE). By providing dynamic, context-aware workload scheduling and traffic routing based on desired objectives (e.g. reduced latency, cost efficiency, compliance, etc.), AEE enables technology teams to more easily implement edge strategies to suit the unique requirements of their applications. Beyond the built-in intelligence that AEE delivers to run workloads in the right place at the right time, Section’s Edge as a Service solutions offer diverse edge application support coupled with a strong DevOps experience to seamlessly integrate into existing workflows. “Through our partnership with Google Cloud, we are helping organizations simplify and accelerate their digital transformation journeys by removing friction associated with edge computing adoption. Section and Google together present an Effortless Edge for organizations to leverage so they can deliver faster, more secure, and more cost effective digital experiences for their customers.” Stewart McGrath, Co-Founder & CEO of Section. To learn more about enabling edge capabilities on Section with Google Cloud, contact us."
"264","2019-06-17","2023-03-24","https://www.section.io/blog/first-day-arpita/","After coming across various legacy CDN issues as a developer and a client manager and working in the performance industry, I discovered Section while working at my previous company. I was intrigued by the level of customizability in what seem like a faster, scalable and dev friendly version of legacy CDNs and hoped to one day work here with a dynamic technology stack and amazing culture. I am excited to join the team and contribute towards enhancing customer experience through exceptional support and a well defined integrated customer success process. As we introduce more customers and features, I want to use my startup, dev and relationship building skills to retain and grow its customer base. Finally, I am ecstatic to be part of a helpful, flexible, dynamic and DevOps friendly culture here at Section. :)"
"265","2019-05-13","2023-03-24","https://www.section.io/blog/nicholas-first-day/","This is my first day, and I am super excited to join Section, who are pioneers and founders of customizable and performant, edge networks for web engineers. I am joining Section after 2 years at a San Francisco Bay Area internet service provider, where I worked in customer tech support and technical project management. I started working 18 years ago in web and internet in Sydney, including working as a web technology specialist for a global news conglomerate. I am sold on the Section vision, and am impressed by the platform architecture and its existing and upcoming features. It’s great to see how Section implements a teamwork first approach for customer success and is building a culture of customer-driven engineering solutions. Section has joined the ranks of great companies where customers LOVE the product and the services wrapped around it. I am stoked to be onboard with the Section mission, of bringing customizable and reliable edge network solutions, for the happiness of web engineers and the commercial success of our customers."
"266","2019-04-16","2023-03-24","https://www.section.io/blog/hello-pavel/","Hello, World! My name is Pavel and I just joined Section. This is my very first blog post. I am a software developer and Golang aficionado. During the past 13 years I have been involved in various projects at different scales, for example, APIs, stream processing applications, websites serving millions of users. I am passionate about high scalability, distributed systems, microservices architecture, Kubernetes and everything CNCF. Over the past few years I have spent a lot of time working on observability (metrics, tracing, logging and alerting) and I expect to continue doing this at Section as well. Few development principles that I follow (in no particular order): No is temporary, yes is forever KISS The best code is no code If something is too complex probably it is wrong I am super excited to be part of this team and I am looking forward to learning new things as well as contributing to the existing and future projects. Stay tuned for more posts!"
"267","2019-05-22","2023-03-24","https://www.section.io/blog/hello-from-ivan2/","I am from the mile high city of Denver, Colorado where it can be sunny at 12pm, rain cats and dogs at 3pm, and have a blizzard at 5pm. I am a rising sophomore in college with my first official introduction to computer science my freshman year. I have worked with both python and c++ to complete class projects and hope to increase my level of knowledge through my college classes and my experience in this internship with Section. Why I decided to join Section? I decided to join Section as an Intern Engineer because I want to be able to get that startup experience and really get to interact with my coworkers. Section has a unique and powerful goal in CDN work, and I wish to contribute to the reputation that Section has built among its customers. I expect to leave this internship with a deep understanding of what HTTP is to better enable my awareness in my everyday life. I also hope to learn as much as I can from my coworkers and getting experience working in a company structure."
"268","2019-04-02","2023-03-24","https://www.section.io/blog/section-brand-evolution/","We’re thrilled to introduce the next evolution of the Section brand. Our prior brand has carried us through the last three years and has represented the emergence of the Section platform, product, and offering. As we continue to drive innovation in the field of edge computing, it’s important that our brand reflects our mission and values. When we first built Section, we set out to solve the problem developers have in gaining access to and control of (Edge) workloads tied up in legacy CDNs. As buyers of CDNs in previous roles, we had experienced this frustration first-hand. We want developers to be able to: Run their choice of software, At the Edge locations they need; and With a proper application development lifecycle and DevOps control of that software at the Edge. What has become apparent over the last three years is that modern developers want control over more than just legacy CDN workloads at the Edge. Modern application architects and developers want to build and run distributed applications. They want to execute code as close as possible to end users to reduce latency, improve security, improve application scaling patterns and reduce data shunting costs and time. The Section platform was built to give developers flexibility and control over the software they run at the Edge. We provide a library of Edge software modules from which developers can choose and we provide the ability for developers to bring their own custom workloads to our edge platform. We also give users a fully Git-backed Edge application development workflow and a DevOps-centric diagnostics platform. Our challenge to modern developers is as follows: Given the opportunity to run your choice of software, in customizable, distributed edge locations, supported by a proper edge application development lifecycle and DevOps-centric control… We are super excited to release the new Section brand. Thanks and kudos to the team who have been working away behind the scenes over the last 6 months to forge our new brand. We made some difficult choices along the way (fonts and colors are always a challenging conversation), but I believe we have landed with a brand which truly represents the Section values, mission and message. We hope you love the new Section brand (and don’t miss the old Section purple too much)! Now, over to you… What will you Create?"
"269","2019-01-17","2023-03-24","https://www.section.io/blog/section-joins-cncf/","We’re thrilled to announce that Section is now a proud supporting member of the Cloud Native Computing Foundation (CNCF). We’re honored to join this community of like minded individuals and organizations who share the same values of transparency, collaboration and community in the spirit of accelerating technological advancements for a common benefit. This commitment directly aligns with the Section Manifesto that has served as our guiding doctrine on our journey to provide a truly developer-centric edge compute platform that gives engineers the flexibility and control to run any workload, anywhere, underpinned by modern DevOps principles. “We’re really proud to join the CNCF community and help sustain their mission of empowering organizations to build and run scalable applications in modern, dynamic environments.” -Stewart McGrath, Co-Founder & CEO, Section What is CNCF? CNCF is an open source software foundation dedicated to making cloud native computing universal and sustainable. Cloud native computing uses an open source software stack to deploy applications as microservices, packaging each part into its own container, and dynamically orchestrating those containers to optimize resource utilization. Cloud native technologies enable software developers to build great products faster. Kubernetes and Prometheus CNCF is home to some of the fastest growing open source projects ever, two of which form the backbone of the Section platform: Kubernetes and Prometheus. As the first CNCF project, Kubernetes is the most widely used container-orchestration platform across the globe with adoption among some of the most respected technology teams in the world. With Kubernetes, we have been able to improve and scale our platform in ways that weren’t previously possible. Running parallel to Kubernetes is another CNCF project, Prometheus, which serves as our monitoring solution, informing all decisions, from container orchestration to business insights for clients. Looking ahead, this year will also see us deploy additional CNCF projects, including OpenTracing, Jaeger and NATS. “Giving back to the open source projects that have enabled us to disrupt an industry and build for the future of edge compute not only feels right, but is a debt that we have a responsibility to repay.” -Dan Bartholomew, Co-Founder & CTO, Section Connect with Us at KubeCon Barcelona & KubeCon San Diego Along with our ongoing commitment to support CNCF as community members, we will also be sponsoring the KubeCon Europe (Barcelona) and KubeCon North America (San Diego) events. If you plan on attending either of these events and would like to connect, get in touch with our team."
"270","2018-11-26","2023-03-24","https://www.section.io/blog/running-agile-across-the-organization/","The Agile methodology was pioneered as an innovative approach to software development, but it can be effectively deployed across many areas. At Section, we choose to apply the Agile methodology as a framework across our entire organizational structure, from software development to customer engineering to marketing and sales. What is the Agile Methodology? Agile was launched by a group of technologists in 2001 who wrote the Agile Manifesto and within it four key principles for developing better software: Prioritizing individuals and a collaborative approach to working over strictly defined processes and tools Having a flexible process that is adaptable to change Focusing on building working software that is up-to-date as opposed to sticking to a rigidly defined waterfall software development process Learning from the user through customer collaboration as opposed to contract negotiation The manifesto was written in response to the waterfall software development process, which was developed in 1970 to bring discipline to software development and quickly became the prevalent mode of use. This highly detailed approach to developing new software involves detailed documentation, followed by coding, integration, and ultimately testing. It could take several years before an application was considered production ready. In the era of the Internet, the waterfall framework was no longer a viable way to work. Originally, the Agile method was initiated by developers at startups where teams were typically smaller, colocated and often hailed from non-traditional computer science backgrounds. These developers began to explore more flexible processes that could bring websites, applications and other capabilities to market more quickly and cheaply. It became harder to attract talented software developers to legacy-bound organizations that followed end-to-end schedules and a rigid waterfall process. Developers instead started to take the lead in building an iterative schedule that had shorter timeframes and a process that emphasized collaboration over documentation, self-organization over rigid project management, and the capability to manage continual change instead of attempting to meet predefined deadlines. Agile Frameworks: The Scrum There are several types of Agile frameworks; the most popular being the scrum. The framework is built around the sprint, a set period of time (usually 1-4 weeks) in which a specific set of priorities has to be completed and prepared for review. The delivery cadence within the sprint involves: Planning - Defining key priorities for the sprint as a team Commitment - Reviewing the backlog of user stories and mapping out the work to be achieved during the sprint Standup Meetings - Daily standup meetings that bring the team together to communicate updates on their development status and strategies for work The Benefits of Agile As the Agile methodology has continued to develop, its many benefits have become clear: User-Focused - Agile focuses on constructing new product features that deliver value to real users rather than just being IT-oriented; software can be beta tested after each sprint. Accountability and Transparency - Developers need to involve clients across the project, from building new features to handling review sessions. Predictable, Frequent Delivery - Due to the fixed schedule of sprints, new features are delivered often. Foreseeable Costs and Schedule - The client can receive cost and schedule estimates ahead of each sprint and revise plans during it. Change-Focused - Teams have the opportunity to revise the priorities of the product backlog and introduce new features when necessary. Focused on Business Value - The focus is on the client’s mission, thus delivering the products that most improve business value. Scalability - As new team members are onboarded, it’s easier for others to jump in and work in relation to clearly defined goals. Cross-Functional Team Collaboration - Prioritization that stems from customer needs, coupled with visibility across the organization, provides continuous opportunity for feedback loops. Remote Teams Brought Together - Agile systemizes collaboration by allowing teams to visualize all the parts and their sum. As more and more software development teams have adopted the Agile methodology within their organizations, other departments have taken notice of the benefits and started to adopt the methodology within their own teams. How Section Applies Agile Across All Teams At Section, we apply the Agile methodology across the entire organization: Platform Engineering Team The platform engineering team uses Agile across the complete lifecycle of the application, working together through the scrum process to build and deliver customer-centric value. All requirements are documented and broken into achievable tasks, then prioritized within a go-to-market schedule. The iterative approach to deployment allows for beta testing and frequent user review and collaboration. Customer Engineering Team Responsible for everything from onboarding, to support, to client success, the customer engineering team plays a critical role as the conduit for client communications that directly impact every other team in our organization. Agile has been very effective in addressing both the strategic vision and the day-to-day tasks of the customer engineering team, making sure that all customers’ needs are being met while also implementing feedback loops that continue to push the product forward. Marketing & Sales Agile as applied to marketing is increasingly popular. The goals of Agile Marketing) are to improve the speed, predictability, transparency, and adaptability to change of the marketing function. At Section, our primary use of Agile is for strategic planning, prioritization, task management and aligning marketing and sales alongside the platform and customer engineering teams in order to keep our focus on the customer. We also use data and analytics to continuously source promising opportunities and/or solutions to problems in real time, deploying tests quickly, evaluating the results, and rapidly iterating new ideas. The Agile Tech Stack Many tools have been developed around the Agile methodology. Software such as JIRA, Trello, Pivotal Tracker, and countless others help ease the management of the process so that teams can focus on what matters most - getting sh*t done. We use TargetProcess as our key piece of software to implement and scale Agile. It offers numerous benefits, including flexibility in the visualization of projects, from a high-level overview down to the granular details, as well as the opportunity to construct custom reports, including features such as cycle time variation, number of bugs per feature and project, etc. TargetProcess integrates with existing development tools to act as a central collaboration and management hub across the Section teams. Everyone has access to each team’s weekly sprints in an easy-to-use organizational dashboard. We perform weekly demos to show the entire organization what was completed within different departments the prior week - this is especially valuable for aligning the platform and customer engineering teams. Adopting an Agile methodology across the organization has allowed us to find innovative ways to meet customers’ needs quickly and effectively across all areas."
"271","2018-10-11","2023-03-24","https://www.section.io/blog/hello-from-matt/","Today is the middle of my third week with Section as a platform engineer, and it’s been a great ride so far! This is a company that lives on the bleeding edge of technology AND the internet that I’m very excited to join. I’ve worked on web technology from e-commerce to mobile applications, but this is the first time that I get to work full-time on a product that is targeted at other developers. I know we’re a tough and demanding audience, but I’m excited for the challenge and the opportunity to give back to the developer community by making the internet better! Going forward I’m going to be focusing on learning, delivering the most powerful edge platform available, and driving innovation for Section!"
"272","2018-09-04","2023-03-24","https://www.section.io/blog/hello-from-molly/","Today is my first day as Marketing Director at Section, and I’m eager for the opportunities ahead! What drew me to this company is the “for engineers, by engineers” mindset that carries through every vein of the organization. In all of my conversations with the team here, it’s glaringly apparent that our clients - software engineers - are at the center of everything we do. From product development to client success, Section is on a mission to make work easier for engineers. Going forward My focus will be on helping to build the Section brand and evangelize the disruptive power of the Section platform. I look forward to the ride!"
"273","2018-08-28","2023-03-24","https://www.section.io/blog/hello-from-gary/","Yesterday was my first day at Section, and I am looking forward to many more. I have worked as a lead software developer and architect for the past 26 years across several industries - most recently at F5 Networks on their cloud enablement effort for their BIG-IP product. I have worked at companies both large and small, but certainly prefer working at smaller companies on a closely-knit team. Once I learned more about Section I knew right away that I wanted to be a part of their team and help the platform reach its full potential. Going forward At Section I am a Principal Engineer on the Platform team, and I am looking forward to working closely with the team to help improve all aspects of the platform and growing our customer base."
"274","2018-08-13","2023-03-24","https://www.section.io/blog/hello-from-bjoern/","A few hours ago, I started my first day at Section as a full time employee in the role of VP Commercials and I couldn’t be more excited to join this team. Over the past few weeks I had the pleasure of working with Stewart and Dan to understand the Section Go-To-Market strategy and value proposition for our clients better. Understanding what Section has already accomplished, and the vision of Stewart and Dan going forward, really convinced me about the opportunity for success. I spent the last 24 years architecting and executing growth-driven negotiations at the commercial level for IT and Engineering outsourcing powerhouses Hewlett-Packard and QuEST Global Engineering Services. Both companies are very big in their field, and it was a pleasure to work there. What’s next? I am looking forward to be working with the team on executing the vision and selling at a bigger level then before to really grow Section with new Enterprise Customers and Partners. I am eager to learn from the team and leverage my expertise as Section continues on its upward path of success. I speak German and English fluently and will learn “Australian” as I come up to speed."
"275","2018-08-06","2023-03-24","https://www.section.io/blog/hello-from-mani/","I start today at Section as a Customer Support Engineer and am very excited to take on customer and platform challenges. I look at this as an opportunity to have a meaningful impact on the web performance side of things by helping customers optimize their web products and provide a better experience for their end users. Having built and launched a number of customer-facing applications I see this as an amazing opportunity to help customers with their pain points and make the internet a better place. Moving forward I love building products and I aim to bring that same passion to the team to help customers with the issues they face in day to day administration and the Section team to resolve existing issues and building out the product further. Learning new things, solving puzzles and building products is what I live for and I am excited that I get to do all that and more at Section. Game on!"
"276","2018-02-20","2023-03-24","https://www.section.io/blog/hello-from-aaron/","I am extremely excited to join the Section team in its mission to empower companies to deliver performant and scalable web content. As a former middle school math teacher, I understand what it’s like to fight with slow and unresponsive technology in a hectic workplace setting, and I look forward to partnering with Section to create a streamlined, painless technological experience for your customers. If technology is not making our lives easier, less stressful, and more productive, then it is not doing its job. Moving forward Having transitioned into the software industry from teaching, I am excited to bring the interpersonal and problem-solving skills that I learned in the classroom to my new role as a customer engineer with Section. I look forward to taking the time to really understand customer needs and provide the optimal solution for their business."
"277","2017-04-10","2023-03-24","https://www.section.io/blog/magento-imagine-2017/","Magento Imagine comes but once a year, and this year we were back in Las Vegas. While I have discovered that you can scratch the surface of many and various “shiny surfaces” in Las Vegas and find a slightly harsher reality, when you scratch the surface of Magento, you find the real heart; the community. After a Magento Partner conference on Sunday and three days of the overall Imagine conference, I had; Tired legs - man those hotels are big! Nearly lost my voice - lots of conversations, or maybe it was singing along to the band covering Daft Punk at the Imagine party Learned much - there is not much difference between a shaken or stirred Negroni but there is when faced with life and death on the slopes of Denali in a wind storm (thanks Jamie Clark) Been inspired by awesome people - thanks Jessica Herrin; and Found a continued desire from the Magento community for there to be a strong, open and accessible Magento product Magento grew as a dominating force in ecommerce due to the open source community which drove innovation, training, adoption, mentoring and promotion of the Magento codebase. It was awesome to again have it confirmed that there is a core desire in the community to maintain and grow this element of Magento. I do wonder if the Magento Cloud product, which reduces vendor choice by pushing Enterprise customers onto a limited feature set which includes hosting and a delivery platform, is aligned with the strengths of the Magento product and community. Thanks so much to Section customer and ecommerce guru Kate Morris who flew from Sydney to share her Adore Beauty conversion optimization story with the Magento community. Kate also shared some thoughts with the community on diversity as we work to increase people diversity in both technology and ecommerce. Thanks also to all our partners at the Magento expo who introduced us to so many great Magento customers looking for better security and performance for their Magento applications. I am looking forward to Imagine next year (but less so Las Vegas!) to see how the heart of Magento responds to the changes which develop throughout this year. No doubt it will be a year of testing the response of the community and the vendors to Magento’s new approach to building it’s revenue with the Magento Cloud product."
"278","2017-01-09","2023-03-24","https://www.section.io/blog/hello-from-cj/","Today I embark on a new journey with Section who empower developers to increase scalability and improve performance for their web applications. Having started my own company and developed the product from the ground up, I understand the importance of building up your CDN alongside your application and Section allows you to do just that! The product for my previous company was a two-sided marketplace, and we had some pretty angry customers when our content was not being delivered in a timely fashion. In fact, we ended up losing those customers. Moving forward I have been working as a developer for almost 6 years now, and I’m joining Section as a Technical Solutions Engineer. I’m excited to work alongside and guide other developers through setting up Section for their application(s) to improve performance, reduce bounce rates, and ultimately increase sales!"
"279","2016-12-21","2023-03-24","https://www.section.io/blog/managing-remote-work-international-teams/","When I first started at Section, which is located in the heart of Boulder, Colorado, I got a lot of questions about my commute and how I was dealing with it. This is because I live in Denver, about 30 miles south of Boulder and connected by one major highway that can get very congested. Although Denver is larger city, Boulder has a burgeoning start-up scene and is where many tech companies have chosen to base themselves. Because of this, I decided I would be open to a Denver-Boulder commute as long as it was for a company I was truly excited about, and one that could be flexible with things including office hours and working from home. Luckily, with Section I found both of these things, so now when people inquire about my commute in sympathetic terms I can say “Actually, it’s not too bad.” Flexible Hours and Working from Home Section is unique in that we have offices in both Boulder and Sydney, Australia, where the company was founded. Only about half of the Colorado team lives in Boulder, and the Australia team is spread across Sydney and up into the mountains 4 hours south. The Australian contingent primarily work from their homes, although we have an office that they can use when they have meetings nearby or want a more traditional office setup. They also have a once monthly “Day in the office” where, you guessed it, everyone spends the day working out of the office. We don’t work from home all the time in Colorado, but since Section is used to remote work it’s easy for any of us to work remotely 1-2 days a week if we need to be closer to home or simply enjoy being in a different environment occasionally. I find that when writing content I can be very productive from my apartment which can be quieter than our open office space in Boulder. Some of us schedule meetings around when we will be in the office, or we can jump on a Skype call or Google Hangout to connect. Because the team is split between the US and Australia, we already use a lot of digital communication methods to ensure all of the teams are up to speed: Slack is our go-to for the vast majority of internal company communications, and we use Google Docs for collaborating on files and Trello boards and Trac tickets for keeping track of tasks and bigger ongoing projects. The international nature of the team also lends itself to flexibility on what hours we work. Boulder and Sydney are a whopping 18 hours apart at the moment, and the gap gets even bigger at times due to daylight savings. That means the Sydney team starts their Tuesday at around 3pm on Boulder’s Monday. Because of this, team meetings are held in the late afternoon MT on Thursdays, and those of us in Boulder who need to talk to the Sydney team often come in on the later side and stay until 6 or 7pm. The ability to work flexible hours at Section has a couple of benefits: Those of us with commutes to Boulder can plan our days to miss the middle of rush hour, which as an added bonus gives us more overlap with the team in Australia. The great thing about the team at Section is we all trust each other to get our own work done. There’s little concern about where you are doing that work as long as it’s getting done in a timely fashion. So far, it’s working out pretty well: in 2016 we completed TechStars Boulder, raised a seed funding round, announced a partnership with Magento, and released several new features on our product. We’re excited to see what the next year brings! Join the Section Team Are you interested in joining our team in 2017? Check our careers page for current openings and we hope to hear from you soon!"
"280","2016-06-03","2023-03-24","https://www.section.io/blog/movin-to-the-country-gonna-eat-a-lot-of-peaches/","At the beginning of 2015 my family realised that living in Sydney, even out in Western Sydney as we were, wasn’t working for us. Too much stress, too much traffic & not enough space. I had been working from home for over a year at that point. My wife & I had talked about getting out of Dodge for years, and we decided that if we didn’t want to regret never trying it we had to do it now. One of the fantastic perks of working for Section is that I can work from pretty much anywhere with an internet connection. So we sold up our small house on 500m2 & bought a 2 acre property in the NSW Snowy Mountains, which is about as far removed from our previous suburban life as I could find while still having access to internet that didn’t require a satellite dish. We now live in a town of about 800 people up in the snowies, and it’s been amazing, I now wake up to kangaroos hopping through our yard, as opposed to what it used to be, which was constant traffic. Other than the kangaroos driving my dogs insane, the transition of my family from suburbanites to country folk has been surprisingly smooth. That isn’t to say that it’s been without its challenges. I don’t think it will surprise anyone who is familiar with the state of internet connectivity in Australia when I say that internet speed out here is woeful. Initially my only option was Telstra ADSL. Now, I’m pretty close to the local exchange, according to http://www.adsl2exchanges.com.au/ there’s only about 1.5km of cable between me & the exchange. One of the reasons I picked this area was that it at least had ADSL2 at the exchange. However, when we first got here, this was the sort of speed we were getting: Despite the surprisingly decent ping time, it’s pretty bad. Skype calls with my colleagues were embarrassing and forget about trying to screen share. It did improve a bit, later I was getting around 5Mb/s down with about 0.5Mb/s up, but still, pretty terrible. Oh, and this was for $93 / month (inc. line rental) with a 200GB cap. With 4 kids in the house and me working from home, we blew through that cap every month, I used up all three of the free topups in the first three months, on the fourth month we got shaped for the last few days. One of the other connectivity options I had investigated before we moved here was CountryTell’s point to point wireless offering. This had looked quite attractive as it was about $80/month for symmetrical 20Mb/s unlimited data. I had few illusions that we would actually get 20Mb/s symmetrical, but people I had spoke to about it said it was stable and fast. Unfortunately, just as we moved here CountryTell had a problem with their supplier for their wireless units and stopped selling their residential wireless service. So I signed up for Telstra, hoping CountryTell would come back up as an option. Which they did. Unfortunately they had redone their plans and pricing by that time and were now offering as 12 down 1 up Mb/s plan for $99/month. Still unlimited though. A bit disappointing, but much better than what I was getting, so we got setup with them, and this is what I’m getting now Of course, this all pales into comparison with what you can get in, say, Sydney: Ah well, it’s working now, so I’m happy. I’d rather be here with the sub-optimal speeds than having to be in the middle of the city to get over 100Mb/s. As I sit here watching the sun set over the mountains, I can’t really complain. * Note: Peaches not guaranteed, we do have apple trees though"
"281","2016-05-18","2023-03-24","https://www.section.io/blog/moving-an-aussie-company-to-the-usa/","Nearly 12 months ago our company started the journey to move our company’s head office to the US. This post is to provide a quick outline on some of the key items we have tackled along the way. I hope it helps other Aussie companies find their way to a broader market if it makes as much sense for them as it has for Section. Why Move? We made the decision to move here to the US for a number of reasons; The US market for our product is so much bigger (number and size of customers) The availability and sophistication of funding and advisory is greater in the US The agile development and CDN thought leadership coming from the US is ahead of where our Aussie community was up to. Why Boulder? After working through a number of potential landing points we settled on Boulder / Denver Colorado. We spent time in NYC, the Valley, have been through Seattle and investigated Austin. Why did we choose Boulder? Well a number of reasons again; Geographically, Boulder is centrally located providing access East and West Coast USA inside a day trip (or maybe two) The cost of building a tech business in Boulder is lower than high-cost areas such as the Valley and NYC (consider Rent, People, Travel, Perks, Hiring etc) The tech community here is booming. We found investors and advisory in the Boulder / Denver area who know, understand and can see the Section vision. The culture of the Boulder business and tech community is much more closely aligned with our values regarding how we believe a business should be built in a scalable sustainable manner, and finally Boulder is an awesome place for our families to live. We believe we will be more successful personally and as a business where our families are happy and healthy. This has been a massive move for our company with some exceptionally positive outcomes already. We are working with some extremely talented people here and some awesome customers. We also ended up in Techstars here in Boulder – more on that to come in a future post. My accent has landed me in moderate trouble a few times and Steve Irwin has not done much for our reputation here, but I reckon all will work in the long run as I finally convince these folks that no Aussies drink Fosters. What have we tackled in this process? Lots! Corporate Flip Lawyers, accountants, tax, shareholders, government agencies and more lawyers. Get ready for a variety of stakeholders playing in all the spaces from IP agreements to options and warrants and employee agreements. We engaged an awesome legal team here in Colorado (Cooley) to augment our Aussie advisors. Our flip needed a little special love and attention as we had two classes of shares in Australia as well as a few options to deal with. A few pain points here included: We had an employee shareholder loan incentive plan in Australia thanks to the restrictive tax structures which used to be in place in Australia with respect to options. (Not the case anymore). Options would have been easier. We had a couple of options in the Aussie entity with investors which need to be converted to warrants – This is a bit painful To provide the Aussie shareholders with some help to understand what we were doing with the flip and why we were doing it, we put together a broad information pack covering the purpose and process of the flip. Our Aussie shareholders needed some advice and comfort on the tax position as they were about to become shareholders in a US entity rather than an Aussie entity. While ultimately this is the responsibility of the individual shareholder to take care of their own personal tax position, we wanted to do the right thing by all and provide some broad “advice”. So we engaged an Aussie accounting firm to provide this advice as part of the information pack. Cooley facilitated the incorporation of the US entity and the share issue within the US entity to the Aussie investors. Cooley also then facilitated the issuance of shares to our new investors. Don’t forget to lodge all the relevant forms with ASIC to reflect the new ownership structure. You may need to apply for corporate restructuring relief if you have a company registered in NSW and hence a potential stamp duty bill. Working with investors in the US when not “there”. We have been fortunate to find a great set of investors who understand the CDN / reverse proxy space and believed us when we said we were moving here. There was an interesting reversal of investor mindset we found when we landed in the US. Whereas in Australia, our awesome angel investors know very little (if anything) about agile development, CDNs and reverse proxy technology or direction, they knew and trusted Daniel and myself; in the US, our investors knew and trusted the technology direction we are making a reality but needed to get to know and trust Daniel and me. We took on a number of trips through the US in 2015 before moving here permanently in Feb 2016. We found that while phone calls, emails and skype sessions with investors were useful, we got so much more done faster when we booked a trip. A couple we booked with just a few meetings lined up but because we had the trips booked, more meetings fell into place quickly. Investors made time to grab a coffee, a meeting or a lunch with us because we had made the effort to come to them. As part of the DD process, our investors needed to get their heads around an Australian corporate entity and the nuances of shareholding for an Aussie subsidiary. The lead investor also engaged Australian legal advice (on top of their US advisors) to close the loop on the Aussie side of things. It was right and proper to get this done from their side but of course, it added cost, time and work involved from our side as well as theirs. Aussie and US team integration We have a team of brilliant folks in Australia continuing to build out our product and platform in addition to winning and helping our customers. One of the key things we are keen to do is make sure we operate as a global team. Several of our Aussie crew have visited Boulder already and more are on the way. The folks who have joined our team here in Boulder will be working closely with the Aussie team so it’s important they have the tools and techniques in place to maintain comms in an appropriate format – Slack, Hangouts, and our Wiki are all key parts of that comms process. We have a once a week company-wide stand up on hangouts which is not particularly formal (yet). On the upside, our Aussie team are all remote workers most of the time so as a company, we are already well set up and structured to manage the wider “remoteness” of Australia versus Boulder. Accounting Consolidated accounts – we have ledgers for our Aussie and US entities – you will need a good accountant to get the process in place to manage the multicurrency budgeting and reporting. Trust me, it’s worth getting this stuff right early. yOu will have better business insights and an easier time talking to investors etc. Personal Stuff Visa’s Gotta get em. We considered L1 and E3 visas and settled on the E3 as this fits what we needed. The process was actually pretty straightforward in the end but does require lots of paperwork so be prepared to get this stuff done. You can pay lawyers to do it for you but we found that with the helpful and comprehensive advice found on Geoff McQueen’s blog https://geoffmcqueen.com/2011/09/28/e-3-visa-for-australians-how-to/ that it was cost effective to get this done ourselves. Seriously, if you know or meet Geoff, buy him a beer for me. The interview was simple at the consulate. No stress. Family visas? More paperwork and form filling in but the process again was less hard than time-consuming. The interview process for my wife and kids was also really simple. Social Security More forms and standing in line – I have one now but my wife and kids cannot get them as they are on E3D visas Driving Licence Get one – you will only have a certain number of days driving on the Aussie licence before it is illegal. I believe each state is different. I sat my Learners permit here then booked my driving test the very next day. The test was much easier than an Aussie test – no three point turns or reverse parking! You will need a valid proof of address like a bank statement and you I94. Bank Accounts You can open one at a bank like Chase with a rental agreement to show your address. Wells Fargo was a little more lenient on what constitutes a valid address. Interestingly you could open a Wells Fargo account then walk over the road to Chase and open an account with a Wells Fargo letter as your proof of address. Rentals You will need a reference or additional deposit to get this up and running. We managed to get by with references from our investors and a new local director appointed to our board. Removalists We packed up the gear from our Aussie home and expect two months before it arrives. Furniture rentals are the go in the interim. The Bloody dog – yes it’s easy to get the dog into the US. Just a rabies shot and a vet report and you are away. United can ship the dog as excess baggage if you are not doing an extra hop. As we had to make an extra hop to Denver, our dog overnighted at LAX so it got pricey and complicated! Power Lots of appliances won’t work on US power due to frequency of the power and the 110 vs 240V – Kitchen appliances like blenders etc will have problems. Many modern TVs are ok on both 110 or 240V. An easy tip to avoid plug problems; pliers can bend / straighten any Aussie two-prong power plugs gently into a US shape. Not that I would advise this of course as it may not comply with some form of electrical regulation?? Mobile phones No credit rating here means a $400 deposit on Verizon per handset. We checked with several carriers and found that none would provide a sim only plan for the handsets we brought from Australia – apparently not compatible with the US cellular network. As more comes to mind I will update this post. Otherwise, I hope the above helps you on your journey and if I can help you further, do let me know in your comments on this post."
"282","2016-05-17","2023-03-24","https://www.section.io/blog/techstars-was-it-worth-it/","We are 24 hours away from the end of 13 weeks, on the 10th birthday, of Boulder’s Techstars program. Demo day is tomorrow. Demo day is the culmination of 13 weeks of company acceleration on the Techstars program. The question I am being asked most often at the moment, by investors, partners, employees and friends is; “was Techstars worth it?”. Let me walk through a bit of background here before I answer that question directly (well sort of). Some Background Section was previously based in Sydney Australia and, while serving customers from all around the world on our global platform, our sales had been focussed primarily on the Australian market. By world standards, the Australian market for Internet technologies is rather small. So we committed to move our head office to Boulder Colorado; a perfect place to build a tech business for many reasons which I will be blogging about shortly. As we investigated Boulder, we were introduced to Techstars and were highly impressed with the Techstars team and the program. Following a series of sessions with the Techstars team, we were fortunate enough to be offered a place in the 10th annual Techstars Boulder program. As a company with a team of 10, quality customers, revenue and a solid product, we figured we were probably further along than most companies who entered Techstars. We held some concerns over what it would mean for us to enter the program. Would the program provide us the right type of acceleration given we are more sales and marketing focussed at present than product focussed? Would Techstars prove a distraction to “getting stuff done”? What I Found at Techstars My initial impressions of the Techstars program were mixed. I was pleasantly surprised by the experience of being forced to work “on” our business rather than “in” our business for the first couple of weeks. There are many things to wrap your hands around as a founder and it is very easy to lose yourself in the forest. By committing to the Techstars program, our team were immediately prepared to step up and support Dan (co-founder) and myself as we were taken away from many of the daily Section tasks. Working on the business again has been an invaluable take away. On the other hand, as a somewhat reserved Aussie, the pump it up team building exercises and group check ins were not exactly what I had in mind as a good use of our time building a business. As we progressed through the program I have been constantly both floored and uplifted by the generosity of the Techstars community. Through “mentor madness” week we met hundreds of awesome mentors who provided opinions, criticisms and encouragement. Subsequently we have been fortunate enough to be helped by a number of these mentors through further introductions and assistance. These mentors are going out of their way to help despite not receiving anything or the prospect of anything in return. Techstars says their motto is “Give First” and of course as a sceptical Australian, I thought “sure, we’ll see about that”. I was wrong. We have been helped. Now I want to give first too. Some of the companies who joined Techstars with our cohort were super early; no product let alone customers. Some are as far along as Section. About 3 weeks in, as we looked about, I figured there were a few companies who would be lucky to survive to the end of the program based on the quality of the idea they had and the stage they were up to. We had a few teams with founders leave in the middle and multiple pivots. However, by the end of the program, as the pitches came together around the restructured teams and the revised ideas, I was amazed at the quality of the outcomes. At Section, we have revised and honed our go to market messaging on the back of the Techstars experience. I don’t expect we would have been able to gather as much high quality feedback as quickly in any other environment. The companies who have pivoted have developed some awesome ideas in quality markets. Lots of execution to come, but I am in awe of the teams and their Techstars advisors/mentors who have been able to bring together such sound pivots in such a short amount of time. Demo Practice Q&A Yesterday we presented our Demo pitches to about 100 folks from the Boulder New Tech Group as a bit of a practice run for tomorrow. We pitched and then opened up to the audience to ask questions. After walking through Section and describing the benefits to organisations of working with a CDN which is built specifically to support agile development workflows we had a question from the audience – “How much faster is Section?”. I answered by providing a range – our customers generally run between 20-30% faster than other CDNs. We have customers running 10% faster and some 85% faster. Section provides a highly powerful and flexible tool which engineers can use to squeeze more speed, scalabilty and security out of their website; more than any other CDN. As the first and only CDN for agile, engineers can test and iterate with the Section CDN locally to push their application and the CDN to the max. Following the session, as we discussed this question and my response, my co-founder Dan sagely noted that the question asked, while reasonable, was actually the wrong question. What should have been asked was “How much faster can I make my website with Section?” The Wrong Question Similarly, the question “was it worth it” with respect to our Techstars experience may well be the wrong question. Perhaps we should ask ourselves “can you make Techstars worth it for your business?” While Demo day is the culmination of the 13 week program, it’s not, as I see it, the end of the program. There are pieces we have taken from the program which we are building into our daily and weekly business practices. There are introductions to mentors, customers, partners and investors from the program which we will continue to develop. Some of these introductions have the capacity to radically lift our business. Despite the fact that we arrived here as just two Aussie blokes parachuting into the Boulder, we now have a super supportive community around us. Colleagues, advisors and friends. The Right Answer I believe every business regardless of stage has an opportunity to be accelerated. Techstars have provided an awesome platform for the acceleration of our business. Now, it is our job to make sure that every bit of Techstars was, and will be, worth it. My TLDR List of Techstars Good Stuff (No Particular Order) Quality of Mentors Insights of the Techstars Directors Introductions Structured Program of Acceleration Incredible Network Beer Keg in Office Free Rent for 13 Weeks Give First (It’s Real) Classmates Perks Quality and Commitment of Associates Sponsors Sponsored Dinners & Lunches Demo Day Pitch Learnings Agile Methodologies"
"283","2016-09-09","2023-03-24","https://www.section.io/blog/colorado-life/","A New Corporate Objective A few months ago, my co founder and CTO, Dan, declared a need to climb Longs Peak here in Colorado. Why? “because it’s there” (that classic Mountain climbers’ cry). It turns out that during his morning drive into our Boulder office from Lousiville, every day since we moved to Colorado from Australia, Longs was standing tall and taunting on his skyline. I joined Dan on the journey to build Section from the ground up so of course I had to jump in on the journey up Longs. These days however, it is not just Dan and me on this journey with Section, we have an awesome established team in Australia and now a new team in the USA who have joined us for the ride. Unfortunately, our Aussie team are just a bit far away to tackle Colorado peaks but our new USA crew were keen. Very quickly, had a new major item on our corporate objectives planning board “Climb a Colorado 14er”. Longs Peak Vs Mt Bierstadt Longs Peak is one of Colorado’s 58 14ers. A peak which rises 14,259 feet above sea level. There are no peaks in Colorado which rise above 15,000 feet so climbing a 14er is what it’s all about. They range in difficulty with Longs rated as the 14th most difficult. As we researched Longs we found a recurring theme; risk of fatality. Ouch. Not good for corporate growth I’d expect. We also discovered that in addition to the risks involved, Longs is a big commit on the day to a very long and very physical walk. With just a few months to get in shape for a walk before the 14er walking season becomes more of a snowshoe shuffle than a hike, we figured we should tackle a peak within the realms of our immediately prospective capabilities. So we settled on Mt Bierstadt as a prelim 14er. While still a 14er and presenting the normal 14er challenges of altitude and elevation, Mt Bierstadt is a shorter and less technical hike than Longs. Preparation As an agile organisation, our daily stand ups around the objectives board include the usual elements such as sales, product and marketing. With the 14er on the table, stand up focus now also included a 14er. Our corporate objective was to climb Bierstadt as a team. Secondary objectives; a healthier and happier team and a chance to spend quality time together outside of the office (and Boulder’s bars). We checked in on our training progress with each stand up. I’d like to say we all scored a tick on training all the time but life gets in the way. We did find that on average, the team became more active than was previously the case. More walks, longer walks and a fitter, healthier team. Even if we did not make the top of the 14er, already this was a really positive outcome. As the day approached for the climb we sorted out gear required and travel arrangements and then organised ourselves with a few more post it notes on the “to do” board. Of course, we discussed the age old rule of not bringing any new gear on an expedition and of course, folks nipped out to REI on the night before for a brand new jacket, pants and a set of walking poles. Was there ever any doubt! Pre Game Jitters Packs packed and travel arrangements made, some of us (as software folks) had to over engineer the process and critically study the impending weather conditions; 49% chance of thunderstorms before midday. Standing on a 14er in a thunderstorms is not a recommended practice (see earlier mention of fatality issues) so weather is a major player on whether you make it to the summit on any given day. With some more calculation and an entrepreneurial spirit, we figured we had a 51% chance there would be no thunderstorms and that the standard deviation of the error rate in forecasts would provide a margin for a window of success. Then we discussed taking a lean approach on letting the weather affect the go /no go. Finally, we figured we would just turn up and wind down the window. It seems the first challenge on the day of the hike with many 14ers is getting to the parking lot early enough to secure a spot! We were scheduled for a 7am departure and found the lot nearly full. Gotta get going early. Go Time As it turned out, were… lucky / fortunate / made our own luck … as, despite a little early rain, our walk was perfect. We kept a steady pace together all the way to the top and, just as we scrambled over the final boulders and crags to the summit, the clouds parted to provide us with a crisp, expansive, 14,000 foot Colorado vista. Beautiful. What better way to celebrate the summiting of a 14er than by hunkering down out of the wind and occasional bursts of snow to hand around Vegemite sambos. Bloody good stuff that. 3 hours up and 2.5 hours down. Thank goodness we had packed a few of Colorado’s finest brews in the cooler for a carpark celebration. What’s Next Bierstadt was the opening salvo in our campaign on Longs Peak. We have ticked one 14er off the list but Longs still beckons. Next season… Most importantly however, the process kicked off by our CTO was the opening salvo in the USA in our search for an office culture of healthy, happy and balanced people. A culture we fostered in Australia initially with our remote office workplace allowing our team to spend circa 2 hours less each day travelling to and from a central Sydney office. We love turning up to work at Section every day and hanging out with good folks here in the USA and back in Australia. People who are happy to get out of their comfort zones, aren’t afraid of a bit of hard work and can have a laugh in the process. After all, we reckon there is no such thing as achieving a work / life balance. We reckon it’s all life. What’s up next Aussie and USA teams?"
"284","2016-12-01","2023-03-24","https://www.section.io/blog/sectionio-partner-program/","Section works with digital agencies, development shops, and hosting services to improve website performance and security Section is pleased to announce that we are launching the Section Partner Program. This program will make it easy for digital agencies, hosting services, and development shops to bring Section’s website performance and security tools to their clients’ websites, while giving partners themselves marketing and revenue benefits. In September, we announced our flagship strategic partnership with leading ecommerce platform Magento. Building on this, we are excited to make it simple for Magento agencies and developers to partner with Section to speed up clients’ websites, make them more secure, and improve their conversion rates. Section also welcomes partners focused on other ecommerce or website platforms, and hosting agencies who are looking to increase customer experience by adding performance and security features. Some of the benefits you’ll get if you join Section’s partnership program include: A quick way to speed up your client websites by providing them with a global CDN and Varnish Cache, a best-in-class website acceleration tool. Improved security of client websites with included HTTPS certificates and HTTP/2, and the option of adding an advanced Web Application Firewall. Referral fees and/or monthly ongoing revenue opportunities for each registered account you bring to Section Collaboration on marketing and sales materials, including the opportunity to post blogs on the Section website, be provided with Section-authored content, and co-brand whitepapers or webinars. Technical training for your engineers on how to get the most out of Section, Varnish Cache, and ModSecurity. Several companies have already joined the Section Partner Program, including OnlyGrowth, who specializes in Shopify websites and are Shopify Plus Experts, Trellis, which provides full-service ecommerce and CMS services, Magento-focused agency Wagento, and Magento development agencies WeltPixel and Customer Paradigm. Brent Peterson of Wagento is pleased to partner with Section because “At Wagento we understand the importance of good website performance for user experience, customer satisfaction, SEO and much more. Section’s platform makes it easy for our developers to add Section’s security and performance tools onto our client websites and gives us more features to discuss with prospective clients.” For more details on the Section partner program, download the partner information pack, or contact us to be put in touch with our partner team."
"285","2016-07-20","2023-03-24","https://www.section.io/blog/seed-funding-round/","I am very pleased to confirm that Section has just closed our $1.5m seed funding round. I have been very fortunate over the past few months to have experienced the process of meeting with some great venture capital groups as we worked towards closing our seed round. As a graduate from the Techstars Boulder program, we are pleased to welcome Techstars Ventures onto our Cap table joining a number of other great funds such as Tahoma Ventures, Blue Note Ventures, Galvanize Ventures, PV Ventures, Venture Blue and various other incredibly astute and experienced angel investors. Not all of the excellent VCs I have been working with over the last few months made it onto our cap table in the end but I am very thankful for their advisory and feedback. This raise we have pulled together will provide capital to drive growth. The Section product is awesome. Our engineering team have created an amazing new way for web businesses to work with reverse proxy servers (the core tech backing Content Delivery Networks). Never before have agile developers been able to work with CDNs with such control and flexibility. For agile development teams to have immediate access to their choice of reverse proxies (from our library of reverse proxy servers) in their development and test environments and on a globally distributed platform is a new paradigm for CDNs and more specifically for reverse proxy server management. Throw in free TLS/SSL certificates, free Global Anycast DNS and a robust, real time (but open) metrics and logs platform and you have a tool set built to finally empower engineers to control, manage and fully exploit a reverse proxy platform (or CDN) in front of their website. Now, putting our seed capital to work, we are committed to tell market why Section is a better way to secure, scale and speed up all web applications. Jim Franklin has been generous with his time and expertise and has joined our corporate board. Jim brings with him tremendous experience as CEO of Sendgrid where, in a similar fashion, engineers were introduced to a new and better way of working; in that case pertaining to email management. Thanks to Mike Platt and team at Cooley who have provided support and advisory throughout our raise process, patiently guiding an Aussie through the US raise process and required legals and paperwork. With our team based in Boulder, Colorado and Sydney, Australia, we are loving the opportunity to serve a global market and continue delivering against our core mission; To improve the Internet by empowering modern engineers to fully exploit reverse proxy server technology."
"286","2016-04-09","2023-03-24","https://www.section.io/blog/magneto-live-conference-and-magento-2/","Source - http://sphero.com The Section team are heading to Vegas on Sunday to get set for the Magento Imagine Conference and we are pumped. Magento 2 is out and loving Varnish Cache. What better Varnish Cache to run in front of your Magento 2 website than our globally distributed CDN; the only CDN on the planet with full developer workflow integration out of the box. Our team will be in the reception as you are signing in and floating around the conference. We also have a Sphero BB-8 giveaway on the go so make sure you grab us to say hi! Check out Magento’s view of using Varnish Cache with your Magento 2 application; ""We strongly recommend you use Varnish Cache in production. The built-in full-page caching (to either the file system or database) is much slower than Varnish Cache, and Varnish Cache is designed to accelerate HTTP traffic."" And Magento’s recommended topology: ![Magento 2 Varnish Cache Topology](/assets/images/magento2_varnish _topology.jpg) Source - http://devdocs.magento.com/guides/v2.0/config-guide/varnish/config-varnish.html The great news is that with Section, all the heavy lifting is already done. Within minutes, out of the box and ready to go you have a global Varnish Cache solution with: Scalability High Availability Varnish Cache redundancy System Tuning Real Time Logs Metrics Developer Workflow Integration. Want a Sphero BB-8?? We will be giving away 20 of Sphero’s cool BB-8 droids at the conference so come and see a Section crew member at the conference or keep your eye out for Section stickers for your chance to scoop up a cool new toy. More Info Check out some more info on the Section Magento 2 story and let us know if you have any questions."
"287","2016-09-22","2023-03-24","https://www.section.io/blog/announcing-community-section-cdn-content-delivery/","We’re pleased the introduce the Section Community Forum, where Section users and those looking for information about Content Delivery Networks, Varnish Cache, and general website performance and security enhancements can post and answer questions. Anyone can read questions by others on the Community Forum, and to ask your own questions sign up for a free Section account. Get your Edge questions answered Along with this blog’s Edge posts, the Community Forum is a great place to learn about edge computing’s benefits and setup. Here’s an answer on if having a greater number of server points of presence (PoPs) is better for your website performance or not. Learn about improving website performance and website security If you’re unsure where to start when learning about improving your page speed and website security, ask the experts in the Section community. When you come across new phrases or metrics, such as Time to Fist Byte, and need to know what they mean and if they matter to you, this forum is here to help. Ask experts about Varnish Cache implementation We know installing and configuring Varnish Cache for your site can be complicated, and with leading platforms such as Magento strongly recommending the use of Varnish Cache for better performance and scalability, more and more websites are having to set it up. The Section technical team are experts in Varnish Cache and configuration and VCL. Improve your website performance, security, and scalability with Section At Section we’re committed to being a resource on all things performance, scalability, and security, so we encourage you to visit the Community Forum and ask any questions you may have about our platform and website enhancements in general. To get an easy-to-setup, globally distributed website performance, scalability, and security solution that integrates with developer workflows, get started for free."
"288","2016-06-10","2023-03-24","https://www.section.io/blog/my-section.io-anniversary/","As at today I’ve been working with Section for two years. When I joined the company it provided a fully-managed service to its clients. For many this was the ideal arrangement: by leaving web performance to the people with the skills and the tools our customers were free to focus on building new features. My professional background before I joined Section involved providing consulting services in the Application Lifecycle Management space. This included agile training and tooling and helping teams adopt Continuous Delivery and Infrastructure-as-Code practices and to realise the benefits. Wrong way, go back It was clear with my past experience that the fully-managed service approach was in conflict with the agile practices I had encouraged in previous teams and were being used internally at Section. Our fully-managed service was only in front of our customers’ production websites and rarely one pre-production test environment too. Much of the service was a black box to the customers, not intentionally, but the way the system worked wasn’t immediately visible to those dependent upon it. We would always happily share details of the configuration, it wasn’t a secret but it became a barrier. Even for the few customers who had opted to run our service in front of their pre-production, this was far too late in the delivery cycle to be discovering bugs due to assumptions made about every browser request reaching the origin web server during development - an assumption that could not be more wrong behind a caching service. In fact, these are the kind of bugs that are often best fixed through a different design, and changing the design when you reach pre-production, or worse production, is very expensive. But it wasn’t just about assumptions made at the origin. Our own process of configuring the Section platform to achieve the best results for a customer’s website required us to analyse and essentially reverse-engineer the intent of various requests just from the network traffic and browser interactions. And while the Section team know web performance well, we just can’t know each customer’s website as well as the people building it. Empower the users The customer knows best what their site it supposed to do and we want to enable them to ensure their site is fast by giving them the tools to when it takes too long, and where it takes too long, and why, and help them to change both their origin website behaviour and their CDN configuration to work together for the best possible outcome. By combining the tools and the data offered by Section with agile development practices, great web performance can be built-in to a website from the beginning instead of tacked-on at the end. With the ability to work with the CDN during the early stages of development, our customers can go beyond just caching static resources and realize the wins of offloading the dynamic HTML resources too. This is what Section is providing today and will continue to improve upon and it makes me very happy to working for this company, knowing that it is solving one of the web’s biggest challenges to shipping quality websites frequently - a CDN market stuck with Production-only deployments, poor visibility, and slow to respond to change."
"289","2016-04-19","2023-03-24","https://www.section.io/blog/magento-imagine-our-top-six-presentations/","Section was proud to sponsor a small part of Magento Imagine 2016. Our goal in sponsoring the event was two-fold - we want to connect with Magento users and we want to make sure that Section’s systems are the ideal Magento CDN. Three people from Section attended Magento Imagine in Las Vegas, two from our Syndey Australia office and one from our Boulder Colorado office. I asked the guys to let me know what they thought the best three presentations they saw. Here are our thoughts one week after the event: Stewart McGrath, CEO. @stewmcgrath Magic Johnson Keynote. “What a personality. Clearly a motivated and motivating person, the Magic Johnson brand is really pushing the envelope. I’m not a Lakers fan, in fact not even a (basketball fan truth be told) but Magic was fantastic to see.” Fully Stacked: Less Oops, More Ops for Magento Development by Mathew Beane - Director of Systems Engineering, Robofirm “Mathew’s exploration of the implementation of working with a DevOps stack for Magento was excellent. Clearly the driving forces of software development (agile, DevOps, Continuous Integration and Continous Delivery) have been embraced by the Robofirm team. Great session.” General Session and Keynote Day 2 “The Release of the Magento 2 platform built by the platform.sh team was a major shift for the Magento platform. Platform.sh have built an excellent DevOps focused toolset for Magento 2 users.” Matt Johnson, VP Delivery Services Magento 2 Developer Deep Dive “Great technical journey into fundamentals of developing for Magento 2” Breakout Sessions I - Magento Enterprise Cloud Edition – A Platform-as-a-Service for Your Business “Launch of Magento cloud. We think it could have used a more developer-centric CDN but platform wise incredible to see branching and environments in Magento 2” Fully Stacked: Less Oops, More Ops for Magento Development “Awesome to see usage of docker and resolution of common development workflow issues targetted at Magento 2” Wrapping Up We are so pleased to see the maturity of the Magento ecosystem progress from a skin/theme/plugin model into a rich developer-centric environment. It was these drivers that led us to build technology into our platform to make Section a great Magento CDN. If you’d like to know more about why Section is set apart from other CDN choices for Magento, then sign up, join a weekly webinar, or contact our engineering team."
"290","2022-12-08","2023-03-24","https://www.section.io/blog/q4-ecommerce/","I have a love/hate relationship with Q4. Before I joined the team at Section, I spent over a decade living and dying by the success or failures my company experienced during what we would call “peak season.” For most of us, “peak season” means the week preceding Black Friday through mid-January when post-holiday gift cards would be spent. I loved this time of year because it represented a culmination of 9 months of effort, from traditional marketing to streamlined supply chain to the “witchcraft” one encounters when a talented digital marketer brings ideal customers into your funnel while boosting your SEO ranking and overall engagement. The pot of gold at the end of all of this effort could simply be distilled down to “conversions.” The “hate” part is different. More often than not the most applicable term would be “winner’s remorse.” Despite the success you encounter during Q4, you often find yourself during the last two weeks of January licking your wounds over the conversions that could have been. I’ve worked for both startups and large enterprise and more often than not, found that headless commerce was the best way to satisfy the most pressing needs for the business. Flexibility. Freedom to create and iterate on an incredible customer experience. And most importantly, the ability to supercharge a store so that it performs at the highest level. The love comes from these things. The hate comes from the myriad challenges and multiple vendors/platforms needed to create optimum performance, and beyond that, the challenges with the API layer of your headless commerce architecture that can seem impossible to overcome. And so… I was pleasantly surprised when I joined the Section team and learned that not only could deploying containers globally across a federation of infrastructure partners help any company with the efficiency of their stack, but that this technology could solve multiple problems for a company that seeks to create an amazing experience via headless commerce. Let’s step back for a moment and look at what this means. If you’re part of the ecommerce world you already get it. You need to serve customers in multiple ways via what the industry calls the “front end.” In the Internet of Everything, customers can order your product via the web, mobile, social, and voice-controlled connected devices in their home. In conjunction with this, you work hard on a daily basis to ensure they can message you, engage customer service, or consume the content you provide and convert that into a brick and mortar retail experience. On the “back end,” you have the operational aspect of your business. Inventory and supply chain. Your cart so that customers can easily and simply check out. You have a payment processor, a catalogue to manage, constant updates to pricing, promotions to support, and then add in the internationalization component. All of this matters without considering the impending doom of a possible ddos attack, bot invasions, and the myriad challenges that come with ensuring that all your customers’ data is safe. And then there is the “middle layer,” comprised of API’s that connect front to back and enable your company to serve all of this to your customer as quickly and efficiently as possible. The more complex your headless setup, the more complex this becomes. So what does this have to do with Section? When I arrived here I had no idea. But then I saw the light. It took less than a day. I quickly learned by looking at what successful ecommerce companies were doing to leverage our technology. The answer to so many problems I’d encountered in my previous lives became simple. Anything that lives in a container can be improved with Section’s platform. I looked at a company that has experienced significant growth and needed to scale quickly. They deployed a nuxt.js app for their front end on a global scale, anywhere they need it. Doing so was easy. They combined that with image optimization, web application firewall, bot mitigation, an intelligent caching layer via Varnish that helped to enable localization, and then I learned of the coup de gras - their work with our engineers to deploy GraphQL endpoints globally so that they could dramatically enhance the performance between front and back end. It was eye opening, and I immediately found myself again with “winner’s remorse” over the fact I’d not leveraged a wholistic solution like this in my previous lives. My story had always been “too many vendors, so much budget to manage.” This customer was only one example of the many who are leveraging Section technology to address multiple needs for their business. If you’re seeking a solution that allows you to work with a single vendor while deploying any containerized aspect of your headless commerce architecture as close to your customers as possible, I can speak from personal experience. Be it your presentation layer/front end, your API layer, or microservices on the back end, there is an incredible opportunity to improve your customers’ experience while efficiently managing cost under a single provider. The platform is risk-averse. Try it for free, or reach out and let us know what you’re looking to accomplish."
"291","2022-11-17","2023-03-24","https://www.section.io/blog/distributing-graphql/","Recent reports have found that as much as 83% of all web traffic today is the result of API calls. Although REST APIs are still widely used, GraphQL is a flexible and efficient alternative query language and server-side runtime for cloud APIs – and its usage has skyrocketed from 6% in 2016 to 47% in 2020. While its advantages are significant, many developers become frustrated with latency challenges when implementing GraphQL. The nature of the GraphQL structure means that caching responses for improved performance can be a significant challenge (you may have heard someone say “GraphQL breaks caching” or “GraphQL is not cacheable”). The secret to making GraphQL more efficient is distributing those GraphQL API servers so they operate – only and always – closer to end users, where and when needed. If you can reduce last-mile latency between your users and your code, they will see an immediate boost in their user experience. This is why we recently launched a Distributed GraphQL service that allows organizations to quickly launch and easily scale location-optimized, multi-cloud API servers. With this new service, organizations can host GraphQL in data centers across town or around the world to improve API performance and reliability, lower costs, decrease impact on back-end servers, and improve scalability, resilience, compliance, security and more. And they can do all of this without impacting their current cloud native development processes or tools. Section Co-Founder & CTO Daniel Bartholomew recently contributed to technology journalist Adrian Bridgwater’s recurring “API series” column for the Computer Weekly Developer Network. In Bridgwater’s column, Bartholomew goes into more detail around the ‘why’ and ‘how’ organizations would want to distribute GraphQL servers. In this article, Bartholomew also explores some of the challenges associated with GraphQL when trying to connect data across a distributed architecture. Read more about how to solve GraphQL latency challenges by deploying your application closer to your users."
"292","2022-08-09","2023-03-24","https://www.section.io/blog/thoughts-on-it-skills-with-zdnet/","Joe McKendrick recently wrote in ZDNet how IT roles are being impacted by the growing demand for skills related to emerging technologies such as AI, machine learning and cloud-based services. While coding skills remain in high demand, companies today are looking for more from their technology managers and professionals. As a result, there’s an increasing need for IT pros with skills in project planning, business use case analysis, architecture and design. Given the exponential growth in Kubernetes adoption in recent years, the natural inclination would be that Kubernetes skills are among the most in-demand for IT professionals today. However, soaring Kubernetes adoption has also led to the emergence of more fully-managed Kubernetes solutions from hyperscalers, edge computing platforms, and managed service providers. The maturation of these managed solutions has abstracted much of the low-level operations work, where the skills required to effectively run Kubernetes clusters have become more specialized among specific tooling, such as Helm, Prometheus Operators, CI/CD tools, and so on. While a solid understanding of Kubernetes is still important, Section Co-Founder & CTO Daniel Bartholomew shared with McKendrick that: “Containerization – the dev side of DevOps – is critical. Being able to build and integrate microservices into DevOps lifecycles with critical components like automatic feature rollouts with zero downtime and container health checks is a skill that has more versatility across organizations who are using Kubernetes in production today. At the same time, the demand for security specialists continues to grow, particularly those with experience hardening Kubernetes environments. These roles require a broad knowledge of security and systems coupled with a deep understanding of containers and Kubernetes.” You can read the rest of McKendrick’s article here. University students in computer science related fields of study interested in connecting and enriching their skills as they enter the workforce should be sure to check out Section’s Engineering Education Program."
"293","2019-06-06","2023-03-24","https://www.section.io/blog/conde-naste-international-kubecon-keynote-distributed-kubernetes/","At KubeCon + Cloud Native Conference Europe 2019 last month, one of the most interesting keynotes came from Katie Gamanji, Cloud Platform Engineer at Condé Naste International. With well-known brands like Wired, Vogue, GQ and Vanity Fair under its umbrella, the prestigious print and digital publishing company operates in over 12 markets, including the U.S., Europe, China and Russia. Its 62 websites generate over 300 million unique users each month with 1.5 billion monthly page views. The company previously allowed its different global territories to operate independently, which had led to a fragmented technical strategy across the company’s international markets. Gamanji joined the company a year ago to be part of a far-reaching project focused on the creation of a centralized platform intended to “embrace cloud native principles” and “further emancipate our international teams”. In her talk, Gamanji summarized her team’s journey towards deploying a unified platform globally. Her talk particularly resonated with our team because of the many parallels between their architectural decisions and how we’ve designed the Section platform to help development teams achieve similar objectives. The Condé Naste International Journey Condé Naste International have been on a challenging journey towards delivering a multi-cluster distributed Kubernetes platform with a centralized management mechanism and self-service CI/CD process on a global basis. In her keynote, Gamanji covered the problems they were previously facing from a fragmented technological landscape, how they focused on building a solution for this and the resulting tech stack, in addition to challenges they continue to face. A Fragmented Technological Landscape Initially, the global market teams at Condé Naste acted like independent business units. From an engineering point of view, this meant that each market had an individualized tech stack with few shared components. It also led to fragmented design and visuals, which compromised the unified experience the publishing company wanted to be able to offer their worldwide customers. China and Russia represent 17% of Condé Naste’s total digital readership and present unique content delivery challenges, spotlighting the need for the company to adopt a cloud agnostic infrastructure, and the benefits of deploying a self-hosted Kubernetes solution. Issues around origin latency was another reason why Condé Naste began its journey towards global distribution of its servers. “What we aim for”, Gamanji said, is “market proximity and the highest user experience for our customers.” As such, it was a business decision to replicate its clusters in five different regions, making a total of nine worldwide to avoid latency for the end user. A Unified, Cloud Native Platform Two years ago, Condé Naste International began its journey towards building a unified platform across its multiple regions, embracing cloud native principles throughout. As with the Section platform, Kubernetes underpins the entire infrastructure. A Tectonic installer is used for their cluster deployment, which allows them to operate a self-hosted control plane for Kubernetes in addition to being able to plug it into numerous cloud providers to consume compute and networking resources. Similarly to Section, this allows Condé Naste International to avoid vendor lock-in while still deploying self-hosted multi-master, multi-node 100% komposers on Kubernetes. Condé Naste International currently operates more than 100 instances in AWS, its cloud provider. Gamanji explained that they make use of auto scaling groups to “make sure that we scale up and down based on interaction with our customers with our brands”. Their infrastructure is deployed using Terraform. Like Section, they use a wide range of open source tools to provide authentication, log-in, monitoring and other tasks; one such example is Helm, their de facto deployment package manager to Kubernetes. The new platform has been live since October of last year when Condé Naste International first launched GQ France. Since then, another ten websites have been launched with the overall goal being to migrate 34 websites. Empowering Developers Similarly to our goal at Section, Condé Naste International’s engineering team was focused on becoming a developer-first company, which empowers its local and international teams through the new unified platform. Gamanji discussed how having a self-service continuous integration/continuous delivery (CI/CD) process was an important step towards achieving this. We have likewise found that CI/CD pipelines are useful to help manage environments that have risk attached by breaking changes into small, verifiable units. This reduces the possibility of a major software flaw being implemented to an entire system, and helps to encourage a more responsive, quicker-time-to-market across the entire software delivery cycle, giving developers the flexibility to quickly, yet safely test out their code in production. Continued Challenges for Condé Naste International Gamanji finished her talk by highlighting some of the challenges they continue to face. These include service continuity; when migrating services to a new platform, the engineers need to maintain availability of content with minimal or zero disruption to users. Condé Naste International’s team have found that the best way to do this is to use Varnish configuration files, allowing them to implement changes to the cache with immediate effect. Gamanji also admitted that upgrades are a continued “pain point”. This is the result of their having a self-hosted solution for Kubernetes, she explained, which doesn’t allow them to do upgrades in an automated, sophisticated way. In particular, Gamanji mentioned the challenges of deploying infrastructure into China and Russia. Looking ahead, her team wants to introduce tracing, observability and service meshes into their clusters, an objective that the Section platform team is actively focused on. She concluded by attributing her team’s success thus far in creating a cluster with a centralized management mechanism, which is globally distributed, to their “solid, well-defined CI/CD process that further empowers our local and international teams and their product”. How Section Helps Development Teams Achieve Similar Outcomes For those companies that may not have the internal resources to build a similar solution or indeed want to devote the time and finances towards it, Section’s Edge Compute Platform provides approachable tooling to manage a distributed microservices architecture. The Section platform allows developers to focus on application development while we concentrate on ensuring that your workloads run in the most sensible locations to optimize performance, security, and scalability objectives. Browse our case studies to see how multiple industries, from testing to gaming to sales, are working with Section to gain control over their workflows and make edge programming a reality. The full presentation can be viewed here: https://www.youtube.com/watch?v=D7pbISekc8g&feature=youtu.be Presentation slide deck: https://static.sched.com/hosted_files/kccnceu19/c0/Katie%20Gamanji%20May%2022.pdf"
"294","2019-01-29","2023-03-24","https://www.section.io/blog/debug-headers-best-practices/","Debug headers are a valuable tool for understanding what is happening with content on a webpage so you can make informed decisions in your performance optimization efforts. In this article, we will look at how to set up and interpret debug headers and provide best practices for implementing them on the Section platform. First off, however, let’s review what response and request headers are and how to read them. Chrome Developer Tools provide information on how the browser is interacting with a particular application or website. These tools help you assess performance and debug certain aspects of the page load by allowing you to see how a website is built, where its content is served from, its page load times, and so on. Response and Request Headers Response and request headers provide information on where a request is being served from, in addition to further data such as whether it was served from a cache or a Content Delivery Network (CDN). Traditionally, almost all response and request headers will give you information linked to the cache status of an object, in addition to where that object is being served from (in multi-tiered CDNs). Critical information that is normally provided includes cache state, cache location, cache key, and transaction ID. All of this information helps in determining how to debug request headers. Within Chrome DevTools, receiving and viewing the debug headers is relatively straightforward; however, knowing how to interpret the information returned in the response can be rather complex. Sometimes, single characters can provide a host of data that can be interpreted only by referring to particular Internet specifications; in other instances, a detailed CDN guide may be needed to break down what each header specifically means. The response headers tell you how the web server or CDN is returning each HTTP request to the browser. Each header has a unique request even if they share the same server; for instance, Section assigns a unique ID under section-io-id to each request; Varnish Cache also gives each request its own identifier using x-varnish. The response headers tell you if the request was a cache hit or cache miss; a cache hit will be labeled x-cache: Hit. You can also determine the cached age of the requested item from a response header, in addition to the time of the last modification and the content type, such as image (png) or text (HTML). The request headers are those sent by the browser when it first initiates a connection to your remote address. They provide information on how the request was made, such as browser type, operating system, IP address of the issuing computer, etc. Request headers can help you identify whether or not your web application is having challenges with particular operating systems and identify potential solutions, such as performing redirects in response to the device type or setting new caching rules. Benefits of Debug Headers Debug headers help to provide a deeper understanding (at a granular level) of where and how specific objects are being cached, so you can troubleshoot, define an optimal debugging strategy, and optimize content delivery. They also let you spot check what’s happening with newly published files, updated features, or configuration changes. Transaction IDs are particularly helpful for this as they provide a unique string, helpful for identifying a transaction within logs. This allows you to find out exactly what happened in a specific request for a single asset and see how a specific object behaves within the entire content delivery workflow. Furthermore, debug headers allow you to see individual requests coming into the browser in real-time. By clicking on each individual request, you are able to access the headers, cookies, timing, and content of each response. Implementation Best Practices at Section While every CDN typically provides debug headers, each has their own unique take on how they deliver the information. Section’s Edge CDN services also follow a defined structure. As mentioned above, a section-io-id is assigned by default to each request that moves through the Section platform. As all requests are logged, this allows you to track down issues related to each individual request. We ensure that access to headers is secure for each customer. Some examples of these include: x-varnish: assigned by Varnish to every request x-cache: as indicated above, this is marked as hit or miss, which helps you identify whether a request has been served from cache or not. We can also tell you: Which edge server handled the request In addition to hit/miss, other information related to cache can be obtained, including cache location, hierarchical tiering and origin shield information Time to serve Time to fetch from origin The transaction ID can be used for further troubleshooting on the CDN side. The section-io-tag can be set by any edge module and the section-io-cache-tag can be set in Varnish Cache. This can be used to debug/capture all information related to the request/response. Unlike many traditional CDNs that lock configurations in a black box, Section gives developers full control to customize headers according to your choice of modules. Headers can be added, which are unique to the modules deployed on the Section stack. For instance, the Kraken-optimized header will let you know if the image has been optimized by Kraken or not. In the logs, you can track patterns across requests/responses for all indexed headers. An example of response headers for an HTML request: accept-ranges: bytes
access-control-allow-credentials: true
access-control-allow-headers: DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,XRequested-With,If-Modified-Since,Cache-Control,Content-Type
access-control-allow-methods: GET, POST, OPTIONS access-control-allow-origin: *
age: 33603
cache-control: no-store, no-cache, must-revalidate, max-age=0
content-encoding: gzip
content-length: 30291
content-type: text/html; charset=UTF-8
date: Sun, 20 Jan 2019 23:25:19 GMT
expires: -1
grace: none
pragma: no-cache
section-io-cache: Hit
section-io-id: 403514207947711548026719.492
section-visitors: 50
set-cookie: sqshowmaintenance.1479168057=false;Path=/; Expires=Mon, 21 Jan 2019 00:25:19 GMT;
status: 200
vary: Accept-Encoding
x-content-type-options: nosniff
x-frame-options: SAMEORIGIN
x-magento-cache-control: max-age=155520000, public, s-maxage=155520000
x-magento-cache-debug: HIT
x-xss-protection: 1; mode=block
 An example of response headers for an image request: accept-ranges: bytes
bytes-saved: 46631
cache-control: public, max-age=31622400
content-length: 13856
content-type: image/jpeg
date: Sun, 20 Jan 2019 23:25:20 GMT
kraken-optimized: true
kraken-size: 13856
original-size: 60487
section-io-cache: Hit
section-io-id: 4035142079477351548026720.027
section-visitors: 50
server: openresty
status: 200
via: 1.1 varnish-v4
x-varnish: 55546054 9713805
 For more guidance around the technical implementation of Section’s debug headers, see our Custom Logging Documentation."
"295","2018-10-24","2023-03-24","https://www.section.io/blog/fix-mixed-content-errors-https/","Mixed content takes place whenever initial HTML is loaded over a secure HTTPS connection while other resources (e.g. scripts, images, videos) are loaded over an unsecure HTTP connection. Due to the fact that the initial request was secure over HTTPS, and both HTTP and HTTPS need to be loaded to display the same page, this is deemed mixed content. The Benefits to HTTPS HTTPS is the secure version of HTTP, the protocol via which data is sent between your web browser and the website you are visiting. The ‘S’ stands for ‘Secure’, referencing the fact that all communications between your browser and website are encrypted. There are three key benefits to the HTTPS protocol: Website authentication Data integrity Secrecy The lock icon in the browser address bar indicates that the site you are visiting has a functional SSL certificate and all resources have been loaded securely over HTTPS. How HTTP Weakens Security When a site has mixed content, however, the security of the whole page is weakened. HTTP requests are vulnerable to man-in-the-middle attacks in which an attacker can eavesdrop on a network connection and witness or change the communication between two parties. By the time a browser reports mixed content warnings to the user, the page’s security has likely already been compromised. This scenario is fairly common, and while browsers do block all HTTP requests when a site is accessed via HTTPS, the on-page impacts are not always immediately visually apparent, which is why the browser delivers the mixed content warning alerting the developer to fix the errors in his/her application. Example of a Common Mixed Content Error There are several scenarios that can lead to mixed content errors, but the most common culprits are absolute links that point to images or files via an HTTP request (perhaps relics from an older version of the site). In this case, when the site is accessed via HTTPS, it can lead to missing images, absent styles and a lack of functionality in the dynamic content. Web browsers will issue warnings about this kind of content in the developer console, letting the user know the page has insecure resources, such as: Mixed Content: The page at 'https://example.com/' was loaded over HTTPS, but requested an insecure image 'http://image.com/image.jpg'. This content should also be served over HTTPS. How to Address Mixed Content Errors There are several options for addressing mixed content errors: You can fix the URLs embedded in the content that your origin served by replacing the http:// prefix with https:// on any absolute URLs. HTTP pages can reference HTTPS resources, but this doesn’t work the other way round. It’s also worth noting that while protocol-relative urls might sound like a good idea for ensuring that links use the same protocol as the page, it is always best to use HTTPS (even on HTTP pages) because the protocol-relative approach will result in browser-cache-misses when the site moves from a HTTP page to a HTTPS page. HTTPS is fast and free on Section. In addition to security, favoring the HTTPS resource will always have cache benefits in the browser and at Section. If you are unable to change the content served by your origin and you have a Section proxy configured that is able to insert additional response headers, you can ask web browsers to solve it themselves. If you have Varnish Cache, for example, this custom VCL can be used: sub vcl_deliver {
    if (req.http.X-Forwarded-Proto == ""https"" && !resp.http.Content-Security-Policy) {
      set resp.http.Content-Security-Policy = ""upgrade-insecure-requests"";
    }
}
 This works by adding a Content-Security-Policy: upgrade-insecure-requests header that all modern browsers will honor, thereby automatically translating any broken http:// URLs on your pages to https://. Finally, if you still have to support older browsers or devices that don’t understand the Content-Security-Policy header and you have configured the OpenResty proxy via Section, you can fix the URLs as the content passes through the Section platform with Lua script similar to this: if ngx.header[""content-type""] ~= nil and string.find(ngx.header[""content-type""], ""^text/html"") then
  local chunk, eof = gz.inflate_chunk()

  chunk = chunk:gsub(""src=\""http://"", ""src=\""https://"")

  gz.deflate_chunk(chunk)
  end"
"296","2018-10-16","2023-03-24","https://www.section.io/blog/vary-user-agent-http-response-header-varnish-cache/","The HTTP response header “Vary: User-Agent” plays an important role in how your website cache works. When set correctly in Varnish Cache, it will ensure that your visitors see the correct content, regardless of whether caching is applied or not. If set up incorrectly, however, it can have a negative impact on the caching hit rate. If you want to improve your cache hit rate and user experience, it is worth considering, but be sure to use it with great care. The Role of Vary: User-Agent If your Vary header has User-Agent, Cookie, Referer or *(wildcard) as a value, this in turn can reduce the effect of your caching system. The most commonly used value is User-Agent. It tells Dynamic caching which version of your page should be served depending on the browser type, which can prevent a cached desktop version being served to a mobile visitor. However, it has become a bit outdated since most sites today don’t serve different HTML for their mobile sites; instead CSS typically performs this function. Unless you are using a specific plugin such as WP Touch or you definitely have a difference in the HTML output of your site, you are unlikely to need to send the Vary: User-Agent header. How Vary: User-Agent Can Negatively Impact Cache Hits This is important as the use of Vary: User-Agent can negatively impact your site because it uses up resources. The User-Agent HTTP head has an extremely wide range of values, which effectively splits the cache into many different buckets and can significantly lower the chance for a cache hit. Given that there are thousands of User-Agent strings, any site with a reasonable amount of traffic will see thousands of different User-Agent strings per hour. Platform-wide, using the Section HTTP logs, we see approximately 25,000 variations of User-Agent every hour. These requests can create a resource-intensive strain and the information contained in these user-agent requests, while widely ranging, typically has no or little impact on the asset the origin generates. The User-Agent header, for example, will indicate which OS or browser version is issuing the request and different values of the header will lead to different buckets in the cache. If the Vary: User-Agent header is present, for example, the following four different User-Agent values will result in four different buckets of cache: Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36 Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36 When requesting an asset from the web server using this set of User-Agents, the same URL would be identical irrespective of the User-Agent header value. Thus, it isn’t necessary to use the User-Agent parameter as a Vary header. How to Improve the Cache Hit Rate If the content of an asset does differ based on User-Agent, to minimize cache buckets, you can bucket into similar groups; for example, a common set of buckets we see at Section is Desktop, Mobile Large Screen, Mobile Small Screen, and Other. If you were to use these buckets, the four User-Agents in the example above can all be served from the “Desktop” bucket. By decreasing the number of requests on the cache, you can increase the cache hit rate. When the Vary: User-Agent header is used correctly, we often see cache hit rate increases of 50% or more. While Vary: User-Agent can play an important role when necessary, ensure it is set up correctly to not create an overuse of your resources. By implementing it properly in the header, you can increase your overall cache hit rate, which in turn leads to a reduced load/data on origin web application servers and will improve user experience."
"297","2018-02-25","2023-03-24","https://www.section.io/blog/working-with-environment-branches/","The Section platform was built with the understanding that changes to sites are often deployed to a series of environments before being deployed to Production. We anticipated that with each set of site changes, there may also be some corresponding changes required to how Section applies caching, optimization, and security rules to the site. The Section platform handles this by using multiple branches within a single git repository per site, i.e. Section Application. Let’s use www.example.com and Varnish Cache as a demonstration. Typically, you start with a Section Application for www.example.com with a Production branch in the git repository. This branch would initially contain a varnish directory with your Varnish Cache VCL files and a section.config.json file in the repository root with details about the Production origin server. On the Section Console ‘s Overview page for the Production environment, you will see a Create environment button. Clicking this will prompt you for the name of the new environment and the name this environment domain. For this example, I’ll use “Staging” as the new environment name. Upon clicking Create a new branch named “Staging” is created in the same Section Application git repository based on the existing “Production” branch. Additionally, the section.config.json file is modified to contain a copy of the Production origin details for your new Staging environment. At this point, you can edit the Origins page in the Section Console to update the Staging configuration to refer to your Staging origin server. Then use the DNS page to find what DNS records you will need to change to make your Staging site use Section. With your Staging site now live on Section, your team can test how your new features in Staging will work with Section in front of it. You may discover, for example, that a new feature page which should be served from cache is not. You can edit the Varnish Cache VCL in your Section Staging branch to correct this and test that it now works as expected, without any impact on your Production site. When you have determined that your new Staging origin changes are ready to go to Production, you can follow your usual internal process for deployment and promote your Section configuration changes from the Section Staging environment to the Section Production environment. Promoting changes across branches in Section is currently done via client-side git tooling. You may prefer GitHub Desktop, Atlassian Sourcetree, Axosoft Gitkraken, or any of the many other clients. I’m going to use the git command line here since it is universal but your chosen git client may have a simpler equivalent workflow. If you haven’t already cloned your Section Application, you will find the git clone command on the Developer PoP page in the Section Console. It will something similar to: git clone https://aperture.section.io/account/1/application/1/www.example.com.git
 Typically this will have checked out your Production branch by default, which is convenient here since this is the target environment for this example. Just to be sure,though, we can explicitly check out the Production branch with: git checkout Production
 You can then merge all of the changes from the Staging branch into your local Production branch with this command: git merge origin/Staging
 Optionally, if you have setup the Section Developer Pop locally, you could test these changes against the Production domain and origin in isolation before deploying globally. When you’re ready to deploy the merged Staging changes to the Section platform, you can push with this command: git push origin Production
 Within moments your Section configuration changes, previously tested in Staging, will be applied to your Production site, and your new feature page will be effectively served from cache, and provide a faster experience to your users."
"298","2017-09-06","2023-03-24","https://www.section.io/blog/tips-to-renew-dns/","During my time at Section I have seen the websites of our customers and our partners go offline due to a seemingly innocuous administration error: failing to renew their domain name registration on time. This issue was recently brought to the forefront recently when the popular marketing tool Marketo failed to renew their domain name, and were shut out of their system for several hours. When your domain name registration expires, it can be quite a catastrophic event. All DNS queries for your domain name will begin to fail shortly after the expiration time is reached. This means that the naked-domain and the www subdomain you most likely use for your public website will not resolve, making it unreachable. It also means that the mail exchanger (MX) DNS records will not resolve and people outside your organization will not be able to send you email, eg to let you know they can’t access your site. If you use names like mail.yourdomain.com to connect to your mail server, you also won’t be able to check your mail or send mail to your colleagues. You may even find any other internal or B2B systems that use the same domain name will begin failing too. One upside to this situation, due to a policy introduced by ICANN a few years ago, is that an expired domain name will typically have a 30-day grace period for you to renew before the domain becomes available for someone else to buy from under you. Don’t Let Your DNS Expire To help avoid a lapsed domain registration yourself, here are some tips that we have found useful: Assign a contact email for your domain management that goes to a group mailbox, not to an individual. This is helpful if the relevant person is on leave when renewal is due or if people leave the organization and roles change between renewal periods. If your DNS registrar offers auto-renewal of domain names, turn it on. If your registrar does not offer this feature, consider changing registrar. Some systems including Section offer DNS hosting, but are not registrars. When you buy your domain name, or next renew it, add a reminder to your team calendar so that you will get a notification about a week before the renewal is due. Do this even if your registrar is configured to auto-renew since your payment details on file with the registrar may have expired. Consider renewing your domain for the minimum period unless the cost savings would be especially significant. When tasks are done more frequently, they become routine and are less likely to be missed, and the process for performing them becomes easier to remember. If you’re not sure when your domain name is expected to expire, find an online service to perform a WHOIS query on your domain name. Contact Us"
"299","2017-06-23","2023-03-24","https://www.section.io/blog/what-is-the-elk-stack-elasticsearch-kibana/","The ELK Stack, recently rebranded as the Elastic Stack, is a combination of several open-source tools: ElasticSearch, LogStash, and Kibana, that are combined to create a popular log management tool. The ELK Stack is an extremely useful set of tools for developers which has a wide range of uses. The most popular use case of the ELK Stack is getting increased visibility into how applications are working through an advanced log management. Below is an explanation of the ELK Stack components and information on how Section uses the ELK Stack to give developers more visibility into how their content delivery configuration and associated reverse proxies (like Varnish Cache and ModSecurity) handle requests. What makes up the ELK Stack? ElasticSearch ElasticSearch is a near-real time search engine that, as the name implies, is highly scalable and flexible. It centrally stores data so documents can be searched quickly, and allows for advanced queries so developers can get detailed analysis. ElasticSearch is based on the Lucene search engine, another open-source software, and built with RESTful APIs for simple deployment. LogStash LogStash is the data collection pipeline which sits in front of ElasticSearch to collect data inputs and pipe said data to a variety of different destinations - ElasticSearch being the destination for this data when utilizing the ELK Stack. LogStash supports a wide range of data types and sources (including web applications, hosting services, content delivery solutions, and web application firewalls or caching servers), and can collect them all at once so you have all the data you need immediately. Kibana Kibana visualizes ElasticSearch documents so it’s easy for developers to have immediate insight into the documents stored and how the system is operating. Kibana offers interactive diagrams that can visualize complex queries done through ElasticSearch, along with Geospatial data and timelines that show you different services are performing over time. Kibana also makes it easy for developers to create and save custom graphs that fit the needs of their specific applications. ELK Stack vs Splunk The ELK Stack has become popular with developers over legacy tools such as Splunk. This is mainly due to the increased visibility offered by the ELK Stack and its friendliness to DevOps teams - using the combined power of ElasticSearch, LogStash, and Kibana development and operations teams are able to quickly and easy collect, parse, and view data to troubleshoot and resolve complex problems. As applications become more complex and utilize tools like bot blockers, WAFs, and caching proxies, the amount of data going into a log management system has grown, so having a fast and scalable system is important. This has also impacted price, making paid services like Splunk more expensive and more suitable to enterprise level companies. In addition, the open-source nature of the ELK Stack and the flexibility it offers is highly appealing to modern developers. The ELK Stack is self-managed which can have some downsides, as teams will need to dedicate time to learning how to use it to its full potential. However, given its popularity there are numerous tutorials and guides to using the ELK Stack. ELK Stack and Content Delivery Content delivery solutions provide users with a huge amount of data representing the different proxies utilized in any content delivery solution - Section offers our users a wide range of reverse proxies including Varnish Cache, PageSpeed, and ModSecurity, and each of these tools comes with their own set of logs, in addition to the logs from Section’s global network of servers. At Section we feel it’s imperative that users have access to fast, easily visualized logging tools built into the content delivery system for visibility across systems, and so we leverage the ELK Stack for this purpose. Through the ELK Stack Section users get real-time logs and metrics that can quickly be sorted through to identify and troubleshoot issues with caching, WAF, or other configurations. In the below video, you can see how to utilize Kibana within Section to query your logs. DevOps-Integrated Content Delivery To learn more about how Section supports DevOps workflows through ELK Stack log management, real time metrics through Graphite visualized in Grapaha, and a local developer PoP for testing, please contact us or request a demo of Section’s platform. Get Started Today"
"300","2017-06-06","2023-03-24","https://www.section.io/blog/developer-testing-cdn-configuration/","Section’s Developer Point of Presence is the first and only solution that allows developers to pull their full content delivery, caching, and WAF configuration into a local environment for testing. Using MiniKube, a tool which allows you to create a local Kubernetes cluster on your machine, Section gives developers and operations teams a local Point of Presence that is exactly the same as Section’s global PoPs. This allows for a faster feedback loop, less problems on your live site, and better website security and performance outcomes. Testing CDN Configuration Traditionally, CDNs have been difficult to fully test before going to production. Because CDNs wrap up reverse proxies in a production only “black-box,” developers are unable to predict how configuration changes will impact their live website. That results in these CDN issues: Items breaking in production The “it worked on my machine” moment is common when making configuration changes to content delivery. Here are some problems that can arise without testing: Broken images and headers: missing images or misconfigured headers result in poor user experience. Session leakage: Delivering the wrong personalized content can be disastrous for your business and lead to a loss of customer trust. Mis-configured WAFs: If your Web Application Firewall is not properly blocking threats it can leave you open to attack, or you could unwittingly block legitimate users. Slower load times: A mis-configured cache can actually slow down your page load time, and it’s often difficult to troubleshoot. Not realizing the full potential of Content Delivery Many of the most valuable features of content delivery are not turned on out of fear they will break a production site once deployed. With a risk-free solution you can: Cache HTML: Caching HTML documents is the best way to improve back end load time and overall page speed, but it’s regarded as risky for users who aren’t able to test. Block threats more effectively: Testing your WAF configuration regularly allows for better threat blocking. Implement more front end optimizations: Front End Optimizations can reduce image sizes and manipulate JavaScript, but are often not be leveraged completely due to an inability to test how they’ll change your site load. Test CDN Changes Locally with Section Section’s DevPoP solves these issues while also improving developer efficiency. Changes to Varnish Cache Configuration Language (VCL), WAF configurations, or PageSpeed settings will be reflected in the DevPoP within 1 minute, and once you’ve confirmed changes are ready to go, simply push up changes using the same Git-backed process. Learn more here, orcontact us to get a demo or request access to the DevPoP beta so you can try it for yourself. Get Started Today"
"301","2016-11-16","2023-03-24","https://www.section.io/blog/free-yxorp-reverse-proxies/","This week, Section is at the DeFrag Conference in Broomfield, Colorado, a technology conference that focuses on those at the core of every technology company: developers. Section was founded by developers, for developers, and was built out of a frustration with the way Content Delivery Networks make it difficult for developers to easily drive and test the performance and security tools CDNs offer. At DeFrag we are launching our “Free YXORP” campaign, a movement to unleash the power of reverse proxies, which have long been held captive by Content Delivery Networks. CDNs and Reverse Proxies Content Delivery Networks are made up of two layers, a DNS layer that finds the CDN server or PoP closest to a user’s browser, and a reverse proxy layer which may include tools for speed, security, and more. A reverse proxy or YXORP takes external requests to your website and handles them on behalf of your website server. Examples include ModSecurity, which blocks malicious traffic, and Varnish Cache, an http accelerator which speeds up websites. Many reverse proxies are open-source and can be installed locally by developers on top of their main web server, but the more common method of installing reverse proxies is to utilize a Content Delivery Network. CDNs take reverse proxies and distribute them on their global PoPs. Each CDN uses different reverse proxies, and most CDNs modify open-source reverse proxies or create their own proprietary ones, locking them in a “black box” which is impossible for developers to break into. This makes it extremely frustrating for developers who are trying to configure and test reverse proxies on CDNs without knowing how they are coded. When configured correctly, reverse proxies are incredibly powerful tools that can be customized for a specific website’s needs. But the way they are currently locked in Content Delivery Networks limits their powers. Developers are unable to properly drive and test reverse proxies, meaning many of their most valuable features are underutilized for fear of them causing issues on live websites. What a Free YXORP World Looks Like Section is breaking reverse proxies free by allowing developers to choose the proxies and versions they want, and easily configure and test them in a local development environment. If everyone had access to open, testable reverse proxies that could be distributed on global server networks, here’s what the Internet would look like: On demand content delivery IP DDoS-proof Internet Content accelerated all the way from creation to delivery Bad bots blocked Frictionless deployments A more scalable, flexible, faster and more secure Internet Join the movement by signing up for a Section account . You’ll get a 14-day free trial to test our reverse proxy configuration tools, and anyone who adds an application gets a Free YXORP t-shirt from us."
"302","2016-05-30","2023-03-24","https://www.section.io/blog/how-caching-fails/","A highly effective website cache is optimised to store and share every possible response that only contains information relevant to all users, whilst ensuring that responses containing user-specific data are only ever served to the correct user. The nature of state management on the web (ie cookies) means that maintaining this separation is challenging and when mistakes happen it can potentially have severe consequences as has been demonstrated by Valve’s Steam Store last December, and Menulog this month. Essentially there are three different ways that caching can fail to work as intended: Not caching Arguably the preferred failure mode is when responses that should be cached and shared are not and instead are served by the origin web server for every request. This can lead to slower page load times for users and also much more traffic to the origin. Sometimes this could escalate into a complete origin failure if the web servers do not have the capacity to handle all these requests. This is the stance taken in a default configuration of Varnish Cache, the caching proxy offered by Section. It is the result of Varnish Cache considering the presence of a cookie in either the request or the response as a potential indicator of user-specific data and bypassing the cache to avoid either of the next two failure modes. Sharing user-specific response bodies The next possible failure scenario is when an origin web server response is intended for only one specific user but is cached and then served to other users, exposing potentially sensitive data. In a trivial case this may just mean that Alice sees “Hi Carol” at the top of a page but a much worse example would be Alice seeing Carol’s contact details and purchase history on the account page. This is what happened to both Valve and Menulog. It usually results from stripping cookies from responses in the cache configuration to achieve better cache utilisation on assets that should be shared but then accidentally applying the same logic to user-specific assets. Having a CDN like Section where the cache configuration can be tested in environments prior to Production can help to catch these issues before they affect real users. Sharing user session cookies The last failure mode, and probably the worst, is when an origin web server response, including a Set-Cookie response header containing a user’s specific session identifier is cached and then served to other users. This means that other users receive a new session identifier, which represents someone other than themselves, and then continue to browse the website under the context of the user whose session id was cached. This means two or more unrelated users can end up adding items to the same shopping cart and when one user logs in with their credentials, the other users with the same session cookie become logged in as that user too and able to perform actions on the site as that user such as changing their delivery address and perhaps even making a purchase with saved payment details. This situation can result from a cache configuration that leaves the Set-Cookie header in a response while explicitly marking the response as cacheable - more often than not this will be an unexpected edge case in some complex conditional logic as it is rare to want to cache and share a Set-Cookie header under most circumstances. Summary While none of the effects of failing to cache are ideal, falling back to increased load on the origin is usually the best of bad options. Be mindful when configuring your cache solution to err in that direction in the event that changes to the shape of responses happen unexpectedly. With Varnish Cache this usually means working with the built-in VCL instead of bypassing it. Be sure to test your caching configuration before releasing new changes to production, and preferrably integrate your caching solution into the workflow of developing new website features. Lastly, setup monitoring on your cache miss rate so you can be notified quickly when excess traffic is passing to your origin web servers."
"303","2016-05-03","2023-03-24","https://www.section.io/blog/can-cdns-remain-relevant-in-an-agile-world/","83% of businesses have or are implementing agile development methodologies. The race here is run and agile is the answer. With the majority of web shops now embracing agile methodologies, are CDNs are drifting away from mainstream processes? Let’s consider what a Content Delivery Network is quickly. A DNS layer which detects, decides and directs traffic to the best destination for that request. A layer of reverse proxies which intercept the traffic on the way through and in some way manipulate the request - inspect, block, respond, reroute, modify etc. All current CDNs are still built on 18 years old network first principles. Back then, it made sense to run a waterfall CDN deploy alongside a waterfall software build. The CDN industry have not moved away from the black box networks which support waterfall deployments. Static object caching is boring and we all know that the biggest bang for buck when running on a CDN is when you have the application well integrated with the CDN. More HTML offload and more tailored WAF rulesets can be run when this is the case. The best place to integrate the application with CDN? In development, not in production. Why? fast feedback loops and the ability to comfortably make mistakes. We all know the cost differential of mistakes in production versus development. Reverse proxies are a smart way to add value to a web application, whether that be security, performance or scalability. I am confident that they will continue to be so but they should be relevant in an agile world. For CDNs to remain relevant in an agile world, engineers need to be provided the opportunity to write application code or configure the CDN settings in the safety of the development environment in the same way they do for database or application changes. CDN for agile is a new trend – Section is the only CDN which is hooked up to provide an immediately seamless integration with agile development workflows. Backed by Git and with the same, open, real time logs and metrics stack in Dev, Test, Staging and Production, developers and engineers have all the right tools to manage the CDN completely."
"304","2016-04-21","2023-03-24","https://www.section.io/blog/contributing-back-to-the-community/","Varnish Cache was the first reverse proxy offered by Section and, for many of us, it is our favourite because it always just works and gives great results. Yesterday though we encountered an issue where our Traces page in the Section Console failed to retrieve the Varnish Cache Logs for a particular Section-hosted Varnish Cache instance. It had timed out before the results returned. Our platform team investigated. Broken Our system gathers the Varnish Cache logs on-demand by executing the varnishlog tool in the relevant proxy container and capturing the output. By default, varnishlog would tail the log indefinitely, reporting activity as traffic is handled by Varnish Cache, but because we want a sample to return to the portal quickly, we specify the -d option to varnishlog which instructs it to output the recently recorded logs, then exit. Upon investigation though we found that varnishlog -d hadn’t exited, it was still running and still outputting logs as traffic flowed. My colleague, Ivan, discovered a Varnish bug reported in 2013 that varnishlog was ignoring the -d option entirely. This bug report included to a reference to a code change that resolved the issue, so we investigated that code further. Ivan also noted that the output from the still active varnishlog process included the text Log overrun and Log reacquired which lead us to another part of varnishlog's source code where we found logic almost identical to that which was changed in the aforementioned bug fix. This led us to conclude that there was an edge case where the -d option would become ignored if the circular buffer used Varnish Cache for its log was overrun whilst trying to read it. This edge case also appeared to exist in all versions of Varnish Cache since 4.0 inclusive. Fixed We did not know how long it would take to fix this issue in Varnish Cache’s log tool itself, or if any such fix would even be applied to all affected Varnish Cache versions. The chance of the issue re-occuring is also somewhat unpredictable, so we needed to work around it in our system. We decided on an approach that would improve upon our platform’s general reliability and began to implement the changes. Meanwhile, we submitted a description of the problem to the Varnish Cache team along with a suggested code change to fix the issue. Within about 4 hours, our change had been accepted to be included in the eventual release of Varnish Cache 5 and very likely in any updates to both Varnish Cache 4.1 and 4.0. I think I like Varnish Cache even more now."
"305","2016-04-14","2023-03-24","https://www.section.io/blog/reducing-the-risk-of-change/","TL;DR All Varnish Cache VCL changes submitted will be validated, and if compilation fails, the changes will be rejected. All running Varnish Cache instances will maintain the last known good configuration. This applies to both in-browser editing and using git push from a local workspace. In the beginning The core tenets at Section are to provide a platform that is open, easy and gives control to the user. All three of these are present in the Varnish Cache Configuration Language editor in our Section Console. We expose the raw VCL, in the browser, for the user to customise, save, and deploy just as the user could if they managed their own Varnish Cache instance. This approach is also key to honouring another Section mantra - “It’s just Varnish Cache”. Unfortunately, our original implementation differed from normal Varnish Cache usage in an important way: deploying broken VCL to any typical Varnish Cache instance would be rejected and Varnish Cache would continue to operate with the last known good configuration. However Section’s Varnish Cache instances would instead lose the last known good configuration and begin serving an error response for all requests. How did this happen? This difference in handling broken configurations was a by-product of ensuring that the committed files at tip of the repository branch (eg Production) should represent the active configuration of the reverse-proxies handling live traffic. The intent was to reduce the conceptual complexity that would arise from having the most recent commit show one state whilst needing another branch (or tag) to track the state that was last successfully deployed. This behaviour, while not helping the chance of breaking your site through web-based VCL editing, has always been mitigated by Section’s recommended local development experience. In local development, VCL changes can be tested, not only to ensure correct syntax, but also to confirm that the actual impact on your site’s behaviour is what you had intended, and tested against the same Varnish Cache version as used in production. We have wanted to improve upon this situation for a while, to make Section even easier and safer to use with confidence. Now we have. A better experience The platform now performs Varnish Cache VCL validation when saving changes made in the Section Console web-based editor and when pushing commits from your local development git workspace. This means that you cannot save broken configuration to your application repository that might have previously resulted in serving error pages to your visitors. For those with a technical interest, this validation is implemented by passing your desired configuration changes to a duplicate Docker container of the same Varnish Cache proxy image that is used to handle your live traffic. This provides high confidence that the configuration is valid in the context of your specific Varnish Cache version and its settings and available modules. While we still strongly encourage implementing and testing changes in a development environment before promoting them through to Production, you can now also make quick changes in the browser without fear of accidentally forgetting a semicolon."
"306","2015-12-10","2023-03-24","https://www.section.io/blog/http2-by-default/","Section has supported the SPDY protocol since the platform’s inception and we’ve been trialing HTTP/2 in recent months. Today HTTP/2 is enabled for all sites using Section. To benefit from HTTP/2 on your Section-proxied site now, just ensure you have configured a valid certificate for HTTPS in the Section Console. After your certificate is saved and deployed, your site will support HTTP/2 for HTTPS connections on Section. While the HTTP/2 standard supports unencrypted connections, modern browsers are currently only supporting it with HTTPS. This is just one more reason to ensure your site works on, and prefers, HTTPS for all pages. What are the benefits of HTTP/2? Ultimately HTTP/2 will lead to faster page load times for your users. HTTP/2 will transfer multiple resources concurrently on a single connection. This reduces the TLS/TCP handshake cost traditionally involved with the browser establishing multiple connections to your site and also removes the need to implement domain-sharding to bypass the per-domain connection limits imposed by the browser. In fact, HTTP/2 will work better if you avoid domain-sharding. HTTP/2 uses binary-encoding and header compression and when these are combined with the above multiplexing, the typical delays from having many small resources are significantly reduced. The decision to implement image spriting and/or JavaScript and CSS concatenation becomes much more a question of browser cache-invalidation patterns instead of the number of requests required. While not implemented by Section yet, HTTP/2 also defines a mechanism called Server Push. This will allow, for example, the server to begin sending to the browser, the JavaScript and CSS referenced in a page, before the browser has completed downloading and parsing the HTML to realise it needs to request these resources. When the browser does discover it needs these resources, it will find that the server has already transferred them and the browser will be able to render the page much sooner. We plan to support Server Push with Section in the future. But my origin server isn’t HTTP/2 enabled That’s ok. Many of the features in the HTTP/2 protocol are focussed on improving the browser experience. The connections between the Section platform and your origin server will continue to use HTTP/1.1 regardless of the protocol the browser is using. Section will maintain a set of open connections to your origin server to reduce handshake costs, and isn’t limited to the same concurrent connection limit or bandwidth as most browsers. Effective use of a Section Varnish Cache proxy will help greatly as the platform can quickly serve cached responses to the browser over HTTP/2 without incurring a round-trip to the origin server."
"307","2015-08-13","2023-03-24","https://www.section.io/blog/continuous-delivery-and-content-delivery-networks/","Prior to joining the Section team, I worked as a consultant focusing on processes and tooling to help software teams deliver the right product on schedule. While there are many facets to achieving this, one approach always contributed significantly to success: deploy to Production (or Production-like environment) early and repeatedly in the development cycle. One reason why this is effective is because Production environments are typically more complex compared to Development environments. So unless you are developing for your production environment, incorrect assumptions may be made and complexities overlooked. Development environments are rarely configured across seperate physical tiers and usage is typically leaner than Production. Hence, by forcing the team to deploy early rather than discovering issues late in the development cycle, bugs and fixes are much easier and cheaper to address. Especially if assumptions are buried too deep in the code and are hard to tease apart. It Worked in the Dev Environment! How many bugs have you seen discovered in staging and production environments where the question “why didn’t we discover this earlier” was answered with “because the dev/test environment is provisioned differently”? In my experience, in almost all cases! With this in mind, now consider the websites you have helped build where the Production environment is using a Content Delivery Network (CDN), or some other caching proxy (like Varnish Cache) but the environments prior to Production did not have that same service in front? If your caching solution is only dealing with static assets (eg images, stylesheets) then there may only ever be a few bugs that occur in Production. At the same time, you’re probably not getting the most out of your caching solution. Once you start leveraging your caching solution’s ability to handle dynamic resources (eg page HTML, AJAX responses) you can deliver great performance improvements to your website and increase the number of concurrent users your infrastructure can handle. However, this may come at a price. Caching rules will now be breaking “simple” assumptions made in development and introducing issues that will only be found in a more “complex” Production environment. One example I’ve seen recently was a product search results page on an e-commerce site. It was cached so that a user who searched for the same keyword(s) that another user had also searched for would be served a cached response. This would therefore avoid the expensive queries and computations on the origin web server. However, the site had been developed assuming the page would always be loaded from the origin and thus it would update the user’s session data to note their most recent search query. Unfortunately, with the caching rules in place that were in production only, the origin never saw the second user’s search request and did not update that user’s session data. Ultimately this led to the follow-on pages not functioning as designed as the session data wasn’t the same. When Problems Occur….just TURN IT OFF! In scenarios like this, where the user experience becomes broken, the fastest resolution is typically to disable the Production-specific behaviour that is triggering the problem. This is how powerful caching solutions atrophy into simple static asset caches. There are many other scenarios where a CDN can introduce these subtle behaviour changes. An effective cache configuration will be coupled to the nuances of a particular site design and the best result is achieved when the website code is designed to leverage the capabilities of the cache proxy in front of it. It’s a symbiotic relationship. To design and build a website that can deliver a fast experience to a single user and maintain that same fast experience to many many users concurrently means that your CDN or Varnish Cache deployment cannot be limited to your Production environment. We would also suggest that this outcome is not the sole responsibility of your infrastructure/operations team either. You need to bring the same caching rules that are used in Production, back into all of your pre-Production environments and keep the cache configurations in sync with Production. Only then will you provide visibility of the cache configurations and the run-time results of the cache to your development team. This will enable the website to be built with an understanding of what will be cached, why something fails to cache, and help with achieving optimal cache durations. Technology is Only Half the Challenge As I well know from my time as a consultant, adopting this approach is as much as people-problem as it is a technical challenge. Especially considering the traditional barriers and conflicts between Dev and Ops. With Section, the technical aspect of achieving this has been done for you. Every Section application is provisioned with a Production and Development environment. The configuration of Section is designed to use Git workflows to keep the environments in sync and promote changes. The section.cli tool enables you to launch the Varnish Cache proxy in your development environment that is the same Varnish Cache proxy used in Production. The Section Console gives users access to metrics, logs, and other diagnostics for each environment. Whatever you’re using as your preferred caching solution for your website, remember to build with the cache in mind. Then verify how the cache affects your site and do this EARLY and OFTEN!"
"308","2015-08-13","2023-03-24","https://www.section.io/blog/hark-a-vagrant/","Coming from a background as a Windows developer, one of the biggest issues I faced when starting to build *nix apps was how to develop locally. Trying to develop a sails.js app or build a Varnish Cache caching proxy on a Windows machine gets pretty fustrating pretty fast. Additionally even if you are running Ubuntu locally, unless you only work on a single project you’ll run into the issue of multiple apps needing port 80 for testing. This is where virtual machines come in. By running virtual machines locally you can replicate the production deploy environment on your local machine while keeping it isolated from your local machine’s OS and config. Vagrant is quite possibly the simplest way of doing this. Lets say you want to build a Varnish Cache cache solution for your site. This will most likely get hosted on something like an Ubuntu machine. To get this running on your local dev environment, first install Vagrant & VirtualBox. Then open a command prompt at the root of your repository for this project (you are using source control aren’t you?) and run these two commands: vagrant init ubuntu/trusty64
 This will go and download the Ubuntu 14.04 64 bit virtual machine image from the Vagrant box catlog. You probably don’t want to run this while you’re on mobile data. It will also create a Vagrantfile in this path. Open this file up with your favourite text editor and have a look. There are a lot of options you can put in here, and we’re not going to go into them now. The only thing you’ll want to do right now is uncomment this line: #config.vm.network ""private_network"", ip: ""192.168.33.10""
 ie, remove the # at the start of the line. This tells vagrant to make the virtual machine accessable to you at that IP address. So once you’ve got Varnish Cache installed on this VM you can just browse to http://192.168.33.10:6081 to view it (Varnish listens on port 6081 by default). Now run vagrant up
 At this point you have a complete Ubuntu machine running as a VM on your host. To get on to the box you run vagrant ssh
 Your comand prompt is now in the Ubuntu virtual machine: Now you can now install Varnish Cache as you would in an actual Ubuntu server, by running sudo apt-get install Varnish Cache
 So, now you can open up http://192.168.33.10:6081 and see Varnish Cache running. Oops. By default, varish sets its backend to localhost:8080. Seeing as the only thing we’ve got runnning on this VM is vagrant, there’s no backend to talk to. You’ll want to edit the /etc/varnish/default.vcl file and set the backend to something that actually works. However editing that file in the VM isn’t a great solution since it won’t be source controlled. Better to create a file in the same folder as the Vagrant file called default.vcl. You can grab an example of what it should look like from the varnish source. Vagrant has automatically mapped the folder on your host that you started vagrant in into the VM as /vagrant. If you create the default.vcl file in that folder, it will be in the VM as /vagrant/default.vcl. You can then write a simple bash script that will copy that into /etc/varnish and reload Varnish Cache. This way you can edit the vcl locally using a proper text editor, but run it in a simulation of your production environment. Wrapping it up There is heaps more you can do with Vagrant. The Vagrantfile is just Ruby, so you can program it too. The simplicity and power of Vagrant is pretty impressive, and it’s why we use it to provide the local dev experience for Section users. ‘Hark! A Vagrant Ref’: Apologies to Kate Beaton"
"309","2022-07-05","2023-03-24","https://www.section.io/blog/roadmap-to-adaptive-edge-balance-cost-effciency-usability-performance/","The network edge presents new challenges in optimizing application deployments, as the deeper an application is pushed into the internet, the greater the associated costs to run those workloads. Recently, as part of Kubernetes on EDGE Day at this year’s KubeCon + CloudNativeCon event in Europe, Section Senior Product Manager Tom McCollough covered a set of optimization considerations and strategies that allow workloads to be as close to users as possible, bounded by a business value construct. At the same time, he discussed the importance of maintaining simplicity in the usability of the system, so that developers can specify how deeply they want their application to run without overallocation of resources. This blog post presents an excerpt from Tom’s presentation, discussing the use of Cloud Native Computing Foundation (CNCF) core technologies, including Kubernetes and Prometheus as a base, to establish a framework for evaluating projects within the CNCF landscape and their suitability for edge use cases. Tom also examines Kubernetes Federation techniques, multi-cluster orchestration systems, and traffic direction and service discovery strategies against a selection criterion to assist architects in making “good-fit” decisions. Building the Roadmap: Where to Start When we talk about balancing performance against cost, we’re talking about running hundreds of instances of an application at the edge to deliver, on average, shorter distance, lower latency, fewer dropped shopping carts, and so on. But there’s a cost associated with that – not just in terms of cycles, but in terms of operating all those clusters. While you might love to run clusters everywhere, the cost is likely too high for that to be a realistic option. So, how do you get the effect of running everywhere without actually running everywhere? First, let’s establish our bearings by considering some of the “what ifs” at play and of note: What if we could be adaptive? What if we could move the workload to where it needs to be? What if we could maintain a set of running workloads that are optimal for particular needs – optimal according to distance from an end user, optimal according to budget, etc.? What if we could continuously adapt that set of running workload locations – adapt it according to signals such as health signals, utilization signals, etc.? What if we could use predictive analytics to know ahead of time that we can shut down a workload in one location, start it up in another, and continuously revisit this set of locations and update it as time goes by? That’s the idea. Consider this as a rough outline for the roadmap you’re building for this first solution for an adaptive optimized edge. Surveying the Landscape Now, let’s survey the existing CNCF landscape and how it can be leveraged here. We’ll use Kubernetes, running a containerized workload, as our starting point. There are still other pieces we’re going to need: We’re going to need to be able to run our workload on multiple clusters. We’re going to need to be able to move workload from one cluster to another. We’re going to need a way to solve the problem related to signals and optimization. And then finally, we’re going to need to be able to direct traffic. As a workload moves from one location to another, based on the signals, we will need to redirect the traffic to follow along. Multi-Cluster – The multi-cluster problem is well addressed in the CNCF landscape. There are a number of solutions out there; some of them are forks from the Federation special interest group. Karmada is just one suitable example we can use, as it allows you to run and manage multiple clusters without the cognitive load of having to think about managing each cluster, individually. That solution is ready to go. Moving Workload – With regard to moving a workload, Open Cluster Management is another tool that lets you manage workloads running on multiple clusters, and administer the multiple clusters themselves. But it adds something different – it adds a placement decision. This gives you ability to customize where your workload runs. It’s a code you write yourself and then provide to Open Cluster Management to tell it where a workload needs to be prioritized. Using this capability and the addition of signals (which we’ll get into more below) you can develop the ability to place a workload. The thing about Open Cluster Management’s placement decision, however, is that it operates on the initial deployment of that workload. It doesn’t do what we want, which is revisit it on a cadence. And since it doesn’t revisit the decision, you’re going to have to somehow add that capability to open cluster management to rerun this placement, and then actually move the workload. Signals and Optimization – This is where we’re left empty handed. While CNCF does have an optimization section, it’s devoted towards optimizing within a single Kubernetes cluster – all the examples are about optimizing for cost (how to run this cluster in an optimal way, minimizing compute resource, etc.). What we want to do is optimize across multiple clusters – and we also want to take performance into account. There’s nothing in the landscape that provides that. So, what now? Unfortunately, you’ll need to develop a custom solution. Directing Traffic – Finally we come to directing traffic, and now we’re back to having a good representation within the CNCF landscape. Core DNS is a programmable DNS solution that will route traffic from an end user to the nearest location where that traffic can be served. However, DNS has common issues that come into play. For instance, an ISP, as a piece of the DNS stack, may decide to cache a single DNS answer for its entire user base – and therefore not provide the necessary user location granularity; you really do want each end user to be routed to their closest location, not to the closest location of some other end user in that same ISP’s network. And then of course, there’s the issue of time to live (TTL). We want use the capability we’re developing here to make highly available services, but if a TTL isn’t being honored (say you’ve asked for a minute and an ISP is giving you an hour), then that’s the best you’re going to be able to do; in the worst case, your switch over time would be an hour for directing traffic. In short, DNS has issues. Kubernetes global load balancer, however, brings anycast into play for us. Anycast is a capability built into IP that’s purpose-built to route traffic from an end user to the closest location where that workload is being served. And Kubernetes global load balancer will use Border Gateway Protocol (BGP) protocol to control the anycast capability of IP, which is the perfect solution for what we want to do. You can learn more on this particular strategy in our recent series on Building for the Inevitable Next Cloud Outage. Charting Your Course to the Adaptive Edge Now, if you take a step back, what you have developed is a high-level roadmap for all the pieces that you need, pulled from the CNCF landscape, to build an optimized adaptive edge. With this you will be able to balance performance against cost to get an optimal solution. You’ll get the effect of running everywhere without actually running everywhere – and without the cost of running everywhere. That said, while we’ve been talking about performance against cost, it’s also important to note that businesses are unique. Businesses want to put themselves forward in a distinctive way – chart their own course, so to speak. Maybe you want to prioritize something else at your business. Maybe, for example, you want to minimize your carbon footprint. You could build into your optimization engine the ability to prefer green data centers. Or, let’s say that compliance is important. You could build into your optimization to prefer PCI compliant or other types of compliance. Ultimately, your business can customize the optimization engine as needed to suit your purposes. Section’s Adaptive Edge Engine intelligently and continuously tunes and reconfigures your edge delivery network to ensure your edge workloads are running the optimal compute for your application. If you’re as excited about discovering the adaptive edge as we are, we’d love to chat with you further. Get in touch with us today."
"310","2022-06-28","2023-03-24","https://www.section.io/blog/building-for-the-next-cloud-outage-part1/","The following is based on a talk by Section’s Pavel Nikolov at the KubeCon+CloudNativeCon Europe 2022 event. This first post will discuss the challenges in building for the next cloud outage. Part Two will demonstrate how to deploy a Kubernetes application across clusters in multiple clouds and regions with built-in failover to automatically adapt to cloud outages. You can also read Pavel’s column on this topic in TechBeacon. Every few months we read about the widespread impact of a major cloud outage. These events are unpredictable and inevitable, and, quite frankly, keep site reliability engineering (SRE) teams up at night. No matter your type of business, it is prohibitively expensive to deploy your applications everywhere around the world at the same time while still ensuring high availability. Public cloud remains the most popular data center approach among the cloud native community, with multi-cloud growing in adoption. However, adopting a multi-cloud strategy isn’t as simple as hitting the “go” button. What’s more, despite best efforts at building out redundancy, the cloud providers cannot guarantee 100% uptime. As such, it’s not a question of if your servers or services will go down but rather when. And it will probably happen when you are either not prepared or least expect it (hello middle of the night support calls). This is true for a number of reasons. For one, there are external factors, such as your Domain Name System (DNS) going down or upstream internet provider connectivity issues, that are outside the control of the public clouds. Then, too, there are the human factors involved, like when we make mistakes in code deployment that can be difficult to roll back. Of course, there are also natural disasters that can take down entire regions or cause significant headaches for services around the globe. As a result, organizations spend a significant amount of time and money prepping disaster recovery plans while preparing for that next inevitable cloud outage. Disaster Recovery to the Rescue (maybe) The vast majority of organizations fall into one of four disaster recovery categories when it comes to responding to an outage: Active / active deployment strategy: If your primary server goes down, you flip the switch on your DNS and your request goes to a second active server. While this is the fastest and least-disruptive disaster recovery, you’re among the lucky few if your IT budget supports this option! Active / passive deployment strategy: This is very similar to active / active but it’s cheaper because you’re not paying for the hosting of the passive instance or cluster when you’re not using it. However, you have to spin up the passive instance and flip the switch on your DNS before service is restored, delaying the return to service. Periodic backup of your databases: In this instance, when your service goes down you must first spin up your code, restore the backups, and then continue serving as normal. While viable, this should not be considered a rapid response and can potentially extend service outages over more than 24 hours. The only thing worse is… No disaster recovery strategy: Truth be told, far too many organizations fall into this category. It’s understandable; you’re busy building features and don’t have time to think about disaster recovery. When something happens, you’ll figure it out! The challenge with any of these disaster recovery strategies (except for the fourth one, of course) is that they require a high level of discipline. Your entire team needs to understand what will happen and know what they must do when an outage occurs, and even the best laid plans will likely require some level of human intervention to restore service. In addition, as you add new features or components to your system, you’ll need to test your disaster recovery plan to account for changes that have occurred. Ideally, this should happen at least every quarter – preferably every month – and it’s easy to get caught up in our day-to-day delivery deadlines, putting off review of the disaster recovery plan until it’s too late. Multi-Cluster Disaster Recovery Since you’re reading this blog, let’s assume you’re running a modern Kubernetes containerized application. Let’s further assume that your application is running on multiple distributed clusters to maximize availability and performance. How does that impact disaster recovery? Just because you have multiple clusters does not mean automatic failover during an outage. The culprit is often DNS. First off, DNS servers can (and often do) become unavailable. But even if the servers themselves don’t go down, DNS configuration can cause problems during outages. DNS uses TTL (time to live) settings to handle routing, and the problem is that there is no guarantee that, worldwide, all providers will honor your TTL. This can effectively mean that distributed clusters are available but effectively invisible during an outage. But what if there was another approach to disaster recovery? In our next post we’ll discuss a strategy using BGP + Anycast to significantly improve availability and recovery. If you’re eager to jump ahead, feel free to watch Pavel’s KubeCon talk. Section’s Cloud-Native Hosting Solution Addresses Reliability (and much more) On the other hand, if you need a solution today, why not turn to Section? As we know all too well, outages will happen eventually. It can be prohibitively expensive and labor intensive to maintain disaster recovery strategies for your organization. Fortunately, Section offers a wide range of Cloud-Native Hosting solutions that address the complexity of building and operating distributed networks. The complexities of routing across multi-layer edge-cloud topologies are perhaps the most daunting when it comes to building distributed systems. This is why organizations are increasingly turning to solutions like Section that take care of this for you. In particular, Section’s Kubernetes Edge Interface (KEI), Adaptive Edge Engine (AEE) and Composable Edge Cloud work together to improve application availability. With KEI you can set policy-based controls using simple commands in tools like kubectl that control, among other things, cluster reliability and availability. AEE uses advanced artificial intelligence to interpret those commands and automatically handle configuration and routing in the background. Finally, Section’s Composable Edge Cloud features a heterogeneous mix of different cloud providers worldwide, ensuring application availability even when a provider network goes down. To learn more, get in touch and we’ll show you how the Section platform can help you achieve the reliability, scalability, speed, security or other custom edge compute functionality that your applications demand. Read Part 2 »"
"311","2022-06-29","2023-03-24","https://www.section.io/blog/building-for-the-next-cloud-outage-part2/","This is the second post in a two part series based on a talk by Section’s Pavel Nikolov at the recent Europe 2022 KubeCon+CloudNativeCon event. In the first post we discussed the challenges in building for the next cloud outage. This second installment demonstrates how to deploy a Kubernetes application across clusters in multiple clouds and regions with built-in failover to automatically adapt to cloud outages. You can also read Pavel’s column on this topic in TechBeacon. Previously we discussed how the Domain Name System (DNS) can become a single point of failure for your system, even in a multi-cluster environment, either because DNS servers go down or due to issues with worldwide TTL (time to live) settings. Obviously neither of these situations are ideal in regard to an outage. BGP + Anycast = A match made in heaven But what if there was another approach to disaster recovery? Imagine, if you will, an approach that is self-healing, does not require human intervention, does not involve any single point of failure, and anticipates that anything could go down at any time, including your DNS servers. In fact, Border Gateway Protocol (BGP) and Anycast Internet Protocol (IP) addresses can be used together to provide a viable alternative for disaster recovery. For starters, you’ll need to purchase your own IP address range and your cloud provider must allow you to bring your own IP range, which most public clouds support. And there is, of course, a learning curve that comes with implementing BGP for your organization. As we know, the internet is essentially a network of networks, with the larger networks referred to as autonomous systems. BGP ensures that autonomous systems communicate with each other in the most efficient way possible. If, for example, you have one server that needs to reach another server, BGP ensures that the Transmission Control Protocol (TCP) packets from server A finds the most efficient route to its destination on the internet. This happens by way of BGP “speakers” that announce the range of IP addresses within their autonomous system to all other autonomous systems. Within a matter of seconds, the entire internet knows where each specific IP range resides. So, when you have a packet that needs to reach a specific IP address, every system in the world knows where to send it based on the IP range it falls within. When the packet reaches the autonomous system with the correct IP range, internal routing finds the exact server with the exact IP address and sends your packet through to its destination. As a failover response, BGP offers a significant time saving benefit compared with DNS. It’s not uncommon for the DNS server to take five minutes or more to recover from a disaster. With BGP, however, convergence takes just seconds. When the BGP speaker announces an IP address, the whole world knows about it. Similarly, when it stops announcing an IP address, the whole world also knows about it. When it comes to sending packets across networks, there are several different addressing methods: Unicast (one server that sends one TCP packet over the network to exactly one destination server), Multicast (one server sends a packet that reaches many different destinations) and Anycast (there are many servers around the world with the exact same public IP address and your packet is guaranteed to find the nearest one). As you might imagine, Anycast and BGP enable a world of possibilities, offering built-in failover to automatically adapt to cloud outages. BGP + Anycast in action To better understand the benefits, let’s look at a very simple scenario where you create a small Kubernetes test application that requests a response every second. In this example, we’ll deploy this cluster in three different clouds and regions – one in New York City, another in Amsterdam, and a third in Sydney. When deployed in a healthy state, the clusters will all have the same Anycast public IP address. If you are located in Zurich, for example, you will receive a response from Amsterdam since it is the closest location. If you are instead located in Cairo, you will also get a response from Amsterdam since it’s still the closest. Now, if you repeat this scenario to send a request to each of the clusters every second and stop announcing the IP range for Amsterdam (to simulate one of your regions going down), your app will start getting a response from New York – the next closest location – in less than a second. Repeat the same process and take down the New York cluster and you’ll instantly start receiving a response from Sydney. You have now automatically rescheduled workloads and rerouted traffic to healthy clusters in real-time without having to touch a single thing – no disaster recovery strategy required! While you can’t guarantee 100% uptime, combining BGP with Anycast will bring you close to the holy grail with minimal effort. In the case of our example, the system recovered on its own within milliseconds. Section’s Cloud-Native Hosting Solution addresses reliability (and much more) As we know all too well, outages will happen eventually. It can be prohibitively expensive and labor intensive to maintain disaster recovery strategies for your organization. Fortunately, Section offers a wide range of Cloud-Native Hosting solutions that address the complexity of building and operating distributed networks. The complexities of routing across multi-layer edge-cloud topologies are perhaps the most daunting when it comes to building distributed systems. This is why organizations are increasingly turning to solutions like Section that take care of this for you. In particular, Section’s Kubernetes Edge Interface (KEI), Adaptive Edge Engine (AEE) and Composable Edge Cloud work together to improve application availability. With KEI you can set policy-based controls using simple commands in tools like kubectl that control, among other things, cluster reliability and availability. AEE uses advanced artificial intelligence to interpret those commands and automatically handle configuration and routing in the background. Finally, Section’s Composable Edge Cloud features a heterogeneous mix of different cloud providers worldwide, ensuring application availability even when a provider network goes down. To learn more, get in touch and we’ll show you how the Section platform can help you achieve the reliability, scalability, speed, security or other custom edge compute functionality that your applications demand."
"312","2022-06-08","2023-03-24","https://www.section.io/blog/cncf-launches-telco-enablement-program/","At this year’s KubeCon + CloudNativeCon conference in Europe, the Cloud Native Computing Foundation (CNCF) came out with another interesting announcement: they are launching a new program to help telcos and other communication service providers adopt Kubernetes and other cloud native technologies and best practices. We think this is a great idea. As we highlighted in our recent whitepaper on Why Organizations are Modernizing their Applications with Distributed, Multi-Cluster Kubernetes Deployments, there is a strong correlation between container and Kubernetes adoption and edge deployment. “The lightweight portability of containers makes them ideally suited to distribution, while their abstraction facilitates deployment to non-homogenous federated compute platforms. Moreover, Kubernetes adds the orchestration needed to best coordinate this sort of distributed multi-region, multi-cluster topology. In other words, organizations that have already adopted Kubernetes are primed to rapidly adopt modern edge deployments for their application workloads. Even those that are still at the single-cluster stage are in position to rapidly leap-frog to the distributed edge. Recent research by SlashData on behalf of the Cloud Native Computing Foundation confirms this correlation between edge, containers, and Kubernetes, noting developers working on edge computing having the highest usage of both containers (76%) and Kubernetes (63%) of surveyed segments.” Seen from this perspective, anything that better aligns telco networks with Kubernetes plays into this larger trend to accelerate distributed cluster deployments. The new program from CNCF is targeted at network equipment providers, offering a new certification (the Cloud Native Network Function, or CNF) so these vendors can verify that their equipment follows cloud native best practices, and thus making it easier for provider networks to choose network equipment technologies which support provider networks’ move to Kubernetes and other cloud native tools. “Moving to cloud native infrastructures has long been difficult for telecom providers who have transitioned to VNFs and found themselves with siloed resources and specialized solutions not built for the cloud. The CNF Certification program is designed to fill this gap by creating solutions optimized for cloud native environments. Some of the world’s largest telecom organizations, including Huawei, Nokia, T-Mobile and Vodafone, already use Kubernetes and other cloud native technologies, and this program will make it easier for others to do the same.” Priyanka Sharma, Executive Director, CNCF Of course, Section has already built a federated worldwide Composable Edge Cloud that allows any organization to rapidly deploy containerized application workloads across a multi-cloud edge network of Kubernetes clusters. Our provider partners – Lumen, Equinix, Digital Ocean, RackCorp, AWS, Microsoft Azure and Google Cloud – all run networks and equipment that Section’s platform orchestrates to make it easy and cost effective to deploy distributed containerized workloads today. By extending the same workload portability to telecom provider networks, platform providers like Section will enable DevOps teams to expand and deepen workload placement beyond cloud to more localized infrastructure. This will open up even more opportunities for developers to create more powerful application experiences. Bravo, CNCF!"
"313","2022-06-06","2023-03-24","https://www.section.io/blog/two-types-multicluster-architectures/","For those new to deploying multiple distributed Kubernetes clusters, it helps to understand that there are two basic choices: a replicated architecture or service-based (sometimes called segmented) approach. We cover this in our white paper on Why Organizations are Modernizing their Applications with Distributed, Multi-Cluster Deployments, along with a deep discussion on multi-cluster definitions, benefits and more. If you’re interested in learning more, it’s a good read. That said, which approach you select (replicated or service-based) can have profound implications for whether and how you can take advantage of all that a distributed multi-cluster topology has to offer. Replicated Architecture: The Basic Multi-Cluster Structure The simplest way to handle a distributed multi-cluster deployment is to replicate the complete application across multiple different Kubernetes clusters. Each cluster is an exact copy – a replica – of the others, and can be physically proximate or distant from each other, as required. Load balancers allocate traffic between the clusters, typically based on origination. Image source: Implementing replication using a multi-cluster Kubernetes architecture (Bob Reselman, CC BY-SA 4.0) Assuming clusters are reasonably distributed, a replicated architecture delivers most, but not all, of the potential advantages we describe in our white paper. Since a replicated architecture is simpler to create and manage (as each cluster is identical), it is often the initial choice of DevOps teams who are willing to sacrifice other benefits in favor of ease of use. Similarly, if performance and resilience are your only reasons to move to a multi-cluster deployment, then a replicated architecture might be a good choice. Service-based Architecture: The More Powerful But Complex Multi-Cluster Option In a service-based application architecture, an application gets segmented into its constituent components (each a Kubernetes service) which are then assigned to specific clusters based on operational requirements. Services interact with each other across clusters, and the communication layer must be designed to appropriately route user and application traffic. If this sounds familiar, it’s because it mirrors the loose coupling typically seen with a micro-service oriented architecture. Image source: A multi-cluster segmentation architecture (Bob Reselman, CC BY-SA 4.0) A service-based architecture has distinct advantages and disadvantages over a replicated architecture. Its advantages are primarily improved isolation and granular control of data, workloads and compute resources, which in turn facilitate performance tuning/optimization, compliance, testing/production, troubleshooting, security, etc. It’s simply more versatile and flexible. The downside of this approach is added complexity in design and management. Service-based architectures are thus the go-to choice for more sophisticated teams looking to take advantage of all that multi-cluster deployments have to offer; these organizations are willing to tackle the additional complexity to reap all the benefits. Section Distributed Multi-Cluster: All the Benefits without the Complexity However – and it’s a big caveat – the added complexity of a service-based approach is eliminated in working with a provider like Section. The Section Edge Platform is built using a service-based architecture, so you get all of the benefits mentioned above, including the opportunity for fine-grained control of the compute environment. Yet our patent-pending Kubernetes Edge Interface and our patented Adaptive Edge Engine work in concert to simplify deployment and provide easy policy-based control of edge workloads. All of the benefits of a services-based architecture without the complexity. That’s the Section edge advantage. For a more in-depth look at benefits of multi-cluster architectures, check out our white paper – we promise it’s a quick and informative read. If you’re ready to see how easy it can be to move your application workloads to Section’s distributed multi-cluster edge platform, schedule a demo today."
"314","2022-05-03","2023-03-24","https://www.section.io/blog/multicluster-kubernetes-complexity-conundrum/","The following is an excerpt from our white paper on The Edge Experience – Streamlining Multi-Cluster Kubernetes Deployment & Management. Complexity is frequently cited as one of the main challenges in adopting a multi-cluster topology, and the more distributed that topology, the more complex it can be to manage. This creates a conundrum, because as explained in our earlier paper, the more distributed your multi-cluster environment, the more benefits you reap. Take latency, for example. This is one of the most important considerations for user experience, and one of the factors that’s directly impacted by edge computing. Simply put, the closer your application is to your user base, the more responsive that workload will be to user requests. This holds true regardless of whether your base is concentrated in a single region or spread globally; but the broader the distribution, the more latency and workload placement need to be carefully considered. Yet no less an authority than Google says that, while latency is one of the biggest factors in selecting compute regions (so important, they say, that you should regularly reevaluate and rebalance regions to reduce latency), concern over complexity can prevent addressing this issue. According to Google’s Best Practices for Region Selection “Even if your app serves a global user base, in many cases, a single region is still the best choice. The lower latency benefits [of distribution] might not outweigh the added complexity of multi-region deployment.” Clearly, complexity (and inherent cost) is a major challenge for distributed deployments, particularly when those deployments are federated – so much so that it can prevent organizations from making decisions that are clearly in their best interest. Where does this complexity come from? Two factors are in play. The first involves the tools and processes that are used to run the application workload itself in a multi-cluster Kubernetes deployment. The second is the management and operation of the underlying edge network, which gets progressively more complicated and cost-sensitive as distribution (across regions and networks/operators) increases. While there is considerable overlap in a modern DevOps environment, the development team (the app owner) tends to be more focused on the process pipeline, while the operations team (the infrastructure owner) wrestles with cluster management and network optimization. Image source: edureka Both are important to how organizations manage overall application delivery, and each has its own complexity challenges to overcome. Application Deployment at the Edge – The Developer View Moving application workloads to the edge differs from a centralized cloud deployment on two vectors: first, it’s multi-cluster instead of single cluster, and second, it’s distributed (and as part of that, often federated across different regions/operators). From a developer’s perspective, this distributed multi-cluster Kubernetes deployment impacts four elements of application ownership: CI/CD Integration Cluster Discovery Orchestration and Failover Between Clusters Credential Management Application Management at the Edge – the Ops View Not surprisingly, managing multiple distributed clusters is much more complex than managing a centralized deployment, especially as those clusters spread across regions and operators in a federated environment. The things that ops teams are concerned about include: Cluster Connection and Tracking End-to-End Security Policy Lock-Down (RBAC, etc.) Resource Management For deeper insight on these challenges, and how you can dramatically simplify deployment, please read the paper on Streamlining Multi-Cluster Kubernetes Deployment and Management – or even better, talk with someone at Section!"
"315","2022-01-12","2023-03-24","https://www.section.io/blog/edge-release-deploy-operate-monitor/","In a previous post we took a hard look at three key challenges for developers at the edge. But development is only half of the modern DevOps paradigm. There’s also the Ops side, specifically application release, deployment, operations and monitoring (the yellow bits in the image below). Image source: edureka Let’s assume that you’ve already embraced containerization, microservices, Kubernetes, CI/CD and other aspects of modern applications (and if you haven’t yet, you can learn more about our perspective here). Release/Deploy at the Edge In the discussion around developer considerations in our earlier post, one of the central observations we made was the need for familiar tooling, noting that if developers are forced to adopt entirely different tools and processes for distributed edge deployment, it creates a significant barrier. Efficient edge deployment requires that the overall process is the same as, or similar to, cloud or centralized on-prem deployments. Viewed through a DevOps lens, this means use of familiar tools like CI/CD solutions (e.g. Jenkins, CircleCI), API-backed CLIs (e.g. kubectl), package managers (e.g. Helm, Kustomize), and more. While that’s achievable with a single cloud instance, it becomes significantly more complex at the edge, where you can be running hundreds of clusters, with different microservices being served from different edge locations at different times. How do you decide on which edge endpoints your code should be running at different times? How do you manage across heterogeneous provider networks? In an ideal world, you wouldn’t – that should be the job of an edge platform provider. All you should have to do is simply hand over your application manifest, make some strategic settings choices, and then leave it to the edge provider to run your applications reliably, securely, and efficiently. Edge Operations A mid-2021 survey of IT decision makers, sponsored by Section partner Lumen Technologies, found organizations heavily focused on the benefits of edge computing. In fact, more than 90% of those surveyed believe that “moving their organization’s high-performance applications from cloud-based apps to metro edge will reduce lag time and improve performance… and [edge] provides the best balance of cost and performance.” That said, there’s a difference between wanting to move to the edge and an actual competency and willingness to oversee edge operations. 73% of those surveyed “would, if possible, rather manage applications only and never manage infrastructure…” Managing the basics of a distributed network is hard enough, as our white paper on Solving the Edge Puzzle makes clear. Optimizing global operations and ensuring security are even more complex, yet just as important. Again, these are areas where organizations need to be able to count on an edge platform provider for execution and support. Distributed System Observability and Monitoring For DevOps teams, it’s critical to have a holistic understanding of the state of an application at any given moment to ensure performance, usability, security and more. But this observability becomes increasingly difficult to achieve for distributed delivery nodes across diverse infrastructure from different providers. This can create a black-box scenario where organizations don’t actually know what’s going on with their application globally until they receive complaints or experience security issues. As reported in a recent Dynatrace survey of 700 CIOs, “The dynamic nature of today’s hybrid, multicloud ecosystems amplifies complexity. 61% of CIOs say their IT environment changes every minute or less, while 32% say their environment changes at least once every second.” To add to the complexities of trying to keep up with dynamic systems, that same report revealed that: “On average, organizations are using 10 monitoring solutions across their technology stacks. However, digital teams only have full observability into 11% of their application and infrastructure environments.” This lack of observability is unacceptable in an edge platform whose primary purpose is to improve application performance and usability. An effective platform for multi-cloud/edge deployment should be able to provide a single pane of glass to observe the state of an application, drawing together data from many locations and infrastructure providers. This kind of visibility is essential for developers to gain insight into the entire application development and delivery lifecycle. The right centralized telemetry solution will allow engineers and operations teams to evaluate performance, diagnose problems, observe traffic patterns, and share value and insights with key stakeholders. Edge as a Service: Managing the Distributed Network To accelerate edge adoption, operations teams need familiar tooling for application release and deployment, the ability to offload management of the underlying distributed network infrastructure to the platform vendor, and clear global observability and monitoring so they can diagnose issues and tune as needed. That’s why we offer an Edge as a Service platform – so DevOps teams can concentrate on managing the application, not the infrastructure. Section’s EaaS platform offers GitOps-based workflows, Kubernetes-native tooling, CI/CD pipeline integration, a broad range of security and operational tools, and a complete single-pane-of-glass edge observability suite. This, combined with the other benefits of EaaS for application deployment, gives DevOps teams the cost and performance benefits they’re looking for in an edge platform, without the need to master distributed systems management."
"316","2022-04-25","2023-03-24","https://www.section.io/blog/7-benefits-distributed-multicluster-deployment-2/","Last week we published Part 1 of a two-part series on the benefits of distributed multi-cluster deployments for modern application workloads, based on a recent Section white paper that dives deep into these topics. This is part two. The context for this discussion is the launch of our breakthrough Kubernetes Edge Interface, which makes it incredibly easy to move workloads to the edge using familiar tools and processes, and then manage that edge deployment with simple policy-based controls. But why move to the edge if you’ve already got a centralized data center or cloud deployment? If you haven’t read the first three benefits in the previous post, please take a moment to do so. Here are four more. Scalability In our previous post, we noted that improvements to performance and latency are a key benefit of moving application workloads to the edge (compared to centralized cloud deployments). A closely related factor: running multiple distributed clusters also improves an organization’s ability to fine tune and scale workloads as needed. This scaling is required when an application can no longer handle additional requests effectively, either due to steadily growing volume or episodic spikes. It’s important to note that scaling can happen horizontally (scaling out), by adding more machines to the pool of resources, or vertically (scaling up), by adding more power in the form of CPU, RAM, storage, etc. to an existing machine. There are advantages to each, and there’s something to be said for combining both approaches. A distributed multi-cluster topology helps facilitate greater flexibility in scaling. Among other things, when run on different clusters/providers/regions, it becomes significantly easier to identify and understand which particular workloads require scaling (and whether best served by horizontal or vertical scaling), whether that workload scaling is provider- or region-dependent, and ensure adequate resource availability while minimizing load on backend services and databases. Those familiar with Kubernetes will recognize that one of its strengths is its ability to perform effective autoscaling of resources in response to real time changes in demand. Kubernetes doesn’t support just one autoscaler or autoscaling approach, but three: Pod replica count - This involves adding or removing pod replicas in direct response to changes in demand for applications using the Horizontal Pod Autoscaler (HPA). Cluster autoscaler - As opposed to scaling the number of running pods in a cluster, the cluster autoscaler is used to change the number of nodes in a cluster. This helps manage the costs of running Kubernetes clusters across cloud and edge infrastructure. Vertical pod autoscaling - the Vertical Pod Autoscaler (VPA) works by increasing or decreasing the CPU and memory resource requests of pod containers. The goal is to match cluster resource allotment to actual usage. Worth noting for those running centralized workloads on single clusters: a multi-cluster approach might become a requirement for scaling if a company’s application threatens to hit the ceiling for Kubernetes nodes per cluster, currently limited to 5,000. Cost Another advantage to a distributed multi-cluster strategy is fine-grained control of costly compute resources. As mentioned above, this can be time-dependent, where global resources are spun down after hours. It can be provider-dependent, where workloads can take advantage of lower cost resources in other regions. Or it can be performance-dependent, whereby services that aren’t particularly sensitive to factors like performance or latency can take advantage of less performant, and less costly, compute resources. Isolation A multi-cluster deployment physically and conceptually isolates workloads from each other, blocking communication and sharing of resources. There are several reasons this is desirable. First, it improves security, mitigating risk by limiting the ability of a security issue with one cluster to impact others. It can also have implications for compliance and data privacy (more on that below). It further ensures that the compute resources you intend for a specific cluster are, in fact, available to that cluster, and not consumed by a noisy neighbor. But by far the most common rationale behind workload isolation is the desire to separate development, staging/testing and production environments. By separating clusters, any change to one cluster affects only that particular cluster. This improves issue identification and resolution, supports testing and debugging, and promotes experimentation and optimization. In short, you can ensure a change is working as expected on an isolated cluster before rolling it into production. As noted, this isolation is an inherent benefit of multi-cluster vs single cluster deployments, and not particularly dependent on how widely distributed those clusters are – you could have isolated stage/test/production clusters in a centralized hosting environment, for example. That said, when you distribute multiple clusters, this isolation can have important implications. Compliance Requirements Compliance is a critical consideration for many organizations. It can be closely related to isolation – workload isolation is required for compliance with standards such as PCI-DSS, for example – but it also has broader implications in regard to distributed or multi-region workloads. In this instance, different countries and regions have unique rules and regulations governing data handling, and a distributed multi-cluster topology facilitates a region-by-region approach to compliance. A notable example is the European Union’s General Data Protection Regulation (GDPR), which governs data protection and privacy for any user that resides in the European Economic Area (EEA). Importantly, it applies to any company, regardless of location, that is processing and/or transferring the personal information of individuals residing in the EEA. For instance, GDPR may regulate how and where data is stored, something that is readily addressed through multiple clusters. Similarly, other regulatory or governance requirements might stipulate that certain types of data or workloads must remain on-premises; a distributed multi-cluster topology allows an organization to meet this requirement while simultaneously deploying clusters elsewhere to address other business needs (to the edge for lower latency, for example). Image source: DLA Piper Data Protection Laws of the World interactive tool So there you have it, seven key benefits of edge deployments: Improved Availability and resiliency Avoiding vendor lock-in Better performance and latency Greater scalability Lower cost Increased workload isolation Flexible compliance Ready to see how easy it is to move your workloads to the edge with our new Kubernetes Edge Interface? Request a demo."
"317","2022-04-20","2023-03-24","https://www.section.io/blog/7-benefits-distributed-multicluster-deployment/","With the recent release of Section’s patent-pending Kubernetes Edge Interface, it’s important to step back and look at why organizations need a solution that makes it easy to move application workloads to the edge. What are the benefits of distributed multi-cluster Kubernetes topologies over centralized cloud or data center deployments for modern organizations? Granted KEI makes it simple, but why go through the effort at all? We recently published a white paper on modernizing applications with distributed multi-cluster Kubernetes that dives deep into these topics, discussing everything from topology definitions to differing application architecture approaches. But at its heart, it outlines seven key benefits to distributed multi-cluster deployments. In this post, we’ll explore the first three. Stay tuned next week for the final four… or feel free to check out the white paper to read ahead. As noted in the white paper, organizations who have embarked on the path of containerizing their applications are faced with many decisions around how and where to deploy their containerized workloads. Many follow a maturity model that is characterized by a series of progressive transitions from single cluster to centralized multi-cluster to distributed multi-cluster to, ultimately, multi-region edge deployments. The question is: why? What are the advantages and benefits of a distributed topology over centralized clusters? As importantly, does the breadth of distribution matter? Are organizations that adopt a multi-region, multi-provider approach gaining advantage over those that don’t? Let’s dive in. Availability and Resiliency By mirroring workloads across clusters, you increase availability and resiliency through elimination of single points of failure. At its most basic level, this means using a secondary cluster as a backup or failover should another cluster fail. As you increase cluster distribution outside of a single data center/cloud instance to stretch across clouds and providers, you further minimize risk – from failure of single endpoints within a provider network, or even failure of an entire provider network – by ensuring your application can fail over to other endpoints or providers. Every year there are reports of wide swaths of the internet ‘going dark’ and taking down well-known brands and applications. Invariably, those issues are traced back to problems within a particular provider network. Distributed multi-cluster deployments help mitigate those risks. Vendor Lock-in Avoiding reliance on a single vendor is an operational mantra for many organizations, making the ability to distribute workloads not only across locations, but across providers a key advantage. This multi-vendor approach improves pricing flexibility, as well as ensuring better continuity and quality of service. Similarly, this can help mitigate data lock-in, whereby it can become prohibitively expensive to migrate data off a provider’s network. In fact, a distributed multi-cluster, multi-vendor approach even helps mitigate managed Kubernetes lock-in, ensuring you are not committed to a particular provider’s version of Kubernetes and any proprietary extensions supported by a specific provider’s managed Kubernetes service. Performance and Latency According to a recent survey of IT decision makers by Lumen, 86% of organizations identify application latency as a key differentiator. And the single best way to reduce latency is to reduce geographic distance by physically placing applications closer to the user base. Distributed multi-cluster Kubernetes facilitates this strategy, allowing organizations to use an edge topology to process data and application interactions closer to the source. This becomes especially important – in fact, arguably a necessity – for applications that have a global user base, where multi-region edge deployments can geographically distribute workloads to best reduce latency while efficiently managing resources (e.g. spinning up/down infrastructure to adapt to “follow the sun” or other regional and ad hoc traffic patterns). Organizations that elect a centralized approach are, by definition, treating users outside of the primary geography as second-class citizens when it comes to application performance. In fact, this is the primary consideration when organizations choose a particular cloud instance for deployment (e.g. AWS EC2 US-East): where are most of my users based, so those within or close to that region will enjoy a premium experience? The unspoken corollary is that as customers get geographically further from that particular region, application performance, responsiveness, resilience and availability will naturally degrade. In short, this is the default “good enough” cloud deployment for application workloads that cater largely to a home-grown user base. As these applications mature and broaden adoption, this strategy becomes increasingly tenuous, and organizations find themselves compelled to move workloads to the edge. Stay tuned for the final four benefits in our next blog post – or, if you’ve heard enough and you’re ready to get started, drop us a line and let Section show you how easy it can be to move to the edge using your familiar Kubernetes tools, workflows and processes."
"318","2022-04-12","2023-03-24","https://www.section.io/blog/achieving-edge-roi/","One of the top five must-haves in an Edge as a Service (EaaS) solution is outstanding business value – meaning your EaaS solution provider should deliver this along with all the other benefits of edge computing, including improved performance, security, reliability, – so ultimately – it must deliver return on investment (ROI). There are many different metrics that can be used to determine ROI. Some, such as the revenue payback from increased performance and improved uptime, or the decreased cost associated with team efficiency and resource utilization efficiency, are more obvious. Others, like time to market, or reduced attack surface, can be more difficult to recognize and assess. Let’s look at a few of the latter as they apply to edge computing. Consistent Security Plane Security is a top concern for every company today. Yet a consistent security plane might not immediately come to mind as a crucial ROI component for an edge platform. An EaaS solution should be designed with protection in mind, providing excellent defense against a diverse set of threats across the infrastructure, network, transport, and application layers. And in an edge computing paradigm that’s made up of heterogeneous networks of providers and infrastructure, EaaS solutions must be able to protect applications across a diverse footprint. As cybersecurity threats continue to increase in sophistication and scale, it’s becoming both harder and more critical to protect against attacks and prevent breaches. Yet sooner or later, organizations that fail to do so will suffer a significant blow to not just their information systems, but inevitably their reputations and their bottom lines. When evaluating edge solutions, application builders should look for solutions that take a security-first approach at the platform level. Core Product Focus Let’s say you’re a technical product lead tasked to improve the application experience for customers. Common areas of focus will be application latency and responsiveness, and moving workloads and data closer to the user (i.e., to the edge) is an obvious solution. Less obvious, however, is that the wrong edge deployment strategy may put you in the business of managing a distributed network. As importantly, this means it is taking away from resources that could be better spent improving the application itself. In simple terms, what you’ve gained in performance and scale, you’ve lost in application functionality and features. With an EaaS solution to handle all your edge computing requirements, your development teams will have more capacity to focus on core product innovation to drive company growth. As your business grows, your EaaS solution should expand with you – enabling you to continually provide responsive and reliable applications. Fastest Time to Market Many organizations are already rushing to modernize applications with multi-cluster Kubernetes deployments. But if your software solution is currently hosted in a single cloud instance, what happens when your customer base starts to scale and expand globally? The right edge solution can be a natural and immediate extension of an existing multi-cluster strategy, allowing you to leapfrog centralized cloud deployments to achieve significant benefits in performance, scalability, resilience, isolation and more. The key is that the chosen edge platform needs to allow you to deploy your distributed application fast, without the headaches of network provisioning and ideally without having to change your development cycles, process or tooling. Conversely, a more complex deployment will delay time to revenue (and thus ROI) because of complex onboarding / deployment processes. If there’s a bottleneck in onboarding because it’s too complex for your customers or there’s a long cycle to get an existing application to production on edge, the ratio of Time to Value over Time to Revenue could suffer. The modern edge provides for a significantly better application experience, but only if it can be simple and affordable to adopt. Given the extreme complexities involved with building and operating distributed systems, it’s imperative to find an EaaS solution that will help cost-effectively accelerate your path to edge. Sunk Cost In economic terms, a sunk cost is one that has already been incurred and cannot be recovered. In making decisions, it’s important to recognize that these costs must be treated as bygone and not taken into consideration when investing in a project. This is an important consideration when considering deployment strategies because all things being equal, the distributed edge always beats centralized cloud. Always, and across every metric – except simplicity. If you can conquer that one factor, your application is always better off at the edge. If you can’t, then the decision is less clear. From an ROI perspective these issues of simplicity and sunk cost can be eye opening. If the edge becomes as simple to deploy to as centralized cloud, and you know that your application workload will improve across every meaningful metric – scalability, performance, latency, resiliency, compliance, cost, etc. – by moving to the edge, then your centralized cloud deployment should be treated as a sunk cost, one in which further investment actually decreases returns. Think of it as good money after bad. Long story short… All things being equal, the distributed edge beats centralized cloud every time. But is it possible to realize edge outcomes quickly and easily without having to sacrifice control, simplicity or flexibility? Yes! And we can help. Here at Section, we’re committed to helping organizations make their edge ROI dreams a reality, simply."
"319","2022-03-01","2023-03-24","https://www.section.io/blog/the-business-case-for-edge-deployment/","When considering the merits of moving application workloads to the edge, it’s necessary to look not just at technical considerations – which we’ve spent a lot of time examining in previous posts – but overall business drivers and benefits, and how those apply to a particular organization, application and user base. At its simplest level, the business decision around where to place application workloads boils down to: Am I able to cost-effectively deliver the application experience my users expect? Yet this relatively benign question hides a myriad of important business considerations. Let’s start with setting the playing field: user expectations. User Expectations For most organizations, the pandemic kicked digital transformation into overdrive. This is a topic that’s been beaten to death, but it’s still worth considering carefully – businesses have shifted more of their user interactions and engagement to application workloads, and users simply expect more from the applications they use. More features, more functionality, more responsiveness, more availability. These expectations are only growing thanks to the ubiquity of mobile and SaaS apps in day-to-day life, and… these expectations will never reset. Like, ever. The bar is high, and will only climb higher. Organizations that haven’t come to grips with this new reality will inevitably fall behind. User Experience Alright, so users expect a lot. Fine. How does that impact application deployment decisions? Consider this: no matter how good your application is, it will never be able to overcome a poor user experience. That experience fundamentally boils down to application responsiveness and availability. Amongst other factors, responsiveness is a function of latency, or how long it takes for data to transfer from one point on a network to another. According to a 2021 survey by Quadrant Strategies and Lumen, 86% of C-suite executives and senior IT decision makers agree that low-latency applications help differentiate their organizations from the competition. At the same time, 80% are concerned that high latency is impacting the quality of their applications. More than 60% of respondents further defined low latency for mission critical apps as 10 milliseconds or less. Let’s consider a simple example: deploying an application to the cloud with a user base that is equally distributed across the U.S. Where do you host the application? On the east coast, knowing that performance for 50% of your user base located outside the east coast will suffer? Somewhere in the middle of the country, so that the experience for everyone is equally sub-par? Once you’ve accounted for all other factors, the only way to improve latency is to physically move workloads and data closer to users – in other words, toward the edge. The more geographically dispersed your user base, the more important this becomes. For a global user base, for instance, centralized cloud deployments quickly become untenable as workload scales; the only answer is edge deployment. Availability is the other side of the experience coin, and the dirty secret is that any given network will, inevitably, go down. The result is a steady stream of headlines about major cloud outages and frustrated users. The way around this is to build in redundancy and resiliency for application workloads. Centralized cloud deployments have little resilience, as they are dependent on that cloud provider. When the network experiences an outage, so do the applications. Edge deployments, on the other hand, can readily work around this, provided that the deployment isn’t tied to a single network operator. Workloads must be broadly distributed across heterogenous providers, such that if (or rather, when) one goes down, the problem can be routed around to ensure continued application availability. Cost Which brings us to the third leg of the stool: how cost effectively can I deliver the expected experience? This question can quickly devolve into technical considerations around workload scalability, allocation of compute resources, network operations, workload isolation, data compliance, etc. There are inevitable pros and cons to different deployment strategies that must be considered. However, all things being equal, the distributed edge beats centralized cloud every time. But since we’re focused on business benefits, let’s try to step back and simplify the question by rephrasing it: Do I want my organization focused on delivering the best possible business applications and services, or on the cost and complexity of operating a distributed network? Because, as we’ve written about numerous times before, rolling your own edge is not for the faint of heart. Many organizations are already rushing to modernize applications with multi-cluster Kubernetes deployments, and the edge is a natural extension of that strategy, delivering significant benefits in performance, scalability, resilience, isolation and more. Those twin considerations – the modern edge provides for a significantly better application experience, but only if it can be simple and affordable to adopt – are the drivers behind the creation of Section’s Edge Hosting platform. By delivering Edge as a Service that can simply be deployed as part of an existing containerized environment using familiar Kubernetes tooling, Section eliminates the challenge of deploying applications to the distributed edge. Section’s Composable Edge Cloud offers a federated on-demand multi-cloud network, ensuring high availability and resiliency worldwide. And the patent-pending Adaptive Edge Engine automatically orchestrates and intelligently scales workloads to meet real-time traffic demand, ensuring cost-effective low-latency responsiveness for users, no matter how many there are or where they’re located. The business case for edge networking is that the edge is simply better, as long as it is simply deployed. That’s why we built Section’s Edge Hosting Platform."
"320","2022-01-14","2023-03-24","https://www.section.io/blog/importance-of-application-latency/","If you’ve already explored every corner of our Solving the Edge Puzzle white paper and are in need of another great edge resource to sink your teeth into, look no further than Section partner Lumen Technologies’ recent Trend Report: The Edge Computing Imperative. Based on an April 2021 Quadrant Strategies survey, this report features data gleaned from more than 1,700 C-suite executives and senior IT decision makers from large and midsize organizations worldwide, and covers business requirements and use cases driving the adoption of edge compute infrastructure, software, services and platforms. Among the things you’ll learn from Lumen’s report are: Factors driving your peers to the edge Benefits of edge compute workloads vs. on-prem and public cloud Why network latency and security matter in the 4th Industrial Revolution Why organizations need trusted partners to help them maximize the value of their edge solutions For us, the findings on the importance of application latency grabbed our attention. Let’s take a closer look. Edge Location, Edge Location, Edge Location An effective presence at the edge is based on having a robust location strategy. By moving workloads as close as possible to the end user, latency is reduced – and as we’ll see shortly, application latency is a critical metric for most organizations. In fact, according to the Lumen report, “Global IT decision makers are twice as likely to say edge compute would do a better job of lowering latency and improving overall application performance than the public cloud.” Image sourced from Lumen Trend Report: The Edge Computing Imperative Yet selecting and managing the appropriate geographies for your specific application within a distributed compute topology involves careful planning. Businesses (who haven’t done so already) tend to start with migration of selected applications to a centralized public cloud, prior to deploying next-generation applications across a distributed edge footprint. Lumen points out this can be problematic on a number of levels: Cloud deployments can result in disappointment in terms of performance and cost, especially for applications (like dynamic AI) that require many round trips, which tend to drive up bandwidth cost and overall latency. Security is also a big worry for cloud/edge deployments, with 80% of those surveyed citing security as a critical concern when moving on-prem applications to the cloud. That said, Lumen notes that only 28% of global IT decision makers intend to keep on-prem computing as an integral part of their business, and as a result, those surveyed expect to double use of distributed applications in the next five years. IT leaders now see edge services as a logical extension of the value and agility of cloud services. In other words, notes Lumen, edge extends the benefits of the cloud. But determining how to efficiently deploy and manage distributed systems can be a challenge. It’s no wonder nearly three quarters of the US-based respondents in Lumen’s report noted that they’d prefer to only manage applications vs. managing infrastructure. The Latency Imperative Why is edge computing so important? As evident in the Lumen report, the importance of application latency cannot be understated. To quote directly from the text: 86% of global IT decision makers agree that low-latency applications help differentiate their organizations from competitors, over 30% say low latency solutions are necessary for the success of their organizations. It is not surprising then, that 80% also are concerned that high latency is impacting the quality of their applications. When asked for the latency requirements of their most mission-critical applications, 8% say 30+ milliseconds, 21% say 20 milliseconds, 44% say 10 milliseconds, and 17% say 5 milliseconds or less. As dynamic interaction with users or devices increases, business leaders and IT decision makers expect to require even lower latency: more than half of global IT decision makers say their most mission-critical application will require 5 milliseconds or less latency five years from now. If IT decision makers are overwhelmingly focused on the importance of latency for their own systems and applications, they’re darn sure going to care about latency in any SaaS apps they’re using from others. In short, ignore latency at your peril. That said, while latency is the primary driver for edge adoption among the IT decision makers surveyed in Lumen’s report, there are other factors that are also pushing this architectural paradigm shift. In case you missed it, we discussed these in more detail in a previous blog post about how latency, reliability, and security are accelerating edge computing innovation and adoption – and why the demand for operational simplicity is a parallel thread to total Edge application delivery."
"321","2021-04-07","2023-03-24","https://www.section.io/blog/latency-reliability-security-edge-computing/","Latency is often considered the primary driver of the edge computing movement. However, there are other factors, beyond speed, that are pushing this architectural paradigm shift. In this post, we’ll look at how latency, reliability, and security are accelerating edge computing innovation and adoption, and why the demand for operational simplicity is a parallel thread to total Edge application delivery. Latency: The need for speed Applications today are far different than they were ten years ago. Adding to core web application and gaming needs, even in the last two years, we’ve seen dramatic innovations in technology being driven by emerging use cases such as autonomous vehicles, smart cities and homes, real-time communications, and many more. What all of these use cases have in common is the need for a move from human to machine speed. Edge computing enables developers to design applications that capture, store, and process data closer to where it’s generated, delivering dramatic performance improvements, alongside bandwidth savings and other benefits. However, as we’ll discover in the next sections, reduced latency isn’t the only driver of the move to distributed computing architectures. Reliability: Designing for failure Before optimizing for speed, application architects are faced with the challenge of designing systems that enable workloads to perform their intended functions accurately and consistently when expected. Reliability is a cornerstone of application design and one of the primary considerations leading developers to the edge. Each year, media sources chronicle all the major cloud outages that took place over the course of the year, e.g. 5 Cloud Outages That Shook the World in 2020. In many of these instances, core services were impacted, often going offline for an extended period of time and taking down many highly trafficked applications in their wake. This centralized blast radius inevitably makes application architects question deployment models. The only guarantee when it comes to computing and networking infrastructure is that there will be failures, and downtime translates to money lost. When all operations are concentrated in a centralized system, all it takes is one point of failure to disrupt or bring down the entire system. Designing fault tolerant, resilient systems is no small feat. Application architects are increasingly looking to hybrid, multi-cloud, and cloud-edge models of deployment in order to mitigate the risk of degraded service and outages. Building redundancy into systems allows for failover to healthy systems when something goes wrong, minimizing the impact of incidents. Security: Compliance and threat intelligence As the pace of technology innovation continues to accelerate, so too does the growing sophistication of threats, targeting, and attacks. On top of that, increased regulatory requirements are forcing organizations to rethink how, where, and when they process and store data. Edge computing topologies are extending flexibility and granular control to application architects to meet compliance requirements, while also mitigating the impact of potential security breaches. Processing data closer to end devices allows for earlier threat detection and mitigation, before attack agents are able to penetrate mission-critical operations. Furthermore, the amount of data needing to traverse the network is significantly reduced, minimizing the attack surface and allowing application architects to focus on protecting the most vulnerable vectors. When an attack occurs, the distributed nature of edge computing infrastructure enables security protocols that are designed to seal off compromised services without shutting everything down. Operational Simplicity Historically, the notion of moving elements of an application to the Edge has been associated with increased complexity. An additional delivery plan to manage means an additional deployment and diagnostic plan to consider for operational management. However, latency, reliability, and scalability benefits of moving parts of the application to the Edge have outweighed the downside of increased operational complexity. Edge developments over the last few years have now turned this paradigm on its head. New Edge as a Service (EaaS) solutions present the opportunity for engineers to place the entire application at the Edge and deliver it from one plane, simplifying the operational and deployment model. Compared with the legacy cloud-edge hybrid model, moving 100% to Edge is bringing operational simplicity. Conclusion As the global population depends more and more on interconnectivity of data and applications, deployment models need to keep up to ensure fast, reliable and secure experiences for end users. There is no one size fits all when it comes to architecting optimized systems, and there are often trade offs based on prioritization of objectives. When asking what’s most important for a particular organization or application, it’s important to evaluate all factors alongside each other. For example, when does resilience matter most? When is reliability most important? Latency? Security? There is no single answer to any of these questions, and the answers are always evolving. The extension of cloud to edge is giving application architects more flexibility in being able to address all of the above questions in more granular terms. However, with increased granularity comes greater complexity. Edge as a Service (EaaS) is helping to simplify this edge-cloud puzzle. Want to learn more? Start customizing your edge solution today."
"322","2021-03-23","2023-03-24","https://www.section.io/blog/orbitdb-ipfs-edge-computing/","Does InterPlanetary File System (IPFS) hold the keys to solving full application hosting at the Edge? One of our latest Section Labs initiatives has us exploring IPFS as a potential solution to solve some of the inherent challenges with distributed databases across hundreds of points of presence, from multi-cloud to regional data centers to edge. To deepen our understanding of the problem space and how a practical implementation might work, we’re actively developing a proof of concept using OrbitDB, an open source serverless, distributed, peer-to-peer database. The Challenge Up until recently, most edge computing use cases have been stateless. However, as developers seek to push more advanced logic to the edge, the requirement for handling stateful data is inevitable and on our doorstep. Over the past year, we’ve been exploring established solutions to solve the distributed database challenge, including conventional distributed databases and geo-distributed databases. Although encompassing various elements of a ‘distributed’ architecture, our findings to date reveal that current models still depend on some form of a centralized state management system. Furthermore, our experiments with a handful of existing market solutions have shown them to fall over when it comes to handling the scale of persistent data across 30+ endpoints (let alone the hundreds, or thousands that are expected as the edge computing market matures). The Solution After running into challenges with more conventional solution offerings, we opted to come at the problem from a different angle. To this end, we’ve been working up an initial proof of concept that involves deploying an IPFS swarm using our points of presence as peers and enabling distributed Node.js apps to access the IPFS swarm to access their OrbitDB data sets. What is OrbitDB? OrbitDB is a serverless, distributed, peer-to-peer database. OrbitDB uses IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. It’s an eventually consistent database that uses CRDTs for conflict-free database merges making OrbitDB an excellent choice for decentralized apps (dApps), blockchain applications and offline-first web applications. The architecture of OrbitDB is fairly straightforward. As a peer-to-peer database, each peer has its own instance of a specific database. A database is replicated between the peers automatically, resulting in an eventually consistent data set. Each supported database (it currently supports five types – log, feed, docs, keyvalue, counter) has its own API, each with its own methods to create, delete, retrieve and update data. With its simplistic approach, OrbitDB also serves as a good foundational framework for others to build more sophisticated data models on top (e.g. AvionDB). Our Implementation For our MVP implementation, we’re exploring how we can deploy OrbitDB across Section’s Composable Edge Cloud. We’re still early in this exploration, but our approach involves running a private IPFS swarm across all of our edge endpoints (i.e. points of presence, PoPs), so that any app running on Section can access the peer network. Since we’re particularly interested in full application hosting on Section, we’ve deployed a distributed Node.js application with OrbitDB running alongside, across a series of edge endpoints. The Node.js app connects to IPFS daemons running on the edge endpoint where the app is hosted. We use the OrbitDB framework to create the database structure and schema in IPFS storage. For our MVP, we’re testing a basic key/value store, but one of the challenges that we foresee is the current lack of support for relational databases (perhaps a successful basic implementation could lead us down a path to solve for this later… onward!) By replicating the databases across all edge endpoints, some of the challenges that we’re running up against include: Network latency Time it takes to populate state (depends on size of data store) Memory-heavy Security Many of these challenges have been documented and are being worked on. If interested, you can track them in this Github issue. Where to from here? Our product and solutions engineering teams continue to talk with customers around the feasibility of this approach, including implementation of test use cases to validate whether OrbitDB is a viable distributed database solution for the edge. At the same time, we’re preparing to roll out some production workload to test replacement of our current key/value store used for Virtual Waiting Room with OrbitDB key/value store. (Some might say ‘eating our own dog food’.) If you’re interested in exploring distributed database solutions, whether with OrbitDB or an alternative approach, we’d love to chat! Please reach out."
"323","2021-03-18","2023-03-24","https://www.section.io/blog/state-of-the-edge-2021-highlights/","In this blog, we highlight some of the key findings from the 2021 State of the Edge Report produced by the State of the Edge project within LF Edge. The only collaborative report of its kind in the industry, this year’s edition is organized around four areas of innovation: critical infrastructure, hardware, networks and networking, and software. After a quick overview of key findings, we’ll take a closer look at the software at the edge section. Overview: Key Findings “The edge, with all of its complexities has become a fast-moving, forceful and demanding industry in its own right.” - Matt Trifiro, CMO, Vapor IO and Jacob Smith, VP Strategy and Marketing, Equinix Some of the key findings in this year’s highly anticipated report include: Despite COVID, and “in some cases driven by it”, the deployment of new edge infrastructure and applications did not stop across 2020. In fact, seven out of ten areas saw increased forecasts over the last year of lockdown. Between 2019-2028, LF Edge predicts that some $800 billion USD will be spent on new and replacement IT server equipment and edge computing facilities. Tolaga Research, which led the market forecasting for the report, says this colossal investment in edge infrastructure will be necessary to support continually growing demand from edge devices and infrastructure. Accordingly, infrastructure edge deployments will see a hefty increase in the global IT power footprint over the next several years. It is conservatively forecasted to grow from 1 GW in 2019 to over 40 GW by 2028. Thinking about this from a sustainability perspective now is critical. As opposed to an Internet of Things, we should be thinking instead about an “Internet of Systems”, in which devices serving different vertical applications within different systems must communicate directly with each other to exchange knowledge. This needs to be achieved “autonomously and securely with no single point of failure”. Traditional security policies are typically put into place using vendor-specific point solutions. However, this is no longer viable at the edge due to scalability needs and potential vulnerabilities, particularly at scale. Next-gen Software-Defined Wide Area Networking (SD-WAN) tooling and the Secure Access Service Edge (SASE) framework will amp up, bringing security, resiliency and session-awareness to enterprise connectivity. SASE has a cloud native design and is capable of integrating networking and security capabilities into a single architecture through management of the connections between individual endpoints and service edge nodes. Hybrid IT, “the edge cloud construct” is increasingly viewed as an essential enabler for the “Fourth Industrial Revolution’'. This will involve the increased use of IoT, the rise in the global sharing economy and the growth of zero marginal cost manufacturing to deliver “unprecedented communication-driven opportunities with massive economies of scale.” Software at the Edge Software is central to edge computing. It enables application delivery, management of edge hardware, and facilitates how workloads move around networks. In some regards, developing and running software at the edge is comparable to other dev experiences, but perhaps the greatest difference is the sheer diversity of types of edge workload and environment to program for and manage. According to technology journalist Simon Bisson, “The ecosystem is building a new stack to run at the edge of our networks, taking lessons from the hyperscale cloud, from IoT, from metro data centers, and from content delivery networks and the web, mashing them all together and building something new to suit new hardware, new networks, and a new generation of applications.” 5 Key Trends for Developers at the Edge Five key trends outlined in the report that developers working at the edge should take note of are as follows: 1. Open source is driving innovation at the Edge We are seeing a huge wave of innovation in software development, much of it open source. This is enabling visions of a near-term future in which “hybrid edge clouds will enable API-first microservices with integrated services for authentication, authorization and identity management regardless of device type, operating system and network.” This type of integration across different software systems will enable greater innovation through the development of an open ecosystem, which can deliver scalability, extensibility and interoperability. 2. Code needs to be more portable and scalable For developers working at the edge, State of the Edge suggests that the “one message” for developers looking at moving to the edge is to explore ways of making code more portable and scalable by: Recognizing that code at the edge needs to run on a diverse mix of hardware in a diverse mix of locations, ranging from servers in remote field locations to microcontrollers in devices on customer premises. Being aware that code also needs to support workloads migrating from edge node to edge node to the hyperscale cloud and back again. 3. Changing patterns are being seen in how users manage systems and software Management of systems and software at the edge is far from straightforward. State of the Edge breaks the edge stack down into three different layers, each of which requires a different blend of engineering skills: Systems layer - firmware, operating systems and hypervisors: the technologies necessary to work directly with edge hardware. Implementation and management layer - involves the use of tooling that will support modern applications, such as Kubernetes and OpenStack Deployment and operation layer - this top layer enables effective management of distributed applications at scale using methodologies like GitOps and CI/CD. All layers require an observability layer, which needs to offer information and insights tailored to different stakeholders. “In order to accelerate edge computing adoption, edge platforms need to remove the burdens for developers and operations engineers when it comes to managing the complexities associated with infrastructure provisioning, workload orchestration, traffic routing, scaling and monitoring, all while minimizing impact on application design. This is the critical role that edge platforms play in the future of the Internet.” Stewart McGrath, CEO and Co-Founder, Section 4. The rise of cloud native platforms at the Edge of the network Another trend State of the Edge highlights is the development of “easy to install, easy to manage cloud-native platform(s) at the edge of the network.” Vendors are bundling software elements necessary for edge deployment into a single platform, the report notes. The two main objectives are: (i) lower risk for the end user (ii) enable the building and delivery of packaged software that uses a familiar set of tools and methodologies for the developer. The critical advantage for end users and service providers utilizing this kind of platform is removing the complexities involved in deploying and managing edge hardware. Instead, customers can work directly with the cloud native platform to support the customers’ specific set of edge compute requirements. 5. Running workloads at the Edge requires complex decision-making Running real-time workloads across distinct and disparate edge infrastructure introduces many complexities to developers and operators. Questions they must continually ask include Which workloads should run where? How should failovers and geo-redundancy be taken care of? How can you maintain continuity and service-guarantees across devices in motion, e.g. a drone or autonomous vehicle? Latency-critical workloads, such as cloud gaming, real-time IoT analysis, VR, autonomous vehicles, and others are just a few of the use cases driving the need for edge computing. Putting compute as close to the end user as possible in these cases “ensures sufficiently agile responsiveness and reduces the risk of degrading user experience.” Orchestration solutions are emerging to tackle the complex scheduling challenges involved at the edge. A custom scheduler for edge computing needs to “take into account increasingly sophisticated levels of edge criteria for workload placement, automating decisions in real-time, abstracting away the complexity from developers and operators.” To read the full report, visit State of the Edge Report 2021"
"324","2021-02-04","2023-03-24","https://www.section.io/blog/monolith-microservices-edge-computing/","In this blog, we will look at the journey in application development from monolith to microservices to edge, the drivers behind it, and why using microservices at scale is the first step towards technical evolution at the edge. From Monolith to Microservices… Monoliths In a monolith application, a single unit of deployment handles multiple types of business activity with the front-end and back-end being tightly coupled. Often monolithic applications are a good way to get an application started, but can become unwieldy as businesses become more established and start to scale. IBM describes monoliths in these terms: “The application that grew over time, became unmanageable and difficult to understand, and has low cohesion and high coupling.” Typically, businesses find that as their user base grows, more innovation, new features and more integrations are required. The monolithic approach then becomes a bottleneck to growth due to: The core code base becoming highly complex A long time to market A steep learning curve for new developers Large dependencies Longer deployment time Because of these drawbacks, businesses are increasingly breaking monolithic technology stacks down into microservices. Microservices Taking a microservices-based approach to application design means that each core business capability can be deployed as its own separate unit, which performs only a single function. Working in this way gives engineering teams the flexibility to organize their code in step with the logic of the business, allowing separate components of the system to be developed and scaled at different times and by different teams. There are numerous drivers of the shift from monolithic applications to microservices-based applications. These include: An industry-wide shift from on-premises infrastructure to cloud/edge. The evolution of VMs to containers. Open-source tools and cloud native services evolving to meet developer need for microservices-oriented architectures. In addition, industry leaders who have undergone the journey from monolith to microservices have frequently been vocal about sharing their migration stories, challenges and expertise. Industry titans who have shared their monolith to microservices story include: Netflix - who experienced significant benefits in performance, development and scalability. Google - “Google encouraged engineers to try and do something that was audacious, and that led to a lot of the systems that they created”. Amazon - within a year of migrating to the AWS cloud, engineers were deploying code every 11.7 seconds on average. These transparent accounts of their successes and failures have helped the wider community more easily accept and overcome the challenges around the use and development of microservices. The benefits of microservices include: Logic that follows business capabilities. In his seminal blog post on microservices, Martin Fowler highlights the way in which microservices enable the building of products versus projects. Development teams are organized around business capabilities as opposed to technologies, meaning that services can be adapted for use in different contexts. This level of code reusability offers the flexibility to rearrange services and their functionalities, allowing the same data to be processed by different services and teams. Applications are easier to build and maintain. The agility that a microservices-based approach offers is often a key draw for developers. Service boundaries are explicit when each application is split into a series of smaller, composable fragments. Managing code is easier since each microservice is its own separate chunk of code. Services can be implemented using a wide range of functional programming paradigms and frameworks. Additionally, development is language-agnostic. Again, this means that each service can be deployed, built, redeployed and maintained independently of one another. Designed for failure. Since they are loosely coupled, microservices can be independently tested and deployed. According to Google, “The smaller the unit of deployment, the easier the deployment.” Clearer boundaries mean that if one service fails, only that one function goes down, as opposed to the entire application. Nonetheless, there are challenges with a microservices-based approach, which include: Increased complexity of the overall system. Since a microservice-based application is a network of different services, they typically interact in ways that aren’t predictable. Therefore, both the increased number of services and their interactions mean that the overall complexity of the system grows. Communication and potential security challenges. Microservices communicate over a network compared to internal containment in a monolith. This can lead to communication challenges and can introduce new security challenges. Potential performance degradation. Performance can suffer compared to “a monolithic approach because of latencies between services” (Google). This can significantly undermine the benefits of microservices. However, there are solutions to help address these challenges, as we’ll see in the next section. … And From Microservices to Edge It is becoming increasingly evident that edge computing offers further opportunity to optimize performance and management for each service within a microservices-based application, and help overcome the associated challenges. Why the Edge benefits from microservices The growing importance of n-layer as opposed to flat architectures is being driven by latency and data constraints. Edge computing brings into play aggregation layers because it involves hierarchical computing, orchestration and placement of workloads. This is very different to the standard approach today of delivering microservices by flat hyperscalers. Not every workload in an application is suitable for edge deployment. Hence, having an application built of smaller composable units offers much greater flexibility in being able to migrate only the workloads that require the benefits that the edge offers, while still working in concert with cloud-based microservices. Why microservices benefit from edge computing Compared to monolithic application development, microservices have an increased level of network communication since everything relies on network calls to one another for the linking of services. Therefore, benefits of edge computing for microservices include: Performance. The lower latency that edge computing provides helps microservices perform better. Bandwidth. The amount of bandwidth required by many IoT devices is enormous (particularly for streaming data). Local processing saves time and reduces the strain on Internet infrastructure. Improved availability. Edge routing based on user location, client type and other factors, eliminates the need for redundant load balancing in availability zones. Statefulness. Compute, user delay, and complexity are reduced by running stateful routing logic of dynamic requests at the edge. Security and compliance. Processing data at the edge, not sending it back to the cloud, can increase privacy and help aid compliance procedures, such as local storage of data, which is often required by data privacy laws. Shift to multi-cloud/hybrid IT infrastructure. Modern applications are increasingly adopting a microservices-based approach to help them more easily span more than one cloud provider or consume services from different clouds and/or edge providers. Kubernetes clusters, for instance, are often used to simplify multi-cloud/hybrid IT management. The Role of Edge as a Service in the Microservices Journey With workforces looking set to continue working remotely for some time to come, if not on a sustained basis; Internet infrastructures worldwide under increased strain; and the volume of data needing crunching continuously increasing, the game-changing combination of microservices and edge computing is looking to be increasingly essential. We anticipate more and more organizations looking to leverage microservices at the edge. For many (if not most), building a home-grown edge workload orchestration system is not feasible. This is the critical role that Edge as a Service (EaaS) plays in the acceleration of edge computing adoption. For organizations that lack the skill set internally to deploy microservices over the edge, EaaS can provide turnkey solutions. Edge as a Service (EaaS) allows engineers time to focus on innovation and developing the core business while the EaaS provider manages the complexities associated with edge deployment of microservices."
"325","2020-08-02","2023-03-24","https://www.section.io/blog/dominating-edge-compute/","Remember in the early days of the Internet when you would walk into a company’s IT department and the team would proudly show you around the crown jewels, the server room? After signing in and walking your shoes over the tacky mat to remove the dust, the team would regale with great delight the hours of UPS available, the cooling system capacity and you would nod in appreciation of how well color-coded and organized their ethernet cables were. The team had purchased a bunch of hardware, deployed it and, were suitably proud of their shiny boxes. Fast forward 20 years or so and very few IT departments will show you around the server room and fewer still have that glass panel into the server room from the front foyer exposing how high tech they are by display of metal, plastic, and blinking lights. We don’t care how many servers you have because now we live in a software-centric world. As summarized nicely from Akamai’s recent quarterly conference call in Lightreading’s recent article Tom Leighton of Akamai essentially claims that Akamai dominates the edge because they have 300,000 servers. Clearly Akamai has bought a lot of boxes over the years and no doubt has all the cabling nice and tidy but owning your own servers is not what is going to power the Edge of the Internet. Software is where it’s at. Should physical Edge networks be capable of driving deep into the Edge? Absolutely. Many infrastructure providers are building systems and capabilities to provide edge infrastructure which we target with Edge software. AWS Outposts, Vapor.io and the 5G networks, CenturyLink’s edge play, and more, mean there is plenty of deep Edge infrastructure to target before we even think about Edge running on-premises. These environments will be open and accessible as compared with those racks of servers locked down to legacy CDNs. All of these locations are valid targets for Edge workload but not all of them need to be “lit up” for every application all of the time. It may make more sense to run a subset of locations for an application at any one time to deliver the optimal performance and cost outcome for that application. Take a simplified view of object caching for example; Does 300,000 servers mean 300,000 empty caches that need to be filled before they become useful? The Cloud computing movement and the subsequent trend to cloud-native software means we can move software around from computer to computer and reroute requests both predictively and in real-time much more easily than ever before. Physical location is no longer a matter of owning a box in a particular location at all times. We can now move software to locations for the right amount of time for that application. Intelligent edge network management and orchestration in a modern, cloud-native world cloud means being able to address far more than 300,000 servers but at the same time, not needing to keep all those servers on at all times for all customers. We also need to think about the quality of software we are running in any location. It does not really matter how many servers you have or where they are if those servers are running inferior software. Perhaps smarter software and fewer servers or even locations can provide superior application performance, security and scalability. Innovation at the software layer will move us faster than owning more boxes. Security software, caching software, edge intelligence software, API gateways, Edge Auth, compute orchestration, Traffic routing, etc etc are all moving so fast that an Edge platform needs to be open enough to move with it. Closed, proprietary software systems like Akamai are getting left behind. As a case in point, the modern Web Application Firewall (WAFs) providers like Signal Sciences, Wallarm and others have dropped significant changes in WAF tech over the last couple of years. They have moved faster than the legacy CDNs can with their older, legacy, rules-based WAFs. The promise of Edge is more performant, application experiences for less cost. Get the right application components running in the right location for the right users at the right time. Rather than a spray and pray approach, this means smart orchestration to place the best software in the best locations. That may be in the infrastructure edge, the telco edge, on premises or some combination of all of these Edge layers. I agree that Akamai changed the Internet with its early use case for Edge Compute - CDN. They invested heavily in hardware and have been a major player in the Internet. They will continue to deliver results for large object delivery and file streaming. Applications are a software problem and orchestration of a dynamic Edge is not about who owns the most boxes, the biggest UPS or who has the most organized ethernet cords."
"326","2020-07-15","2023-03-24","https://www.section.io/blog/breaking-down-the-edge-continuum/","There are many definitions of “the edge” out there. Sometimes it can seem as if everyone has their own version. LF Edge, an umbrella organization that brings together industry leaders to build “an open source framework for the edge”, has a number of edge projects under its remit, each of which seeks to unify the industry around coalescing principles and thereby accelerate open source edge computing developments. Part of its remit is to define what the edge is, an invaluable resource for the edge community to coalesce around. Latest LFEdge White Paper: Sharpening the Edge In 2018, State of the Edge (which recently became an official project of LF Edge) put out its inaugural report, defining the edge using four criteria: “The edge is a location not a thing; There are lots of edges, but the edge we care about today is the edge of the last mile network; This edge has two sides: an infrastructure edge and a device edge; Compute will exist on both sides, working in coordination with the centralized cloud.” Since that inaugural report, much has evolved within the edge ecosystem. The latest white paper from LF Edge, Sharpening the Edge: Overview of the LF Edge Taxonomy and Framework, expands on these definitions and moves on from simply defining two sides (the infrastructure and the device edge) to use the concept of an edge continuum. The Edge Continuum The concept of the edge continuum describes the distribution of resources and software stacks between centralized data centers and deployed nodes in the field as “a path, on both the service provider and user sides of the last mile network.” In almost the same breath, LF Edge also describes edge computing as essentially “distributed cloud computing, comprising multiple application components interconnected by a network.” We typically think of “the edge” or “the edges” in terms of the physical devices or infrastructure where application elements run. However, the idea of a path between the centralized cloud (also referred to as “the cloud edge” or “Internet edge”) and the device edge instead allows for the conceptualization of multiple steps along the way. The latest white paper concentrates on two main edge categories within the edge continuum: the Service Provider Edge and the User Edge (each of which is broken down into further subcategories). Image source: LF Edge The Service Provider Edge and the User Edge LF Edge positions devices at one extreme of the edge continuum and the cloud at the other. Next along the line of the continuum after the cloud, also described as “the first main edge tier”, is the Service Provider (SP) Edge. Similarly to the public cloud, the infrastructure that runs at the SP Edge (compute, storage and networking) is usually consumed as a service. In addition to the public cloud, there are also cellular-based solutions at the SP Edge, which are typically more secure and private than the public cloud, as a result of the differences between the Internet and cellular systems. The SP Edge leverages substantial investments by Communications Service Providers (CSPs) into the network edge, including hundreds of thousands of servers at Points of Presence (PoPs). Infrastructure at this edge tier is largely more standardized than compute at the User Edge. The second top-level edge tier is the User Edge, which is on the other side of the last mile network. It represents a wider mix of resources in comparison to the SP Edge, and “as a general rule, the closer the edge compute resources get to the physical world, the more constrained and specialized they become.” In comparison to the SP Edge and the cloud where resources are owned by these entities and shared across multiple users, resources at the User Edge tend to be customer-owned and operated. Moving from the Cloud to the Edge What do we mean when we talk about moving from the cloud to the edge? Each of the stages along the edge continuum take you progressively closer to the end user. You have high latency and more compute in the centralized cloud versus low latency and less compute as you get closer to the User Edge. When we talk about moving from the cloud to the edge, it means we want to leverage the whole stack and not solely focus on the centralized cloud. Let’s look at the most obvious use case: content delivery networks (CDNs). In the 1990s, Akamai created content delivery networks to allow localized websites to serve a global audience. A website based in New York could leverage Akamai’s distributed network of proxy servers and data centers around the world to be able to store their static assets globally, including HTML, CSS, JavaScript, video, and images. By caching these in Akamai’s distributed global points of presence (PoP), the website’s end users worldwide were guaranteed high availability and consistent performance. These days, CDNs are considered to be only one layer in a highly complex Internet ecosystem. Content owners such as media companies and e-commerce vendors continue to pay CDN operators to deliver their content to end users. In turn, a CDN pays ISPs, carriers, and network operators for hosting its servers in their data centers. That’s the Service Provider Edge we’re talking about. An edge compute platform is still a geographically distributed network, but instead of simply providing proxy servers and data centers, an edge compute platform also offers compute. How do we define this? Compute can be defined as many things, but essentially, it boils down to the ability to run workloads wherever you need to run them. Compute still gives you high availability and performance, but it also allows for the capability to run packaged and custom workloads positioned relatively spatially to users. An edge compute platform leverages all available compute between the cloud provider and the end user, together with DevOps practices, to deliver traditional CDN and custom workloads. Applying Lessons from the Cloud to the Edge We can take the lessons we’ve learned in the cloud and apply them to the edge. These include: Flexibility - At Section, we describe this as wanting to be able to run “any workload, anywhere”, including packaged and customized workloads; Taking a multi-provider approach to deployments - This offers the opportunity to create a higher layer of abstraction. Infrastructure as Code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files as opposed to physical hardware configuration or interactive configuration tools. At Section, we have 6-7 different providers, from cloud providers to boutique providers to bare metal providers. Applying DevOps practices - In order to provide the capabilities that the cloud has at the infrastructure edge, we need to enable developers to get insight and to run things at the edge at speed, just as they did in the cloud. This is DevOps. It’s important to be able to apply DevOps practices here since, “if you build it, you own it”. You want to make things open, customizable, and API-driven with integrations, so that developers can leverage and build on top of them. Leveraging containerized workloads - Deploying containers at the edge involves multiple challenges, particularly around connectivity, distribution and synchronization, but it can be done, and in doing, allows you to leverage this architecture to deploy your own logic, not just pre-packaged ones. Containerization also offers: Security Standardization Isolation; and A lightweight footprint. Insights and Visibility - We need to give developers deep, robust insight into what’s happening at the edge, just as we do in the cloud. The three pillars of observability are logs, metrics and tracing. An ELK stack can provide this, giving developers the invaluable ability to understand what is happening when things inevitably go wrong. Edge Computing Use Cases in the Wild There are many examples of use cases already operating at the Edge. A few of the many interesting ones out there include: Facebook Live - When you see a live stream in your feed and click on it, you are requesting the manifest. If the manifest isn’t already on your local PoP, the request travels to the data center to get the manifest, and then fetches the media files in 1 second clips. ML algorithms operate on the 1 second clips to optimize them in real time to deliver the best, fastest experience for users. Cloudflare Workers - These are Service Worker API implementations for the Cloudflare platform. They deploy a server-side approach to running JavaSCript workloads on Cloudflare’s global network. Chick-fil-A - A surprising one. Chick-fil-A has been pushing into the device edge over the last couple of years. Each of their 20,000 stores has a Kubernetes cluster that runs there. The goal: “low latency, Internet-independent applications that can reliably run our business”, in addition to high availability for these applications, a platform that enables rapid innovation, and the ability to horizontally scale. We’re Not Throwing Away the Cloud One last thing to make clear: we’re not talking about throwing away the cloud. The cloud is going nowhere. We will be working alongside it, using it. What we’re talking about is moving the boundary of our applications out of the cloud closer to the end user, into the compute that is available there. And, as we’ve seen, we don’t need to throw away the lessons we’ve learned in the cloud; we can still use the tools that we’re used to, plus gain all the advantages that the edge continuum has to offer."
"327","2020-04-15","2023-03-24","https://www.section.io/blog/reinforced-need-for-5g-edge-computing/","Not only are many employees working remotely, but more of us are turning to digital means of communication for leisure and essential communications. Virtual interactions have become critical, highlighting the necessity for organizations to be able to deliver applications performantly and securely at scale. The cloud is enabling digital to meet the moment in a way that would simply not have been possible a decade ago. So far, the cloud has been able to pull off the huge surge in demand with relatively few problems. “Right now, everything is doom and gloom out there,” 2nd Watch Executive Vice President Jeff Aden told CRN. “But the majority of companies have been able to continue to operate because of the cloud.” Edge computing and 5G can deliver even greater performance benefits with more bandwidth, powering more data and more devices. Can 5G meet the moment? In a recent piece on ways in which the coronavirus will affect the connectivity and tech industry this year, Business Insider identifies the way in which “the growing need for remote interactions amid the coronavirus pandemic has highlighted a need for 5G technology, potentially accelerating adoption in the long term.” Business Insider highlights the merits of 5G’s “lightning-fast speeds, near-instantaneous communications, and increased connection density”, all of which make it “primed for remote interactions.” The positive mention of 5G will particularly be welcome for some in the telecom industry at a time when arson attacks against 5G masts in the UK have become sufficiently serious for the four main carriers (EE, 02, Three and Vodafone) to issue a joint statement urging people to stop spreading false conspiracy theories linking 5G to COVID-19, threatening engineers, and even going so far as to set fire to cellular masts. Two key areas driving demand for 5G BI identifies two areas, crucial to maintaining communications during the pandemic, that will prove the value of 5G: Telehealth With healthcare providers in the full throes of tackling the COVID-19 pandemic and states issuing stay-in-place orders, more hospitals than ever have been turning to telehealth initiatives to diagnose and treat mild and moderate cases of coronavirus and help other patients with their needs. In China in January, two telecom providers teamed up to build a 5G-powered system that enabled experts on coronavirus in one hospital to remotely participate in consultations and diagnoses with 27 other hospitals. Teleconferencing There has been a huge surge in demand for teleconferencing tools like Zoom, Microsoft Teams and Google Hangout, with employees worldwide working from home. Business Insider predicts that the new reliance on such tools will reveal the value of 5G connectivity in our homes as well as the office, demonstrating how “a 5G connection will be able to provide real-time and uninterrupted communication that’s not possible with most wired connections today.” The intersection of 5G and edge computing By definition, edge computing focuses on pushing compute resources and processing closer to end users, resulting in lower latency and reduced data backhaul. The emerging 5G buildout provides critical infrastructure that will serve to expand upon existing network footprints to create a massive global edge network across which developers will be able to run workloads. There are a number of services currently surging in demand that can benefit from the lag-free compute that edge computing and 5G can deliver together. These include: Telemedicine – helping to deliver smooth teleconferencing. There’s even the potential in the future for remote surgery, protecting healthcare workers and spreading expertise to understaffed hospitals in ways not possible today. Streaming video, recorded and live - by helping reduce jitter and lag, edge computing can help move the viewing experience closer to broadcast quality. Online gaming, large multiplayer games - with a significant reduction in latency, streamed games can achieve near-instantaneous response times between user controls and on-screen action. VR – edge computing can help bolster virtual reality, at a time in which it can help smooth disruptions for employees by offering a more immediate means of remote collaboration and hands-on training despite physical distance. Looking to the future New York’s Governor Andrew Cuomo talks of a “new normal” following the pandemic. This is also likely to be the case in relation to digital transformation of the workplace and home, with technology playing a critical role in supporting and advancing these changes. “Crisis can be sort of a catalyst or can speed up changes that are on the way — it almost can serve as an accelerant,” Arun Sundararajan, an NYU Stern School of Business professor researching how digital technologies transform society, told Protocol. Two particular areas of digital transformation that edge computing and 5G can help enable, which may directly help with current and future crises include: Investment in smart cities Business Insider discusses this as the fifth way in which the coronavirus will impact the connectivity and tech industry, “proving the space to be a worthwhile investment in a time of crisis.” Tech is proving to be a critical tool in crisis management for governments and public health officials, whether through the development of a contact tracing app in Germany that logs a user’s proximity to others, or the use of drones with thermal sensors in China to detect people in public areas running a fever. The need to urgently respond to the pandemic is accelerating the rate at which governments worldwide deploy such solutions. 5G and edge computing are major enablers of smart cities and IoT, and could play a critical role in supporting them. Increased automation It is becoming increasingly clear that a higher degree of automation is optimal in many areas of work, from retail to manufacturing to delivery. Supply chains and workforce gaps have become relatively common in some critical industries due to illness and/or challenges connected to remote working. This is accelerating a trend that has been underway for some time. Edge computing enables “machine-to-machine” speeds in a way that’s not possible with the current cloud."
"328","2020-03-16","2023-03-24","https://www.section.io/blog/section-response-covid19/","We’re all in this together. Our world has been shaken, and it’s critical that we all do our part to slow the spread of the COVID-19 pandemic. Remaining Calm, Yet Precautionary In a recent blog post, we highlighted the benefits of Section’s ‘work-where-you’re-awesome’ policy, but we never could have predicted the full importance of our remote work culture in the face of a global crisis. As we’ve moved from remote-optional to remote-mandated, here are some additional steps that we’re taking to ensure that we’re acting as responsible global citizens while continuing to support our clients and the growing demands on the Internet. Delivering Reliability, Performance, and Security Now, perhaps more than ever, the responsibility of delivering reliable, performant and secure Internet traffic is one that we don’t take lightly. We’re committed to continuing to support our clients with solutions that enable optimized digital experiences. Uninterrupted Support for Clients Just as we’ve always done, we will continue to offer 24x7 support. Travel Restrictions We have restricted all business-related travel among our team and asked that personal travel be heavily considered and limited to only essential travel. Stay Home Beyond closing down our available office spaces, we’ve asked everyone in our organization to limit any unnecessary risk of exposure. This means being mindful and respectful of the severity of the circumstances and making conscious decisions that serve the best interests of ourselves and our global community. Virtual All-Company Offsite As a globally distributed team, each year, we take the opportunity to gather everyone for an all-company offsite to connect and align on strategic planning for the coming year. This year, we made the early decision to transition to a virtual offsite agenda with the hope of postponing the in-person gathering to a post-COVID-19 timeframe. To lighten the disappointment, we’ve circulated offsite care packages to our team - yay Section schwag! Support for Circumstances in Flux While the adaptation of remote working is virtually seamless for our team, we recognize that many are faced with new challenges of partners, children and roommates all converging into a newly shared workplace. We’re here to support our team and their families during this time. We’ve got extra laptops available to loan and are supportive and understanding of shifted schedules and unplanned interruptions. Staying Positive Beyond taking the necessary safety precautions, let’s not forget to try to stay calm and be kind, supportive, happy and positive."
"329","2020-03-02","2023-03-24","https://www.section.io/blog/evaluating-multi-cdn-strategies/","Many application architects, particularly those in the enterprise space, are increasingly choosing to pursue a multi-CDN strategy. In this article, we examine the key drivers behind the trend, the benefits it can offer, and why modern edge compute platforms are an attractive alternative (or in some cases complementary) strategy. Today’s computing landscape grows more complex daily and traffic volume is ballooning. Last year, Cisco forecasted that global IP traffic would grow to 4.8ZB per year by 2022. At the same time, delivering an impeccable end user experience has become ever more important; performance degradation can quickly lead to lost business and downtime is unacceptable. Nonetheless, many of the world’s biggest tech companies, including Google, AWS, Facebook, and Cloudflare experienced significant outages last year, many of which were directly tied to CDN failure. For these reasons, there is growing popularity around deploying a multi-CDN strategy. What is a multi-CDN strategy? In a single-CDN architecture, once the default model, a website’s content is delivered by just one provider’s network. In a multi-CDN setup, web traffic is distributed over multiple CDNs, meaning your content is delivered across multiple networks, immediately providing failover and redundancy. When a multi-CDN model is deployed, the number of CDNs in play varies depending on the application and its specific needs. Heavy video sites, particularly those with live video streaming, typically have multiple CDNs in place as quality of experience (QoE) is so crucial to success. Netflix, for instance, is thought to have three different CDN providers; no surprise given that last year Netflix represented over 12% of all worldwide downstream traffic. Key drivers pushing multi-CDN adoption Enterprises not only want to avoid downtime, but they are also seeking to reduce costs and optimize the performance of their websites and applications. Redundancy Perhaps the main driver behind a multi-CDN model is that by adding redundancy to your IT stack, there is no longer a single point of failure that can slow down your application, or worse, bring all services and products to a halt. The possibility of reputation damage and lost revenue is thereby greatly diminished. Geographic footprint Companies extending their service into new geographic areas can also benefit from having multiple content delivery networks in play by leveraging an optimal mix of availability and reliability across different networks in different geographies. Many enterprises work with multiple CDNs to deliver the best worldwide performance based on where their users are located. Tuning site delivery to perform equally well from Beijing to Boston is challenging within today’s complex computing landscape. A CDN that has coast-to-coast presence across North America may have fewer PoPs in Asia or Europe. By adopting a best-of-breed approach, you can take advantage of the fastest, most reliable CDN for each part of your network. Avoid vendor lock-in Another driver behind the trend is increased flexibility that translates to reduced financial and technical risks by avoiding vendor lock-in. Rather than being tied to a single provider’s fixed network and/or software stack, engineers gain more flexibility to meet the unique demands of their applications. Unique value proposition Additionally, different CDNs offer a different set of advantages and business value proposition. As an example, Section’s focus as a developer-centric platform aligns with modern DevOps practices while remaining vendor agnostic in network and software offerings. Hence, we are often brought in by DevOps teams who want to use their existing tooling while also benefiting from maximum flexibility and control. The benefits of a multi-CDN approach There are various benefits to adopting a multi-CDN strategy, including: Greater resiliency If one provider has an outage, the other can pick up the slack, reducing the risk of downtime. You can minimize the impact of third-party vendor issues that you already have limited control over. Optimized performance Providers can adapt to changing network conditions by choosing the best delivery path in real-time. The option to: Use different CDNs in different geographical regions. Use different CDNs for different content types e.g. static content vs. dynamic. Speed matters. Improving performance can increase a site’s conversion rates and positively impact SEO (Google includes overall site speed as part of its Page Rank algorithm). Scalability At times of traffic surges, such as on Black Friday or Cyber Monday, you can rest assured that by distributing traffic across multiple CDNs, your applications will be able to scale to accommodate the unusual conditions. You can plan ahead for events which are likely to require particularly high bandwidth and a large volume of users. When anticipating a large live stream, for example, you can prepare to spread the traffic across more than one CDN provider. Opportunity to lower costs By gaining greater control over your delivery expenditure, you can decide on key services and the best delivery model for the long-term in terms of workload and budget. The potential to move traffic to providers with lower rates after meeting your pre-negotiated commitments with each provider. The drawbacks of a multi-CDN approach Achieving the proposed benefits of a multi-CDN strategy can be problematic due to the disparate, black box approach each of the legacy CDNs have pursued. Operational consistency Working with a multi-CDN architecture as an operations team can present challenges. Operations teams typically look for a common diagnostic platform to holistically understand the performance, errors and availability of the entire network alongside the individual components so that they can manage and debug the system. Each CDN presents their own diagnostic paths, levels of available information and frequency of information availability. This presents operations engineers with the challenge of normalizing that data and consolidating it into their preferred system. Lowest common denominator Each of the legacy CDNs have also provided their own black box solution when it comes to configuring the CDN. Solutions sit behind custom code interfaces or APIs which are not consistent between providers. The usual approach for engineers to solve for a multi-CDN approach is to adopt whatever is the lowest or simplest solution for caching or security provided by one of the CDNs in the stack and then translate that over to the other offerings. Engineers are limited in terms of the scope of what they can achieve by the weakest link in the multi-CDN chain, and their ability to then seamlessly translate and maintain configurations from one CDN to the next is compromised. Edge computing platforms offer an alternative approach Modern edge compute platforms are equipped to handle traditional CDN workloads such as caching and image optimization, but extend far beyond static asset delivery by offering more advanced workload options and the opportunity to benefit from a wider distribution of web application architecture. Like CDNs, edge computing aims to improve performance by getting as close to the end user as possible; however, contrary to CDNs, which focus exclusively on delivering content, edge computing platforms also enable developers to move more advanced logic closer to users. By localized processing, applications benefit from reduced latency and bandwidth savings as a result of limiting the amount of requests that need to travel back to the centralized infrastructure. In addition, vendor-agnostic platforms like Section provide redundancy without the need for multiple CDNs. Appliances Online, Australia’s largest online appliances retailer, discovered this for themselves when they implemented Section alongside one of the largest CDN providers in the market to help alleviate frustrations they were experiencing around A/B testing with their incumbent provider. Initially, Section was set up behind their existing CDN provider, which was left in place to cache dynamic content. After realizing immediate performance gains, the Appliances Online team was eager to turn off their legacy CDN and cache all content through Section to benefit from better performance at a reduced cost. Do a test run If you’re still not sure about the best option for you, whether to bring on multiple CDNs, or explore working with an edge compute platform, consider doing a test run. When asked, most CDN providers or edge compute platforms will provide estimates about performance and uptime, but a trusted vendor will also allow any potential customer the opportunity to test and monitor their actual application traffic. Start by sending 10% of traffic to the vendor, and watch what happens over a fixed period of time. Test during peak and off-peak times to understand how the CDN provider(s) and/or edge compute platform handles your application in varied environments. Use this time to build a relationship with sales and support in order to set the right Service Level Agreement if you decide to move forward."
"330","2020-01-27","2023-03-24","https://www.section.io/blog/lessons-from-2019-cloud-outages/","Last year, the Internet experienced a run of major cloud outages, which had significant ripple effects on end users around the world. Most of these outages took place across the summer, disrupting many top tech companies, including China Telecom, Verizon, Cloudflare, AWS, Google Cloud, WhatsApp and Facebook. The Summer of Outages 9-hour outage demonstrates China Telecom’s global reach The summer of outages started in May when China Telecom experienced substantial packet loss across its backbone over a nine hour period, mainly taking down network infrastructure in mainland China, but also impacting its network in Singapore and multiple parts of the US, including Los Angeles. Over one hundred services were affected with many big western sites, such as Apple, Slack, Amazon and Microsoft reporting disruption to their services across that time. In terms of the bigger picture, the incident revealed the reach of China Telecom well beyond the geographic limits of mainland China. BGP route leak takes down Cloudflare and major US sites In June, a tiny Internet service provider (ISP) in Pennsylvania yielded another lesson, again showing how fragile the modern Internet can be. DQE Communications, a small commercial ISP that services around 2,000 buildings in Pittsburgh, Pennsylvania, put out a mistaken signal, which led to a BGP route leak. According to Cloudflare CTO John Graham-Cumming, “This little company said, ‘These 2,400 networks, including some bits of Cloudflare, some bits of Amazon, some bits of Google and Facebook, whole swathes of the Internet,’ they said those networks are ours, you can send us their traffic.” The misconfiguration was likely an error due to automatic route optimization software rather than an intentional act; nonetheless, the effect was the same. As soon as the new route was announced, the route leak spread all the way up to Verizon who accepted the faulty routes and passed them on. As one of the world’s largest transit providers, the problem grew from there. Internet traffic originally destined for Cloudflare, AWS and Google instead went through DQE, Allegheny and Verizon. Consequently, a huge swatch of the Internet’s traffic (including to major destinations such as Google, Facebook, Reddit and AWS) suddenly went nowhere. It was as if Google Maps had sent large numbers of drivers down an unmarked cliff. Image source: https://www.theregister.co.uk/2019/06/24/verizon_bgp_misconfiguration_cloudflare/ The Increasing Complexity of the Internet Both these examples point to how interconnected the modern Internet has become with a large and complex ecosystem of Internet-facing services, from DNS to APIs to public cloud services. These services need to work in tandem to provide consistent performance for end-users. It also reveals the way in which we still rely on a relatively naïve, trust-based system devised when the Internet first came into being. In the instance of the Cloudflare and AWS outages, the real problem, as Slate pointed out, was the Border Gateway Protocol (BGP) Internet routing system, which essentially relies on trust for its correct implementation. BGP has worked incredibly well for more than 25 years, but increasingly, problems are arising when even a small element goes wrong. With IP traffic expected to reach 4.8 ZB per year by 2022, representing a threefold increase in global IP traffic over five years, more outages and slowdowns are to be expected. What Can We Do Better in 2020? According to Mehdi Daoudi, CEO of digital experience monitoring firm Catchpoint, “We may be at a tipping point where the complexity and interdependence of our systems have grown to where the risks to business are greater.” Writing in Forbes last year, Daoudi advised “a new type of vigilance” was necessary. Increase Use of Grassroots Protocols One cause for hope looking forward to the rest of 2020 is the increasing use of grassroots protocols to help solve some of these issues. QUIC, for instance, was upgraded last year, with the goal of improved privacy, greater online experiences, and better security. There has also been more widespread deployment of the latest version of TLS, offering improved performance and upgraded security between users and websites. Following the Cloudflare Verizon outage, Graham-Cumming recommended the use of RPKI to avoid similar problems in the future, and let networks better filter faulty BGP routes. As Cloudflare said at the time, if Verizon had used RPKI, it wouldn’t have allowed the invalid routes from DQE through, and they would have been automatically dropped. Various providers, including AT&T, have started to embrace RPKI frameworks. Build in Redundancy With the growth in complexity of the Internet, enterprises have also been changing how they work with CDNs. One of Mehdi Daoudi’s recommendations is to “build in redundancies or have backups at the ready”. Multi-CDN usage is becoming increasingly popular with increasing numbers of companies pursuing a dual or multi-CDN distribution strategy in order to not risk content delivery on a single point of failure. The goal is to maintain the highest-quality delivery while also building in redundancy and resiliency. Cost consideration is also typically a factor in companies opting for a multi-CDN strategy. Embrace Changing CDN Structures Many CDNs have also been restructuring to change the focus on the types of services they offer in order to meet the demands of a decentralized web, moving towards providing edge compute resources. In its 2020 report, State of the Edge calls edge compute “the third act of the Internet” as it “seeks to resolve the problems with our current infrastructure and the challenges that come with supporting the applications we desire”. Section’s Edge-First Approach Section was designed as an edge compute platform from the get-go. When Stewart McGrath and Daniel Bartholomew founded the company, they wanted to build a new type of platform that not only delivered speed, security and scalability, but also the flexibility for developers to build software at the edge. In our CEO Stewart’s words, “Section is the only [edge platform] around that is hooked up to provide an immediately seamless integration with agile development workflows”…”giving developers and engineers all the right tools to manage [edge configurations] completely”. Reduce Complexity and Improve Performance at the Edge In the Forrester Analytics 2019 survey, 57% of mobility decision makers said they have edge computing on their roadmap for 2020, a significant statistic given edge computing’s relatively recent emergence in the tech space. As Forrester concluded, edge computing can be a major part of simplifying the challenges of the modern-day Internet since “computing at the edge avoids network latency and allows faster responses”. The emerging edge compute paradigm can also bring about improvements in scalability, flexibility and security. While 2019 was seen as “the year of outages”, perhaps there’s still hope that 2020 will manage to buck the trend…"
"331","2019-06-03","2023-03-24","https://www.section.io/blog/telco-cooperation-accelerates-5g-edge-computing/","Cooperation among telco providers has been ramping up over the last decade in order to catalyze innovation in the networking industry. A variety of working groups have formed and in doing so, have successfully delivered a host of platforms and solutions that leverage open source software, merchant silicon, and disaggregation, thereby offering a disruptive value proposition to service providers. The open source work being developed by these consortiums is fundamental in accelerating the development of 5G and edge computing. Telco Working Groups Working groups in the telco space include: Open Source Networking The Linux Foundation hosts over twenty open source networking projects in order to “support the momentum of this important sector”, helping service providers and enterprises “redefine how they create their networks and deliver a new generation of services”. Projects supported by Open Source Networking include the Akraino Edge Stack (working towards the creation of an open source software stack that supports high-availability cloud services built for edge computing systems and applications), FRRouting (an IP routing protocol suite developed for Linux and Unix platforms, which include protocol daemons for BGP, IS-/S and OSF) and the IO Visor Project (which aims to help enable developers in the creation, innovation and sharing of IO and networking functions). The ten largest networking vendors worldwide are active members of the Linux Foundation. LF Networking (LFN) LFN focuses on seven of the top Linux Foundation networking projects with the goal of increasing “harmonization across platforms, communities, and ecosystems”. The Foundation does so by helping facilitate collaboration and high-quality operational performance across open networking projects. The seven projects, all open to participation - pending acceptance by each group, are: FD.io - The Fast Data Project, focused on data IO speed and efficiency; ONAP - a platform for real-time policy-driven orchestration and automation of physical and virtual network functions (see more below); Open Platform for NFV - OPNFV - a project and community that helps enable NFV; PNDA - a big data analytics platform for services and networks; Streaming Networks Analytics System - a framework that enables the collection, tracking and accessing of tens of millions of routing objects in real time; OpenDaylight Project - an open, extensible and scalable platform built for SDN deployments; Tungsten Fabric Project - a network virtualization solution focused on connectivity and security for virtual, bare-metal or containerized workloads. ONAP One of the most significant LFN projects is the Open Network Automation Platform (ONAP). 50 of the world’s largest tech providers, network and cloud operators make up ONAP, together representing more than 70% of worldwide mobile subscribers. The goal of the open source software platform is to help facilitate the orchestration and management of large-scale network workloads and services based on Software Defined Networking (SDN) and Network Functions Virtualization (NFN). Open Networking Foundation (ONF) The Open Networking Foundation is an independent operator-led consortium, which hosts a variety of projects with its central goal being “to catalyze a transformation of the networking industry” via the promotion of networking through SDN and standardizing the OpenFlow protocol and related tech. Its hosted projects focus on white box economics, network disaggregation, and open source software. ONF recently expanded its reach to include an ecosystem focus on the adoption and deployment of disruptive technologies. The ONF works closely with over 150 member companies; its Board includes members of AT&T, China Unicom, and Google. It merged with the ON.Lab in 2016. The group has released open source components and integrated platforms built from those components, and is working with providers on field trials of these technologies and platforms. Guru Parulkar, Executive Director of ONF, gave a really great overview of the entire ONF suite during his keynote at the Open Networking Summit. Open Mobile Evolved Core (OMEC) One of the most notable projects coming out of ONF is the Open Mobile Evolved Core (OMEC). OMEC is the first open source Evolved Packet Core (EPC) that has a full range of features, and is scalable and capable of high performance. The EPC connects mobile subscribers to the infrastructure of a specific carrier and the Internet. Major service providers, such as AT&T, SK Telecom, and Verizon, have already shown support for CORD (Central Office Re-architected as a Datacenter) which combines NFV, SDN, and the elasticity of commodity clouds to bring datacenter economics and cloud agility to the Telco Central Office. This allows the operator to manage their Central Offices using declarative modeling languages for agile, real-time configuration of new customer services. OMEC has been specifically developed to cope with the “ensuing onslaught of devices coming online as part of the move to 5G and IoT”. It can be used either as a stand-alone EPC or via the COMAC platform. A Shift Toward Container Network Functions Network functions virtualization (NFV) has been an important factor in enabling the move towards a 5G future by helping to virtualize the network’s different appliances, including firewalls, load balancers, and routers. Specifically, it looks to enable network slicing, allowing multiple virtual networks to be created atop a shared physical infrastructure, empowering numerous use cases and new services. NFV has also been seen as important in the enabling of the distributed cloud, aiding in the creation of programmable, flexible networks for 5G. Over the last couple of years, however, there has been a shift towards cloud-native network functions (CNF) and containerization as a lighter-weight alternative to NFV software. This follows a decision made by ONAP and Kubernetes working groups within The Linux Foundation to work together on a CNF architecture. According to a report in Container Journal, “Arpit Joshipura, general manager for networking at The Linux Foundation, says that while telecommunications providers have been making extensive investments in NFV software based on virtual machines, it’s become apparent to many that containers offer a lighter-weight approach to virtualizing network services.” Joshipura says CNFs offer greater portability and scalability and many organizations are beginning to load VNFs in Docker containers or deploy them using Kubernetes technologies like KubeVirt to make them more portable. This has gone even further recently, according to ONF, with telecom operators increasingly adopting cloud-native solutions in their data centers and moving their operations away from traditional Central Office (CO). In a recent blog post, ONF says, “3GPP standardized 5G core will be cloud-native at start, which means telcos should be able to run and operate Containerized Network Functions (CNF) in their CO in the near future.” At a recent ONOS/CORD/K8s tech seminar in Seoul, eight telcos, vendors and research institutions demonstrated this in action, sharing stories about their containerization journeys. Kubernetes Enabling 5G News out of AT&T in February has led to industry speculation that Kubernetes is now set to become the de facto operating system for 5G. The carrier said it would be working in conjunction with open source infrastructure developer Mirantis on Kubernetes-controlled infrastructure to underlie its 5G network. In an interview with Telecompetitor, Mirantis Co-Founder and Chief Marketing Officer Boris Renski said he believes that other carriers will adopt a similar approach as they deploy 5G, and described Kubernetes as the next logical step following cloud computing and virtual machines. As large companies like Google need to run their applications such as its search engine over multiple servers, Kubernetes enables just that. Renski said that Kubernetes is becoming the go-to standard for telco companies looking to implement mobile packet core and functionality like VNFs because it is open source. A further key appeal of Kubernetes and containerization is that it offers the potential for a flexible, affordable and streamlined backend, essential factors to ensuring scalable and deployable 5G economics. There are still a number of areas to be worked out as the shift to Kubernetes happens, as pointed out by AvidThink’s Roy Chua in Fierce Wireless recently. Network functions will need to be rethought to fit the architecture of microservices, for instance, and vendors will have to explore complex questions, such as how to best deploy microservices with a Kubernetes scheduler to enable orchestration and auto-scaling of a range of services. Democratizing the Network Edge Nonetheless, the significant amounts of cooperation between telcos is indeed helping accelerate the move towards 5G and edge computing. The meeting point of cloud and access technologies at the access-edge offers plenty of opportunities for developers to innovate in the space. The softwarization and virtualization of the access network democratizes who can gain access - anyone from smart cities to manufacturing plants to rural communities. Simply by establishing an access-edge cloud and connecting it to the public Internet, the access-edge can open up new environments, and in doing so enable a whole new wave of innovation from cutting-edge developers. There are several reasons to be hopeful about the movement towards this democratization: first, there is demand for 5G and edge computing - enterprises in the warehouse, factory, and automotive space, for instance, increasingly want to use private 5G networks for physical automation use cases; secondly, as we have looked at here, the open-sourcing of hardware and software for the access network offers the same value to anyone and is a key enabler of 5G and edge computing for the telcos, cable and cloud companies; finally, spectrum is becoming increasingly available with countries worldwide starting to make unlicensed or gently licensed spectrum for 5G deployments available. The U.S. and Germany are leading the way."
"332","2019-02-26","2023-03-24","https://www.section.io/blog/containers-vm-serverless-edge-computing/","The rise of edge computing has brought about a shift in system architecture requirements and considerations. As applications demand lower latency and reduced bandwidth, deployment method decisions are increasingly critical. This article examines the differences between using virtual machines (VMs) vs. containers vs. serverless functions in the context of edge computing, and looks at how to decide which method is best for your workload. Definitions at the Edge Let’s first take a look at the definitions for each of these separately, and see how they fit into edge computing. Virtualization Cloud computing has its roots in virtualization. A virtual machine is an emulation of a computer system. There are system VMs (or full virtualization VMs), which offer a substitute for a physical machine and process VMs, which execute computer programs in a platform independent environment. All kinds of virtualization enable the running of your application on fewer physical servers. Through virtualization, each app and operating system live in a separate software container called a virtual machine or VM. VMs are entirely isolated; however, computing resources, storage and networking are pooled together and delivered dynamically to each VM by software called a hypervisor (the software or firmware layer). This means that every app gets what it needs for peak performance and the strong isolation of the system is important for enabling robust security. With servers running at full capacity, fewer are needed. The idea is that performance improves, less maintenance is required, and savings can be made on hardware and overhead. VMs have formed the foundation of Infrastructure as a Service (IaaS), the most popular delivery model of the cloud. Amazon EC2, Azure VMs, and Google Compute Engine are examples of IaaS. Containerization There are inherent advantages to containerization over VMs. Compared to virtualization in which each VM runs a unique operating system atop a hypervisor, containers run on top of physical infrastructure. Hypervisors and OS are reduced to wafer thin layers that act as an interface between the hardware and containers. Containers can be a strong candidate for running edge workloads because they are very lightweight and agile as a result of sharing the same kernel. The shared OS also reduces the number of management tasks, such as patching and upgrades for the OS admin. Containers can be run on virtual machines or bare metal. CenturyLink recently ran a test to determine the network latency of Kubernetes clusters running on both VMs and bare metal, and found that containers running on bare metal servers achieved three times lower latency compared to when running Kubernetes on VMs. CPU consumption was also higher when a cluster was run on VMs. Serverless Following VMs and containerization, the next wave of compute services is serverless, also known as Functions as a Service (FaaS). The goal behind serverless is to make the developer experience more straightforward by reducing the operational overhead in running and managing code. FaaS platforms such as AWS Lambda, Azure Functions and Google Cloud Functions are used as the foundation for contemporary applications designed as microservices. As the unit of deployment in serverless is a function, it is a significantly more efficient solution than a comparatively heavy VM or container. In terms of edge compute, serverless platforms can play a particularly important role in simplifying the DevOps cycle, particularly in relation to operations involving resource intensive compute, such as machine learning (ML). Use Cases Along the Edge Continuum VMs: VNFs and CaaS VMs are often used as the basis for network services. Many vendors have chosen not to containerize because of the level of isolation offered by VMs and the robust security this enables, both in terms of web application firewalls (WAFs) and other security services. Various tasks can be run in VMs via virtual network functions (VNFs) (virtualized tasks previously carried out by proprietary, dedicated hardware). VNFs operate on software that runs on commodity hardware. The kinds of tasks they perform include WAFs, caching, DNS or network address translation. VNFs want to look at every single packet, which containers currently can’t do, although network service mesh may be a solution for containerization to be able to achieve this. AWS Greengrass, a service that extends Amazon Web Services functionality to edge compute, is an example of a Container as a Service (CaaS). Built on top of an existing infrastructure layer based on VMs, Greengrass delivers portability and agility, two necessities of edge computing. If Greengrass loses connectivity from the cloud, it can still communicate with services locally through Greengrass core and provide messaging and security services for local execution of AWS Lambda. IoT SDK enabled devices can also communicate with Greengrass core via the edge network. Containers: Retail in a Box The restaurant Chick-fil-A has begun to use bare metal clustering for Kubernetes at the edge in its restaurants. This equates to approximately 6,000 devices at the edge running Kubernetes in 2,000 restaurants. This gives the restaurant’s edge deployment a unique kind of scale. Instead of having only a few large K8s clusters with tens-to-hundreds-of-thousands of containers, at full scale, it has over 2,000 clusters with tens of containers per cluster. Chick-fil-A’s edge workloads include platform services (such as log collection, monitoring, pub/sub messaging), applications that interact with “things” inside each restaurant, and ML models that are able to synthesize cloud-developed forecasts with real-time events from pub/sub messaging in order to issue decisions at the edge and drive automation processes. Why has Chick-fil-A sought out this kind of deployment? In a follow-up post, the IoT/edge team explained a variety of objectives, including “low-latency, Internet-independent applications”, which “can reliably run our business”. The others mentioned comprise of “high availability for these applications”, a “platform that enables rapid innovation and that allows delivery of business functionality to production as quickly as possible”, and the opportunity to achieve a “horizontal scale” in relation to its infrastructure and app development teams. Use cases for containerization are more extensive than serverless mainly because you can refactor existing monolithic applications to container-based setups. Further uses of containerization include web APIs, machine learning computations and long-running processes. Serverless: Machine Learning at the Device Edge (IoT) Machine Learning (ML) models are being used throughout the process of developing software. Increasingly, machine learning models are being deployed at the edge (closer to where data is generated). By operating and gaining insights as close to the source as possible, machine learning models deployed at the edge can reduce transmission costs, reduce application latency, and isolate privacy concerns. In autonomous vehicles, for instance, a predictive maintenance model could be kept near the automobile as opposed to running in the cloud. By invoking the ML model closest to the application, latency is dramatically reduced versus sending data on a round trip to and from the cloud. Additional use cases for serverless at the edge include performing back-end tasks for mobile apps or websites, thereby freeing up front-end time and resources; cleaning up, parsing and filtering data streams and uploads from real-time devices, thus moving resource-intensive processes out of the main application; and high volume background processes such as migrating data to long-term storage and forwarding metrics to an analytics service. The DevOps POV From a DevOps standpoint, there are pros and cons to all three methods. The two most important factors to consider are development speed and time-to-market. Other factors to weigh are the complexity of your deployment scenario, the time needed to deploy an application plus vendor lock-in and associated costs. In summary, containerization can be a powerful tool when you need flexibility and total control of your system, or when legacy services need migrating. Tools for monitoring and proper alerts such as Prometheus, have also matured with containerization. The main drawback of containers from a DevOps point of view is that having more control and greater flexibility also means greater complexity and more moving parts. It also introduces greater costs as you will be paying for resources all the time, whether there is traffic or not. Furthermore, with containerization, scaling is not enabled by default. This can mean a more complicated setup process; however, it also means the developer benefits from full control of his/her resources as you are entirely in charge of the scaling, which (depending on the provider) theoretically means infinite scalability. Serverless, on the other hand, can be particularly useful if you have sudden spikes in traffic that need to be immediately detected and handled. You pay only for the resources you use, so if there is no traffic, the application will be shut down. Autoscaling is enabled by default. Furthermore with serverless, the infrastructure is abstracted away from the developers creating the function, making it an ideal solution for using serverless databases such as DynamoDB and Aurora Serverless, hosting static websites on S3, and running code without the hassle of managing servers. Serverless eliminates the work involved in deploying your applications (you simply have to deploy code to your provider) and the administration involved in installing OS updates or making security patches. This means fast, flexible time-to-market, an extremely valuable asset for any startup. Serverless is a powerful tool for microservices frameworks, such as web APIs, task runners or the processing of data streams and/or images. For background tasks that only run occasionally, such as Cron jobs, serverless is a good choice. In fact, FaaS is only viable for short-running processes (the maximum time an AWS Lambda function can run is five minutes). For heavier computing tasks, a container or VM based setup would be preferable. Other downsides to serverless include the fact that there are predefined limits for memory and processing power, calling for extremely efficient code to avoid overloading functions if they get too large, which can lead to latency challenges. Serverless is also trailing behind in terms of the maturity of its dev tools. Virtualization is the oldest technology of the three and although VMs have performed a great job over the last decade or more, containerization and serverless technologies are largely superceding them in terms of running edge workloads. That said, there are various developments in infrastructure technologies that are seeking to combine VMs and containers, such as New York-based startup Hyper. HyperContainers are an attempt to provide the best of both worlds by offering the speed and agility of containers with the security for, and isolation of the VM. Security Concerns Previously, customers have preferred VMs to containers for security reasons due to their more complete isolation. However, VMs have their own set of security concerns that need addressing. One simple way of improving security is to limit running services to only what is absolutely necessary. Another is to apply patches as quickly as possible following their release. On a full virtual machine, the need to apply any given patch is usually far higher than in containers and serverless as there are substantially more packages required and installed. Container-specific security concerns can be focused around two specific areas: the trustworthiness of the container source on which the container is built, and the level of access the container is allowed to the host operating system. The container should never be run with root or administrator privileges when running on any host, whether Windows or Linux. Moreover, containerization carries all the same security concerns as serverless. Security issues related to serverless include ensuring that anyone with access follows secure coding best practices. This is especially important in FaaS as any vulnerability will lead to leaked data that can easily spill beyond the scope of the serverless app itself. The other primary security issue in serverless is related to any third-party library included inside the app. They should always be used in concert with a scanning tool that self-updates and routinely scans your built artifacts or a highly organized manual process in order to stay on top of any vulnerability announcements related to any third-party library used. In Conclusion Ultimately, the decision is not whether to use one over the other. Depending on your workload requirements, each has their own benefits and can be mutually supportive within computing architectures. Focus on what your workload demands first, then decide which solution is best to solve that problem."
"333","2019-01-10","2023-03-24","https://www.section.io/blog/lightweight-js-vs-node-js/","There are a number of lightweight code at the edge solutions emerging to complement existing lightweight solutions such as AWS Lambda. Several traditional CDN players have implemented a cut down Javascript layer called V8 (both Cloudflare and Stackpath have taken this route). At Section, we generally want developers to avoid having to rebuild their applications at the edge, though there are many use cases where simple apps created at the edge can be very effective. We offer both lightweight and fully featured Javascript at the edge (in addition to a range of other language options, as well as open source and third party web performance and security modules). Let’s review the benefits and challenges associated with both lightweight and fully featured code at the edge options. Lightweight JS at the Edge Cloudflare and Stackpath have both implemented the use of Chrome V8 at the edge (essentially a lightweight JS), and hailed it as the critical ingredient for their serverless programs. The team at Cloudflare describes V8 Isolates as “the future of serverless and cloud computing in general”. V8 is an open-source JavaScript engine developed for Google Chrome and Chromium web browsers by Danish computer programmer Lars Bak. Its first iteration launched in 2008, initially designed solely to be executed by web browsers. V8 can also now execute JS code outside of a browser, which enables server-side (i.e. back-end) scripting. Cloudflare equates V8 to an “eight-cylinder car engine” because it is “fast and powerful” and capable of translating JavaScript code directly into machine code, which CPUs can understand because it is purely digital. V8 also optimizes JavaScript execution. Cloudflare, which processes millions and millions of requests each second, was in need of a solution which allowed its manifold customers to write code and build applications themselves on the CDN’s 150+ servers worldwide. Cloudflare previously used Lua, but because it wasn’t sandboxed, its customers were not able to independently run code. Cloudflare ruled out containerization technologies such as Kubernetes for being too expensive and overly resource intensive. Its engineering team eventually landed on Chrome V8 Isolates as the backbone for the Cloudflare Workers serverless program: a single process can run hundreds or thousands of Isolates and switch between them, thereby allowing code from many different customers to run in the same OS process. In a post titled “Cloud Computing without Containers” written for Cloudflare, Zack Bloom, Director of Product for Product Strategy, says he believes, “it’s possible with this model to get close to the economics of running code on bare metal, but in an entirely Serverless environment”. Bloom believes that the use of V8 marks not just “an iterative improvement but an actual paradigm shift”. StackPath’s serverless computing platform, EdgeEngine, also leans heavily on V8 as its backbone, allowing the uploading of JavaScript scripts to be automatically deployed “within seconds” to each of StackPath’s global data centers. When a request comes in, the developer’s JS code is executed in an individual, isolated sandbox environment built on top of StackPath’s customized implementation of the V8 JavaScript engine. StackPath describes the setup as a means for developers to get “a simple and easy way to deploy highly-distributed, highly-performant and highly-scalable global applications”. Both StackPath and Cloudflare espouse the speed of V8, and how quickly it allows your code to start. Zack Bloom describes the way in which “any given isolate” is able to “start around a hundred times faster than I can get a Node process to start on my machine.” Fully Featured JS at the Edge On Section’s Edge Compute Platform, we offer options for lightweight code at the edge, but we also support richer JS at the edge that traditional platforms are unable to provide. The richer JS option is a fully fledged Node JS module running at the edge in a completely containerized solution which eliminates the security risks other platforms face. This allows your apps to run natively, and moreover, grants you the full experience. Full featured application functionality means flexibility in your technology stack, and the ability to mix and match best-of-breed security and performance options in order to build the solution you need, not the one you are locked into by traditional CDNs. Section’s holistic solution gives websites maximum overall value and the opportunity to achieve optimal performance and robust security at any required scale. One of the key differentiators with Section’s Edge Compute Platform is its adherence to modern DevOps principles. Our platform provides developers with the tooling they need to clone environments and run Node apps locally for testing before pushing to staging and production, ensuring faster and smoother development lifecycles. The Section platform utilizes a blue-green deployment model which eliminates the need for ‘spinning up’. Many believe that spin up time is an issue for fully featured code at the edge; we believe, however, that this only applies to a subset of solutions, such as AWS Lambda, in which containers are spun up and then discarded once the code operation is complete. Finally, adding our Node JS Module to the edge is highly cost effective when evaluating fully featured and developer integrated solutions at the edge. With Section, you can choose whichever option best suits you - or use both. Edge Hosting Section’s Node.js Edge Hosting empowers DevOps teams to run mission critical Node.js applications at the network edge for blazingly fast results with enterprise level AppSec protection. Learn more and get started on a free plan Practical Use Cases for Node JS at the Edge Some example use cases for running Node JS at the edge include: Deploy your single page app (SPA) to the edge; Handle API requests; Rewrite body content; Implement custom security filters at the edge; Redirect segments of users to different back-ends; Deliver highly personalized content to users; … and many, many more! (What will you create.) Have a use case that you’d like to talk through with our engineers? Contact Us"
"334","2018-11-12","2023-03-24","https://www.section.io/blog/edge-infrastructure-scalability/","At Section, we’re primarily focused on providing value for developers on the programming side of edge compute, but the scalability of our platform is heavily dependent on the edge infrastructure that we utilize. In order to meet the demand for increasingly vast volumes of requests that pass through our platform, we’re constantly looking for ways to optimize our infrastructure to continue to support highly performant systems. One of the biggest advantages of our edge computing software is the flexibility and control that it offers for developers to run any workload, anywhere. In other words, everything that we do from a systems architecture standpoint revolves around the freedom of choice that we extend to clients to be able to leverage edge computing within their own server and network infrastructures, from cloud to on-premise, and in-between. As Section continues to attract, onboard, and support larger clients, we’ve been enjoying greater flexibility and control over the physical infrastructure that supports our platform. Each infrastructure decision for us and our clients, from providers such as AWS, Azure, Google Cloud and Digital Ocean, offers unique advantages and serves a specific purpose, and it’s the combination of those services that ultimately determines performance, efficiency, and scalability. We’ve recently been hard at work implementing a large-scale platform upgrade to support our growth, requiring us to make critical infrastructure decisions to meet the growing demands. One such decision that has been particularly noteworthy is our engagement with Packet, which has allowed us to scale and expand flexibility for our clients. Packet is a bare metal cloud, offering a fresh approach to cloud computing by providing on demand access to dedicated (e.g. bare metal) infrastructure all over the world, including at the network edge very close to end users. Packet’s proprietary technology automates physical servers and networks without recourse to multi-tenancy or virtualization, which means customers ‘own’ the hardware they deploy on the cloud, allowing them to use all the resources of that hardware. Infrastructure to Support Edge Workloads Section leverages Packet’s bare metal as a high-performing infrastructure foundation, layering our programming interface on top which gives developers access to the edge. When developers understand that instead of scaling up their centralized infrastructure, they can better improve efficiency by using an edge compute platform to run custom logic at the edge, and only trickle back the necessary workloads to the cloud, it results in not only a better user experience, but also a reduction in centralized infrastructure costs. Control and Reliability Are Key We were initially drawn to Packet for their high performance servers and the level of control that dedicated infrastructure afforded us. However, Packet’s support for both global and local BGP (border gateway protocol) was another draw, enabling Section to use its own IP space on Packet and route traffic reliably to the nearest datacenter on a global basis. As developers ourselves, we don’t want to own or run any hardware, but Packet allows us to manage physical servers and various “carrier-grade” networking features with the automation we all expect from the public cloud. This kind of control is even more important as we grow to support ever larger clients on the Section platform. As you might expect, with more demanding use cases we find that our customers require more specialized solutions. The control and performance available with Packet’s bare metal is a powerful tool in our toolbox as we scale. Endless Edge Opportunities A current hot topic in the edge computing space is that while many providers are building edge compute infrastructure, there aren’t many clear pathways for developers to understand how the edge works and how to access that edge. That’s the bridge Section crosses. By partnering with companies like Packet, AWS, and Digital Ocean, to name a few, to build out our edge infrastructure, Section provides a programming model that opens the door to the edge for developers. Packet is holding its annual conference, IFX 2018, November 28-29, 2018 at Bally’s Parking Lot in Las Vegas, where Section CTO Daniel Bartholomew will be delivering a talk titled How Do We Bring Developers to the Edge. Programming (including intimate talks with C-level execs and presentations by leading engineers and partners) will emanate from the core topic areas: plenty of 5G, edge computing, speciality silicon and open source software."
"335","2016-08-24","2023-03-24","https://www.section.io/blog/five-questions-to-ask-before-buying-a-cdn/","What is a Content Delivery Network? Currently, more than 50 percent of the Web’s traffic is being run through Content Delivery Networks. Most CDNs have similar basic features that provide improved website security, performance, and scalability, but some go about this in different ways than others, and some CDNs focus on one specific aspect of web optimization - for example, blocking bots and protecting against attacks. To help you negotiate the CDN landscape, here are five questions we think you should consider before investing in your next CDN. Question 1 – Why do I need a CDN? Buying a CDN can be a confusing process. This is often because a CDN is a very technical product that ultimately has a real business benefit. In the long run, there are three core reasons to buy a CDN: Improved scalability so your website doesn’t go down at peak traffic times. Better performance and speed: a faster website means more conversions and more revenue. Protection against security issues, from downtime due to attacks such as DDOS, to exposing confidential user data. There are around 40 major CDNs available today, some of which have very specific niches. So you must understand your business needs: are you looking for one specific benefit? Or would you like the option to improve scalability, speed, and security through one solution? Question 2 – What CDN will work for my developers? This is an important question which is often overlooked. Assessing what CDN is right for your operations and development teams will be important down the line, when you’re looking to get the maximum benefits out of your Content Delivery Network. If you buy a tool that is difficult to set up and work with, it will take hours of development time to get it right, and that often means it won’t be utilized to it’s full potential. Speak with your technical team about the features they are looking for in a CDN and ask yourselves these questions: Can the team integrate the CDN into its development workflow? How many hours will it take to set up? Can the team diagnose caching or firewall issues quickly and effectively? Can the team identify and fix issues in testing environments before you go to production on your live website? Question 3 – How do I choose a CDN for my customers? It’s important to think about who will be accessing your website and from where when you buy a CDN. Legacy CDNs typically have the most servers, or points of presence (POP), spread out across the most locations. Most newer CDNs have taken a different approach, building Super POPs. Super POPs are clusters of servers, or gateways between users and content, which are closer to the Internet backbone (the main routes between major computer networks across the globe). While more POPs may sounds more attractive, the reality is that each website will have a different number of POP profiles to suit their visitors. Only the very largest websites, with the most customers distributed across the globe, tend to be able to utilize all parts of the largest CDN network. Most sites will perform better with a more concentrated caching tier closer to the Internet backbone. Question 4 – What CDN is right for my website or application? Some CDNs offer features that increase their effectiveness for a particular application type. For example, at Section we have features that are optimized to help websites using the Magento eCommerce platform. Consider what sort of applications you are running and examine your current network requirements, growth rate, and the content you are aiming to optimize. Most of all, make sure your development team can use the CDN to customize its features for your specific application and needs. Question 5 – How much should a CDN cost? These days, CDN pricing is a little complicated. There are a myriad of ways CDNs charge for services, and this can vary depending on things like bandwidth, performance, security features, SSL certificates, the number of requests per page, the number of times your team needs to “flush the cache,” and more. Some CDNs publish their pricing broken down by feature, while others will charge you whatever they can. We suggest you choose the CDN whose pricing mechanism you best understand, and that makes the most sense cost-wise as your business grows. Let Section take the guesswork out of CDN buying Section is the only CDN on the market that is designed to put you and your developers in control: choose tools to enhance speed, scalability, security, or combine all three, and test features in a development environment before pushing them live. With a flexible pricing model and no long-term agreements, we invite you to see Section in action for yourself with a free trial, or contact us for a custom demo. Try Section for free"
"336","2018-10-31","2023-03-24","https://www.section.io/blog/saas-companies-on-section-platform/","Among the primary customers that we serve at Section are SaaS (Software as a Service) companies that license and deliver software on a subscription basis from a central hosting platform. SaaS has become a popular delivery model for many business segments - from payroll processing to HR management, along with all types of other enterprise applications. The Section platform is a very compatible solution for SaaS companies for three key reasons. 1. Free SSL Certificates and Flexibility to Upload Custom Certifications Typically, traditional CDNs (even those who offer free monthly services such as Cloudflare) will require you to pay for their most advanced plans, which are the ones that you need in order to benefit from unlimited SSL features. Such enterprise plans typically cost in the region of several thousands of dollars per month. For a small SaaS company getting up and running, or even for a larger SaaS company, this can be an expensive piece of the monthly pie. So why do you need an SSL certificate? SaaS companies frequently act as placeholders for other clients white labeling their software. That client will typically want to use their own custom domain or a subdomain of the system they are using, and for each domain, they will need a separate SSL certificate in order to create a secure link between a website and a user’s browser. The SSL certificate authenticates a website’s identity and encrypts information sent to the server using SSL technology. SSL stands for Secure Sockets Layer, which is the protocol that provides the encryption. For example, Status Page is a SaaS business that offers customer relationship management software to update and inform its customers of digital problems. Section happens to be one of those customers; in fact, we white label Status Page’s software. In SSL terms, this means that they have to provide us with an SSL certificate so that when a user goes to our Status Page subdomain, the information is encrypted over the Internet. Contrary to other CDNs, Section provides free SSL certificates for every single domain that you require; this is included in our base price of $500. When you sign up with Section, you can benefit from unlimited free SSL certificates, which for many SaaS companies is a significant boon. This feature also plays into the second key benefit of using Section. 2. Unlimited Number of Domains per Account Many CDNs charge per domain and will limit the number of domains you can get on their free plans. Once you start paying for a larger number of domains, it can quickly get very expensive. With Section’s highly reasonable base rate price, you get an unlimited number of domains and SSL certificates; you only pay more on a per-traffic basis. If you have lots of traffic, you will pay extra, but you will nonetheless greatly benefit from avoiding a base rate of around $3-4K/month, which is what other CDNs will charge for their plans that include unlimited domains. How is Section able to do this you may ask? Well, we use Let’s Encrypt, which is a free, automated and open source of SSL certificates. It allows us to charge you just for the administrational overhead. If you use Let’s Encrypt directly, you have to manually manage the auto renewing of the SSL certificates that they provide. These expire every three months; thus in order to lessen your operational task list, our platform provisions a certificate for you whenever you need one and handles those time-burdensome renewals. This is how we are able to provide the service and hand on those cost-saving benefits to you. 3. RESTful API The third reason that we tend to work very well for the needs of SaaS customers is that we use a RESTful API that automates the process of provisioning SSL certificates and adding domains to the Section platform. Other CDNs have limitations on automating internal processes. The Section API, however, allows SaaS companies (and indeed any other company) to bake the SSL certification process directly into your application’s logic, allowing you to automate that management process. Once you code it into your application, you will never have to touch it again. Other examples of how the API works, in addition to provisioning SSL certificates and adding domains automatically, include managing your cache remotely (e.g. automatically deleting cache based on a certain trigger event from a user); or using it to poll (or ping) Section in order to grab certain metrics that you might want to bring into your own platform to display directly to your customers. All these examples serve to reduce operational overhead, allowing you to automate tasks that would otherwise take up the valuable time of your operations teams and letting them focus instead on other things, such as building and running awesome applications. SaaS Customer Leveraging the Section Platform for SSL White Labeling and Beyond We’ve helped multiple SaaS companies automate functionality into their applications that previously was either cumbersome or too expensive with other solutions. One such example is ScreenSteps, which provides an easy to use knowledge base for a business’ employees and customer base. They offer a feature for each business to add a custom domain and provide a free SSL certificate, all of which is automated through the Section API. ScreenSteps was able to test the API in a staging environment and eventually deploy to production giving them peace of mind before making critical changes in production. In this particular implementation, the custom UI is built directly into the ScreenSteps application, making Section completely invisible to their user base. By choosing Section for this functionality they were also able to offer their customers a globally distributed caching layer with Varnish Cache to improve performance, visibility into HTTP traffic via ElasticSearch and Kibana, and the ability to add additional modules to their proxy stack when the need arises."
"337","2018-10-30","2023-03-24","https://www.section.io/blog/gartner-top-10-technology-trends-2019/","Our team was in attendance at the recent Gartner Symposium/ITxpo 2018. The agenda was packed with insights across 16 different areas of tech - from Application Modernization to Work, People & Culture. One of the highlights of the annual event is hearing what the Gartner experts have identified as the Top 10 Strategic Technology Trends for the coming year. This influential list highlights the key “changing or not yet widely recognized trends that will impact and transform industries through 2023”. Not surprisingly, this year’s list includes the Edge. “The future will be characterized by smart devices delivering increasingly insightful digital services everywhere,” said David Cearley, Gartner Vice President and Fellow, at Gartner 2018 Symposium/ITxpo in Orlando, Florida. “We call this the intelligent digital mesh.” “Intelligent” because AI sits within almost every category and equally is continually defining new categories. “Digital” refers to the blend of the digital and physical worlds “to create an immersive world” and “mesh” to the expanding connections between different sets of people, businesses, devices, content and services. Trend No. 5: Empowered Edge The trend that especially captured our interest is the ‘Empowered Edge’, coming in at No. 5. As we discussed recently in another post, the edge is currently dominating many conversations across the tech landscape. Cearley described the trend as “a topology where information processing and content collection and delivery are placed closer to the sources of the information, with the idea that keeping traffic local will reduce latency”. It was clear from the Symposium and the top 10 list that there are overlaps within various trends, for instance IoT combines with AI, and augmented analytics overlaps with edge compute. Interestingly, Gartner said too much of the current focus of edge compute was on the need to enable IoT systems to deliver “disconnected or distributed capabilities” into the embedded IoT world, and that industry focus on it should widen. We elaborated on this same topic in a recent blog post. Its ability to address a wide range of challenges, from high WAN costs to unacceptable latency levels, was highlighted. Gartner identified three critical elements within the Empowered Edge: Cloud to the Edge - Using cloud architectures to deliver and manage capabilities out to the Edge; seeing cloud as a supporting force rather than a competitive one. Leveling up devices - Empowering Edge devices with more resources, such as AI chips, more compute capabilities, advanced processing, more storage, etc. Communicating to the Edge - Other developments that will drive the Edge forward, such as the release of 5G. We’ve discussed this aspect of the trend recently here. Through 2028, Gartner said it expects to see “a steady increase in the embedding of sensor, storage, compute and advanced AI capabilities in edge devices”. Gartner’s Top 10 Strategic Technology Trends for 2019 Here is an overview of the other nine trends that make up the list: Autonomous Things: Gartner groups things that use AI to perform tasks normally done by humans into five types: robotics, autonomous vehicles, drones, appliances and agents. “As autonomous things proliferate, we expect a shift from stand-alone intelligent things to a swarm of collaborative intelligent things, with multiple devices working together, either independently of people or with human input.” Cearley said. Augmented Analytics: This trend represents a third major wave for data and analytics capabilities. For Gartner, it is focused on a particular area of augmented intelligence - using machine learning (ML) to explore more hypotheses and eventually to build automated insights into enterprise applications to optimize decision-making across all departments. AI-Driven Development: This trend looks at tools, technologies and best practices for ways of embedding AI into applications and deploying AI to create AI-powered tools for the development process. Gartner imagines application developers working alone (as opposed to with professional data scientists) to develop most AI-enhanced solutions using predefined models delivered as a service. It also looks at the trend for AI being applied to the development process to automate aspects of data science, application development and testing functions. Digital Twins: The expression refers to the digital representation of a real-world entity or system. Gartner predicts there will be over 20 billion connected sensors and endpoints by 2020, and accordingly, the existence of digital twins for billions of things. Organizations will initially implement digital twins simply and they will develop them over time as they improve their ability to collect and visualize the right data, apply the correct analytics and rules, and respond usefully to business goals. As discussed above, the Empowered Edge comes in at No. 5. Immersive Experience: Gartner points out the way in which conversational platforms - virtual reality (VR), augmented reality (AR) and mixed reality (XR) are altering the way in which people interact with and perceive the digital world. “Over time, we will shift from thinking about individual devices and fragmented user interface technologies to a multichannel and multimodal experience.” Cearley said. Blockchain: Gartner predicts that blockchain has the potential to transform entire industries by enabling trust, offering transparency and easing collaboration, potentially lowering costs, lessening transaction-settlement times and bettering cash flow. By removing the need for central authorities to arbitrate transactions, blockchain offers an alternative trust mode and way of doing business. Smart Spaces: A smart space is “a physical or digital environment in which humans and technology-enabled systems interact in increasingly open, connected, coordinated and intelligent ecosystems”. Multiple components, including people, processes, services and things, align in a smart space. The most extensive and integrated example is smart cities, in which digital and real-world business, residential and industrial communities interact in their very design. Digital Ethics and Privacy: This area is a growing concern for individuals, organizations and governments alike. As people become increasingly concerned about how their personal information is being used and stored online, the risk of “consumer backlash will only increase”. Cearly highlights, “Conversations regarding privacy must be grounded in ethics and trust. The conversation should move from ‘Are we compliant?’ toward ‘Are we doing the right thing?'” Quantum Computing (QC): Gartner says QC is a longer-term trend that organizations shouldn’t expect to be able to exploit until post 2022 at the earliest, but that it is worth the investment of time to understand its potential applications and consider security issues. Gartner highlights the potential of QC for parallel execution of tasks and its exponential scalability, which means “quantum computers are able to theoretically work on millions of computations at once”."
"338","2018-10-19","2023-03-24","https://www.section.io/blog/edge-computing-5g-iot-ai/","The growth of edge computing is not only creating opportunities for processing to take place closer to where data originates, but as more devices connect to the cloud via edge compute, it is also paving the way for further development of 5G, IoT and AI, thereby opening up virtually boundless possibilities in the future world of technology. The Oncoming 5G Revolution 5G, the fifth generation of wireless technology, promises to not only dramatically increase speed, but also significantly enhance the coverage and responsiveness of wireless networks. 5G speeds will be anywhere from 10 to 100 times faster than the average cellular connection. 5G promises to have a low latency with lag times decreasing from around 20 milliseconds (with current networks) to as little as 1 millisecond. Since Verizon said it would be the first big telecom company to deploy 5G tests around three years ago, hype around the technology and further development from other major telecom figures such as AT&T has been mounting. If it fulfils its promise, 5G (and edge computing along with it) will serve as a foundation to boost many areas of new technology and innovation, including the ascension of self-driving cars, VR, AI, and telemedicine. How to Define the Edge With 5G and edge computing, it is first necessary to define where the edge is located. Edge computing resources can be located on the operator or the user side of the last mile network. The infrastructure edge refers to the operator-side, and the device edge refers to operations on the user side. Indeed as we discussed in our recent article, “What and Where is the Edge”, there is no single edge to the Internet. Enterprise customers may define the edge differently than mobile network operators. While enterprise customers might define the edge as where the IoT device is installed or at the location of the end user, mobile networks typically define the edge more broadly as anything that isn’t in the data center. Multi-Access Edge Computing (MEC) is the term the European Telecommunications Standards Institute use to describe edge computing. MEC is starting to be deployed around the world within networks, beginning with the Radio Access Network (RAN). Edge Computing is Paving the Way for 5G, IoT and AI According to the Open Fog Consortium, a worldwide coalition of thought leaders in the edge/fog compute space, edge computing will help pave the way for 5G, IoT and AI. Roger Billings, a Global Solutions Architect at Cradlepoint, outlines the reasons for this as: The provisioning of load balancing Supporting multiple levels of nodes for hierarchical networking Enabling the pooling of resources, universal orchestration & management Numerous access modes giving each edge network node the resource applications it needs Delivering improved reliability, security & resiliency Helping to support virtualization, mobile & IoT applications Offering agility with a horizontal platform & aiding all vertical markets Leading to a more scalable solution by migrating computation, networking, or storage capabilities across or through different levels of hierarchy Edge Computing and 5G Enable IoT and AI According to Gartner, 20.4 billion connected devices are expected to be in use by 2020, and the explosion of devices is already creating a strain on networks. On top of that, a recent article published by G2 Crowd notes that by 2021, the average U.S. consumer will interact with 601 internet connected devices every day. If this seems like a lot, that number is expected to reach 4,785 interactions by 2025 – an 800 percent increase! According to the International Data Corporation (IDC), 45% of the world’s data will be moved closer to the network edge by 2025 in order to cope with the sheer size of machine data. 5G was built to handle equipment for businesses that don’t need a constant connection and only periodically send data. With 5G network capabilities and edge computing improving latency, it is not only that IoT devices can be better accommodated in terms of processing data volume, but for many of these devices, latency is critical. With things like autonomous vehicles and medical equipment, absolute reliability is essential. 5G and edge computing will be the cornerstone for many IoT and AI devices. Edge computing and 5G are essential infrastructure enablers for the growth of Industry 4.0, new business and tech developments centered around the IoT and AI."
"339","2018-10-12","2023-03-24","https://www.section.io/blog/latency-issues-logging-tracing-metrics/","When using monitoring tools to identify latency issues, metrics and logging have their place, but they can be inadequate for offering true visibility across services, particularly with the rise of distributed application architectures. Distributed tracing, however, overcomes those challenges. The Role of Logging and Metrics Traditionally, logging and metrics have played critical roles in identifying and tackling latency issues. Through the creation of a record of events in an operating system or other software, logging offers an account of what happened when - including in the instance of latency, a record of response times. Metrics will also record response times, but as part of a larger set of data combined from measuring events. Metrics help put response times into context; for example, demonstrating whether response times were fast or slow in comparison to others. Metrics can also be useful to identify patterns and/or alerts. The Benefits of Distributed Tracing Logging and metrics, however, can run into challenges when a system reaches a certain size. As functions increase in complexity and companies increasingly move over to distributed architectures, metrics and logging can fail to provide sufficient visibility across services. Distributed tracing is instead increasingly being used for monitoring complex, microservice-based architectures as it records events with causal ordering. As a result, it allows you to ask the question, why is this slow? Distributed tracing can therefore enable you to determine causality in a way that logging and metrics alone cannot, as it can reveal the processes behind the speed of each response. As an application grows to 10+ processes, begins to see increased concurrency or non-trivial interactions between mobile/web clients and servers, tracing allows for visibility into those processes. As an example, think of your homepage. Whenever a user visits it, the web server will make two HTTP calls; each of those calls will branch out to call the database. Debugging this process is fairly straightforward if there are latency issues, as you can assign each request a unique ID and send it downstream via HTTP headers for analysis. If, however, your website experiences a spike in popularity and your application is now spread across multiple machines and services, logs become less useful, providing less and less visibility, the larger the number of machines and services. Use Cases: Uber & The Economist Uber Engineers has been a pioneer in distributed tracing as a result of its software architecture complexity growing with the size of its worldwide business. As of early 2017, Uber had over two thousand microservices due to an increased amount of business features, both user-facing apps such as UberEATS and internal functions such as maps processing and data mining.The company also migrated away from large monolithic applications to a distributed microservices architecture and found accordingly that they needed a different monitoring system that would provide visibility into the system and the complex interactions happening between services. Its engineers have written about the shift in detail, explaining the different phases they moved through as they found their way to their current model. The Economist has also detailed its transition from a monolithic system to a distributed microservice-based architecture, and the realization that “our logging and monitoring approach wasn’t able to keep up”. They found, for instance, that logs from different apps had no schema or differing schema, and HTTP requests were not easy to trace through distributed applications and services. They increasingly lacked transparency into their own systems and the company’s engineers began to lose confidence in the site’s ability to perform at scale. The company developed a new strategy, leaning towards standardized logging, or distributed tracing, in addition to continuing to deploy logs and metrics where necessary. Keep Track of Transactions Distributed tracing allows you to keep track of transactions. A tracer propagates a context that comes into a service, which is then propagated to other processes and attached to transaction data sent to a tracing backend. This not only allows you to monitor your system across process boundaries in which APM agents can’t be installed (an increasingly important tool as the industry shifts to microservices), but the context enables the transactions to be stitched together at a later time. When trying to improve latency across machines and services, this type of visibility is crucial. It allows you to ask the question, what happened and how long did it take? It’s also why Section is making an active shift from a logging to a tracing focus. This will allow us to provide more meaningful insights to our platform users, and offer the most up-to-date and valuable monitoring tools for today’s increasingly complicated computing landscape."
"340","2018-09-26","2023-03-24","https://www.section.io/blog/fog-computing-vs-edge-computing/","In contrast to traditional cloud architectures, fog computing and edge computing are network and system architectures that gather, analyze and process data from assets deployed at the very edge of the network. Fog compute and edge compute share similar goals, including the reduction of network latency, minimizing the amount of data sent to the cloud for processing and optimizing system response time, particularly in remote mission-critical applications. Indeed some consider the two architectures to be synonymous, but there are fundamental differences between the two. In both types of architecture, the data itself is generated from the exact same source - a physical asset or ‘thing’ of some kind, whether that is a sensor, pump or a motor. These devices perform a real-world task such as detecting information about the world around them, pumping water, or switching electrical circuits. Even though both fog computing and edge computing involve moving intelligence and processing capacity closer to where the data originates, the most fundamental difference between them is precisely where that intelligence and computing power is located. Fog computing drives intelligence down to the local area network (LAN) level of network architecture, processing data in either a fog node or IoT gateway. Edge computing meanwhile drives the intelligence, processing power, and communication operations of an edge gateway or appliance directly into the devices themselves such as PACs (programmable automation controllers). Fog computing therefore involves multiple layers of complexity and data conversion. It relies on numerous links in a communication chain to move data from the physical world of the ‘thing’ into the digital world. Edge computing offers a simplified version of this communication chain, thereby reducing possible points of failure. Fog compute can be effectively used in combination with edge and cloud compute. Key real-world use cases of fog computing include smart electrical grids, the Internet of Things, in particular autonomous vehicles and manufacturing in the IIoT (Industrial Internet of Things). Graeme Wright, CTO for Manufacturing, Utilities, and Services at Fujitsu UK described how this works to improve efficiency in an interview with Tech Radar. “Edge computing may be used to control the device that is being monitored by a sensor, and only send data back when something changes,” says Wright. “This could then be complemented by fog computing, to alert other sensors or devices of the status change, and take appropriate action.” Cloud compute can then be put into use to run analytics on the complete system, alerting staff to any necessary maintenance issues. “This setup can not only provide real-time analysis of the data, but also lower data storage, and more importantly improve efficiency,” notes Wright. Cisco is commonly associated with the term fog computing who indeed originally registered the name “Cisco Fog Computing”. Various industry bodies have since arisen to help define fog computing and encourage its use, including the OpenFog Consortium. The group was founded in November 2015 by ARM, Cisco, Dell, Intel, Microsoft and Princeton University to “accelerate the adoption of fog computing and address bandwidth, latency and communications challenges associated with IoT, 5G and AI applications”. The Consortium is about to hold the Fog World Congress in San Francisco from October 1-3, 2018. One of the key values disseminated by the OpenFog Consortium is that “the immersive fog can address many challenges that Cloud alone cannot effectively address”, including the processing of real-world data, supporting time-critical local control, connecting and protecting the many types of resource-constrained devices on the market, and overcoming network bandwidth and availability issues. The Consortium predicts that “over time, cloud and fog will converge into unified end-to-end platforms offering integrated services that combine resources in the clouds, the fogs and the things”."
"341","2018-09-11","2023-03-24","https://www.section.io/blog/comparing-paas-saas-iaas/","A Comparison Between PaaS, SaaS, and IaaS What do the terms PaaS, SaaS and IaaS mean, and how do you choose between them? All three are different types of cloud service, offering infrastructure and resources-on-demand as a service. Each offers a fundamentally different approach to the management of IT resources. PaaS Platform as a Service (PaaS) offers an online framework that developers can build upon to create, test and deliver customized applications. The third-party provider manages the infrastructure and operating systems side (including servers, storage, software updates and networking) allowing the developer to focus on management of the applications and building software. PaaS is: A platform for software creation The platform is delivered online Customizable - businesses can design and create applications built into the PaaS with specific software elements Scalable and highly available - multiple users can access the same development application Reduces the amount of coding necessary Automates business policy and admin Offers services for development, testing and deployment of apps Integrates web services and databases Supports DevOps principles SaaS Software as a Service (SaaS) is a full application, hence SaaS are also known as cloud application services. It is the most commonly used option for businesses in the cloud market. It offers less flexibility than a PaaS or IaaS as the SaaS platform controls most of the look and feel, along with the stored data. Most SaaS applications are run through the web browser so the end user does not need to perform any downloads or installations to make use of the application. SaaS offers: Virtually no in-house management; hardware and software updates are managed instead from a central location (i.e. no downloads or installations required for the user) Hosted on a remote server Accessible online You maintain identity and access management (IAM) SaaS applications can be linked to other software using APIs Offers specific services, such as storage (i.e. Dropbox) or WebEx conferencing (i.e. Cisco WebEx) IaaS Infrastructure as a Service (IaaS) provides cloud computing infrastructure (including storage, computing and networking resources) on a pay-as-you-go basis to enterprises through virtualization technology. Some people describe IaaS providers, such as AWS and Microsoft Azure, as a virtual data center. In addition to provisioning hardware, IaaS providers issue access to other infrastructure services, including network monitoring, security, billing and load balancing. These tend to be provided to the client through a dashboard or an API: Offers automated deployment of computing resources (including servers, networking, storage and processing power) Hardware and resources can be purchased on an as-needed basis Generally includes multiple users on one piece of hardware Deploys middleware Services are scalable Organizations retain control over their applications and infrastructure Further comparisons between PaaS, SaaS and IaaS are available here and here. Why Edge Compute Platform is a better alternative to legacy CDNs An Edge Compute Platform like Section offers you greater flexibility, control and transparency than is possible with the legacy CDNs. Indeed, the founders of Section followed an “internal manifesto” when building the service to do the opposite of the legacy CDNs in certain key respects. Our developer-friendly approach offers dev teams visibility into their reverse proxies, which legacy CDNs do not. This provides the transparency to understand what is happening with your stack. The Section Edge Compute Platform also crucially doesn’t take a one-size-fits-all approach to any aspect of its services. Developers are not locked into any one proxy software stack at a time, as with traditional CDNs. As PaaS is built on container-based technology, resources can be scaled up and down as your business needs change. You can customize your reverse-proxy stack based on the tools that work best for your site at any given time, swapping out different modules (both open-source and proprietary software) as needed. The open-source mindset is one that we embrace at Section: valuing transparency, collaboration and community in a similar spirit to that of Linus Torvalds, the creator of Linux (which became the kernel for the Linux, Android and Chome OS operating systems) and the distributed revision control system Git. By giving developers access to open-source software and embracing an open-source mindset, new possibilities are opened up. With our Edge Compute Platform, you also have the choice of where you want to run your edge based on the best locations for your application and users. You can deploy your edge on our global delivery cloud, on-premises, on a private or public cloud of your choosing, within the telcos and even into target networks. The Section Edge Compute Platform offers agility, flexibility and is more cost-effective than the legacy CDNs, as infrastructure and developer costs are reduced and there is usually a faster-time-to-market for products, resulting in increased revenue for businesses. For developers that want the ability to flexibly customize their own applications without the hassle of managing the underlying infrastructure, PaaS is the ideal solution."
"342","2018-05-24","2023-03-24","https://www.section.io/blog/how-to-secure-a-website/","To maintain a well performing and highly available website, it is important to provide several layers of security. In addition to normal patching, user access management and good development hygiene, we recommend you use a combination of following to ensure your website is protected: DNS Protection HTTPS Network Protection Caching Rate Limiting And IP Blocking Web Application Firewall 1 DNS Protection Use of a quality customer-facing DNS solution is important to prevent attackers from overwhelming your website through a flood of DNS requests. Your DNS provider should be able to confirm an ability to handle DDoS attacks. An even better protection at the DNS layer is to use a provider who can deliver redundant DNS systems for your website so that in the instance of one DNS provider being overwhelmed by any sort of attack, the second provider will be available to continue service for your website. You can set up redundant DNS services yourself or choose a website delivery provider who includes this service for you. 2 HTTPS Delivering properly encrypted traffic from your servers all the way through to your customers’ browsers and back again is a core line of defence in making sure your site and your customers’ details are secure. Deployment and maintenance of a high quality SSL certificate (the certificate you need to demonstrate your site is secure which gives you a HTTPS web address) is important to prevent potential flaws in the encryption which may open the traffic up to interception, interpretation and exploitation. Qualys SSL Labs use a handy rating system to help users discern the quality of their SSL certificate. Lower ratings indicate that your encryption levels may not be satisfactory with respect to areas such as cipher support, protocol support, or key exchange support, or could indicate your certificate is installed incorrectly or not trusted for the domain of your store. Visit www.ssllabs.com/ssltest/ to test your website certificate rating. If you wish to immediately enhance your certificate rating, you should address any shortcomings found from this review. You could also investigate the use of certificates issued by your website delivery platform, as they may provide and manage higher rated certificates on an ongoing basis than you are able to secure directly. An Extended Validation certificate, commonly used by banks or other websites that manage highly sensitive data, is not necessary for your site and will not enhance the security of the web traffic to and from your website. Extended Validation certificates may improve user perception, but do not improve security as they use the same encryption protocols. 3 Network Protection Attacks can also occur at the networking layer. Your website needs to be able to detect and reject networking style attacks including those at the TCP layer. You should partner with hosting and site delivery providers who provide network-level protection at large scale so that your site is not subject to performance degradation or failure as a result of network attacks. 4 Caching A well structured caching layer can prevent your core infrastructure and compute resource from becoming overwhelmed by requests for certain assets. As we reviewed in Chapter 4, caching means an asset or assets can be served from a cache, preferably from an elastic infrastructure which is not part of your core hosting infrastructure. In this way, you can defeat some DDoS attacks simply by having more resource readily available to serve the attacker’s requests than the attacker can muster to generate the requests. Every website should maintain a quality caching tier in front of their web servers for the purposes of both improving performance and scalability of the website directly and for providing an additional security layer. Distributed, elastic cloud solutions will provide the best results for these purposes. This caching tier needs to be well tuned. For example it is very common to be able to bypass all caching efforts by simply adding random parameters to querystrings. Ideally, all the URLs and their possible valid querystring parameters will be addressed to make sure simple efforts to negate the cache are not trivial. 5 Rate Limiting And IP Blocking Detecting and blocking requests based on IP ranges or GeoIP Databases can be helpful for certain styles of attack. While some attacks will avoid IP blocking by moving the attacking vector IPs around or attacking from a large and varied range of IPs (such as with a DDoS attack), other attacks can be handled well by limiting the ranges of IP addresses which can make requests on your website. For example, your customer base may be solely from one country and in this case you may wish to block the IP ranges for other suspect countries to avoid the chance that attacks could be launched from those countries. An additional alternative to all-out blocking of IP addresses or address ranges is to limit the frequency with which an IP address can connect and make requests from your website. Normal customer behaviour usually presents as a much lower frequency of request than a number of different types of attack. This is known as request rate limiting. By installing the right delivery infrastructure for your website, with very limited effort you should be able to manage the IPs which can connect to your web infrastructure and set upper thresholds for the rate at which any particular IP address can make requests to your web application. 6 Web Application Firewall Placing a Web Application Firewall (WAF) in front of your application can be a very effective way of controlling attacks which may occur above the networking layers at the HTTP protocol layer. A WAF can inspect the HTTP requests being sent to your website and, based on a set of rules, determine if the requests may be malicious and block them, or valid and then allow them to continue. The types of attack a WAF can detect and block include those outlined in the OWASP top ten above. By inspecting, detecting and blocking malicious requests, you can avoid system outages. When websites are compromised by these types of attacks, most often, a website will be taken offline voluntarily by the website owner to avoid the potential calamitous complications of leaked or hijacked customer details and payments information. Installing and maintaining a WAF for your website can prevent these outages. Beware of one-size-fits-all WAF vendors. These systems are often optimized to reduce the chance of the system breaking the protected application so the vendor can minimize support requests. You’ll get a better result with a WAF that is tailored to your application. Installing and maintaining a WAF for your website can be a complicated matter. You need to make sure you have the right compute infrastructure and the right tooling to be able to view the activity within the WAF and manage the rule sets for your application. A WAF returning too many false positive blocks will cause real customer frustrations and hence become frustratingly useless very quickly. Conversely, a WAF returning too many false negatives (or requests which should have been blocked but were not) will not provide the level of protection which it promises. Therefore, when installing a WAF, make sure you have the tooling to quickly and simply test the WAF settings in your development and staging environments before turning it on immediately in production. You should have good access to real time reports, metrics and logs for your WAF in addition to flexibility to manage the rulesets and run the WAF in each of your development and staging environments."
"343","2018-05-01","2023-03-24","https://www.section.io/blog/australian-ecommerce-top50/","The recently released top 50 people in Australian ecommerce has again demonstrated Section’s market leadership. Across the board Section is the preferred solution for web performance, availability and security in Australian ecommerce, supporting 5 of the top 10 and 15+ of the top 50 Australian ecommerce leaders; more than any other comparable provider. Section has become the trusted “go to” partner, assisting businesses to improve website performance and security. Section team and platform partner with both internal and external development teams to dramatically enhance the performance and security of any website. Section’s flexible platform and deep knowledge of web performance in Australia means Section achieves results beyond any other web optimisation platform or Content Delivery Network. Matt Johnson - GM of APAC comments: “Most people work to improve performance or security either via web development activities or basic CDN layer but they can’t achieve the results they were hoping for. This can frustrate the engineering and marketing teams as they suffer reduced effectiveness of marketing spend and can make sale events or security breaches a terrifying experience. Brands may be damaged by downtime and poor user experience. We are helping people fix these issues every day.” Industry data has consistently demonstrated that site speed is a critical issue to solve for improved business returns. At the same time, hacks, attacks and ransom events are on the the increase. There are a many solutions in the market for everything from payment gateways to shipping, but until now no one has successfully (and consistently) improved website speed and security in one easy, developer friendly platform."
"344","2018-04-21","2023-03-24","https://www.section.io/blog/rum-by-classification/","Competition as an e-retailer has never been tougher. The marketplace has become so crowded that if I want to buy a boutique reindeer outfit for my dog, I have literally dozens of options. Online shoppers have an overwhelming number of choices, and even the slightest blemishes on their user experience can turn them somewhere else. One of the easiest ways to lose customers is through slow pageload times. Synthetic performance tests are a great place to start optimizing, but in order to get a real picture of what your users are actually experiencing, you need to collect real user data. You want metrics that provide deeper insight into your website’s performance as your users actually experience it — statistics like the average of the slowest 5% of pageloads. This is why Section offers free Real User Monitoring (RUM) for all of our customers. Our goal is to partner with you to deliver the best experience possible for your users. We’re happy to announce that we’ve made our RUM even better. Our most recent release adds RUM by classification, allowing you to see pageload statistics on different areas of your site. Previous iterations of RUM yielded a single, site-wide set of statistics, which, while extremely useful, does not give you specific insight into the user experience on different parts of your website. You could have a lightning fast home page and still alienate users with a crawling checkout process. With our enhanced RUM, Section customers have an even more powerful tool for outpacing the competition. The Details Our new RUM supports five different classifications: Home, Product, Category, Checkout, and Uncategorised. You assign a page to one of these categories by adding a specific HTML class to the document’s body. This tells our RUM script to organize metrics for these pages into different statistical buckets. A few more small configurational changes plus some time to gather data and you’ll have accessible RUM metrics in your Grafana dashboards! Although our new RUM version can work with any website on any platform, our categories are built to integrate seamlessly with Magento’s default HTML structure. For more information on the implementation process, check out our documentation!"
"345","2018-04-09","2023-03-24","https://www.section.io/blog/chrome-ct-compliance/","Announced in 2016 but postponed until now, Google has decided that all web certificates issued after April 30th, 2018 must comply with the Chromium CT Policy in order for the Chrome web browser to honor them. When Chrome visits a website with a certificate issued after this date that does not comply with the Chromium CT (Certificate Transparency) policy, it will display a full page warning notifying the user that their connection is not CT-compliant. To see what this will look like, check out this link This policy has applied to Extended Validation(EV) certificates since 2015 and is being expanded to all certificates on April 30th. In order for a certificate to comply with the CT policy, it must be registered with an external logging registry, referred to as a Certificate Log. There are a number of Certificate Logs, both affiliated with Google and not. They are publicly queryable and append-only, meaning that anyone can view the logs, but no one can alter any information inside. These logs are continually monitored for suspicious activity and designed to limit the possibility of fraudulent certificates. In short, this is happening to make the web more secure. From a technical perspective, CT compliance means that all certificates issued after April 30th must present the Chrome browser with a Signed Certificate Timestamp, or SCT during the connection process. SCTs are issued by a Certificate Log once a certificate has been successfully registered with it and act as proof that the certificate is CT-compliant. There are several accepted methods of delivering the SCT, all of which are detailed in the above linked Chromium CT policy. All Let’s Encrypt certificates issued through the Section platform since March 30th comply with this new policy – if your application is provisioned under Section Let’s Encrypt, you are all set. If you are using a certificate from an outside provider, you should verify that any new certificates will comply with Google’s Chromium CT policy. You can determine the status of your current certificate by loading a page on your site with the security section of the Chrome network tab open as depicted below. If you have no Certificate Transparency section, then your certificate is not CT compliant."
"346","2018-03-23","2023-03-24","https://www.section.io/blog/vcl-hash/","Once you’ve been introduced to Varnish Cache’s power as a web application accelerator (HTTP caching reverse proxy) and understand the basics of the benefits it provides, you’ll quickly want to make sure it is maximizing the number of responses it can handle itself and not pass on to your web application server. To do this, it will need to match the incoming requests to items in its cache as often as possible without ever giving an incorrect or mismatched response. What requests should be considered equal? The flexibility of HTTP means that only the web server constructing the responses truly knows what parts of an HTTP request are important. Downloading binary files? Only the URI path is important. Serving content customized for the requesting device? The HTTP user-agent header is important. Serving content customized for the user’s location? The requesting IP address or geo-location is important. Running multiple sites sharing some common resources (CSS/images)? The website (host) name may not be important. Need to cache per-user (session) content? Cookies (or specific parts) may be important. The patterns for what aspects of the request alter the content of the response vary as much as the features of web applications do. The dilemma of how to deal with all of these parts within Varnish Cache is handled by a function called “hashing” Finding matches without onerous searches or masses of original data Let’s try an analogy: A carpet & floor covering shop wants to keep copies of floor plans for all the housing units it supplies. A number of apartment buildings and housing developments have duplicate buildings/apartment layouts and we don’t want to keep duplicate copies of the same floor plans for each one. How could we store a single unique copy of the housing layout and still connect it to a specific building when we get a call to install some carpet? One approach to this general problem is to take some aspects of the items and mix them together into a form that is unique, simple and sortable. Though perhaps a bit impractical in real life, our hypothetical carpet shop goes about solving this layout problem by asking each customer for one drop of paint of each color used on the building mixed together. This mixed color provides a unique signature for each building that is then kept with our copy of the abstract floor plan and placed in a filing cabinet where we sort them by the swatch’s hue. When we get a new order for carpet and need to pull the right set of plans from our filing cabinet, we request another mixed paint sample and use it to match the plans in our filing cabinet. Because we can match the swatches very precisely, the chances of matching a layout to the wrong building are essentially zero. Varnish Cache uses a similar approach. After a request arrives, a few of the request’s properties are mixed into the “hash” — by analogy our mixed paint sample. By default (https://github.com/varnishcache/varnish-cache/blob/6.0/bin/varnishd/builtin.vcl#L78-L86), this includes the request’s URL (including the hostname if available). If you’d like to differentiate your stored responses by another property (for example, if your web server customizes the content for different types of devices/browsers), you can add your own vcl_hash subroutine that will run before the default: sub vcl_hash {
    if (req.http.user-agent) {
        hash_data(req.http.user-agent);
    }
}
 How specific is the hash (paint mixture)? Varnish Cache can differentiate 115 quattuorvigintillion (that’s a 1 with 75 zeros after it) different hash values. For some perspective, if every atom in the visible universe was numbered, each unique Varnish Cache hash value would only match 1000 atoms. Summary The default operation of Varnish Cache will allow you cache efficiently and without false-matches in most scenarios. To unlock the full power of caching, you’ll need to give Varnish Cache an understanding of what aspects of a request can make the response unique. Understanding hashing and VCL’s vcl_hash is the key to imparting that understanding."
"347","2018-02-25","2023-03-24","https://www.section.io/blog/outage-pages/","Sometimes site downtime is unavoidable. Major upgrades, database server crashes/backup restoration, and a number of other events require that your site temporarily go offline. Whatever necessitates the outage, you don’t want your users getting meaningless server errors or having their browsers timeout waiting for a response from your site. Section has the ability to display a message to your users that will let them know that the site is temporarily down. This page can be designed to match your site’s branding — that way it looks like your website, but it’s hosted by us. During web server outages your users will still see a friendly message that lets them know what’s going on. Setting up your first outage page Section allows you to define multiple outage pages, so you can have different messaging depending on the reason your site is down. The first thing to do is to design your outage page. The page should be simple, with some text explaining that the site is down, but will be back soon. If you know when the site will be back up it may be helpful to include that information. Remember that because your site is down, you won’t be able to use any resources from your site. Images and css should be embedded in the page or hosted on something like Amazon’s S3. The outage page should be a single HTML file. For the purposes of this example, we’ll call it outage.html. There are two ways to get started with your first outage page. If you’re comfortable using git you can follow these next instructions — otherwise contact support and we’ll get you started. Go to https://aperture.Section and navigate to the “Advanced Config” for your site Grab the URL next to “Clone with HTTPS” and use it to do a git clone <url>. In the cloned directory create a new directory called outage_pages Put the outage.html file you created in that directory Commit this change using git add . && git commit -m ""Added new outage page"" Push this change up to Section with git push. Using your outage page Once the page is setup you can see it on https://aperture.Section under “Set up” -> “Outage Pages”. There will be a list of the outage pages you have set up with a “Preview” link next to each one. Clicking on the preview link will let you see what the outage page will look like once enabled. When you’re ready to engage the outage page, click the box next to the page then click “Engage,” and confirm your request. This will deploy your outage page to our servers and ensure that all traffic to your site will see your friendly message instead of a server error or a browser timeout. Once your site is up & healthy you can come back to https://aperture.Section and go to the “Outage Pages” and click “Disengage”. After another minute deploy your site will be live again. This is just one of the ways that Section can help, get setup today and see what we can do for you."
"348","2018-02-25","2023-03-24","https://www.section.io/blog/section-io-http-logs/","When creating a website, you have to set up a lot of moving pieces to actually make it function. For example, creating an HTTP server with software like Nginx or Apache, constructing the application logic sitting behind your web server like a Node app or Magento installation, defining and styling the user facing application with HTML and CSS, and if you have application state you’ll also need to configure a database. Once your application is ready to use you’ll have to start worrying about performance, security, and scalability by using software like Varnish Cache, a Web Application Firewall, and/or a Content Delivery Network. At the end of the day, you have a lot of moving pieces within your stack that you need to monitor over time to make your application more efficient and performant. You also have to be able to debug the setup in case of an outage or incident. Unfortunately these complex stacks are bound to run into issues over time and you need to ensure you have the tools to narrow down the root cause quickly so you can spend your engineering time fixing the problem, not trying to figure out what the problem is. Logging When dealing with logs there is a wide variety of contexts you need to account for. Firstly, there are the metrics from the actual server your stack is running on to ensure it has enough resources to compute responses to the client quickly and efficiently. Secondly, there are the error logs produced by your application in the event of a malfunction, such as your Nginx server crashing. Luckily there are many third-party solutions called Application Performance Monitoring software that capture and organize these logs and can be found in tools like New Relic. Third, you’ll need to worry about all the access logs generated by your software which can be very useful in narrowing down the root cause of issues and analyzing traffic patterns over time. These can be difficult to digest and manage due to sheer volume. There are also 3rd party tools that digest these logs for you but they can get quite expensive even for the starting pricing tiers. ELK Stack The ELK stack can be very useful as it provides a log digestion tool in Logstash, a highly efficient document storage location in ElasticSearch, and a user interface to interact with said documents in Kibana. You can run your own ELK stack instance on a cloud provider like Digital Ocean or AWS, or use one of the 3rd party solutions like AWS Elastic to get up and running with the ELK stack. These solutions are significantly cheaper than some of the 3rd party log digestion tools and also give you complete control over how your application stores it’s logs. Section logs The Section platform provides a hosted ELK stack instance included in the pricing structure for each customer. This ELK stack does not digest your origin logs but comes pre-configured to digest all your HTTP traffic flowing through the Section infrastructure. We log every request hitting our servers with some additional fields like geo-location of the connecting user, all the logs from your proxy stack for a better understanding of what each proxy is doing for you, and also the responses being sent from your origin server. These logs can be very helpful in understanding how your content is being delivered and even assist in the debugging of origin issues at a particular point in time."
"349","2017-02-08","2023-03-24","https://www.section.io/blog/ecommerce-clothing-shoe-stores-website-performance-study/","Fusion Retail Group in Australia is the parent company to several brands including Diana Ferrari clothing, shoes, and accessories, Colorado shoes, and Williams shoes. Recently these brands all over-went website updates with a focus on improved user experience to stand out in a competitive market. Quality images and detailed product descriptions were a clear need, but Fusion also wanted to focus on the speed of their websites as a fundamental element of improving UX and achieving higher conversion rates. Their new sites were set up on the EPIServer CMS and ecommerce platform, and then deployed on Section for website acceleration and server offload with a simple DNS change. Section also worked with the Fusion Retail team to install New Relic, so they were able to easily see their website performance outcomes. Section sits in front of the EPIServer web servers and dynamically improves page speed by providing the following front end and back end optimizations: Rewrites the HTML being produced by the EPiServer application for optimal delivery into the browser requesting the page. Optimizes images, JavaScript files and CSS files to reduce the number of round trips between EPiServer and customer browser. Compresses content and strips out redundant data to reduce to payload being delivered to the browser. Rewrites HTTP headers to improve the cachability of static files on Content Delivery Networks and in the browser. Caches content on a shared cache node and also in Section’s Edge Compute Platform. Rewrites HTML and files for optimal delivery of third party content. When these improvements were implemented, page speeds improved by about 46% and websites were able to handle increases in traffic without scaling up origin infrastructure. Most importantly, brands saw 30% increases in sales as a result of the website performance improvements. Fusion Retail has continued to leverage Section’s solution since then, noting that “The net results for the business make Section a “no-brainer” for any ecommerce website.” Optimize your e-commerce website with Section Section provides performance improvements to ecommerce websites that result in measurable lifts in page views, conversion rates, and revenue. This is achieved through caching dynamic content, serving requests from a global Edge Compute Platform, and improving website security through HTTPS for all pages, network-layer DDOS protection, and the option to add an Intelligent Web Application Firewall. To add Section to your ecommerce site, contact us today and one of our website performance experts will get back to you. Contact Us to Get Started"
"350","2017-02-08","2023-03-24","https://www.section.io/blog/akamais-greatest-asset-fatal-flaw/","It’s no secret that Akamai is the elephant in the room in any conversation regarding CDNs. It is the grandfather of the industry, and pioneered the art of bringing content closer to end users to improve browsing experience for people all around the world. Their first-mover advantage allowed Akamai to build an infrastructure still unmatched in size and scale by any of their competitors. Akamai’s infrastructure quickly became their greatest selling point. Thousands of points of presence (PoPs) distributed around the globe reduced latency and the load on a website’s origin, massively improving user experience during the age of dial up connections. No one could compete with the sheer size of Akamai’s network. During a time where the distance content traveled was paramount, their network size became an unbeatable competitive advantage. How many Points of Presence does your website need? Yet, Akamai’s infrastructure was built for a pre-broadband world and what was once a great asset became a glaring flaw. As the internet sped up, latency caused by physical distance became less important, and the performance difference between a caching server ten miles away versus one hundreds of miles away was negligible. The name of the game became caching optimization, where performance was determined by cache hit ratios and caching more content, rather than content travelling a shorter physical distance. New, modern CDNs built “super PoPs” with larger and more powerful servers located strategically along the backbone of the internet. By reducing the number of PoPs, the odds of a user requesting content that was already cached within their nearest PoP increased dramatically. Suddenly, Akamai’s massive network was actually inhibiting performance rather than improving it. To break this down a little further, we should take a look at how caching works more closely. Caching servers only cache content that has already been requested by someone else. This is a way of intelligently prioritizing the content that needs to be cached by letting a website’s visitors determine which content is being consumed regularly. Because content changes on websites frequently, caching servers also only keep content for a specified amount of time before needing to return to the origin server to collect un-expired content. This ensures that the cache isn’t delivering outdated content if something at the origin has changed. Increase cache hit rates with less PoPs While increasing the number of caching servers in a CDN’s infrastructure does reduce the latency caused by physical distance, it also decreases the odds that content will already be cached in that specific PoP. While truly massive websites with a worldwide audience might still get the most value out of having a network the size of Akamai’s, most companies are going to be hurt by a network of Akamai’s size because their cache hit rate will go down. Another flaw caused by Akamai’s large network is the complexity and time required to clear content from the cache. In the event of a website update, bug fix, or emergency security patch, it can take hours to clear the Akamai’s caching servers of bad content. More modern CDNs now offer instant cache clear allowing a user to purge the caches in a matter of seconds which is becoming a must have feature. So what does Akamai do now? Brand recognition will continue to carry them for a while, but with more cost efficient and higher performing CDNs hitting the market, what will Akamai’s competitive advantage be going forward and how will they modernize their infrastructure to address the needs of today’s internet users? There are no easy answers to these questions, and in the future they will need to contend with software-defined networks that can provide the right balance of latency reduction and cache hit optimization that will deliver the best possible solution for each individual website’s traffic pattern. In the meantime, the writing’s on the wall for Akamai. Either innovate fast, or prepare for decline. How Section and Akamai differ Section’s Edge Compute Platform was built to alleviate the frustrations developers have with configuring and testing on legacy CDNs such as Akamai. By allowing websites to choose what caching and security proxies they run, configure them themselves, and test them locally, Section is redefining modern content delivery."
"351","2017-01-20","2023-03-24","https://www.section.io/blog/tips-to-lower-bounce-rate/","Bounce rate is one of the most important metrics a marketing manager can consider: it is the percentage of visitors who leave your site after viewing just one page, and a good indicator of how your content and site usability is resonating with your audience. The average bounce rate is around 50%, and anything under 40% is considered to be very good. A bounce rate of 70% or over is high for most pages on a website, although if the first interaction a user has is on a blog article then a higher bounce rate is more expected. How To Improve Bounce Rate There are several steps to take to improve bounce rate on your website. Below are a few tips from Section: Improve page speed: Page speed is arguably the most important metric you should look at in relation to bounce rate. Multiple studies have shown that bounce rate increases with higher page load times, and more than 40% of users will abandon your site if it takes over 3 seconds to load. That means if your site loads slowly, you will be starting at a high bounce rate before accounting for users who leave your site for different reasons. This illustration from Soasta demonstrates this point: To get the biggest improvement in page speed, we recommend setting up caching and utilizing a Content Delivery Network. Both of these solutions will speed up your site while providing other benefits such as improved scalability and security. For smaller page speed improvements, look to optimize image sizes, reduce the number of resources on each page, manage your 3rd party JavaScripts, and minify your CSS. Examine the messaging on your site: One of the main reasons a user may leave your site quickly is if they don’t understand your product or service. If your homepage is unclear or vaugue, impatient internet users will likely click off your page rather than try and navigate to different sections to understand what you are offering. Clear and concise messages that immediately indicate to a user why they should browse your site further are crucial to lowering bounce rate. If you are using online advertisements to bring users to your site, ensure that the page each advertisement links to is relevant to the keywords on the ad. For example, if a user clicks on an ad proclaiming “Shoe Sale! 70% off everything” and is directed to a homepage that does not mention the sale, they will be more likely to leave than if they were directed right to the sale page. Adjust ad targeting: Ads through search engines such as Google or Bing and social media platforms including Facebook and Twitter often take some tuning before they will deliver the right results for your business. Certain keywords may bring a high volume of low-quality traffic to your site. These are users who click on an ad and are unlikely to navigate your site further. Look at the conversion and bounce rates for each of your keywords or targeting groups and adjust or remove those which are not performing well. Even if these keywords are bringing traffic to your site, if that traffic is unlikely to ever convert it is a waste of your advertising money. To bring higher-value visitors to your site through advertising, focus on long-tail, specific keywords and direct users to an appropriate landing page. A keyword such as “Women’s winter hiking boots” is better than “Women’s boots.” Create clear actions for users: One reason users may navigate off the first page they visit on your website is that they aren’t sure where to go next. Should they browse product pages, read a recent article, download a guide, or sign up for an account? The calls to action on your website will be determined by how you want users to convert, but no matter what they should be clear both in wording and design. Too many different calls to action or buttons on a page can be confusing, but without any direction users are forced to dig deeper before making their next move. Stick to one or two clear calls to action on each page, and include a menu at the top that stands out visually and has clear categories for users who want to decide themselves where to go next. Look at metrics by device type: Users browse websites differently depending on the device they are on. Those on a laptop or desktop computer are likely have more time to look through a website and may be more patient when it comes to page speed. Mobile users take load time into consideration even more harshly: a study by Google showed that 53% of mobile users navigate off a page if it fails to load with 3 seconds. If your website is not optimized for mobile phones and tablets that will also have a negative impact on bounce rate as pages may not render correctly. If your bounce rate is higher on mobile devices and tablets than on desktop computers, you should look into improving mobile performance and ensure your site is responsive for all types of devices. Improve page speed and bounce rate with Section Section is a website performance and security solution built to enable developers to achieve optimal performance. Through our global Content Delivery Network and flexible, open versions of Varnish Cache and ModSecurity we give sites the power to improve their speed and protect themselves from malicious traffic. Learn more by contacting our team today."
"352","2017-01-17","2023-03-24","https://www.section.io/blog/how-bot-traffic-impacts-page-views/","Some of the questions we get often at Section are “What counts as a page view?” and “How does bot traffic impact my website?” At Section and most other Content Delivery Networks, a page view is counted as any page that serves an HTTP response with a status code of 200 (meaning the page has been correctly delivered) and content type matching text/html. Importantly, this includes bot traffic that is not typically counted in the page view statistics for Google Analytics or other metrics services: A study in 2014 found that bots account for 56% of all Internet traffic, and at Section we have found that especially for smaller sites bots can account for 50-75% of total traffic. Google Analytics and bot traffic So why the discrepancy in page view numbers? It has to do with how Google Analytics counts a page view versus how your website server and Section’s CDN see traffic. Google Analytics works by inserting a JavaScript snippet into the header of your website. This snippet counts a page view whenever a visitor triggers that JavaScript, and most bots do not process JavaScript. “Good bots” such as those used by Google themselves to crawl and index your site for SEO purposes, will follow directions you give them in your robots.txt file to crawl pages and will not send data to Google Analytics or any other tracking that uses JavaScript. However, even those bots which do not trigger the JavaScript on your pages are still being served the same HTML document and subsequent assets a real user gets. Because your website server or CDN does the same amount of work to generate that HTML document, bots are counted in the page views for Section and other CDNs. How to manage bot traffic with headers and caching These good search engine bots are beneficial for your site: It’s been shown that site speed is an element of SEO, so serving them content quickly is good, and you want them to index your site correctly so you rank for the keywords on your pages. However, small websites that have a large number of pages (for example, ecommerce sites with many individual product pages) may balk at the percentage traffic they are getting from bots when it impacts price of hosting or Content Delivery Network services. In the below site example, you can see that the top 2 browsers are both bots, and 6 of the top 10 browsers are bots. To manage the amount of bot traffic your website receives, we recommend using some of the strategies outlined in this article on using headers to improve crawl efficiency. By using cache control, you can tell bots if a document has been modified since their last request. You can also set long expiry times on resources that do not change much. Questions about how many page views you have or how to speed up and secure your site for better SEO? Contact a member of our team today and we’d be happy to help."
"353","2017-01-03","2023-03-24","https://www.section.io/blog/504-503-errors-difference/","We all come across website errors at some point, and every website out there has inevitably experienced their share of 404 and 500 status codes, which are shown when a URL is not found either due to their typo or a broken link (404), and when there is a general server error (500). There are many other less well known status codes in the 400 and 500 range: Those HTTP status codes beginning with 4 are errors on the client side, and codes beginning with 5 are errors on the side of the website server. Some of the common 500 error messages that websites may encounter are 502, 503, and 504, which are all related to the website server being unable to fill a request that should be valid. These codes provide both the user and the website owner with additional information as to why a page is not loading. Websites can utilize logs and alerts to keep track of how often these errors occur and be notified when errors are occurring so that they can be resolved if needed. 502 Bad Gateway A 502 error means that a website server that is serving as a reverse proxy for the website origin server (for example, a CDN PoP) did not receive a valid response from the origin server. This may be because the origin server is experiencing issues, there is an invalid or incorrect DNS name, or because a firewall on the origin server has blocked the reverse proxy server request. This may also occur when requests at the origin server are taking several minutes to complete and a caching tool such as Varnish Cache has been instructed to timeout after a set number of seconds. Varnish Cache has a default timeout of 60 seconds, which Section recommends keeping for security and alerting reasons. 503 Service Unavailable The 503 service unavailable message means that the website origin server is not available and is usually a temporary state. This error could be triggered because something running on the website server side has crashed or your site is purposefully down for maintenance. This error is also commonly served when a site has more traffic than it can handle - a problem with poor website scalability. The famous Twitter “Fail Whale” which was often triggered in the early days of the social media site is one example of a 503 triggered due to excess traffic. To avoid serving 503 errors due to an overload of visitors, we recommend using a Content Delivery Network and caching tool so that more of your requests can be served from the CDN and cache. 504 Gateway Timeout Similar to the 502 error, the 504 Gateway Timeout error occurs if the server that is acting as a proxy for the website origin server did not receive a response from the website origin server within a set time period. This may indicate an issue with the DNS host or hosting company, or with the connection or configuration between the reverse proxy servers and the website origin server. Troubleshooting 500 errors 502 and 504 errors are related to a bad gateway, meaning that while the reverse proxy server is operational, something it needs to collect from the origin server is not working, or the connection between the reverse proxy server and the origin server is broken. To troubleshoot this issue, websites should check that their origin server and all the servers it needs to access are running correctly, and then check the configuration between the origin server and reverse proxy server (such as the CDN PoP or locally installed Varnish Cache server). You should also check that your firewall is not accidentally blocking legitimate traffic, your DNS name is set up correctly, and your origin hosting provider is not down. 503 errors are often expected when your website is going through downtime in order to make updates or changes. They may also be triggered by a large influx of traffic to your website. If your website does not scale well, we suggest exploring utilizing a CDN such as Section’s and/or a caching reverse proxy such as Varnish Cache. To learn more please contact us or get started for free to explore our features free for 2 weeks."
"354","2016-08-03","2023-03-24","https://www.section.io/blog/newrelic-how-static-resources-can-impact-metrics/","New Relic is one of the best tools that many of our clients use for performance monitoring, for both their internal infrastructure as well as performance of their sites through a CDN. To be able to accurately measure performance and impacts from deployments, New Relic needs to be setup correctly so the relevant metrics are not obfuscated by extraneous data. Static vs dynamic transactions One of the common mistakes we see is when New Relic’s APM combines both dynamic and static resource metrics into a single Web Transaction response time. This can happen out of the box when installing New Relic on certain platforms. The problem here is that serving a static file is normally orders of magnitude faster than generating a dynamic asset which may require database queries, external calls to 3rd parties, PHP application etc. Combining these two different types of resources into a single metric for performance monitoring will often mask issues and does not offer true insight on how the application is performing. Here you see a site that shows an excellent Web transaction time of 134ms. To look at this metric might lead someone to think that the origin application is very fast at responding to all requests. But closer inspection shows that much of this metric is made up of static resources. The median time for HTML responses is actually over 1 second. We win!…..oh wait A problem can occur with the above setup when examining the effects of a deployment, e.g. impact of caching performance and load on the origin. Unlike other CDNs, the Section platform allows easy caching of dynamic content such as HTML documents as well as static files. Offloading commonly requested HTML files from the origin frees up resources for higher value tasks. As sites evolve over time, the caching configuration may need to be updated to achieve better offload. In one instance, a client deployed a change to the caching configuration which led to a reduction in cache hit rates for static assets. This meant there was an increase in requests for static resources to the origin, but because of their New Relic setup, it actually showed as a drop in median Web Transaction time. So what was actually a deploy that had a negative impact to performance was celebrated as a win until a deeper investigation revealed the true result. Ok, what else can go wrong? Another problem that can occur, is when there is a sudden change in performance for dynamic requests. For example, if dynamic requests are reliant on a database which suddenly becomes very slow to respond, the Web transaction time should reflect this to warn the dev team that something is wrong. However, if for every dynamic asset, multiple static assets are served, then the degree to which the Web transaction metric increases is now a ratio of static vs dynamic rather than the true increase. This can be detrimental if there are associated monitors and alarms using this metric, for example Apdex score. The site can be slowing down to a crawl, but as long as the static content is served fast, any monitors in place maybe blissfully unaware of the sudden drop in performance. This can cause delays in responding and deploying fixes, which can turn a minor incident into a full blown outage. How to combat this Focusing New Relic to gather metrics that are of value will greatly enhance the information it provides. Does it need to know the speed at which a static resource is served? In most cases, the answer is no. Even when the answer is yes, it should not be assessed together with dynamic resources. Make a separate custom metric for it. Bruce Lee said it best, “Absorb what is useful, discard what is not, add what is uniquely your own”. Section is a leading New Relic partner. We can assist in implementation and configuration of New Relic, through to full stack performance and availability reviews. Contact sales@section.io for more info."
"355","2016-07-21","2023-03-24","https://www.section.io/blog/magento-varnish-metrics-101/","Section recently released a new Magento extension that provides visibility for business managers to see their magento Varnish Cache metrics without having to login to the Section application. You can get the extension for your Magento website by going to the Magento Marketplace. Metrics for Vanish can seem foreign for people who are not yet familiar with caching. We put together a quick writeup on each of the three metrics we show in the extension in order to help make sense of what these metrics mean and how to improve them. Overall Cache Hit Overall Cache Hit count shows the number of cache hits from all content types. A cache hit is when a customer requests content from your site (whether it’s an image, html document, etc) and the cache is able to respond to that request. The more often the cache can answer your customer requests, the higher the overall cache hit count. The higher the number of cache hits, the faster your website loads and the less work is required by your servers. This means you can earn more revenue by providing a better experience for your customers, and you can save money by not having to purchase additional servers with Magento. You can increase the overall cache hit count by caching more content. To setup caching, login to Section Console account and add configurations for Varnish Cache. HTML Cache Hit HTML Cache Hit count shows the number of cache hits for HTML documents. HTML documents are a set of instructions used to create the webpage and determine how all other content types get loaded on the page. As with overall cache hit, the more often the cache can answer requests for the HTML document, the higher the HTML cache hit count. The higher the number of HTML cache hits, the faster your website loads and the less work is required by your servers. HTML documents are generated for you by Magento and they consume a significant amount of your server resources. By focusing on HTML document caching as the first type of content you try to optimize, you are focusing on the content type that can make the biggest initial improvement to both your performance and server resources. This means you can earn more revenue by providing a better experience for your customers, and you can save money by not having to purchase additional servers with Magento. To increase the HTML cache hit count, you should start serving these documents from the cache instead of from your server. To setup caching, login to Section Console account and add configurations for Varnish Cache. HTTP Status Codes HTTP status codes show how your website responded to a customer request. The extension in Magento is meant to give you an overview, but we recommend that a developer look into the logs in the Section application in order to review the codes as described below in order to make improvements. Here is what each of the response types mean and what you should do about them: A 200 code is a normal response, which means the customer can get the content they requested (such as your full website page, an image on the page, etc). You want as many 200 codes as possible, as it means a good experience for your customers. A 3XX code typically means there was a redirect of some kind. This does not mean anything is broken, however, it may mean longer loading times for your customers as at least two items need to be served instead of one (the original and the redirect). You will want to review your 3XX codes and make sure the redirects you have setup for your website are necessary, and then remove the unnecessary ones. You can also update marketing campaigns to point traffic directly to the end destination instead of continuing to point to a site that will trigger a redirect. A 4XX or 5XX code means that your customer’s request was not answered and can indicate broken links or application error. The difference between a 4XX and a 5XX has to do with why the error took place, and a developer can dig into each error to find out the reason. The most common error in the bucket is a 404 which means that the page was not found. You can setup custom error messages in Section to give your customers a better experience when this does happen until you fix the link. A 7XX code has to do with the connection, and may be the result of problems with the connection your customer is using to make the request and not the result of your website."
"356","2016-07-12","2023-03-24","https://www.section.io/blog/marketing-impact-website-performance/","Did you know your marketing team could be impacting your page load times? Or worse, their activities could bring your site down? You may think that the development team is the sole owner of website performance, but actually there are quite a few other teams that can contribute to the speed of your website. One of the biggest influencers is the marketing team, who are responsible for driving traffic to the website. Since they are the drivers of traffic, they actually have quite a lot to do with how well your website can manage that traffic. There are two areas where the marketing team can directly impact both the scalability and speed of the website during typical marketing activities: email and tracking. Email Campaigns When there is a sale on an ecommerce website, the marketing team tries to push as much traffic as possible to the website. One of the key channels used to drive traffic is email because it often has a very high ROI. Email as a channel is very low cost as you don’t pay per view or click beyond your normal monthly email costs, and it is the most targeted audience of potential customers because the people on your email list have already shown interest in your products or services. One challenge with email is that it is notorious for creating a burst of traffic at once, more so than any other marketing channel. When you send an email, somewhere between one third to one quarter of email opens happen within the first hour. This creates a challenge for your website as it now needs to deal with a spike in traffic. The term “scalable” in website performance means how well your website deals with this increase in traffic. To be able to take advantage of email marketing while minimizing its impact on performance, marketing teams can start batching emails. Email batching is the concept of sending a campaign to several smaller batches instead of sending to the entire list all at once. This is an option provided by many email providers and can make a big difference on performance. The smaller the batches and the more time delay between each scheduled batch, the less your website will have to work to handle the traffic. On the development side, we recommend caching your static files using the Cache Statics TTL Varnish Cache Configuration. This allows you to set a certain amount of time (known as Time To Live) to keep objects cached in order to ease the work required by the server. This is hugely important to your website as the burst in traffic only impacts your servers if all the requests are going to your servers. By implementing caching, your requests are answered from the cache and therefore your servers aren’t impacted by the increase in traffic. Campaign Tracking A second marketing area that can greatly impact your website performance is campaign tracking. Data has become critical to making marketing decisions, but before you can analyze the data you must start collecting it by implementing some kind of tracking. This includes tracking on all the links in your email campaign described above, as well as tracking for ads, social posts, and even links for marketing assets on your own website. Tracking is typically done by adding parameters to each url. The most common parameters are UTM, which can be easily consumed by most analytics platforms. For example, if I wanted to drive people to signup from this blog post I might create the following url: section.io/sign-up?utm_source=internal&utm_medium=blog&utm_content=2016-07-05-marketing&utm_campaign=signup Once you get started creating parameters, you realize it’s critical for everyone on the team to use the same labels. For instance, I’m using “internal” as a source to mean that it’s coming from our own marketing site. But if someone decides to indicate the same thing by making “section.io” as the medium, then we won’t be able to properly do our analysis. But here is something you may not have thought about before: the order of your utm parameters matters. It does not matter to your customers or to your analytics platform, but it matters to the server responding to the request. To it: section.io/sign-up?utm_source=internal&utm_medium=blog and section.io/sign-up?utm_medium=blog&utm_source=internal are not the same thing. And in fact, they are both different from section.io/sign-up This is a pretty big deal if you are trying to optimize your website caching because even if the first url has been cached, the second one has not and will need to request the page from the server. As mentioned earlier, your servers are only impacted by the requests going to it, so each of these requests is going to hit the server which increases the likelihood you site can go down and decrease the speed and scalability fo your website. One thing the marketing team can do to minimize the impact is to try to keep all urls in the same order. The same way that your team has agreed upon labels and meanings for those labels, whether via a tool or an internal spreadsheet, you can dictate the order as well. Furthermore, you could create a spreadsheet that will generate the url for you given a set of inputs, which can ensure (if someone is using the spreadsheet) that all parameters are in the same order. On the development side, we recommend enabling a Varnish Cache configuration called Sort Querystring, which re-orders parameters to be in alphabetical order so they are always in the same order, which decreases the requests made to your server. This takes the burden off the marketing team by automating the ordering of parameters so they don’t have to manage it manually. You can take this one step further by writing additional VCL (Varnish Vonfiguration Language) to strip the parameters from the url (normalizing the url in Varnish Cache) if your parameters are used only for tracking and are not meant to change what elements are shown on the page. This does not remove the parameters from your tracking, only for the server, and allows all of the parameter permutations for a single page to be cached and served as one object. Yet another strategy is to use VCL to whitelist parameters the marketing team has created and only allow those requests to come through. These strategies require communication between development and marketing teams and can be covered more in depth in a future blog post. The two Varnish Cache configurations mentioned are part of our basic Varnish Cache configurations in Section that you can enable with a single click. These are two quick ways that development can help minimize the impact that marketing activities have on the speed and scalability of the website, in addition to two changes the marketing team can make. Happy marketing! To read more about Varnish Cache configurations with Section, checkout our documentation."
"357","2016-07-05","2023-03-24","https://www.section.io/blog/setting-up-with-section-io/","If you operate a website, a massive wave of traffic can be a very appealing idea. However, such a wave can also knock your site offline, or leave it crawling at slow speeds. To avoid such an issue, you can employ the use of a CDN, or Content Delivery Network. A CDN is a network of servers that sit in front of your site and cache pages to speed up your site and reduce the load on your servers. This tutorial will be covering how to set up a free CDN with Section, and configure it to work perfectly with your site. Prerequisites This tutorial assumes that you already have the following: A hosted website A domain name pointing to that website Permission to change the DNS records for that domain Signing up for an account Before doing anything else, you should create an account on Section at https://www.section.iohttps://console.section.io/. It features a completely free 14 day trial, no credit card required. After signing up, you will be directed through the “Bootcamp”. This will show you around the Section management dashboard and help you get an understanding of it. Afterwards, press the blue button labeled “Create application” to get started deploying your app on Section. Configuring your domain for Section Enter your domain name, and Section will automatically scan for existing DNS records and use them to set your origin address. However, if his fails you will need to manually insert an origin. An IP address or CNAME that you would normally point a domain towards works. You will also be prompted to choose a reverse proxy. A reverse proxy is a server that takes requests from clients, and requests the data from your server. However, a Varnish Cache reverse proxy can be used for caching content so your server will not be queried every time. For most instances, we recommend using Varnish Cache v4. Changing DNS For the final stage of deployment onto Section infrastructure, you need to point your domain at your Section app. This can be done one of two ways. Press the “Change DNS” button to get started. We recommend changing your nameservers to utilize our hosted DNS. This allows you to use a bare domain (example.com rather than www.example.com) and also ensures stability and speed in site’s DNS. The methods for changing your DNS servers vary between domain registrars, so please refer to the documentation or customer support for your registrar if you encounter any difficulties. The second method is to use a CNAME. Create a CNAME record on your domain and point it at the domain found in the DNS menu. All done! Congratulations! You are now running on the Section CDN. If you have any issues, feel free to contact our support team for help."
"358","2016-06-28","2023-03-24","https://www.section.io/blog/why-do-i-see-long-ttfb-with-http2-in-webpagetest/","WebPageTest is a common tool that is used by us and our clients to examine site performance. It generates a beautifully detailed waterfall with great information on each request. A common question we have been receiving lately, “The Time To First Byte for our static assets seem to be very long when using HTTPS. What is going on?"". So, what is going on? Expectation Under HTTP/1.1 a good performance timeline for assets should look something like this(assuming DNS already resolved, SSL negotiated and connection established) For small assets we would expect a short green bar with a short blue bar. For larger assets, depending on the speed of the connection selected for the page test, we would expect a short green bar with a longer blue bar. In both cases, the green bar which indicates Time To First Byte is very short. This is a good indicator for a fast response from the server to the browser. So why are we seeing a long green bar for HTTP2 assets? Is the server having issues processing the request? Is there network congestion? No and no, the reason is because of the way HTTP2 performs these requests and how WPT represents this graphically. HTTP2 priorities. Unlike HTTP/1.1 where a browser will open multiple connections each requesting a different asset, HTTP2 opens a single connection to a domain. All requests for assets from that domain are made over this connection using streams. The request for an asset is sent soon as the browser determines the asset required, it does not wait until previous assets have been received. Most browsers will assign a priority to each stream which the server can use to determine the order of processing. Generally priorities are ordered HTML(highest), CSS(high), JS(medium), images(low). This is a simplified explanation, the priorities are actually an integer from 1 to 256(higher number == higher priority). Every browser also uses different values for priority. These values can also be effected by their dependancies, but for our purposes we will use the simplified version of the story. These priorities are hints, and the server can choose to ignore them, but if honoured it will send the assets in order of priority. This means that an asset can be requested very early in the page load, but won’t be served until higher priority asset requests have been processed. This is why it has a long TTFB. This is in fact a good thing as prioritising critical assets such as render blocking CSS and JS before images during page load should lead to a better user experience on average. But it does lead to some very worrying looking waterfalls in some performance tests like in WebPageTest. Don’t worry, this is what HTTP2 can look like. The above test using Chrome on www.facebook.com is a good exampled of long TTFBs for .png and .gif files while higher priority assets are being processed. To find out more about HTTP2 and why you should make the upgrade checkout our blog article: HTTP/2 by default Section platform is HTTP2 ready out of the box. Contact us to find out how we can help you make your site faster using HTTP2."
"359","2015-12-14","2023-03-24","https://www.section.io/blog/which-reverse-proxy-should-we-add/","As you know Section currently supports the following reverse proxy servers: Varnish Cache 3 Varnish Cache 4 Varnish Cache 3 pre-configured for Magento Turpentine Integration Varnish Cache 4 specifically for Magento 2.0.0 ModSecurity You also get HTTP/2 support out of the box and TLS management when you point your website traffic through Section (regardless of whether your Varnish Cache or ModSecurity configuration is set up or not!) Given that Section is a reverse proxy management platform which allows you to run reverse proxies on our cloud or in your own AWS account, we will be continuing to add support for more reverse proxies – both open source and commercial if desired. We have been having some discussion internally over what should be the next reverse proxy added to the stack for folks to choose. Here are a few of the candidates: mod_pagespeed – there is lots of info on Google’s mod_pagespeed reverse proxy Apache module. At Section we have been using mod_pagespeed for a few years with awesome results for various websites. We figure using it in a fashion such that you can integrate it with your development workflows will significantly improve the usability of the module. Lua rewriting - We have also been working with the OpenResty Lua module for the last few years and more recently the NginScript JavaScript module. We use these to rewrite HTML on the fly and resolve issues like blocking Javascript. Data driven content management – We have a custom module to provide you with an opportunity to adjust the response of your website based on data feeds. For example, we have been using real time concurrent user data feeds to redirect specific users to “holding” pages for some websites when their backends or origin servers become overwhelmed with transactions. As with Varnish Cache and ModSec, when we add the next reverse proxies to the Section platform, we will make sure we add them so that: You can run them in your local development environment backed by familiar Git deployment models; and You have the relevant metrics and logs and alerting ready to go for prod and dev based on Graphite, Kibana, Logstash, Elasticsearch and Umpire. We would love some user feedback as to the reverse proxies or functionality you would like added to the platform. It’s not a matter of if we add them but when!"
"360","2015-11-09","2023-03-24","https://www.section.io/blog/pci-compliance-and-tls-protocol-versions/","Update On December 18th 2015, the PCI Security Standards Council delayed the date for TLS 1.0 deprecation until June 2018. The full announcement is available here. The Payment Card Industry Data Security Standard (PCI DSS) was updated to version 3.1 in April this year (2015). One stand out requirement (§ 4.1) in the standard is to disable SSL and “early TLS” by June 30th 2016. There is also another PCI document which clarifies that “early TLS” refers to TLS v1.0 and that a minimum of TLS v1.1 should be used but goes on to strongly encourage TLS v1.2 be preferred. SSL v3 has already been long disabled on our platform but disabling all but TLS v1.2 was a little harder to justify as we still see many TLS v1.0 and v1.1 connections to our Edge proxies, although we surprisingly see much less usage of v1.1 than v1.0. However, the number of connections and requests being performed over older TLS versions is insufficient information to advise a site owner of the real impact of choosing to disable those protocol versions. What we really want to know is the business value of those connections. Measure As part of the Section Fully Managed Content Delivery Service we already have Real User Monitoring (RUM) in place to measure other important information like page load times by page type or geo-location and front-end versus back-end times. We also measure revenue so that we can clearly see how changes in website performance lead to changes in purchasing behaviour. In preparation for the PCI compliance deadline we decided to begin measuring revenue based on the TLS protocol version detected at our Edge proxy. And our initial results were interesting so we’re sharing them here. Based on the existing connection statistics alone we selected three different e-commerce sites exhibiting the highest ratio of TLS v1.0 connections and applied the new measurements to their RUM configuration. Over a 4 day period from Thursday to Sunday we recorded a combined total of $271,762.00 revenue across all 3 sites. Of that total, 3.5% percent (ie $9,597) came from browsers connected on TLS v1.0 and another 1.1% (ie $3,064) came from TLS v1.1 connections. With this data we can now have an informed conversation with our customers about the potential impact on their income as a result of complying with the latest PCI standard, with the precise dollar amounts applicable to their site’s actual traffic. While almost 5% revenue is a significant hit to accept, the upside is that (at the time of writing) there are still 7+ months before the deadline for these numbers to change. While they may not reach zero, we expect them to decline as software and devices are upgraded and other popular websites remove early TLS support, forcing users to update. If you’d like to know what these numbers would look like for your site, contact us here at Section for more information."
"361","2013-10-10","2023-03-24","https://www.section.io/blog/website-performance-in-the-marketing-mix/","Most people are aware of the benefits of website performance on their website and business in general. To recap, here are some key figures: 0.5 second page slowdown = 20% drop in revenue at google Every 100ms delay costs 1% of sales at Amazon 46% page speed improvement “= 30% revenue increase at JAG (A Section customer) Its clear that improved page speed increases revenue and is a good thing for any business. A trickier topic is: How should this be funded and who owns it within your business?“Website performance is often considered an “IT Problem” and hence it’s assumed that this should be owned and managed through your IT resources. I would like to present an alternate model for looking at website performance that integrates it into the marketing channel mix. We will see how allocating a portion of your marketing spend on performance can greatly improve the overall effectiveness of your marketing efforts. To start here is a view of a common marketing channel mix: Each channel is going to have a different cost and deliver different returns on investment. We would like to introduce website performance as another option in your marketing mix which can be used as a part of your marketing spend. Using this option increases the effectiveness of all channels and enables balancing of the cost of performance improvements between different sources. You can then experiment with which channel’s fund the improvements and discover a better overall marketing mix. Why does this work? Investing in performance enhances the effectiveness of every channel you are using. This provides you with another option to experiment with regarding marketing ROI. For example Online Advertising may be more expensive than Email Marketing but delivers better leads that convert to customers more often (or higher paying customers). By spending a portion of your Email Marketing budget on website performance you make any money spent on Online Advertising (and all other channels including Email Marketing) more effective. Taking 1% (for example) from your Email Marketing budget and allocating it to website performance will not correlate to a 1% drop in email marketing ROI. The 1% allocated to performance results in much greater effectiveness of all email marketing campaigns. The spend on performance per month is generally a fraction of the overall marketing budget and hence is a huge multiplier for all marketing efforts The flow on effects of the performance spend benefit your business in other ways: SEO - Site speed is factor in organic search results Faster website = Happier customers. This improves engagement, reduces support/customer service requirements and improves your brand image "" Sources: http://glinden.blogspot.com.au/2006/11/marissa-mayer-at-web-20.html https://sites.google.com/site/glinden/Home/StanfordDataMining.2006-11-28.ppt?attredirects=0 https://www.section.io/customers/ "" """
